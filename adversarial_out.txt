Epoch [1/30], Batch [0/6000], Loss: 2.4458
Epoch [1/30], Batch [100/6000], Loss: 1.1305
Epoch [1/30], Batch [200/6000], Loss: 0.5238
Epoch [1/30], Batch [300/6000], Loss: 0.2529
Epoch [1/30], Batch [400/6000], Loss: 0.6761
Epoch [1/30], Batch [500/6000], Loss: 0.9409
Epoch [1/30], Batch [600/6000], Loss: 0.4304
Epoch [1/30], Batch [700/6000], Loss: 0.6182
Epoch [1/30], Batch [800/6000], Loss: 0.1164
Epoch [1/30], Batch [900/6000], Loss: 0.2205
Epoch [1/30], Batch [1000/6000], Loss: 0.1677
Epoch [1/30], Batch [1100/6000], Loss: 0.2083
Epoch [1/30], Batch [1200/6000], Loss: 1.0229
Epoch [1/30], Batch [1300/6000], Loss: 0.3456
Epoch [1/30], Batch [1400/6000], Loss: 0.7857
Epoch [1/30], Batch [1500/6000], Loss: 0.3145
Epoch [1/30], Batch [1600/6000], Loss: 0.2362
Epoch [1/30], Batch [1700/6000], Loss: 0.7753
Epoch [1/30], Batch [1800/6000], Loss: 0.7626
Epoch [1/30], Batch [1900/6000], Loss: 0.6070
Epoch [1/30], Batch [2000/6000], Loss: 0.4057
Epoch [1/30], Batch [2100/6000], Loss: 0.6753
Epoch [1/30], Batch [2200/6000], Loss: 0.1583
Epoch [1/30], Batch [2300/6000], Loss: 0.1874
Epoch [1/30], Batch [2400/6000], Loss: 0.3540
Epoch [1/30], Batch [2500/6000], Loss: 0.3675
Epoch [1/30], Batch [2600/6000], Loss: 0.1273
Epoch [1/30], Batch [2700/6000], Loss: 0.1486
Epoch [1/30], Batch [2800/6000], Loss: 0.0961
Epoch [1/30], Batch [2900/6000], Loss: 0.0954
Epoch [1/30], Batch [3000/6000], Loss: 0.1172
Epoch [1/30], Batch [3100/6000], Loss: 0.2973
Epoch [1/30], Batch [3200/6000], Loss: 0.1987
Epoch [1/30], Batch [3300/6000], Loss: 0.4990
Epoch [1/30], Batch [3400/6000], Loss: 0.0556
Epoch [1/30], Batch [3500/6000], Loss: 0.0955
Epoch [1/30], Batch [3600/6000], Loss: 0.1758
Epoch [1/30], Batch [3700/6000], Loss: 0.9356
Epoch [1/30], Batch [3800/6000], Loss: 0.3118
Epoch [1/30], Batch [3900/6000], Loss: 1.1747
Epoch [1/30], Batch [4000/6000], Loss: 0.1779
Epoch [1/30], Batch [4100/6000], Loss: 0.1336
Epoch [1/30], Batch [4200/6000], Loss: 0.1242
Epoch [1/30], Batch [4300/6000], Loss: 0.1334
Epoch [1/30], Batch [4400/6000], Loss: 0.1443
Epoch [1/30], Batch [4500/6000], Loss: 0.1206
Epoch [1/30], Batch [4600/6000], Loss: 0.4461
Epoch [1/30], Batch [4700/6000], Loss: 0.0713
Epoch [1/30], Batch [4800/6000], Loss: 0.4019
Epoch [1/30], Batch [4900/6000], Loss: 0.4611
Epoch [1/30], Batch [5000/6000], Loss: 0.5683
Epoch [1/30], Batch [5100/6000], Loss: 0.1249
Epoch [1/30], Batch [5200/6000], Loss: 0.2349
Epoch [1/30], Batch [5300/6000], Loss: 0.2324
Epoch [1/30], Batch [5400/6000], Loss: 0.1944
Epoch [1/30], Batch [5500/6000], Loss: 0.6783
Epoch [1/30], Batch [5600/6000], Loss: 0.0690
Epoch [1/30], Batch [5700/6000], Loss: 0.6228
Epoch [1/30], Batch [5800/6000], Loss: 0.0618
Epoch [1/30], Batch [5900/6000], Loss: 0.3172
Epoch [1/30], Loss: 0.3982
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.1080
Epoch [2/30], Batch [100/6000], Loss: 0.0942
Epoch [2/30], Batch [200/6000], Loss: 0.0764
Epoch [2/30], Batch [300/6000], Loss: 0.7241
Epoch [2/30], Batch [400/6000], Loss: 0.0516
Epoch [2/30], Batch [500/6000], Loss: 0.3421
Epoch [2/30], Batch [600/6000], Loss: 0.0940
Epoch [2/30], Batch [700/6000], Loss: 0.0690
Epoch [2/30], Batch [800/6000], Loss: 0.3385
Epoch [2/30], Batch [900/6000], Loss: 0.0469
Epoch [2/30], Batch [1000/6000], Loss: 0.0820
Epoch [2/30], Batch [1100/6000], Loss: 0.0931
Epoch [2/30], Batch [1200/6000], Loss: 0.1525
Epoch [2/30], Batch [1300/6000], Loss: 0.1635
Epoch [2/30], Batch [1400/6000], Loss: 0.1953
Epoch [2/30], Batch [1500/6000], Loss: 0.0721
Epoch [2/30], Batch [1600/6000], Loss: 0.1984
Epoch [2/30], Batch [1700/6000], Loss: 0.1146
Epoch [2/30], Batch [1800/6000], Loss: 0.3509
Epoch [2/30], Batch [1900/6000], Loss: 0.1379
Epoch [2/30], Batch [2000/6000], Loss: 0.2012
Epoch [2/30], Batch [2100/6000], Loss: 0.0627
Epoch [2/30], Batch [2200/6000], Loss: 0.0705
Epoch [2/30], Batch [2300/6000], Loss: 0.3845
Epoch [2/30], Batch [2400/6000], Loss: 0.2903
Epoch [2/30], Batch [2500/6000], Loss: 0.2010
Epoch [2/30], Batch [2600/6000], Loss: 0.5114
Epoch [2/30], Batch [2700/6000], Loss: 0.1765
Epoch [2/30], Batch [2800/6000], Loss: 0.1055
Epoch [2/30], Batch [2900/6000], Loss: 0.0519
Epoch [2/30], Batch [3000/6000], Loss: 0.2180
Epoch [2/30], Batch [3100/6000], Loss: 0.0554
Epoch [2/30], Batch [3200/6000], Loss: 0.0948
Epoch [2/30], Batch [3300/6000], Loss: 0.3030
Epoch [2/30], Batch [3400/6000], Loss: 0.1321
Epoch [2/30], Batch [3500/6000], Loss: 0.0567
Epoch [2/30], Batch [3600/6000], Loss: 0.0653
Epoch [2/30], Batch [3700/6000], Loss: 0.0509
Epoch [2/30], Batch [3800/6000], Loss: 0.1228
Epoch [2/30], Batch [3900/6000], Loss: 0.1357
Epoch [2/30], Batch [4000/6000], Loss: 0.2152
Epoch [2/30], Batch [4100/6000], Loss: 0.0645
Epoch [2/30], Batch [4200/6000], Loss: 0.0523
Epoch [2/30], Batch [4300/6000], Loss: 0.1843
Epoch [2/30], Batch [4400/6000], Loss: 0.1772
Epoch [2/30], Batch [4500/6000], Loss: 0.1675
Epoch [2/30], Batch [4600/6000], Loss: 0.0669
Epoch [2/30], Batch [4700/6000], Loss: 0.0704
Epoch [2/30], Batch [4800/6000], Loss: 0.0595
Epoch [2/30], Batch [4900/6000], Loss: 0.0567
Epoch [2/30], Batch [5000/6000], Loss: 0.2204
Epoch [2/30], Batch [5100/6000], Loss: 0.4288
Epoch [2/30], Batch [5200/6000], Loss: 0.1248
Epoch [2/30], Batch [5300/6000], Loss: 0.0703
Epoch [2/30], Batch [5400/6000], Loss: 0.0689
Epoch [2/30], Batch [5500/6000], Loss: 0.6390
Epoch [2/30], Batch [5600/6000], Loss: 0.0531
Epoch [2/30], Batch [5700/6000], Loss: 0.0622
Epoch [2/30], Batch [5800/6000], Loss: 0.4583
Epoch [2/30], Batch [5900/6000], Loss: 0.3952
Epoch [2/30], Loss: 0.2250
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.3068
Epoch [3/30], Batch [100/6000], Loss: 0.0508
Epoch [3/30], Batch [200/6000], Loss: 0.0405
Epoch [3/30], Batch [300/6000], Loss: 0.3010
Epoch [3/30], Batch [400/6000], Loss: 0.1871
Epoch [3/30], Batch [500/6000], Loss: 0.0635
Epoch [3/30], Batch [600/6000], Loss: 0.0435
Epoch [3/30], Batch [700/6000], Loss: 0.1131
Epoch [3/30], Batch [800/6000], Loss: 0.0380
Epoch [3/30], Batch [900/6000], Loss: 0.0833
Epoch [3/30], Batch [1000/6000], Loss: 0.2235
Epoch [3/30], Batch [1100/6000], Loss: 0.1210
Epoch [3/30], Batch [1200/6000], Loss: 0.0280
Epoch [3/30], Batch [1300/6000], Loss: 0.0521
Epoch [3/30], Batch [1400/6000], Loss: 0.0844
Epoch [3/30], Batch [1500/6000], Loss: 0.2214
Epoch [3/30], Batch [1600/6000], Loss: 0.5540
Epoch [3/30], Batch [1700/6000], Loss: 0.1029
Epoch [3/30], Batch [1800/6000], Loss: 0.2667
Epoch [3/30], Batch [1900/6000], Loss: 0.0701
Epoch [3/30], Batch [2000/6000], Loss: 0.0412
Epoch [3/30], Batch [2100/6000], Loss: 0.1834
Epoch [3/30], Batch [2200/6000], Loss: 0.0391
Epoch [3/30], Batch [2300/6000], Loss: 0.3053
Epoch [3/30], Batch [2400/6000], Loss: 0.0357
Epoch [3/30], Batch [2500/6000], Loss: 0.0431
Epoch [3/30], Batch [2600/6000], Loss: 0.0681
Epoch [3/30], Batch [2700/6000], Loss: 0.2078
Epoch [3/30], Batch [2800/6000], Loss: 0.1158
Epoch [3/30], Batch [2900/6000], Loss: 0.1170
Epoch [3/30], Batch [3000/6000], Loss: 0.0412
Epoch [3/30], Batch [3100/6000], Loss: 0.0552
Epoch [3/30], Batch [3200/6000], Loss: 0.0280
Epoch [3/30], Batch [3300/6000], Loss: 0.0439
Epoch [3/30], Batch [3400/6000], Loss: 0.0397
Epoch [3/30], Batch [3500/6000], Loss: 0.1808
Epoch [3/30], Batch [3600/6000], Loss: 0.0565
Epoch [3/30], Batch [3700/6000], Loss: 0.0654
Epoch [3/30], Batch [3800/6000], Loss: 0.0939
Epoch [3/30], Batch [3900/6000], Loss: 0.4700
Epoch [3/30], Batch [4000/6000], Loss: 0.6415
Epoch [3/30], Batch [4100/6000], Loss: 0.2653
Epoch [3/30], Batch [4200/6000], Loss: 0.1324
Epoch [3/30], Batch [4300/6000], Loss: 0.1115
Epoch [3/30], Batch [4400/6000], Loss: 0.0433
Epoch [3/30], Batch [4500/6000], Loss: 0.0455
Epoch [3/30], Batch [4600/6000], Loss: 0.1212
Epoch [3/30], Batch [4700/6000], Loss: 0.1894
Epoch [3/30], Batch [4800/6000], Loss: 0.3532
Epoch [3/30], Batch [4900/6000], Loss: 0.1944
Epoch [3/30], Batch [5000/6000], Loss: 0.3335
Epoch [3/30], Batch [5100/6000], Loss: 0.0431
Epoch [3/30], Batch [5200/6000], Loss: 0.0570
Epoch [3/30], Batch [5300/6000], Loss: 0.2339
Epoch [3/30], Batch [5400/6000], Loss: 0.0480
Epoch [3/30], Batch [5500/6000], Loss: 0.0728
Epoch [3/30], Batch [5600/6000], Loss: 0.0531
Epoch [3/30], Batch [5700/6000], Loss: 0.2396
Epoch [3/30], Batch [5800/6000], Loss: 0.0747
Epoch [3/30], Batch [5900/6000], Loss: 0.0501
Epoch [3/30], Loss: 0.1681
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.0759
Epoch [4/30], Batch [100/6000], Loss: 0.0700
Epoch [4/30], Batch [200/6000], Loss: 0.0935
Epoch [4/30], Batch [300/6000], Loss: 0.0395
Epoch [4/30], Batch [400/6000], Loss: 0.3775
Epoch [4/30], Batch [500/6000], Loss: 0.0782
Epoch [4/30], Batch [600/6000], Loss: 0.1071
Epoch [4/30], Batch [700/6000], Loss: 0.1193
Epoch [4/30], Batch [800/6000], Loss: 0.1809
Epoch [4/30], Batch [900/6000], Loss: 0.1934
Epoch [4/30], Batch [1000/6000], Loss: 0.4963
Epoch [4/30], Batch [1100/6000], Loss: 0.3039
Epoch [4/30], Batch [1200/6000], Loss: 0.4130
Epoch [4/30], Batch [1300/6000], Loss: 0.1561
Epoch [4/30], Batch [1400/6000], Loss: 0.0729
Epoch [4/30], Batch [1500/6000], Loss: 0.2479
Epoch [4/30], Batch [1600/6000], Loss: 0.2069
Epoch [4/30], Batch [1700/6000], Loss: 0.0367
Epoch [4/30], Batch [1800/6000], Loss: 0.0413
Epoch [4/30], Batch [1900/6000], Loss: 0.2087
Epoch [4/30], Batch [2000/6000], Loss: 0.9808
Epoch [4/30], Batch [2100/6000], Loss: 0.1460
Epoch [4/30], Batch [2200/6000], Loss: 0.1745
Epoch [4/30], Batch [2300/6000], Loss: 0.0806
Epoch [4/30], Batch [2400/6000], Loss: 0.0462
Epoch [4/30], Batch [2500/6000], Loss: 0.0355
Epoch [4/30], Batch [2600/6000], Loss: 0.0884
Epoch [4/30], Batch [2700/6000], Loss: 0.2291
Epoch [4/30], Batch [2800/6000], Loss: 0.0397
Epoch [4/30], Batch [2900/6000], Loss: 0.1046
Epoch [4/30], Batch [3000/6000], Loss: 0.0479
Epoch [4/30], Batch [3100/6000], Loss: 0.3023
Epoch [4/30], Batch [3200/6000], Loss: 0.1503
Epoch [4/30], Batch [3300/6000], Loss: 0.0345
Epoch [4/30], Batch [3400/6000], Loss: 0.0328
Epoch [4/30], Batch [3500/6000], Loss: 0.1325
Epoch [4/30], Batch [3600/6000], Loss: 0.1536
Epoch [4/30], Batch [3700/6000], Loss: 0.1602
Epoch [4/30], Batch [3800/6000], Loss: 0.0391
Epoch [4/30], Batch [3900/6000], Loss: 0.0384
Epoch [4/30], Batch [4000/6000], Loss: 0.0529
Epoch [4/30], Batch [4100/6000], Loss: 0.0628
Epoch [4/30], Batch [4200/6000], Loss: 0.0355
Epoch [4/30], Batch [4300/6000], Loss: 0.0292
Epoch [4/30], Batch [4400/6000], Loss: 0.0401
Epoch [4/30], Batch [4500/6000], Loss: 0.1002
Epoch [4/30], Batch [4600/6000], Loss: 0.0298
Epoch [4/30], Batch [4700/6000], Loss: 0.5643
Epoch [4/30], Batch [4800/6000], Loss: 0.0348
Epoch [4/30], Batch [4900/6000], Loss: 0.0502
Epoch [4/30], Batch [5000/6000], Loss: 0.3457
Epoch [4/30], Batch [5100/6000], Loss: 0.2476
Epoch [4/30], Batch [5200/6000], Loss: 0.2540
Epoch [4/30], Batch [5300/6000], Loss: 0.3261
Epoch [4/30], Batch [5400/6000], Loss: 0.1227
Epoch [4/30], Batch [5500/6000], Loss: 0.0645
Epoch [4/30], Batch [5600/6000], Loss: 0.0628
Epoch [4/30], Batch [5700/6000], Loss: 0.1309
Epoch [4/30], Batch [5800/6000], Loss: 0.0338
Epoch [4/30], Batch [5900/6000], Loss: 0.5050
Epoch [4/30], Loss: 0.1391
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.0368
Epoch [5/30], Batch [100/6000], Loss: 0.1064
Epoch [5/30], Batch [200/6000], Loss: 0.0324
Epoch [5/30], Batch [300/6000], Loss: 0.0306
Epoch [5/30], Batch [400/6000], Loss: 0.0318
Epoch [5/30], Batch [500/6000], Loss: 0.0280
Epoch [5/30], Batch [600/6000], Loss: 0.1566
Epoch [5/30], Batch [700/6000], Loss: 0.1163
Epoch [5/30], Batch [800/6000], Loss: 1.0039
Epoch [5/30], Batch [900/6000], Loss: 0.0416
Epoch [5/30], Batch [1000/6000], Loss: 0.1554
Epoch [5/30], Batch [1100/6000], Loss: 0.0934
Epoch [5/30], Batch [1200/6000], Loss: 0.1031
Epoch [5/30], Batch [1300/6000], Loss: 0.0958
Epoch [5/30], Batch [1400/6000], Loss: 0.0339
Epoch [5/30], Batch [1500/6000], Loss: 0.0271
Epoch [5/30], Batch [1600/6000], Loss: 0.0397
Epoch [5/30], Batch [1700/6000], Loss: 0.0908
Epoch [5/30], Batch [1800/6000], Loss: 0.0317
Epoch [5/30], Batch [1900/6000], Loss: 0.4358
Epoch [5/30], Batch [2000/6000], Loss: 0.1828
Epoch [5/30], Batch [2100/6000], Loss: 0.2370
Epoch [5/30], Batch [2200/6000], Loss: 0.0338
Epoch [5/30], Batch [2300/6000], Loss: 1.5849
Epoch [5/30], Batch [2400/6000], Loss: 0.0295
Epoch [5/30], Batch [2500/6000], Loss: 0.3191
Epoch [5/30], Batch [2600/6000], Loss: 0.1536
Epoch [5/30], Batch [2700/6000], Loss: 0.4591
Epoch [5/30], Batch [2800/6000], Loss: 0.5856
Epoch [5/30], Batch [2900/6000], Loss: 0.1453
Epoch [5/30], Batch [3000/6000], Loss: 0.0340
Epoch [5/30], Batch [3100/6000], Loss: 0.0796
Epoch [5/30], Batch [3200/6000], Loss: 0.0453
Epoch [5/30], Batch [3300/6000], Loss: 0.0396
Epoch [5/30], Batch [3400/6000], Loss: 0.0626
Epoch [5/30], Batch [3500/6000], Loss: 0.0535
Epoch [5/30], Batch [3600/6000], Loss: 0.0302
Epoch [5/30], Batch [3700/6000], Loss: 0.0820
Epoch [5/30], Batch [3800/6000], Loss: 0.0530
Epoch [5/30], Batch [3900/6000], Loss: 0.3183
Epoch [5/30], Batch [4000/6000], Loss: 0.4788
Epoch [5/30], Batch [4100/6000], Loss: 0.3587
Epoch [5/30], Batch [4200/6000], Loss: 0.2401
Epoch [5/30], Batch [4300/6000], Loss: 0.4615
Epoch [5/30], Batch [4400/6000], Loss: 0.0728
Epoch [5/30], Batch [4500/6000], Loss: 0.0379
Epoch [5/30], Batch [4600/6000], Loss: 0.0804
Epoch [5/30], Batch [4700/6000], Loss: 0.0334
Epoch [5/30], Batch [4800/6000], Loss: 0.0263
Epoch [5/30], Batch [4900/6000], Loss: 0.0403
Epoch [5/30], Batch [5000/6000], Loss: 0.0758
Epoch [5/30], Batch [5100/6000], Loss: 0.0530
Epoch [5/30], Batch [5200/6000], Loss: 0.0288
Epoch [5/30], Batch [5300/6000], Loss: 0.0267
Epoch [5/30], Batch [5400/6000], Loss: 0.0352
Epoch [5/30], Batch [5500/6000], Loss: 0.0350
Epoch [5/30], Batch [5600/6000], Loss: 0.0720
Epoch [5/30], Batch [5700/6000], Loss: 0.0569
Epoch [5/30], Batch [5800/6000], Loss: 0.0239
Epoch [5/30], Batch [5900/6000], Loss: 0.3215
Epoch [5/30], Loss: 0.1193
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.0621
Epoch [6/30], Batch [100/6000], Loss: 0.0320
Epoch [6/30], Batch [200/6000], Loss: 0.3839
Epoch [6/30], Batch [300/6000], Loss: 0.4941
Epoch [6/30], Batch [400/6000], Loss: 0.2932
Epoch [6/30], Batch [500/6000], Loss: 0.0256
Epoch [6/30], Batch [600/6000], Loss: 0.0299
Epoch [6/30], Batch [700/6000], Loss: 0.1721
Epoch [6/30], Batch [800/6000], Loss: 0.0519
Epoch [6/30], Batch [900/6000], Loss: 0.7593
Epoch [6/30], Batch [1000/6000], Loss: 0.2684
Epoch [6/30], Batch [1100/6000], Loss: 0.2080
Epoch [6/30], Batch [1200/6000], Loss: 0.0469
Epoch [6/30], Batch [1300/6000], Loss: 0.0463
Epoch [6/30], Batch [1400/6000], Loss: 0.1304
Epoch [6/30], Batch [1500/6000], Loss: 0.0607
Epoch [6/30], Batch [1600/6000], Loss: 0.2868
Epoch [6/30], Batch [1700/6000], Loss: 0.0549
Epoch [6/30], Batch [1800/6000], Loss: 0.0443
Epoch [6/30], Batch [1900/6000], Loss: 0.0292
Epoch [6/30], Batch [2000/6000], Loss: 0.1834
Epoch [6/30], Batch [2100/6000], Loss: 0.0288
Epoch [6/30], Batch [2200/6000], Loss: 0.1387
Epoch [6/30], Batch [2300/6000], Loss: 0.0353
Epoch [6/30], Batch [2400/6000], Loss: 0.0457
Epoch [6/30], Batch [2500/6000], Loss: 0.1574
Epoch [6/30], Batch [2600/6000], Loss: 0.0377
Epoch [6/30], Batch [2700/6000], Loss: 0.0454
Epoch [6/30], Batch [2800/6000], Loss: 0.0413
Epoch [6/30], Batch [2900/6000], Loss: 0.1273
Epoch [6/30], Batch [3000/6000], Loss: 0.0367
Epoch [6/30], Batch [3100/6000], Loss: 0.1458
Epoch [6/30], Batch [3200/6000], Loss: 0.1669
Epoch [6/30], Batch [3300/6000], Loss: 0.0286
Epoch [6/30], Batch [3400/6000], Loss: 0.0513
Epoch [6/30], Batch [3500/6000], Loss: 0.0317
Epoch [6/30], Batch [3600/6000], Loss: 0.0375
Epoch [6/30], Batch [3700/6000], Loss: 0.0282
Epoch [6/30], Batch [3800/6000], Loss: 0.0798
Epoch [6/30], Batch [3900/6000], Loss: 0.0374
Epoch [6/30], Batch [4000/6000], Loss: 0.0281
Epoch [6/30], Batch [4100/6000], Loss: 0.0371
Epoch [6/30], Batch [4200/6000], Loss: 0.0751
Epoch [6/30], Batch [4300/6000], Loss: 0.0398
Epoch [6/30], Batch [4400/6000], Loss: 0.1308
Epoch [6/30], Batch [4500/6000], Loss: 0.0327
Epoch [6/30], Batch [4600/6000], Loss: 0.1336
Epoch [6/30], Batch [4700/6000], Loss: 0.1406
Epoch [6/30], Batch [4800/6000], Loss: 0.1678
Epoch [6/30], Batch [4900/6000], Loss: 0.0270
Epoch [6/30], Batch [5000/6000], Loss: 0.0267
Epoch [6/30], Batch [5100/6000], Loss: 0.2147
Epoch [6/30], Batch [5200/6000], Loss: 0.0362
Epoch [6/30], Batch [5300/6000], Loss: 0.0830
Epoch [6/30], Batch [5400/6000], Loss: 0.0319
Epoch [6/30], Batch [5500/6000], Loss: 0.0842
Epoch [6/30], Batch [5600/6000], Loss: 0.2226
Epoch [6/30], Batch [5700/6000], Loss: 0.0285
Epoch [6/30], Batch [5800/6000], Loss: 0.1824
Epoch [6/30], Batch [5900/6000], Loss: 0.1932
Epoch [6/30], Loss: 0.1031
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.0270
Epoch [7/30], Batch [100/6000], Loss: 0.0358
Epoch [7/30], Batch [200/6000], Loss: 0.0275
Epoch [7/30], Batch [300/6000], Loss: 0.0407
Epoch [7/30], Batch [400/6000], Loss: 0.0283
Epoch [7/30], Batch [500/6000], Loss: 0.3581
Epoch [7/30], Batch [600/6000], Loss: 0.4115
Epoch [7/30], Batch [700/6000], Loss: 0.0321
Epoch [7/30], Batch [800/6000], Loss: 0.2768
Epoch [7/30], Batch [900/6000], Loss: 0.2750
Epoch [7/30], Batch [1000/6000], Loss: 0.1136
Epoch [7/30], Batch [1100/6000], Loss: 0.0273
Epoch [7/30], Batch [1200/6000], Loss: 0.0331
Epoch [7/30], Batch [1300/6000], Loss: 0.0308
Epoch [7/30], Batch [1400/6000], Loss: 0.0479
Epoch [7/30], Batch [1500/6000], Loss: 0.0264
Epoch [7/30], Batch [1600/6000], Loss: 0.0325
Epoch [7/30], Batch [1700/6000], Loss: 0.6839
Epoch [7/30], Batch [1800/6000], Loss: 0.4082
Epoch [7/30], Batch [1900/6000], Loss: 0.0249
Epoch [7/30], Batch [2000/6000], Loss: 0.3044
Epoch [7/30], Batch [2100/6000], Loss: 0.0619
Epoch [7/30], Batch [2200/6000], Loss: 0.0250
Epoch [7/30], Batch [2300/6000], Loss: 0.0636
Epoch [7/30], Batch [2400/6000], Loss: 0.0629
Epoch [7/30], Batch [2500/6000], Loss: 0.0257
Epoch [7/30], Batch [2600/6000], Loss: 0.0695
Epoch [7/30], Batch [2700/6000], Loss: 0.0655
Epoch [7/30], Batch [2800/6000], Loss: 0.0224
Epoch [7/30], Batch [2900/6000], Loss: 0.0270
Epoch [7/30], Batch [3000/6000], Loss: 0.0267
Epoch [7/30], Batch [3100/6000], Loss: 0.0355
Epoch [7/30], Batch [3200/6000], Loss: 0.0607
Epoch [7/30], Batch [3300/6000], Loss: 0.0227
Epoch [7/30], Batch [3400/6000], Loss: 0.0514
Epoch [7/30], Batch [3500/6000], Loss: 0.0159
Epoch [7/30], Batch [3600/6000], Loss: 0.0262
Epoch [7/30], Batch [3700/6000], Loss: 0.0264
Epoch [7/30], Batch [3800/6000], Loss: 0.0406
Epoch [7/30], Batch [3900/6000], Loss: 0.5866
Epoch [7/30], Batch [4000/6000], Loss: 0.0426
Epoch [7/30], Batch [4100/6000], Loss: 0.0366
Epoch [7/30], Batch [4200/6000], Loss: 0.4207
Epoch [7/30], Batch [4300/6000], Loss: 0.0250
Epoch [7/30], Batch [4400/6000], Loss: 0.0309
Epoch [7/30], Batch [4500/6000], Loss: 0.3621
Epoch [7/30], Batch [4600/6000], Loss: 0.0238
Epoch [7/30], Batch [4700/6000], Loss: 0.6082
Epoch [7/30], Batch [4800/6000], Loss: 0.0367
Epoch [7/30], Batch [4900/6000], Loss: 0.0570
Epoch [7/30], Batch [5000/6000], Loss: 0.0260
Epoch [7/30], Batch [5100/6000], Loss: 0.1590
Epoch [7/30], Batch [5200/6000], Loss: 0.0445
Epoch [7/30], Batch [5300/6000], Loss: 0.5396
Epoch [7/30], Batch [5400/6000], Loss: 0.2490
Epoch [7/30], Batch [5500/6000], Loss: 0.4731
Epoch [7/30], Batch [5600/6000], Loss: 0.0538
Epoch [7/30], Batch [5700/6000], Loss: 0.0325
Epoch [7/30], Batch [5800/6000], Loss: 0.0372
Epoch [7/30], Batch [5900/6000], Loss: 0.0265
Epoch [7/30], Loss: 0.0925
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.3026
Epoch [8/30], Batch [100/6000], Loss: 0.0994
Epoch [8/30], Batch [200/6000], Loss: 0.0864
Epoch [8/30], Batch [300/6000], Loss: 0.1182
Epoch [8/30], Batch [400/6000], Loss: 0.0298
Epoch [8/30], Batch [500/6000], Loss: 0.0296
Epoch [8/30], Batch [600/6000], Loss: 0.0317
Epoch [8/30], Batch [700/6000], Loss: 0.0507
Epoch [8/30], Batch [800/6000], Loss: 0.0231
Epoch [8/30], Batch [900/6000], Loss: 0.0980
Epoch [8/30], Batch [1000/6000], Loss: 0.4195
Epoch [8/30], Batch [1100/6000], Loss: 0.3697
Epoch [8/30], Batch [1200/6000], Loss: 0.0572
Epoch [8/30], Batch [1300/6000], Loss: 0.0430
Epoch [8/30], Batch [1400/6000], Loss: 0.0269
Epoch [8/30], Batch [1500/6000], Loss: 0.0341
Epoch [8/30], Batch [1600/6000], Loss: 0.0282
Epoch [8/30], Batch [1700/6000], Loss: 0.0256
Epoch [8/30], Batch [1800/6000], Loss: 0.1413
Epoch [8/30], Batch [1900/6000], Loss: 0.1710
Epoch [8/30], Batch [2000/6000], Loss: 0.0242
Epoch [8/30], Batch [2100/6000], Loss: 0.0382
Epoch [8/30], Batch [2200/6000], Loss: 0.0964
Epoch [8/30], Batch [2300/6000], Loss: 0.0253
Epoch [8/30], Batch [2400/6000], Loss: 0.0342
Epoch [8/30], Batch [2500/6000], Loss: 0.0271
Epoch [8/30], Batch [2600/6000], Loss: 0.0739
Epoch [8/30], Batch [2700/6000], Loss: 0.0268
Epoch [8/30], Batch [2800/6000], Loss: 0.0388
Epoch [8/30], Batch [2900/6000], Loss: 0.0257
Epoch [8/30], Batch [3000/6000], Loss: 0.0262
Epoch [8/30], Batch [3100/6000], Loss: 0.0292
Epoch [8/30], Batch [3200/6000], Loss: 0.0320
Epoch [8/30], Batch [3300/6000], Loss: 0.0271
Epoch [8/30], Batch [3400/6000], Loss: 0.0216
Epoch [8/30], Batch [3500/6000], Loss: 0.1970
Epoch [8/30], Batch [3600/6000], Loss: 0.1130
Epoch [8/30], Batch [3700/6000], Loss: 0.1500
Epoch [8/30], Batch [3800/6000], Loss: 0.0312
Epoch [8/30], Batch [3900/6000], Loss: 0.0309
Epoch [8/30], Batch [4000/6000], Loss: 0.0499
Epoch [8/30], Batch [4100/6000], Loss: 0.0799
Epoch [8/30], Batch [4200/6000], Loss: 0.0356
Epoch [8/30], Batch [4300/6000], Loss: 0.0346
Epoch [8/30], Batch [4400/6000], Loss: 0.4843
Epoch [8/30], Batch [4500/6000], Loss: 0.0296
Epoch [8/30], Batch [4600/6000], Loss: 0.3198
Epoch [8/30], Batch [4700/6000], Loss: 0.0216
Epoch [8/30], Batch [4800/6000], Loss: 0.0231
Epoch [8/30], Batch [4900/6000], Loss: 0.0275
Epoch [8/30], Batch [5000/6000], Loss: 0.0308
Epoch [8/30], Batch [5100/6000], Loss: 0.0269
Epoch [8/30], Batch [5200/6000], Loss: 0.0262
Epoch [8/30], Batch [5300/6000], Loss: 0.0207
Epoch [8/30], Batch [5400/6000], Loss: 0.0245
Epoch [8/30], Batch [5500/6000], Loss: 0.0292
Epoch [8/30], Batch [5600/6000], Loss: 0.0299
Epoch [8/30], Batch [5700/6000], Loss: 0.0272
Epoch [8/30], Batch [5800/6000], Loss: 0.0404
Epoch [8/30], Batch [5900/6000], Loss: 0.0251
Epoch [8/30], Loss: 0.0835
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.5042
Epoch [9/30], Batch [100/6000], Loss: 0.0445
Epoch [9/30], Batch [200/6000], Loss: 0.0253
Epoch [9/30], Batch [300/6000], Loss: 0.0235
Epoch [9/30], Batch [400/6000], Loss: 0.0303
Epoch [9/30], Batch [500/6000], Loss: 0.0340
Epoch [9/30], Batch [600/6000], Loss: 0.0456
Epoch [9/30], Batch [700/6000], Loss: 0.0620
Epoch [9/30], Batch [800/6000], Loss: 0.0266
Epoch [9/30], Batch [900/6000], Loss: 0.0267
Epoch [9/30], Batch [1000/6000], Loss: 0.0983
Epoch [9/30], Batch [1100/6000], Loss: 0.0612
Epoch [9/30], Batch [1200/6000], Loss: 0.1768
Epoch [9/30], Batch [1300/6000], Loss: 0.0230
Epoch [9/30], Batch [1400/6000], Loss: 0.0267
Epoch [9/30], Batch [1500/6000], Loss: 0.0236
Epoch [9/30], Batch [1600/6000], Loss: 0.0301
Epoch [9/30], Batch [1700/6000], Loss: 0.0840
Epoch [9/30], Batch [1800/6000], Loss: 0.1329
Epoch [9/30], Batch [1900/6000], Loss: 0.0334
Epoch [9/30], Batch [2000/6000], Loss: 0.0359
Epoch [9/30], Batch [2100/6000], Loss: 0.0413
Epoch [9/30], Batch [2200/6000], Loss: 0.0503
Epoch [9/30], Batch [2300/6000], Loss: 0.0347
Epoch [9/30], Batch [2400/6000], Loss: 0.0504
Epoch [9/30], Batch [2500/6000], Loss: 0.0296
Epoch [9/30], Batch [2600/6000], Loss: 0.1649
Epoch [9/30], Batch [2700/6000], Loss: 0.0667
Epoch [9/30], Batch [2800/6000], Loss: 0.0287
Epoch [9/30], Batch [2900/6000], Loss: 0.0229
Epoch [9/30], Batch [3000/6000], Loss: 0.0425
Epoch [9/30], Batch [3100/6000], Loss: 0.0421
Epoch [9/30], Batch [3200/6000], Loss: 0.0425
Epoch [9/30], Batch [3300/6000], Loss: 0.0233
Epoch [9/30], Batch [3400/6000], Loss: 0.0325
Epoch [9/30], Batch [3500/6000], Loss: 0.7134
Epoch [9/30], Batch [3600/6000], Loss: 0.0307
Epoch [9/30], Batch [3700/6000], Loss: 0.0372
Epoch [9/30], Batch [3800/6000], Loss: 0.0269
Epoch [9/30], Batch [3900/6000], Loss: 0.0471
Epoch [9/30], Batch [4000/6000], Loss: 0.3562
Epoch [9/30], Batch [4100/6000], Loss: 0.0219
Epoch [9/30], Batch [4200/6000], Loss: 0.2041
Epoch [9/30], Batch [4300/6000], Loss: 0.1926
Epoch [9/30], Batch [4400/6000], Loss: 0.0248
Epoch [9/30], Batch [4500/6000], Loss: 0.0267
Epoch [9/30], Batch [4600/6000], Loss: 0.9035
Epoch [9/30], Batch [4700/6000], Loss: 0.0316
Epoch [9/30], Batch [4800/6000], Loss: 0.1427
Epoch [9/30], Batch [4900/6000], Loss: 0.0289
Epoch [9/30], Batch [5000/6000], Loss: 0.3410
Epoch [9/30], Batch [5100/6000], Loss: 0.0354
Epoch [9/30], Batch [5200/6000], Loss: 0.0234
Epoch [9/30], Batch [5300/6000], Loss: 0.1088
Epoch [9/30], Batch [5400/6000], Loss: 0.0637
Epoch [9/30], Batch [5500/6000], Loss: 0.0212
Epoch [9/30], Batch [5600/6000], Loss: 0.0545
Epoch [9/30], Batch [5700/6000], Loss: 0.0304
Epoch [9/30], Batch [5800/6000], Loss: 0.1624
Epoch [9/30], Batch [5900/6000], Loss: 0.0247
Epoch [9/30], Loss: 0.0733
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.0284
Epoch [10/30], Batch [100/6000], Loss: 0.0420
Epoch [10/30], Batch [200/6000], Loss: 0.0222
Epoch [10/30], Batch [300/6000], Loss: 0.0232
Epoch [10/30], Batch [400/6000], Loss: 0.6042
Epoch [10/30], Batch [500/6000], Loss: 0.0229
Epoch [10/30], Batch [600/6000], Loss: 0.0288
Epoch [10/30], Batch [700/6000], Loss: 0.0188
Epoch [10/30], Batch [800/6000], Loss: 0.2458
Epoch [10/30], Batch [900/6000], Loss: 0.0253
Epoch [10/30], Batch [1000/6000], Loss: 0.0279
Epoch [10/30], Batch [1100/6000], Loss: 0.0492
Epoch [10/30], Batch [1200/6000], Loss: 0.0283
Epoch [10/30], Batch [1300/6000], Loss: 0.0265
Epoch [10/30], Batch [1400/6000], Loss: 0.0508
Epoch [10/30], Batch [1500/6000], Loss: 0.0259
Epoch [10/30], Batch [1600/6000], Loss: 0.0591
Epoch [10/30], Batch [1700/6000], Loss: 0.2356
Epoch [10/30], Batch [1800/6000], Loss: 0.0648
Epoch [10/30], Batch [1900/6000], Loss: 0.0277
Epoch [10/30], Batch [2000/6000], Loss: 0.0247
Epoch [10/30], Batch [2100/6000], Loss: 0.0258
Epoch [10/30], Batch [2200/6000], Loss: 0.0362
Epoch [10/30], Batch [2300/6000], Loss: 0.0396
Epoch [10/30], Batch [2400/6000], Loss: 0.0320
Epoch [10/30], Batch [2500/6000], Loss: 0.0312
Epoch [10/30], Batch [2600/6000], Loss: 0.0392
Epoch [10/30], Batch [2700/6000], Loss: 0.3592
Epoch [10/30], Batch [2800/6000], Loss: 0.0341
Epoch [10/30], Batch [2900/6000], Loss: 0.0335
Epoch [10/30], Batch [3000/6000], Loss: 0.0225
Epoch [10/30], Batch [3100/6000], Loss: 0.0346
Epoch [10/30], Batch [3200/6000], Loss: 0.0306
Epoch [10/30], Batch [3300/6000], Loss: 0.0294
Epoch [10/30], Batch [3400/6000], Loss: 0.1636
Epoch [10/30], Batch [3500/6000], Loss: 0.0500
Epoch [10/30], Batch [3600/6000], Loss: 0.0385
Epoch [10/30], Batch [3700/6000], Loss: 0.0220
Epoch [10/30], Batch [3800/6000], Loss: 0.0186
Epoch [10/30], Batch [3900/6000], Loss: 0.0352
Epoch [10/30], Batch [4000/6000], Loss: 0.0169
Epoch [10/30], Batch [4100/6000], Loss: 0.1661
Epoch [10/30], Batch [4200/6000], Loss: 0.0248
Epoch [10/30], Batch [4300/6000], Loss: 0.0319
Epoch [10/30], Batch [4400/6000], Loss: 0.0307
Epoch [10/30], Batch [4500/6000], Loss: 0.0220
Epoch [10/30], Batch [4600/6000], Loss: 0.0247
Epoch [10/30], Batch [4700/6000], Loss: 0.0273
Epoch [10/30], Batch [4800/6000], Loss: 0.0345
Epoch [10/30], Batch [4900/6000], Loss: 0.0486
Epoch [10/30], Batch [5000/6000], Loss: 0.0336
Epoch [10/30], Batch [5100/6000], Loss: 0.2872
Epoch [10/30], Batch [5200/6000], Loss: 0.0300
Epoch [10/30], Batch [5300/6000], Loss: 0.1752
Epoch [10/30], Batch [5400/6000], Loss: 0.0648
Epoch [10/30], Batch [5500/6000], Loss: 0.0646
Epoch [10/30], Batch [5600/6000], Loss: 0.0283
Epoch [10/30], Batch [5700/6000], Loss: 0.0220
Epoch [10/30], Batch [5800/6000], Loss: 0.0242
Epoch [10/30], Batch [5900/6000], Loss: 0.0236
Epoch [10/30], Loss: 0.0671
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.0185
Epoch [11/30], Batch [100/6000], Loss: 0.2186
Epoch [11/30], Batch [200/6000], Loss: 0.0774
Epoch [11/30], Batch [300/6000], Loss: 0.0213
Epoch [11/30], Batch [400/6000], Loss: 0.0341
Epoch [11/30], Batch [500/6000], Loss: 0.0310
Epoch [11/30], Batch [600/6000], Loss: 0.0272
Epoch [11/30], Batch [700/6000], Loss: 0.0298
Epoch [11/30], Batch [800/6000], Loss: 0.0201
Epoch [11/30], Batch [900/6000], Loss: 0.2361
Epoch [11/30], Batch [1000/6000], Loss: 0.1121
Epoch [11/30], Batch [1100/6000], Loss: 0.0222
Epoch [11/30], Batch [1200/6000], Loss: 0.0603
Epoch [11/30], Batch [1300/6000], Loss: 0.0397
Epoch [11/30], Batch [1400/6000], Loss: 0.0307
Epoch [11/30], Batch [1500/6000], Loss: 0.0246
Epoch [11/30], Batch [1600/6000], Loss: 0.0235
Epoch [11/30], Batch [1700/6000], Loss: 0.0219
Epoch [11/30], Batch [1800/6000], Loss: 0.0508
Epoch [11/30], Batch [1900/6000], Loss: 0.0306
Epoch [11/30], Batch [2000/6000], Loss: 0.0825
Epoch [11/30], Batch [2100/6000], Loss: 0.0240
Epoch [11/30], Batch [2200/6000], Loss: 0.0664
Epoch [11/30], Batch [2300/6000], Loss: 0.0262
Epoch [11/30], Batch [2400/6000], Loss: 0.0243
Epoch [11/30], Batch [2500/6000], Loss: 0.3019
Epoch [11/30], Batch [2600/6000], Loss: 0.0332
Epoch [11/30], Batch [2700/6000], Loss: 0.0239
Epoch [11/30], Batch [2800/6000], Loss: 0.0506
Epoch [11/30], Batch [2900/6000], Loss: 0.0241
Epoch [11/30], Batch [3000/6000], Loss: 0.0271
Epoch [11/30], Batch [3100/6000], Loss: 0.0236
Epoch [11/30], Batch [3200/6000], Loss: 0.0255
Epoch [11/30], Batch [3300/6000], Loss: 0.0277
Epoch [11/30], Batch [3400/6000], Loss: 0.0440
Epoch [11/30], Batch [3500/6000], Loss: 0.0243
Epoch [11/30], Batch [3600/6000], Loss: 0.0350
Epoch [11/30], Batch [3700/6000], Loss: 0.0214
Epoch [11/30], Batch [3800/6000], Loss: 0.0585
Epoch [11/30], Batch [3900/6000], Loss: 0.0801
Epoch [11/30], Batch [4000/6000], Loss: 0.0970
Epoch [11/30], Batch [4100/6000], Loss: 0.0385
Epoch [11/30], Batch [4200/6000], Loss: 0.0836
Epoch [11/30], Batch [4300/6000], Loss: 0.0250
Epoch [11/30], Batch [4400/6000], Loss: 0.0189
Epoch [11/30], Batch [4500/6000], Loss: 0.0207
Epoch [11/30], Batch [4600/6000], Loss: 0.0273
Epoch [11/30], Batch [4700/6000], Loss: 0.2093
Epoch [11/30], Batch [4800/6000], Loss: 0.0298
Epoch [11/30], Batch [4900/6000], Loss: 0.1847
Epoch [11/30], Batch [5000/6000], Loss: 0.0680
Epoch [11/30], Batch [5100/6000], Loss: 0.0248
Epoch [11/30], Batch [5200/6000], Loss: 0.0218
Epoch [11/30], Batch [5300/6000], Loss: 0.1616
Epoch [11/30], Batch [5400/6000], Loss: 0.0231
Epoch [11/30], Batch [5500/6000], Loss: 0.0229
Epoch [11/30], Batch [5600/6000], Loss: 0.0218
Epoch [11/30], Batch [5700/6000], Loss: 0.0212
Epoch [11/30], Batch [5800/6000], Loss: 0.0224
Epoch [11/30], Batch [5900/6000], Loss: 0.0230
Epoch [11/30], Loss: 0.0617
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.0299
Epoch [12/30], Batch [100/6000], Loss: 0.0384
Epoch [12/30], Batch [200/6000], Loss: 0.0309
Epoch [12/30], Batch [300/6000], Loss: 0.0352
Epoch [12/30], Batch [400/6000], Loss: 0.0207
Epoch [12/30], Batch [500/6000], Loss: 0.0290
Epoch [12/30], Batch [600/6000], Loss: 0.0226
Epoch [12/30], Batch [700/6000], Loss: 0.0616
Epoch [12/30], Batch [800/6000], Loss: 0.0249
Epoch [12/30], Batch [900/6000], Loss: 0.0972
Epoch [12/30], Batch [1000/6000], Loss: 0.0784
Epoch [12/30], Batch [1100/6000], Loss: 0.0215
Epoch [12/30], Batch [1200/6000], Loss: 0.0239
Epoch [12/30], Batch [1300/6000], Loss: 0.0282
Epoch [12/30], Batch [1400/6000], Loss: 0.0232
Epoch [12/30], Batch [1500/6000], Loss: 0.0243
Epoch [12/30], Batch [1600/6000], Loss: 0.0208
Epoch [12/30], Batch [1700/6000], Loss: 0.4226
Epoch [12/30], Batch [1800/6000], Loss: 0.0249
Epoch [12/30], Batch [1900/6000], Loss: 0.0241
Epoch [12/30], Batch [2000/6000], Loss: 0.1357
Epoch [12/30], Batch [2100/6000], Loss: 0.0485
Epoch [12/30], Batch [2200/6000], Loss: 0.0278
Epoch [12/30], Batch [2300/6000], Loss: 0.0331
Epoch [12/30], Batch [2400/6000], Loss: 0.0338
Epoch [12/30], Batch [2500/6000], Loss: 0.0295
Epoch [12/30], Batch [2600/6000], Loss: 0.0193
Epoch [12/30], Batch [2700/6000], Loss: 0.0242
Epoch [12/30], Batch [2800/6000], Loss: 0.1310
Epoch [12/30], Batch [2900/6000], Loss: 0.0197
Epoch [12/30], Batch [3000/6000], Loss: 0.0234
Epoch [12/30], Batch [3100/6000], Loss: 0.0200
Epoch [12/30], Batch [3200/6000], Loss: 0.0674
Epoch [12/30], Batch [3300/6000], Loss: 0.0290
Epoch [12/30], Batch [3400/6000], Loss: 0.0171
Epoch [12/30], Batch [3500/6000], Loss: 0.0202
Epoch [12/30], Batch [3600/6000], Loss: 0.2459
Epoch [12/30], Batch [3700/6000], Loss: 0.0682
Epoch [12/30], Batch [3800/6000], Loss: 0.0668
Epoch [12/30], Batch [3900/6000], Loss: 0.0223
Epoch [12/30], Batch [4000/6000], Loss: 0.0299
Epoch [12/30], Batch [4100/6000], Loss: 0.0297
Epoch [12/30], Batch [4200/6000], Loss: 0.0215
Epoch [12/30], Batch [4300/6000], Loss: 0.0532
Epoch [12/30], Batch [4400/6000], Loss: 0.0284
Epoch [12/30], Batch [4500/6000], Loss: 0.0248
Epoch [12/30], Batch [4600/6000], Loss: 0.0267
Epoch [12/30], Batch [4700/6000], Loss: 0.0219
Epoch [12/30], Batch [4800/6000], Loss: 0.0348
Epoch [12/30], Batch [4900/6000], Loss: 0.0253
Epoch [12/30], Batch [5000/6000], Loss: 0.0434
Epoch [12/30], Batch [5100/6000], Loss: 0.0415
Epoch [12/30], Batch [5200/6000], Loss: 0.0233
Epoch [12/30], Batch [5300/6000], Loss: 0.0195
Epoch [12/30], Batch [5400/6000], Loss: 0.0175
Epoch [12/30], Batch [5500/6000], Loss: 0.0243
Epoch [12/30], Batch [5600/6000], Loss: 0.0438
Epoch [12/30], Batch [5700/6000], Loss: 0.0212
Epoch [12/30], Batch [5800/6000], Loss: 0.0243
Epoch [12/30], Batch [5900/6000], Loss: 0.0317
Epoch [12/30], Loss: 0.0561
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.0308
Epoch [13/30], Batch [100/6000], Loss: 0.0340
Epoch [13/30], Batch [200/6000], Loss: 0.0215
Epoch [13/30], Batch [300/6000], Loss: 0.0185
Epoch [13/30], Batch [400/6000], Loss: 0.0396
Epoch [13/30], Batch [500/6000], Loss: 0.0975
Epoch [13/30], Batch [600/6000], Loss: 0.0264
Epoch [13/30], Batch [700/6000], Loss: 0.0277
Epoch [13/30], Batch [800/6000], Loss: 0.0218
Epoch [13/30], Batch [900/6000], Loss: 0.0944
Epoch [13/30], Batch [1000/6000], Loss: 0.0248
Epoch [13/30], Batch [1100/6000], Loss: 0.3779
Epoch [13/30], Batch [1200/6000], Loss: 0.0303
Epoch [13/30], Batch [1300/6000], Loss: 0.0497
Epoch [13/30], Batch [1400/6000], Loss: 0.0593
Epoch [13/30], Batch [1500/6000], Loss: 0.0285
Epoch [13/30], Batch [1600/6000], Loss: 0.0451
Epoch [13/30], Batch [1700/6000], Loss: 0.0283
Epoch [13/30], Batch [1800/6000], Loss: 0.0417
Epoch [13/30], Batch [1900/6000], Loss: 0.1045
Epoch [13/30], Batch [2000/6000], Loss: 0.1952
Epoch [13/30], Batch [2100/6000], Loss: 0.0233
Epoch [13/30], Batch [2200/6000], Loss: 0.0278
Epoch [13/30], Batch [2300/6000], Loss: 0.0214
Epoch [13/30], Batch [2400/6000], Loss: 0.0633
Epoch [13/30], Batch [2500/6000], Loss: 0.0459
Epoch [13/30], Batch [2600/6000], Loss: 0.0219
Epoch [13/30], Batch [2700/6000], Loss: 0.0221
Epoch [13/30], Batch [2800/6000], Loss: 0.0311
Epoch [13/30], Batch [2900/6000], Loss: 0.0286
Epoch [13/30], Batch [3000/6000], Loss: 0.0201
Epoch [13/30], Batch [3100/6000], Loss: 0.0694
Epoch [13/30], Batch [3200/6000], Loss: 0.0225
Epoch [13/30], Batch [3300/6000], Loss: 0.0223
Epoch [13/30], Batch [3400/6000], Loss: 0.0820
Epoch [13/30], Batch [3500/6000], Loss: 0.0713
Epoch [13/30], Batch [3600/6000], Loss: 0.0234
Epoch [13/30], Batch [3700/6000], Loss: 0.2812
Epoch [13/30], Batch [3800/6000], Loss: 0.0316
Epoch [13/30], Batch [3900/6000], Loss: 0.0228
Epoch [13/30], Batch [4000/6000], Loss: 0.0372
Epoch [13/30], Batch [4100/6000], Loss: 0.0430
Epoch [13/30], Batch [4200/6000], Loss: 0.0272
Epoch [13/30], Batch [4300/6000], Loss: 0.0953
Epoch [13/30], Batch [4400/6000], Loss: 0.0230
Epoch [13/30], Batch [4500/6000], Loss: 0.0218
Epoch [13/30], Batch [4600/6000], Loss: 0.0239
Epoch [13/30], Batch [4700/6000], Loss: 0.0409
Epoch [13/30], Batch [4800/6000], Loss: 0.0186
Epoch [13/30], Batch [4900/6000], Loss: 0.0928
Epoch [13/30], Batch [5000/6000], Loss: 0.0256
Epoch [13/30], Batch [5100/6000], Loss: 0.0213
Epoch [13/30], Batch [5200/6000], Loss: 0.0244
Epoch [13/30], Batch [5300/6000], Loss: 0.0284
Epoch [13/30], Batch [5400/6000], Loss: 0.0265
Epoch [13/30], Batch [5500/6000], Loss: 0.0238
Epoch [13/30], Batch [5600/6000], Loss: 0.0201
Epoch [13/30], Batch [5700/6000], Loss: 0.0532
Epoch [13/30], Batch [5800/6000], Loss: 0.0177
Epoch [13/30], Batch [5900/6000], Loss: 0.0167
Epoch [13/30], Loss: 0.0528
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.0237
Epoch [14/30], Batch [100/6000], Loss: 0.0170
Epoch [14/30], Batch [200/6000], Loss: 0.0388
Epoch [14/30], Batch [300/6000], Loss: 0.0249
Epoch [14/30], Batch [400/6000], Loss: 0.0244
Epoch [14/30], Batch [500/6000], Loss: 0.0247
Epoch [14/30], Batch [600/6000], Loss: 0.0200
Epoch [14/30], Batch [700/6000], Loss: 0.0236
Epoch [14/30], Batch [800/6000], Loss: 0.0176
Epoch [14/30], Batch [900/6000], Loss: 0.0242
Epoch [14/30], Batch [1000/6000], Loss: 0.0186
Epoch [14/30], Batch [1100/6000], Loss: 0.0210
Epoch [14/30], Batch [1200/6000], Loss: 0.0775
Epoch [14/30], Batch [1300/6000], Loss: 0.0597
Epoch [14/30], Batch [1400/6000], Loss: 0.0266
Epoch [14/30], Batch [1500/6000], Loss: 0.1410
Epoch [14/30], Batch [1600/6000], Loss: 0.0226
Epoch [14/30], Batch [1700/6000], Loss: 0.0222
Epoch [14/30], Batch [1800/6000], Loss: 0.0213
Epoch [14/30], Batch [1900/6000], Loss: 0.0379
Epoch [14/30], Batch [2000/6000], Loss: 0.0281
Epoch [14/30], Batch [2100/6000], Loss: 0.0201
Epoch [14/30], Batch [2200/6000], Loss: 0.3726
Epoch [14/30], Batch [2300/6000], Loss: 0.0246
Epoch [14/30], Batch [2400/6000], Loss: 0.0168
Epoch [14/30], Batch [2500/6000], Loss: 0.0260
Epoch [14/30], Batch [2600/6000], Loss: 0.0274
Epoch [14/30], Batch [2700/6000], Loss: 0.0200
Epoch [14/30], Batch [2800/6000], Loss: 0.0214
Epoch [14/30], Batch [2900/6000], Loss: 0.0351
Epoch [14/30], Batch [3000/6000], Loss: 0.0199
Epoch [14/30], Batch [3100/6000], Loss: 0.0284
Epoch [14/30], Batch [3200/6000], Loss: 0.0224
Epoch [14/30], Batch [3300/6000], Loss: 0.0502
Epoch [14/30], Batch [3400/6000], Loss: 0.0204
Epoch [14/30], Batch [3500/6000], Loss: 0.0232
Epoch [14/30], Batch [3600/6000], Loss: 0.0236
Epoch [14/30], Batch [3700/6000], Loss: 0.0234
Epoch [14/30], Batch [3800/6000], Loss: 0.0184
Epoch [14/30], Batch [3900/6000], Loss: 0.0531
Epoch [14/30], Batch [4000/6000], Loss: 0.0197
Epoch [14/30], Batch [4100/6000], Loss: 0.0256
Epoch [14/30], Batch [4200/6000], Loss: 0.1294
Epoch [14/30], Batch [4300/6000], Loss: 0.0266
Epoch [14/30], Batch [4400/6000], Loss: 0.0215
Epoch [14/30], Batch [4500/6000], Loss: 0.1244
Epoch [14/30], Batch [4600/6000], Loss: 0.0594
Epoch [14/30], Batch [4700/6000], Loss: 0.0213
Epoch [14/30], Batch [4800/6000], Loss: 0.0743
Epoch [14/30], Batch [4900/6000], Loss: 0.0218
Epoch [14/30], Batch [5000/6000], Loss: 0.0275
Epoch [14/30], Batch [5100/6000], Loss: 0.0214
Epoch [14/30], Batch [5200/6000], Loss: 0.0176
Epoch [14/30], Batch [5300/6000], Loss: 0.0763
Epoch [14/30], Batch [5400/6000], Loss: 0.0505
Epoch [14/30], Batch [5500/6000], Loss: 0.0277
Epoch [14/30], Batch [5600/6000], Loss: 0.0203
Epoch [14/30], Batch [5700/6000], Loss: 0.0239
Epoch [14/30], Batch [5800/6000], Loss: 0.1623
Epoch [14/30], Batch [5900/6000], Loss: 0.0205
Epoch [14/30], Loss: 0.0487
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.0256
Epoch [15/30], Batch [100/6000], Loss: 0.0322
Epoch [15/30], Batch [200/6000], Loss: 0.0192
Epoch [15/30], Batch [300/6000], Loss: 0.0999
Epoch [15/30], Batch [400/6000], Loss: 0.0231
Epoch [15/30], Batch [500/6000], Loss: 0.0645
Epoch [15/30], Batch [600/6000], Loss: 0.0197
Epoch [15/30], Batch [700/6000], Loss: 0.0205
Epoch [15/30], Batch [800/6000], Loss: 0.0429
Epoch [15/30], Batch [900/6000], Loss: 0.0244
Epoch [15/30], Batch [1000/6000], Loss: 0.0198
Epoch [15/30], Batch [1100/6000], Loss: 0.0188
Epoch [15/30], Batch [1200/6000], Loss: 0.0193
Epoch [15/30], Batch [1300/6000], Loss: 0.0232
Epoch [15/30], Batch [1400/6000], Loss: 0.0198
Epoch [15/30], Batch [1500/6000], Loss: 0.0240
Epoch [15/30], Batch [1600/6000], Loss: 0.0328
Epoch [15/30], Batch [1700/6000], Loss: 0.0158
Epoch [15/30], Batch [1800/6000], Loss: 0.0198
Epoch [15/30], Batch [1900/6000], Loss: 0.0528
Epoch [15/30], Batch [2000/6000], Loss: 0.0342
Epoch [15/30], Batch [2100/6000], Loss: 0.0406
Epoch [15/30], Batch [2200/6000], Loss: 0.0246
Epoch [15/30], Batch [2300/6000], Loss: 0.0696
Epoch [15/30], Batch [2400/6000], Loss: 0.0257
Epoch [15/30], Batch [2500/6000], Loss: 0.0196
Epoch [15/30], Batch [2600/6000], Loss: 0.1607
Epoch [15/30], Batch [2700/6000], Loss: 0.0276
Epoch [15/30], Batch [2800/6000], Loss: 0.0218
Epoch [15/30], Batch [2900/6000], Loss: 0.0207
Epoch [15/30], Batch [3000/6000], Loss: 0.0219
Epoch [15/30], Batch [3100/6000], Loss: 0.0332
Epoch [15/30], Batch [3200/6000], Loss: 0.0262
Epoch [15/30], Batch [3300/6000], Loss: 0.0345
Epoch [15/30], Batch [3400/6000], Loss: 0.0257
Epoch [15/30], Batch [3500/6000], Loss: 0.0253
Epoch [15/30], Batch [3600/6000], Loss: 0.0190
Epoch [15/30], Batch [3700/6000], Loss: 0.0379
Epoch [15/30], Batch [3800/6000], Loss: 0.0197
Epoch [15/30], Batch [3900/6000], Loss: 0.0171
Epoch [15/30], Batch [4000/6000], Loss: 0.0433
Epoch [15/30], Batch [4100/6000], Loss: 0.0159
Epoch [15/30], Batch [4200/6000], Loss: 0.0237
Epoch [15/30], Batch [4300/6000], Loss: 0.0346
Epoch [15/30], Batch [4400/6000], Loss: 0.0183
Epoch [15/30], Batch [4500/6000], Loss: 0.0337
Epoch [15/30], Batch [4600/6000], Loss: 0.0133
Epoch [15/30], Batch [4700/6000], Loss: 0.0190
Epoch [15/30], Batch [4800/6000], Loss: 0.0319
Epoch [15/30], Batch [4900/6000], Loss: 0.1217
Epoch [15/30], Batch [5000/6000], Loss: 0.0240
Epoch [15/30], Batch [5100/6000], Loss: 0.0575
Epoch [15/30], Batch [5200/6000], Loss: 0.1436
Epoch [15/30], Batch [5300/6000], Loss: 0.0222
Epoch [15/30], Batch [5400/6000], Loss: 0.0589
Epoch [15/30], Batch [5500/6000], Loss: 0.0525
Epoch [15/30], Batch [5600/6000], Loss: 0.0251
Epoch [15/30], Batch [5700/6000], Loss: 0.0256
Epoch [15/30], Batch [5800/6000], Loss: 0.0216
Epoch [15/30], Batch [5900/6000], Loss: 0.0195
Epoch [15/30], Loss: 0.0459
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.0373
Epoch [16/30], Batch [100/6000], Loss: 0.0251
Epoch [16/30], Batch [200/6000], Loss: 0.0222
Epoch [16/30], Batch [300/6000], Loss: 0.0205
Epoch [16/30], Batch [400/6000], Loss: 0.0395
Epoch [16/30], Batch [500/6000], Loss: 0.0705
Epoch [16/30], Batch [600/6000], Loss: 0.1825
Epoch [16/30], Batch [700/6000], Loss: 0.0240
Epoch [16/30], Batch [800/6000], Loss: 0.0247
Epoch [16/30], Batch [900/6000], Loss: 0.0150
Epoch [16/30], Batch [1000/6000], Loss: 0.0251
Epoch [16/30], Batch [1100/6000], Loss: 0.0202
Epoch [16/30], Batch [1200/6000], Loss: 0.0249
Epoch [16/30], Batch [1300/6000], Loss: 0.0158
Epoch [16/30], Batch [1400/6000], Loss: 0.0246
Epoch [16/30], Batch [1500/6000], Loss: 0.0920
Epoch [16/30], Batch [1600/6000], Loss: 0.0273
Epoch [16/30], Batch [1700/6000], Loss: 0.0278
Epoch [16/30], Batch [1800/6000], Loss: 0.0261
Epoch [16/30], Batch [1900/6000], Loss: 0.0200
Epoch [16/30], Batch [2000/6000], Loss: 0.0275
Epoch [16/30], Batch [2100/6000], Loss: 0.0538
Epoch [16/30], Batch [2200/6000], Loss: 0.0210
Epoch [16/30], Batch [2300/6000], Loss: 0.0199
Epoch [16/30], Batch [2400/6000], Loss: 0.0202
Epoch [16/30], Batch [2500/6000], Loss: 0.0302
Epoch [16/30], Batch [2600/6000], Loss: 0.0190
Epoch [16/30], Batch [2700/6000], Loss: 0.0194
Epoch [16/30], Batch [2800/6000], Loss: 0.0206
Epoch [16/30], Batch [2900/6000], Loss: 0.0273
Epoch [16/30], Batch [3000/6000], Loss: 0.0188
Epoch [16/30], Batch [3100/6000], Loss: 0.0197
Epoch [16/30], Batch [3200/6000], Loss: 0.2557
Epoch [16/30], Batch [3300/6000], Loss: 0.2073
Epoch [16/30], Batch [3400/6000], Loss: 0.0184
Epoch [16/30], Batch [3500/6000], Loss: 0.0446
Epoch [16/30], Batch [3600/6000], Loss: 0.0244
Epoch [16/30], Batch [3700/6000], Loss: 0.0208
Epoch [16/30], Batch [3800/6000], Loss: 0.0586
Epoch [16/30], Batch [3900/6000], Loss: 0.0304
Epoch [16/30], Batch [4000/6000], Loss: 0.0430
Epoch [16/30], Batch [4100/6000], Loss: 0.0189
Epoch [16/30], Batch [4200/6000], Loss: 0.0248
Epoch [16/30], Batch [4300/6000], Loss: 0.0462
Epoch [16/30], Batch [4400/6000], Loss: 0.0802
Epoch [16/30], Batch [4500/6000], Loss: 0.0236
Epoch [16/30], Batch [4600/6000], Loss: 0.0202
Epoch [16/30], Batch [4700/6000], Loss: 0.3011
Epoch [16/30], Batch [4800/6000], Loss: 0.0237
Epoch [16/30], Batch [4900/6000], Loss: 0.3950
Epoch [16/30], Batch [5000/6000], Loss: 0.0227
Epoch [16/30], Batch [5100/6000], Loss: 0.0407
Epoch [16/30], Batch [5200/6000], Loss: 0.0253
Epoch [16/30], Batch [5300/6000], Loss: 0.0227
Epoch [16/30], Batch [5400/6000], Loss: 0.0192
Epoch [16/30], Batch [5500/6000], Loss: 0.0181
Epoch [16/30], Batch [5600/6000], Loss: 0.0258
Epoch [16/30], Batch [5700/6000], Loss: 0.1849
Epoch [16/30], Batch [5800/6000], Loss: 0.0242
Epoch [16/30], Batch [5900/6000], Loss: 0.0186
Epoch [16/30], Loss: 0.0433
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.0235
Epoch [17/30], Batch [100/6000], Loss: 0.0206
Epoch [17/30], Batch [200/6000], Loss: 0.0273
Epoch [17/30], Batch [300/6000], Loss: 0.0452
Epoch [17/30], Batch [400/6000], Loss: 0.0235
Epoch [17/30], Batch [500/6000], Loss: 0.0376
Epoch [17/30], Batch [600/6000], Loss: 0.0235
Epoch [17/30], Batch [700/6000], Loss: 0.0198
Epoch [17/30], Batch [800/6000], Loss: 0.0189
Epoch [17/30], Batch [900/6000], Loss: 0.0199
Epoch [17/30], Batch [1000/6000], Loss: 0.0202
Epoch [17/30], Batch [1100/6000], Loss: 0.0324
Epoch [17/30], Batch [1200/6000], Loss: 0.0219
Epoch [17/30], Batch [1300/6000], Loss: 0.0205
Epoch [17/30], Batch [1400/6000], Loss: 0.0218
Epoch [17/30], Batch [1500/6000], Loss: 0.0201
Epoch [17/30], Batch [1600/6000], Loss: 0.0456
Epoch [17/30], Batch [1700/6000], Loss: 0.3868
Epoch [17/30], Batch [1800/6000], Loss: 0.0218
Epoch [17/30], Batch [1900/6000], Loss: 0.0309
Epoch [17/30], Batch [2000/6000], Loss: 0.0219
Epoch [17/30], Batch [2100/6000], Loss: 0.0502
Epoch [17/30], Batch [2200/6000], Loss: 0.0213
Epoch [17/30], Batch [2300/6000], Loss: 0.0252
Epoch [17/30], Batch [2400/6000], Loss: 0.0155
Epoch [17/30], Batch [2500/6000], Loss: 0.0195
Epoch [17/30], Batch [2600/6000], Loss: 0.0221
Epoch [17/30], Batch [2700/6000], Loss: 0.0217
Epoch [17/30], Batch [2800/6000], Loss: 0.0193
Epoch [17/30], Batch [2900/6000], Loss: 0.1969
Epoch [17/30], Batch [3000/6000], Loss: 0.0213
Epoch [17/30], Batch [3100/6000], Loss: 0.0215
Epoch [17/30], Batch [3200/6000], Loss: 0.0221
Epoch [17/30], Batch [3300/6000], Loss: 0.0250
Epoch [17/30], Batch [3400/6000], Loss: 0.0213
Epoch [17/30], Batch [3500/6000], Loss: 0.0326
Epoch [17/30], Batch [3600/6000], Loss: 0.0290
Epoch [17/30], Batch [3700/6000], Loss: 0.0354
Epoch [17/30], Batch [3800/6000], Loss: 0.0215
Epoch [17/30], Batch [3900/6000], Loss: 0.1219
Epoch [17/30], Batch [4000/6000], Loss: 0.0467
Epoch [17/30], Batch [4100/6000], Loss: 0.0345
Epoch [17/30], Batch [4200/6000], Loss: 0.0138
Epoch [17/30], Batch [4300/6000], Loss: 0.0208
Epoch [17/30], Batch [4400/6000], Loss: 0.0228
Epoch [17/30], Batch [4500/6000], Loss: 0.0178
Epoch [17/30], Batch [4600/6000], Loss: 0.0251
Epoch [17/30], Batch [4700/6000], Loss: 0.0203
Epoch [17/30], Batch [4800/6000], Loss: 0.0222
Epoch [17/30], Batch [4900/6000], Loss: 0.0215
Epoch [17/30], Batch [5000/6000], Loss: 0.0402
Epoch [17/30], Batch [5100/6000], Loss: 0.0221
Epoch [17/30], Batch [5200/6000], Loss: 0.0209
Epoch [17/30], Batch [5300/6000], Loss: 0.0323
Epoch [17/30], Batch [5400/6000], Loss: 0.0493
Epoch [17/30], Batch [5500/6000], Loss: 0.0199
Epoch [17/30], Batch [5600/6000], Loss: 0.0209
Epoch [17/30], Batch [5700/6000], Loss: 0.0223
Epoch [17/30], Batch [5800/6000], Loss: 0.0247
Epoch [17/30], Batch [5900/6000], Loss: 0.0330
Epoch [17/30], Loss: 0.0416
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.0153
Epoch [18/30], Batch [100/6000], Loss: 0.0402
Epoch [18/30], Batch [200/6000], Loss: 0.0170
Epoch [18/30], Batch [300/6000], Loss: 0.0240
Epoch [18/30], Batch [400/6000], Loss: 0.0202
Epoch [18/30], Batch [500/6000], Loss: 0.0219
Epoch [18/30], Batch [600/6000], Loss: 0.0237
Epoch [18/30], Batch [700/6000], Loss: 0.0240
Epoch [18/30], Batch [800/6000], Loss: 0.0226
Epoch [18/30], Batch [900/6000], Loss: 0.0197
Epoch [18/30], Batch [1000/6000], Loss: 0.0190
Epoch [18/30], Batch [1100/6000], Loss: 0.0228
Epoch [18/30], Batch [1200/6000], Loss: 0.0211
Epoch [18/30], Batch [1300/6000], Loss: 0.0213
Epoch [18/30], Batch [1400/6000], Loss: 0.0198
Epoch [18/30], Batch [1500/6000], Loss: 0.0203
Epoch [18/30], Batch [1600/6000], Loss: 0.0174
Epoch [18/30], Batch [1700/6000], Loss: 0.0269
Epoch [18/30], Batch [1800/6000], Loss: 0.0228
Epoch [18/30], Batch [1900/6000], Loss: 0.0204
Epoch [18/30], Batch [2000/6000], Loss: 0.0297
Epoch [18/30], Batch [2100/6000], Loss: 0.0721
Epoch [18/30], Batch [2200/6000], Loss: 0.0199
Epoch [18/30], Batch [2300/6000], Loss: 0.0258
Epoch [18/30], Batch [2400/6000], Loss: 0.0213
Epoch [18/30], Batch [2500/6000], Loss: 0.0188
Epoch [18/30], Batch [2600/6000], Loss: 0.0261
Epoch [18/30], Batch [2700/6000], Loss: 0.0190
Epoch [18/30], Batch [2800/6000], Loss: 0.0266
Epoch [18/30], Batch [2900/6000], Loss: 0.0318
Epoch [18/30], Batch [3000/6000], Loss: 0.0223
Epoch [18/30], Batch [3100/6000], Loss: 0.0240
Epoch [18/30], Batch [3200/6000], Loss: 0.0154
Epoch [18/30], Batch [3300/6000], Loss: 0.0229
Epoch [18/30], Batch [3400/6000], Loss: 0.0349
Epoch [18/30], Batch [3500/6000], Loss: 0.0208
Epoch [18/30], Batch [3600/6000], Loss: 0.0204
Epoch [18/30], Batch [3700/6000], Loss: 0.0205
Epoch [18/30], Batch [3800/6000], Loss: 0.0194
Epoch [18/30], Batch [3900/6000], Loss: 0.0599
Epoch [18/30], Batch [4000/6000], Loss: 0.0221
Epoch [18/30], Batch [4100/6000], Loss: 0.0204
Epoch [18/30], Batch [4200/6000], Loss: 0.0436
Epoch [18/30], Batch [4300/6000], Loss: 0.0196
Epoch [18/30], Batch [4400/6000], Loss: 0.0555
Epoch [18/30], Batch [4500/6000], Loss: 0.0670
Epoch [18/30], Batch [4600/6000], Loss: 0.0215
Epoch [18/30], Batch [4700/6000], Loss: 0.0229
Epoch [18/30], Batch [4800/6000], Loss: 0.0247
Epoch [18/30], Batch [4900/6000], Loss: 0.0205
Epoch [18/30], Batch [5000/6000], Loss: 0.0242
Epoch [18/30], Batch [5100/6000], Loss: 0.0230
Epoch [18/30], Batch [5200/6000], Loss: 0.0235
Epoch [18/30], Batch [5300/6000], Loss: 0.0240
Epoch [18/30], Batch [5400/6000], Loss: 0.0210
Epoch [18/30], Batch [5500/6000], Loss: 0.0213
Epoch [18/30], Batch [5600/6000], Loss: 0.0224
Epoch [18/30], Batch [5700/6000], Loss: 0.0266
Epoch [18/30], Batch [5800/6000], Loss: 0.4579
Epoch [18/30], Batch [5900/6000], Loss: 0.0741
Epoch [18/30], Loss: 0.0381
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.0218
Epoch [19/30], Batch [100/6000], Loss: 0.0212
Epoch [19/30], Batch [200/6000], Loss: 0.0241
Epoch [19/30], Batch [300/6000], Loss: 0.0236
Epoch [19/30], Batch [400/6000], Loss: 0.0189
Epoch [19/30], Batch [500/6000], Loss: 0.0212
Epoch [19/30], Batch [600/6000], Loss: 0.0285
Epoch [19/30], Batch [700/6000], Loss: 0.0173
Epoch [19/30], Batch [800/6000], Loss: 0.0184
Epoch [19/30], Batch [900/6000], Loss: 0.0157
Epoch [19/30], Batch [1000/6000], Loss: 0.0185
Epoch [19/30], Batch [1100/6000], Loss: 0.1727
Epoch [19/30], Batch [1200/6000], Loss: 0.0235
Epoch [19/30], Batch [1300/6000], Loss: 0.0873
Epoch [19/30], Batch [1400/6000], Loss: 0.0222
Epoch [19/30], Batch [1500/6000], Loss: 0.0220
Epoch [19/30], Batch [1600/6000], Loss: 0.0195
Epoch [19/30], Batch [1700/6000], Loss: 0.0241
Epoch [19/30], Batch [1800/6000], Loss: 0.0556
Epoch [19/30], Batch [1900/6000], Loss: 0.0224
Epoch [19/30], Batch [2000/6000], Loss: 0.0201
Epoch [19/30], Batch [2100/6000], Loss: 0.0188
Epoch [19/30], Batch [2200/6000], Loss: 0.0377
Epoch [19/30], Batch [2300/6000], Loss: 0.1163
Epoch [19/30], Batch [2400/6000], Loss: 0.0349
Epoch [19/30], Batch [2500/6000], Loss: 0.0206
Epoch [19/30], Batch [2600/6000], Loss: 0.0261
Epoch [19/30], Batch [2700/6000], Loss: 0.0272
Epoch [19/30], Batch [2800/6000], Loss: 0.0144
Epoch [19/30], Batch [2900/6000], Loss: 0.0171
Epoch [19/30], Batch [3000/6000], Loss: 0.0386
Epoch [19/30], Batch [3100/6000], Loss: 0.0603
Epoch [19/30], Batch [3200/6000], Loss: 0.0212
Epoch [19/30], Batch [3300/6000], Loss: 0.0151
Epoch [19/30], Batch [3400/6000], Loss: 0.0198
Epoch [19/30], Batch [3500/6000], Loss: 0.0173
Epoch [19/30], Batch [3600/6000], Loss: 0.0229
Epoch [19/30], Batch [3700/6000], Loss: 0.0184
Epoch [19/30], Batch [3800/6000], Loss: 0.0213
Epoch [19/30], Batch [3900/6000], Loss: 0.0263
Epoch [19/30], Batch [4000/6000], Loss: 0.0227
Epoch [19/30], Batch [4100/6000], Loss: 0.0175
Epoch [19/30], Batch [4200/6000], Loss: 0.1323
Epoch [19/30], Batch [4300/6000], Loss: 0.0244
Epoch [19/30], Batch [4400/6000], Loss: 0.1044
Epoch [19/30], Batch [4500/6000], Loss: 0.0180
Epoch [19/30], Batch [4600/6000], Loss: 0.0282
Epoch [19/30], Batch [4700/6000], Loss: 0.0233
Epoch [19/30], Batch [4800/6000], Loss: 0.0279
Epoch [19/30], Batch [4900/6000], Loss: 0.0164
Epoch [19/30], Batch [5000/6000], Loss: 0.0261
Epoch [19/30], Batch [5100/6000], Loss: 0.0540
Epoch [19/30], Batch [5200/6000], Loss: 0.0175
Epoch [19/30], Batch [5300/6000], Loss: 0.0280
Epoch [19/30], Batch [5400/6000], Loss: 0.0318
Epoch [19/30], Batch [5500/6000], Loss: 0.0184
Epoch [19/30], Batch [5600/6000], Loss: 0.0154
Epoch [19/30], Batch [5700/6000], Loss: 0.0621
Epoch [19/30], Batch [5800/6000], Loss: 0.0209
Epoch [19/30], Batch [5900/6000], Loss: 0.0238
Epoch [19/30], Loss: 0.0373
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.0162
Epoch [20/30], Batch [100/6000], Loss: 0.0237
Epoch [20/30], Batch [200/6000], Loss: 0.0313
Epoch [20/30], Batch [300/6000], Loss: 0.0253
Epoch [20/30], Batch [400/6000], Loss: 0.0179
Epoch [20/30], Batch [500/6000], Loss: 0.0174
Epoch [20/30], Batch [600/6000], Loss: 0.0173
Epoch [20/30], Batch [700/6000], Loss: 0.0154
Epoch [20/30], Batch [800/6000], Loss: 0.0212
Epoch [20/30], Batch [900/6000], Loss: 0.0235
Epoch [20/30], Batch [1000/6000], Loss: 0.0240
Epoch [20/30], Batch [1100/6000], Loss: 0.0179
Epoch [20/30], Batch [1200/6000], Loss: 0.0258
Epoch [20/30], Batch [1300/6000], Loss: 0.0173
Epoch [20/30], Batch [1400/6000], Loss: 0.0209
Epoch [20/30], Batch [1500/6000], Loss: 0.0162
Epoch [20/30], Batch [1600/6000], Loss: 0.0237
Epoch [20/30], Batch [1700/6000], Loss: 0.0191
Epoch [20/30], Batch [1800/6000], Loss: 0.0179
Epoch [20/30], Batch [1900/6000], Loss: 0.0222
Epoch [20/30], Batch [2000/6000], Loss: 0.0189
Epoch [20/30], Batch [2100/6000], Loss: 0.0191
Epoch [20/30], Batch [2200/6000], Loss: 0.0182
Epoch [20/30], Batch [2300/6000], Loss: 0.0196
Epoch [20/30], Batch [2400/6000], Loss: 0.0216
Epoch [20/30], Batch [2500/6000], Loss: 0.1546
Epoch [20/30], Batch [2600/6000], Loss: 0.0205
Epoch [20/30], Batch [2700/6000], Loss: 0.0166
Epoch [20/30], Batch [2800/6000], Loss: 0.0291
Epoch [20/30], Batch [2900/6000], Loss: 0.0484
Epoch [20/30], Batch [3000/6000], Loss: 0.0193
Epoch [20/30], Batch [3100/6000], Loss: 0.0279
Epoch [20/30], Batch [3200/6000], Loss: 0.0162
Epoch [20/30], Batch [3300/6000], Loss: 0.0225
Epoch [20/30], Batch [3400/6000], Loss: 0.0215
Epoch [20/30], Batch [3500/6000], Loss: 0.0186
Epoch [20/30], Batch [3600/6000], Loss: 0.0377
Epoch [20/30], Batch [3700/6000], Loss: 0.0307
Epoch [20/30], Batch [3800/6000], Loss: 0.0179
Epoch [20/30], Batch [3900/6000], Loss: 0.0197
Epoch [20/30], Batch [4000/6000], Loss: 0.0208
Epoch [20/30], Batch [4100/6000], Loss: 0.6424
Epoch [20/30], Batch [4200/6000], Loss: 0.0197
Epoch [20/30], Batch [4300/6000], Loss: 0.0240
Epoch [20/30], Batch [4400/6000], Loss: 0.0189
Epoch [20/30], Batch [4500/6000], Loss: 0.0218
Epoch [20/30], Batch [4600/6000], Loss: 0.0240
Epoch [20/30], Batch [4700/6000], Loss: 0.0228
Epoch [20/30], Batch [4800/6000], Loss: 0.0231
Epoch [20/30], Batch [4900/6000], Loss: 0.0298
Epoch [20/30], Batch [5000/6000], Loss: 0.0168
Epoch [20/30], Batch [5100/6000], Loss: 0.0310
Epoch [20/30], Batch [5200/6000], Loss: 0.0374
Epoch [20/30], Batch [5300/6000], Loss: 0.0203
Epoch [20/30], Batch [5400/6000], Loss: 0.0199
Epoch [20/30], Batch [5500/6000], Loss: 0.0222
Epoch [20/30], Batch [5600/6000], Loss: 0.0224
Epoch [20/30], Batch [5700/6000], Loss: 0.1090
Epoch [20/30], Batch [5800/6000], Loss: 0.0202
Epoch [20/30], Batch [5900/6000], Loss: 0.1144
Epoch [20/30], Loss: 0.0351
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.0215
Epoch [21/30], Batch [100/6000], Loss: 0.0182
Epoch [21/30], Batch [200/6000], Loss: 0.0193
Epoch [21/30], Batch [300/6000], Loss: 0.0231
Epoch [21/30], Batch [400/6000], Loss: 0.0254
Epoch [21/30], Batch [500/6000], Loss: 0.0211
Epoch [21/30], Batch [600/6000], Loss: 0.0311
Epoch [21/30], Batch [700/6000], Loss: 0.0202
Epoch [21/30], Batch [800/6000], Loss: 0.0191
Epoch [21/30], Batch [900/6000], Loss: 0.0225
Epoch [21/30], Batch [1000/6000], Loss: 0.0210
Epoch [21/30], Batch [1100/6000], Loss: 0.0213
Epoch [21/30], Batch [1200/6000], Loss: 0.0188
Epoch [21/30], Batch [1300/6000], Loss: 0.0144
Epoch [21/30], Batch [1400/6000], Loss: 0.0253
Epoch [21/30], Batch [1500/6000], Loss: 0.0778
Epoch [21/30], Batch [1600/6000], Loss: 0.0174
Epoch [21/30], Batch [1700/6000], Loss: 0.0186
Epoch [21/30], Batch [1800/6000], Loss: 0.0259
Epoch [21/30], Batch [1900/6000], Loss: 0.0209
Epoch [21/30], Batch [2000/6000], Loss: 0.0246
Epoch [21/30], Batch [2100/6000], Loss: 0.0193
Epoch [21/30], Batch [2200/6000], Loss: 0.0205
Epoch [21/30], Batch [2300/6000], Loss: 0.2522
Epoch [21/30], Batch [2400/6000], Loss: 0.0194
Epoch [21/30], Batch [2500/6000], Loss: 0.0328
Epoch [21/30], Batch [2600/6000], Loss: 0.1353
Epoch [21/30], Batch [2700/6000], Loss: 0.0209
Epoch [21/30], Batch [2800/6000], Loss: 0.0436
Epoch [21/30], Batch [2900/6000], Loss: 0.0132
Epoch [21/30], Batch [3000/6000], Loss: 0.0191
Epoch [21/30], Batch [3100/6000], Loss: 0.0195
Epoch [21/30], Batch [3200/6000], Loss: 0.0244
Epoch [21/30], Batch [3300/6000], Loss: 0.0234
Epoch [21/30], Batch [3400/6000], Loss: 0.0463
Epoch [21/30], Batch [3500/6000], Loss: 0.0195
Epoch [21/30], Batch [3600/6000], Loss: 0.0166
Epoch [21/30], Batch [3700/6000], Loss: 0.3380
Epoch [21/30], Batch [3800/6000], Loss: 0.0246
Epoch [21/30], Batch [3900/6000], Loss: 0.0231
Epoch [21/30], Batch [4000/6000], Loss: 0.0220
Epoch [21/30], Batch [4100/6000], Loss: 0.0457
Epoch [21/30], Batch [4200/6000], Loss: 0.0148
Epoch [21/30], Batch [4300/6000], Loss: 0.0232
Epoch [21/30], Batch [4400/6000], Loss: 0.0806
Epoch [21/30], Batch [4500/6000], Loss: 0.0144
Epoch [21/30], Batch [4600/6000], Loss: 0.0195
Epoch [21/30], Batch [4700/6000], Loss: 0.1271
Epoch [21/30], Batch [4800/6000], Loss: 0.0139
Epoch [21/30], Batch [4900/6000], Loss: 0.0220
Epoch [21/30], Batch [5000/6000], Loss: 0.0191
Epoch [21/30], Batch [5100/6000], Loss: 0.0210
Epoch [21/30], Batch [5200/6000], Loss: 0.0174
Epoch [21/30], Batch [5300/6000], Loss: 0.0267
Epoch [21/30], Batch [5400/6000], Loss: 0.0241
Epoch [21/30], Batch [5500/6000], Loss: 0.0256
Epoch [21/30], Batch [5600/6000], Loss: 0.0227
Epoch [21/30], Batch [5700/6000], Loss: 0.0190
Epoch [21/30], Batch [5800/6000], Loss: 0.0819
Epoch [21/30], Batch [5900/6000], Loss: 0.0192
Epoch [21/30], Loss: 0.0338
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.0153
Epoch [22/30], Batch [100/6000], Loss: 0.0197
Epoch [22/30], Batch [200/6000], Loss: 0.0242
Epoch [22/30], Batch [300/6000], Loss: 0.0672
Epoch [22/30], Batch [400/6000], Loss: 0.0225
Epoch [22/30], Batch [500/6000], Loss: 0.0181
Epoch [22/30], Batch [600/6000], Loss: 0.0193
Epoch [22/30], Batch [700/6000], Loss: 0.0186
Epoch [22/30], Batch [800/6000], Loss: 0.0148
Epoch [22/30], Batch [900/6000], Loss: 0.0692
Epoch [22/30], Batch [1000/6000], Loss: 0.0168
Epoch [22/30], Batch [1100/6000], Loss: 0.0195
Epoch [22/30], Batch [1200/6000], Loss: 0.0198
Epoch [22/30], Batch [1300/6000], Loss: 0.0209
Epoch [22/30], Batch [1400/6000], Loss: 0.0365
Epoch [22/30], Batch [1500/6000], Loss: 0.0255
Epoch [22/30], Batch [1600/6000], Loss: 0.0157
Epoch [22/30], Batch [1700/6000], Loss: 0.0228
Epoch [22/30], Batch [1800/6000], Loss: 0.0155
Epoch [22/30], Batch [1900/6000], Loss: 0.0214
Epoch [22/30], Batch [2000/6000], Loss: 0.0203
Epoch [22/30], Batch [2100/6000], Loss: 0.0242
Epoch [22/30], Batch [2200/6000], Loss: 0.0215
Epoch [22/30], Batch [2300/6000], Loss: 0.0224
Epoch [22/30], Batch [2400/6000], Loss: 0.0212
Epoch [22/30], Batch [2500/6000], Loss: 0.0158
Epoch [22/30], Batch [2600/6000], Loss: 0.0533
Epoch [22/30], Batch [2700/6000], Loss: 0.0170
Epoch [22/30], Batch [2800/6000], Loss: 0.0216
Epoch [22/30], Batch [2900/6000], Loss: 0.0159
Epoch [22/30], Batch [3000/6000], Loss: 0.0254
Epoch [22/30], Batch [3100/6000], Loss: 0.0197
Epoch [22/30], Batch [3200/6000], Loss: 0.0214
Epoch [22/30], Batch [3300/6000], Loss: 0.0208
Epoch [22/30], Batch [3400/6000], Loss: 0.1001
Epoch [22/30], Batch [3500/6000], Loss: 0.0166
Epoch [22/30], Batch [3600/6000], Loss: 0.4271
Epoch [22/30], Batch [3700/6000], Loss: 0.0213
Epoch [22/30], Batch [3800/6000], Loss: 0.0210
Epoch [22/30], Batch [3900/6000], Loss: 0.0218
Epoch [22/30], Batch [4000/6000], Loss: 0.0152
Epoch [22/30], Batch [4100/6000], Loss: 0.0185
Epoch [22/30], Batch [4200/6000], Loss: 0.0241
Epoch [22/30], Batch [4300/6000], Loss: 0.0422
Epoch [22/30], Batch [4400/6000], Loss: 0.0201
Epoch [22/30], Batch [4500/6000], Loss: 0.0183
Epoch [22/30], Batch [4600/6000], Loss: 0.0185
Epoch [22/30], Batch [4700/6000], Loss: 0.0277
Epoch [22/30], Batch [4800/6000], Loss: 0.0177
Epoch [22/30], Batch [4900/6000], Loss: 0.0167
Epoch [22/30], Batch [5000/6000], Loss: 0.0187
Epoch [22/30], Batch [5100/6000], Loss: 0.0209
Epoch [22/30], Batch [5200/6000], Loss: 0.0152
Epoch [22/30], Batch [5300/6000], Loss: 0.0228
Epoch [22/30], Batch [5400/6000], Loss: 0.0196
Epoch [22/30], Batch [5500/6000], Loss: 0.0150
Epoch [22/30], Batch [5600/6000], Loss: 0.0156
Epoch [22/30], Batch [5700/6000], Loss: 0.0215
Epoch [22/30], Batch [5800/6000], Loss: 0.0224
Epoch [22/30], Batch [5900/6000], Loss: 0.0164
Epoch [22/30], Loss: 0.0323
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.0163
Epoch [23/30], Batch [100/6000], Loss: 0.0209
Epoch [23/30], Batch [200/6000], Loss: 0.0154
Epoch [23/30], Batch [300/6000], Loss: 0.0170
Epoch [23/30], Batch [400/6000], Loss: 0.0158
Epoch [23/30], Batch [500/6000], Loss: 0.0205
Epoch [23/30], Batch [600/6000], Loss: 0.0270
Epoch [23/30], Batch [700/6000], Loss: 0.0202
Epoch [23/30], Batch [800/6000], Loss: 0.0176
Epoch [23/30], Batch [900/6000], Loss: 0.0541
Epoch [23/30], Batch [1000/6000], Loss: 0.0141
Epoch [23/30], Batch [1100/6000], Loss: 0.0220
Epoch [23/30], Batch [1200/6000], Loss: 0.0166
Epoch [23/30], Batch [1300/6000], Loss: 0.0243
Epoch [23/30], Batch [1400/6000], Loss: 0.0850
Epoch [23/30], Batch [1500/6000], Loss: 0.0455
Epoch [23/30], Batch [1600/6000], Loss: 0.0176
Epoch [23/30], Batch [1700/6000], Loss: 0.0181
Epoch [23/30], Batch [1800/6000], Loss: 0.0198
Epoch [23/30], Batch [1900/6000], Loss: 0.0167
Epoch [23/30], Batch [2000/6000], Loss: 0.0163
Epoch [23/30], Batch [2100/6000], Loss: 0.0225
Epoch [23/30], Batch [2200/6000], Loss: 0.0212
Epoch [23/30], Batch [2300/6000], Loss: 0.0247
Epoch [23/30], Batch [2400/6000], Loss: 0.0210
Epoch [23/30], Batch [2500/6000], Loss: 0.0162
Epoch [23/30], Batch [2600/6000], Loss: 0.0264
Epoch [23/30], Batch [2700/6000], Loss: 0.0183
Epoch [23/30], Batch [2800/6000], Loss: 0.0263
Epoch [23/30], Batch [2900/6000], Loss: 0.0172
Epoch [23/30], Batch [3000/6000], Loss: 0.0212
Epoch [23/30], Batch [3100/6000], Loss: 0.0201
Epoch [23/30], Batch [3200/6000], Loss: 0.2968
Epoch [23/30], Batch [3300/6000], Loss: 0.0201
Epoch [23/30], Batch [3400/6000], Loss: 0.0223
Epoch [23/30], Batch [3500/6000], Loss: 0.0164
Epoch [23/30], Batch [3600/6000], Loss: 0.0247
Epoch [23/30], Batch [3700/6000], Loss: 0.0192
Epoch [23/30], Batch [3800/6000], Loss: 0.0165
Epoch [23/30], Batch [3900/6000], Loss: 0.0205
Epoch [23/30], Batch [4000/6000], Loss: 0.1170
Epoch [23/30], Batch [4100/6000], Loss: 0.0221
Epoch [23/30], Batch [4200/6000], Loss: 0.0198
Epoch [23/30], Batch [4300/6000], Loss: 0.0224
Epoch [23/30], Batch [4400/6000], Loss: 0.0235
Epoch [23/30], Batch [4500/6000], Loss: 0.0185
Epoch [23/30], Batch [4600/6000], Loss: 0.0151
Epoch [23/30], Batch [4700/6000], Loss: 0.0301
Epoch [23/30], Batch [4800/6000], Loss: 0.0185
Epoch [23/30], Batch [4900/6000], Loss: 0.0189
Epoch [23/30], Batch [5000/6000], Loss: 0.0224
Epoch [23/30], Batch [5100/6000], Loss: 0.3945
Epoch [23/30], Batch [5200/6000], Loss: 0.0198
Epoch [23/30], Batch [5300/6000], Loss: 0.0215
Epoch [23/30], Batch [5400/6000], Loss: 0.0225
Epoch [23/30], Batch [5500/6000], Loss: 0.0384
Epoch [23/30], Batch [5600/6000], Loss: 0.0176
Epoch [23/30], Batch [5700/6000], Loss: 0.0179
Epoch [23/30], Batch [5800/6000], Loss: 0.0278
Epoch [23/30], Batch [5900/6000], Loss: 0.0186
Epoch [23/30], Loss: 0.0315
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.0193
Epoch [24/30], Batch [100/6000], Loss: 0.0138
Epoch [24/30], Batch [200/6000], Loss: 0.0189
Epoch [24/30], Batch [300/6000], Loss: 0.0159
Epoch [24/30], Batch [400/6000], Loss: 0.0231
Epoch [24/30], Batch [500/6000], Loss: 0.0174
Epoch [24/30], Batch [600/6000], Loss: 0.0205
Epoch [24/30], Batch [700/6000], Loss: 0.0161
Epoch [24/30], Batch [800/6000], Loss: 0.0184
Epoch [24/30], Batch [900/6000], Loss: 0.0202
Epoch [24/30], Batch [1000/6000], Loss: 0.0175
Epoch [24/30], Batch [1100/6000], Loss: 0.0199
Epoch [24/30], Batch [1200/6000], Loss: 0.0184
Epoch [24/30], Batch [1300/6000], Loss: 0.0185
Epoch [24/30], Batch [1400/6000], Loss: 0.0212
Epoch [24/30], Batch [1500/6000], Loss: 0.0215
Epoch [24/30], Batch [1600/6000], Loss: 0.0230
Epoch [24/30], Batch [1700/6000], Loss: 0.0168
Epoch [24/30], Batch [1800/6000], Loss: 0.0364
Epoch [24/30], Batch [1900/6000], Loss: 0.0219
Epoch [24/30], Batch [2000/6000], Loss: 0.0219
Epoch [24/30], Batch [2100/6000], Loss: 0.1661
Epoch [24/30], Batch [2200/6000], Loss: 0.0191
Epoch [24/30], Batch [2300/6000], Loss: 0.0246
Epoch [24/30], Batch [2400/6000], Loss: 0.0187
Epoch [24/30], Batch [2500/6000], Loss: 0.0188
Epoch [24/30], Batch [2600/6000], Loss: 0.0141
Epoch [24/30], Batch [2700/6000], Loss: 0.0212
Epoch [24/30], Batch [2800/6000], Loss: 0.0207
Epoch [24/30], Batch [2900/6000], Loss: 0.0193
Epoch [24/30], Batch [3000/6000], Loss: 0.0223
Epoch [24/30], Batch [3100/6000], Loss: 0.2614
Epoch [24/30], Batch [3200/6000], Loss: 0.0517
Epoch [24/30], Batch [3300/6000], Loss: 0.0184
Epoch [24/30], Batch [3400/6000], Loss: 0.0158
Epoch [24/30], Batch [3500/6000], Loss: 0.0217
Epoch [24/30], Batch [3600/6000], Loss: 0.0274
Epoch [24/30], Batch [3700/6000], Loss: 0.0315
Epoch [24/30], Batch [3800/6000], Loss: 0.0202
Epoch [24/30], Batch [3900/6000], Loss: 0.0199
Epoch [24/30], Batch [4000/6000], Loss: 0.0177
Epoch [24/30], Batch [4100/6000], Loss: 0.0181
Epoch [24/30], Batch [4200/6000], Loss: 0.0449
Epoch [24/30], Batch [4300/6000], Loss: 0.0265
Epoch [24/30], Batch [4400/6000], Loss: 0.0301
Epoch [24/30], Batch [4500/6000], Loss: 0.0819
Epoch [24/30], Batch [4600/6000], Loss: 0.0205
Epoch [24/30], Batch [4700/6000], Loss: 0.0206
Epoch [24/30], Batch [4800/6000], Loss: 0.0232
Epoch [24/30], Batch [4900/6000], Loss: 0.0245
Epoch [24/30], Batch [5000/6000], Loss: 0.0891
Epoch [24/30], Batch [5100/6000], Loss: 0.0283
Epoch [24/30], Batch [5200/6000], Loss: 0.0220
Epoch [24/30], Batch [5300/6000], Loss: 0.0173
Epoch [24/30], Batch [5400/6000], Loss: 0.0513
Epoch [24/30], Batch [5500/6000], Loss: 0.0190
Epoch [24/30], Batch [5600/6000], Loss: 0.0223
Epoch [24/30], Batch [5700/6000], Loss: 0.0183
Epoch [24/30], Batch [5800/6000], Loss: 0.0175
Epoch [24/30], Batch [5900/6000], Loss: 0.0235
Epoch [24/30], Loss: 0.0311
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.0212
Epoch [25/30], Batch [100/6000], Loss: 0.0247
Epoch [25/30], Batch [200/6000], Loss: 0.0215
Epoch [25/30], Batch [300/6000], Loss: 0.0207
Epoch [25/30], Batch [400/6000], Loss: 0.0213
Epoch [25/30], Batch [500/6000], Loss: 0.0191
Epoch [25/30], Batch [600/6000], Loss: 0.0189
Epoch [25/30], Batch [700/6000], Loss: 0.0200
Epoch [25/30], Batch [800/6000], Loss: 0.0121
Epoch [25/30], Batch [900/6000], Loss: 0.0162
Epoch [25/30], Batch [1000/6000], Loss: 0.0172
Epoch [25/30], Batch [1100/6000], Loss: 0.0192
Epoch [25/30], Batch [1200/6000], Loss: 0.0154
Epoch [25/30], Batch [1300/6000], Loss: 0.0197
Epoch [25/30], Batch [1400/6000], Loss: 0.0211
Epoch [25/30], Batch [1500/6000], Loss: 0.0253
Epoch [25/30], Batch [1600/6000], Loss: 0.0176
Epoch [25/30], Batch [1700/6000], Loss: 0.0213
Epoch [25/30], Batch [1800/6000], Loss: 0.0187
Epoch [25/30], Batch [1900/6000], Loss: 0.0199
Epoch [25/30], Batch [2000/6000], Loss: 0.0214
Epoch [25/30], Batch [2100/6000], Loss: 0.0138
Epoch [25/30], Batch [2200/6000], Loss: 0.0168
Epoch [25/30], Batch [2300/6000], Loss: 0.0180
Epoch [25/30], Batch [2400/6000], Loss: 0.0155
Epoch [25/30], Batch [2500/6000], Loss: 0.0229
Epoch [25/30], Batch [2600/6000], Loss: 0.0181
Epoch [25/30], Batch [2700/6000], Loss: 0.0265
Epoch [25/30], Batch [2800/6000], Loss: 0.0173
Epoch [25/30], Batch [2900/6000], Loss: 0.0171
Epoch [25/30], Batch [3000/6000], Loss: 0.0358
Epoch [25/30], Batch [3100/6000], Loss: 0.0204
Epoch [25/30], Batch [3200/6000], Loss: 0.0210
Epoch [25/30], Batch [3300/6000], Loss: 0.0184
Epoch [25/30], Batch [3400/6000], Loss: 0.0176
Epoch [25/30], Batch [3500/6000], Loss: 0.0242
Epoch [25/30], Batch [3600/6000], Loss: 0.0188
Epoch [25/30], Batch [3700/6000], Loss: 0.1797
Epoch [25/30], Batch [3800/6000], Loss: 0.0186
Epoch [25/30], Batch [3900/6000], Loss: 0.0234
Epoch [25/30], Batch [4000/6000], Loss: 0.0184
Epoch [25/30], Batch [4100/6000], Loss: 0.0215
Epoch [25/30], Batch [4200/6000], Loss: 0.0201
Epoch [25/30], Batch [4300/6000], Loss: 0.0202
Epoch [25/30], Batch [4400/6000], Loss: 0.0161
Epoch [25/30], Batch [4500/6000], Loss: 0.0191
Epoch [25/30], Batch [4600/6000], Loss: 0.0179
Epoch [25/30], Batch [4700/6000], Loss: 0.0785
Epoch [25/30], Batch [4800/6000], Loss: 0.0208
Epoch [25/30], Batch [4900/6000], Loss: 0.0196
Epoch [25/30], Batch [5000/6000], Loss: 0.0186
Epoch [25/30], Batch [5100/6000], Loss: 0.0211
Epoch [25/30], Batch [5200/6000], Loss: 0.0172
Epoch [25/30], Batch [5300/6000], Loss: 0.0186
Epoch [25/30], Batch [5400/6000], Loss: 0.0243
Epoch [25/30], Batch [5500/6000], Loss: 0.0176
Epoch [25/30], Batch [5600/6000], Loss: 0.0229
Epoch [25/30], Batch [5700/6000], Loss: 0.0153
Epoch [25/30], Batch [5800/6000], Loss: 0.0160
Epoch [25/30], Batch [5900/6000], Loss: 0.0288
Epoch [25/30], Loss: 0.0289
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.0567
Epoch [26/30], Batch [100/6000], Loss: 0.0164
Epoch [26/30], Batch [200/6000], Loss: 0.0191
Epoch [26/30], Batch [300/6000], Loss: 0.0296
Epoch [26/30], Batch [400/6000], Loss: 0.0171
Epoch [26/30], Batch [500/6000], Loss: 0.0219
Epoch [26/30], Batch [600/6000], Loss: 0.0209
Epoch [26/30], Batch [700/6000], Loss: 0.0155
Epoch [26/30], Batch [800/6000], Loss: 0.0150
Epoch [26/30], Batch [900/6000], Loss: 0.0165
Epoch [26/30], Batch [1000/6000], Loss: 0.0145
Epoch [26/30], Batch [1100/6000], Loss: 0.0209
Epoch [26/30], Batch [1200/6000], Loss: 0.0290
Epoch [26/30], Batch [1300/6000], Loss: 0.0261
Epoch [26/30], Batch [1400/6000], Loss: 0.0161
Epoch [26/30], Batch [1500/6000], Loss: 0.0163
Epoch [26/30], Batch [1600/6000], Loss: 0.0174
Epoch [26/30], Batch [1700/6000], Loss: 0.0174
Epoch [26/30], Batch [1800/6000], Loss: 0.0718
Epoch [26/30], Batch [1900/6000], Loss: 0.0159
Epoch [26/30], Batch [2000/6000], Loss: 0.0495
Epoch [26/30], Batch [2100/6000], Loss: 0.0170
Epoch [26/30], Batch [2200/6000], Loss: 0.0180
Epoch [26/30], Batch [2300/6000], Loss: 0.0218
Epoch [26/30], Batch [2400/6000], Loss: 0.0189
Epoch [26/30], Batch [2500/6000], Loss: 0.1015
Epoch [26/30], Batch [2600/6000], Loss: 0.0162
Epoch [26/30], Batch [2700/6000], Loss: 0.0171
Epoch [26/30], Batch [2800/6000], Loss: 0.0224
Epoch [26/30], Batch [2900/6000], Loss: 0.0204
Epoch [26/30], Batch [3000/6000], Loss: 0.0156
Epoch [26/30], Batch [3100/6000], Loss: 0.0206
Epoch [26/30], Batch [3200/6000], Loss: 0.0164
Epoch [26/30], Batch [3300/6000], Loss: 0.0163
Epoch [26/30], Batch [3400/6000], Loss: 0.0179
Epoch [26/30], Batch [3500/6000], Loss: 0.0162
Epoch [26/30], Batch [3600/6000], Loss: 0.0383
Epoch [26/30], Batch [3700/6000], Loss: 0.0212
Epoch [26/30], Batch [3800/6000], Loss: 0.0205
Epoch [26/30], Batch [3900/6000], Loss: 0.0204
Epoch [26/30], Batch [4000/6000], Loss: 0.0189
Epoch [26/30], Batch [4100/6000], Loss: 0.0347
Epoch [26/30], Batch [4200/6000], Loss: 0.0212
Epoch [26/30], Batch [4300/6000], Loss: 0.0216
Epoch [26/30], Batch [4400/6000], Loss: 0.0245
Epoch [26/30], Batch [4500/6000], Loss: 0.0171
Epoch [26/30], Batch [4600/6000], Loss: 0.0189
Epoch [26/30], Batch [4700/6000], Loss: 0.0295
Epoch [26/30], Batch [4800/6000], Loss: 0.0194
Epoch [26/30], Batch [4900/6000], Loss: 0.0157
Epoch [26/30], Batch [5000/6000], Loss: 0.0169
Epoch [26/30], Batch [5100/6000], Loss: 0.0166
Epoch [26/30], Batch [5200/6000], Loss: 0.0170
Epoch [26/30], Batch [5300/6000], Loss: 0.0160
Epoch [26/30], Batch [5400/6000], Loss: 0.1014
Epoch [26/30], Batch [5500/6000], Loss: 0.0164
Epoch [26/30], Batch [5600/6000], Loss: 0.0176
Epoch [26/30], Batch [5700/6000], Loss: 0.0238
Epoch [26/30], Batch [5800/6000], Loss: 0.0220
Epoch [26/30], Batch [5900/6000], Loss: 0.0157
Epoch [26/30], Loss: 0.0286
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0197
Epoch [27/30], Batch [100/6000], Loss: 0.0208
Epoch [27/30], Batch [200/6000], Loss: 0.0201
Epoch [27/30], Batch [300/6000], Loss: 0.0206
Epoch [27/30], Batch [400/6000], Loss: 0.0190
Epoch [27/30], Batch [500/6000], Loss: 0.0226
Epoch [27/30], Batch [600/6000], Loss: 0.0162
Epoch [27/30], Batch [700/6000], Loss: 0.0200
Epoch [27/30], Batch [800/6000], Loss: 0.0143
Epoch [27/30], Batch [900/6000], Loss: 0.0221
Epoch [27/30], Batch [1000/6000], Loss: 0.0185
Epoch [27/30], Batch [1100/6000], Loss: 0.0188
Epoch [27/30], Batch [1200/6000], Loss: 0.0171
Epoch [27/30], Batch [1300/6000], Loss: 0.0186
Epoch [27/30], Batch [1400/6000], Loss: 0.0830
Epoch [27/30], Batch [1500/6000], Loss: 0.0376
Epoch [27/30], Batch [1600/6000], Loss: 0.0136
Epoch [27/30], Batch [1700/6000], Loss: 0.0354
Epoch [27/30], Batch [1800/6000], Loss: 0.0199
Epoch [27/30], Batch [1900/6000], Loss: 0.0143
Epoch [27/30], Batch [2000/6000], Loss: 0.0193
Epoch [27/30], Batch [2100/6000], Loss: 0.0182
Epoch [27/30], Batch [2200/6000], Loss: 0.0182
Epoch [27/30], Batch [2300/6000], Loss: 0.0182
Epoch [27/30], Batch [2400/6000], Loss: 0.0197
Epoch [27/30], Batch [2500/6000], Loss: 0.0171
Epoch [27/30], Batch [2600/6000], Loss: 0.0192
Epoch [27/30], Batch [2700/6000], Loss: 0.0205
Epoch [27/30], Batch [2800/6000], Loss: 0.0163
Epoch [27/30], Batch [2900/6000], Loss: 0.0170
Epoch [27/30], Batch [3000/6000], Loss: 0.0507
Epoch [27/30], Batch [3100/6000], Loss: 0.0149
Epoch [27/30], Batch [3200/6000], Loss: 0.0219
Epoch [27/30], Batch [3300/6000], Loss: 0.0164
Epoch [27/30], Batch [3400/6000], Loss: 0.0181
Epoch [27/30], Batch [3500/6000], Loss: 0.0200
Epoch [27/30], Batch [3600/6000], Loss: 0.0247
Epoch [27/30], Batch [3700/6000], Loss: 0.0152
Epoch [27/30], Batch [3800/6000], Loss: 0.0180
Epoch [27/30], Batch [3900/6000], Loss: 0.0211
Epoch [27/30], Batch [4000/6000], Loss: 0.2316
Epoch [27/30], Batch [4100/6000], Loss: 0.0172
Epoch [27/30], Batch [4200/6000], Loss: 0.0165
Epoch [27/30], Batch [4300/6000], Loss: 0.0189
Epoch [27/30], Batch [4400/6000], Loss: 0.0209
Epoch [27/30], Batch [4500/6000], Loss: 0.0181
Epoch [27/30], Batch [4600/6000], Loss: 0.0182
Epoch [27/30], Batch [4700/6000], Loss: 0.0208
Epoch [27/30], Batch [4800/6000], Loss: 0.0208
Epoch [27/30], Batch [4900/6000], Loss: 0.3522
Epoch [27/30], Batch [5000/6000], Loss: 0.0184
Epoch [27/30], Batch [5100/6000], Loss: 0.1223
Epoch [27/30], Batch [5200/6000], Loss: 0.0149
Epoch [27/30], Batch [5300/6000], Loss: 0.0224
Epoch [27/30], Batch [5400/6000], Loss: 0.0206
Epoch [27/30], Batch [5500/6000], Loss: 0.0143
Epoch [27/30], Batch [5600/6000], Loss: 0.0141
Epoch [27/30], Batch [5700/6000], Loss: 0.0196
Epoch [27/30], Batch [5800/6000], Loss: 0.0194
Epoch [27/30], Batch [5900/6000], Loss: 0.0158
Epoch [27/30], Loss: 0.0285
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.0311
Epoch [28/30], Batch [100/6000], Loss: 0.0125
Epoch [28/30], Batch [200/6000], Loss: 0.0237
Epoch [28/30], Batch [300/6000], Loss: 0.0148
Epoch [28/30], Batch [400/6000], Loss: 0.0181
Epoch [28/30], Batch [500/6000], Loss: 0.0128
Epoch [28/30], Batch [600/6000], Loss: 0.0209
Epoch [28/30], Batch [700/6000], Loss: 0.0190
Epoch [28/30], Batch [800/6000], Loss: 0.0183
Epoch [28/30], Batch [900/6000], Loss: 0.0148
Epoch [28/30], Batch [1000/6000], Loss: 0.0173
Epoch [28/30], Batch [1100/6000], Loss: 0.0228
Epoch [28/30], Batch [1200/6000], Loss: 0.0174
Epoch [28/30], Batch [1300/6000], Loss: 0.0129
Epoch [28/30], Batch [1400/6000], Loss: 0.0123
Epoch [28/30], Batch [1500/6000], Loss: 0.0196
Epoch [28/30], Batch [1600/6000], Loss: 0.0167
Epoch [28/30], Batch [1700/6000], Loss: 0.0166
Epoch [28/30], Batch [1800/6000], Loss: 0.0201
Epoch [28/30], Batch [1900/6000], Loss: 0.0206
Epoch [28/30], Batch [2000/6000], Loss: 0.0199
Epoch [28/30], Batch [2100/6000], Loss: 0.1586
Epoch [28/30], Batch [2200/6000], Loss: 0.0199
Epoch [28/30], Batch [2300/6000], Loss: 0.0129
Epoch [28/30], Batch [2400/6000], Loss: 0.0170
Epoch [28/30], Batch [2500/6000], Loss: 0.0185
Epoch [28/30], Batch [2600/6000], Loss: 0.0162
Epoch [28/30], Batch [2700/6000], Loss: 0.0197
Epoch [28/30], Batch [2800/6000], Loss: 0.0188
Epoch [28/30], Batch [2900/6000], Loss: 0.0370
Epoch [28/30], Batch [3000/6000], Loss: 0.0194
Epoch [28/30], Batch [3100/6000], Loss: 0.0177
Epoch [28/30], Batch [3200/6000], Loss: 0.0157
Epoch [28/30], Batch [3300/6000], Loss: 0.0395
Epoch [28/30], Batch [3400/6000], Loss: 0.0227
Epoch [28/30], Batch [3500/6000], Loss: 0.0156
Epoch [28/30], Batch [3600/6000], Loss: 0.0222
Epoch [28/30], Batch [3700/6000], Loss: 0.0186
Epoch [28/30], Batch [3800/6000], Loss: 0.0267
Epoch [28/30], Batch [3900/6000], Loss: 0.0172
Epoch [28/30], Batch [4000/6000], Loss: 0.0151
Epoch [28/30], Batch [4100/6000], Loss: 0.0169
Epoch [28/30], Batch [4200/6000], Loss: 0.0156
Epoch [28/30], Batch [4300/6000], Loss: 0.0463
Epoch [28/30], Batch [4400/6000], Loss: 0.0189
Epoch [28/30], Batch [4500/6000], Loss: 0.0224
Epoch [28/30], Batch [4600/6000], Loss: 0.0160
Epoch [28/30], Batch [4700/6000], Loss: 0.0142
Epoch [28/30], Batch [4800/6000], Loss: 0.0215
Epoch [28/30], Batch [4900/6000], Loss: 0.0202
Epoch [28/30], Batch [5000/6000], Loss: 0.0180
Epoch [28/30], Batch [5100/6000], Loss: 0.0160
Epoch [28/30], Batch [5200/6000], Loss: 0.0254
Epoch [28/30], Batch [5300/6000], Loss: 0.0198
Epoch [28/30], Batch [5400/6000], Loss: 0.0193
Epoch [28/30], Batch [5500/6000], Loss: 0.0164
Epoch [28/30], Batch [5600/6000], Loss: 0.0177
Epoch [28/30], Batch [5700/6000], Loss: 0.0801
Epoch [28/30], Batch [5800/6000], Loss: 0.0169
Epoch [28/30], Batch [5900/6000], Loss: 0.0221
Epoch [28/30], Loss: 0.0281
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0144
Epoch [29/30], Batch [100/6000], Loss: 0.0186
Epoch [29/30], Batch [200/6000], Loss: 0.0145
Epoch [29/30], Batch [300/6000], Loss: 0.0145
Epoch [29/30], Batch [400/6000], Loss: 0.0181
Epoch [29/30], Batch [500/6000], Loss: 0.0178
Epoch [29/30], Batch [600/6000], Loss: 0.0283
Epoch [29/30], Batch [700/6000], Loss: 0.0201
Epoch [29/30], Batch [800/6000], Loss: 0.0198
Epoch [29/30], Batch [900/6000], Loss: 0.0186
Epoch [29/30], Batch [1000/6000], Loss: 0.0203
Epoch [29/30], Batch [1100/6000], Loss: 0.0179
Epoch [29/30], Batch [1200/6000], Loss: 0.0196
Epoch [29/30], Batch [1300/6000], Loss: 0.0206
Epoch [29/30], Batch [1400/6000], Loss: 0.0142
Epoch [29/30], Batch [1500/6000], Loss: 0.0173
Epoch [29/30], Batch [1600/6000], Loss: 0.0152
Epoch [29/30], Batch [1700/6000], Loss: 0.0312
Epoch [29/30], Batch [1800/6000], Loss: 0.0177
Epoch [29/30], Batch [1900/6000], Loss: 0.0232
Epoch [29/30], Batch [2000/6000], Loss: 0.0162
Epoch [29/30], Batch [2100/6000], Loss: 0.0208
Epoch [29/30], Batch [2200/6000], Loss: 0.0346
Epoch [29/30], Batch [2300/6000], Loss: 0.0438
Epoch [29/30], Batch [2400/6000], Loss: 0.0174
Epoch [29/30], Batch [2500/6000], Loss: 0.0209
Epoch [29/30], Batch [2600/6000], Loss: 0.0165
Epoch [29/30], Batch [2700/6000], Loss: 0.0187
Epoch [29/30], Batch [2800/6000], Loss: 0.0613
Epoch [29/30], Batch [2900/6000], Loss: 0.0200
Epoch [29/30], Batch [3000/6000], Loss: 0.0145
Epoch [29/30], Batch [3100/6000], Loss: 0.0300
Epoch [29/30], Batch [3200/6000], Loss: 0.0172
Epoch [29/30], Batch [3300/6000], Loss: 0.0227
Epoch [29/30], Batch [3400/6000], Loss: 0.0185
Epoch [29/30], Batch [3500/6000], Loss: 0.0144
Epoch [29/30], Batch [3600/6000], Loss: 0.0273
Epoch [29/30], Batch [3700/6000], Loss: 0.0155
Epoch [29/30], Batch [3800/6000], Loss: 0.0212
Epoch [29/30], Batch [3900/6000], Loss: 0.0163
Epoch [29/30], Batch [4000/6000], Loss: 0.0157
Epoch [29/30], Batch [4100/6000], Loss: 0.0191
Epoch [29/30], Batch [4200/6000], Loss: 0.0171
Epoch [29/30], Batch [4300/6000], Loss: 0.1233
Epoch [29/30], Batch [4400/6000], Loss: 0.0492
Epoch [29/30], Batch [4500/6000], Loss: 0.1411
Epoch [29/30], Batch [4600/6000], Loss: 0.0179
Epoch [29/30], Batch [4700/6000], Loss: 0.0142
Epoch [29/30], Batch [4800/6000], Loss: 0.0177
Epoch [29/30], Batch [4900/6000], Loss: 0.0347
Epoch [29/30], Batch [5000/6000], Loss: 0.0182
Epoch [29/30], Batch [5100/6000], Loss: 0.0196
Epoch [29/30], Batch [5200/6000], Loss: 0.0192
Epoch [29/30], Batch [5300/6000], Loss: 0.0195
Epoch [29/30], Batch [5400/6000], Loss: 0.0154
Epoch [29/30], Batch [5500/6000], Loss: 0.5159
Epoch [29/30], Batch [5600/6000], Loss: 0.0219
Epoch [29/30], Batch [5700/6000], Loss: 0.0179
Epoch [29/30], Batch [5800/6000], Loss: 0.0192
Epoch [29/30], Batch [5900/6000], Loss: 0.0169
Epoch [29/30], Loss: 0.0272
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0242
Epoch [30/30], Batch [100/6000], Loss: 0.0151
Epoch [30/30], Batch [200/6000], Loss: 0.0206
Epoch [30/30], Batch [300/6000], Loss: 0.0143
Epoch [30/30], Batch [400/6000], Loss: 0.0144
Epoch [30/30], Batch [500/6000], Loss: 0.0226
Epoch [30/30], Batch [600/6000], Loss: 0.0165
Epoch [30/30], Batch [700/6000], Loss: 0.0182
Epoch [30/30], Batch [800/6000], Loss: 0.0162
Epoch [30/30], Batch [900/6000], Loss: 0.0122
Epoch [30/30], Batch [1000/6000], Loss: 0.0186
Epoch [30/30], Batch [1100/6000], Loss: 0.0150
Epoch [30/30], Batch [1200/6000], Loss: 0.0184
Epoch [30/30], Batch [1300/6000], Loss: 0.0334
Epoch [30/30], Batch [1400/6000], Loss: 0.0236
Epoch [30/30], Batch [1500/6000], Loss: 0.0265
Epoch [30/30], Batch [1600/6000], Loss: 0.0225
Epoch [30/30], Batch [1700/6000], Loss: 0.0136
Epoch [30/30], Batch [1800/6000], Loss: 0.0199
Epoch [30/30], Batch [1900/6000], Loss: 0.0186
Epoch [30/30], Batch [2000/6000], Loss: 0.0183
Epoch [30/30], Batch [2100/6000], Loss: 0.0181
Epoch [30/30], Batch [2200/6000], Loss: 0.0172
Epoch [30/30], Batch [2300/6000], Loss: 0.0149
Epoch [30/30], Batch [2400/6000], Loss: 0.0207
Epoch [30/30], Batch [2500/6000], Loss: 0.0159
Epoch [30/30], Batch [2600/6000], Loss: 0.0222
Epoch [30/30], Batch [2700/6000], Loss: 0.0249
Epoch [30/30], Batch [2800/6000], Loss: 0.0181
Epoch [30/30], Batch [2900/6000], Loss: 0.2023
Epoch [30/30], Batch [3000/6000], Loss: 0.0176
Epoch [30/30], Batch [3100/6000], Loss: 0.0178
Epoch [30/30], Batch [3200/6000], Loss: 0.0172
Epoch [30/30], Batch [3300/6000], Loss: 0.0187
Epoch [30/30], Batch [3400/6000], Loss: 0.0174
Epoch [30/30], Batch [3500/6000], Loss: 0.0135
Epoch [30/30], Batch [3600/6000], Loss: 0.0175
Epoch [30/30], Batch [3700/6000], Loss: 0.0148
Epoch [30/30], Batch [3800/6000], Loss: 0.0170
Epoch [30/30], Batch [3900/6000], Loss: 0.0140
Epoch [30/30], Batch [4000/6000], Loss: 0.0186
Epoch [30/30], Batch [4100/6000], Loss: 0.0219
Epoch [30/30], Batch [4200/6000], Loss: 0.0177
Epoch [30/30], Batch [4300/6000], Loss: 0.0190
Epoch [30/30], Batch [4400/6000], Loss: 0.0124
Epoch [30/30], Batch [4500/6000], Loss: 0.0196
Epoch [30/30], Batch [4600/6000], Loss: 0.2483
Epoch [30/30], Batch [4700/6000], Loss: 0.0135
Epoch [30/30], Batch [4800/6000], Loss: 0.0163
Epoch [30/30], Batch [4900/6000], Loss: 0.1231
Epoch [30/30], Batch [5000/6000], Loss: 0.0153
Epoch [30/30], Batch [5100/6000], Loss: 0.0190
Epoch [30/30], Batch [5200/6000], Loss: 0.0148
Epoch [30/30], Batch [5300/6000], Loss: 0.0219
Epoch [30/30], Batch [5400/6000], Loss: 0.0182
Epoch [30/30], Batch [5500/6000], Loss: 0.0175
Epoch [30/30], Batch [5600/6000], Loss: 0.0195
Epoch [30/30], Batch [5700/6000], Loss: 0.0181
Epoch [30/30], Batch [5800/6000], Loss: 0.0187
Epoch [30/30], Batch [5900/6000], Loss: 0.0188
Epoch [30/30], Loss: 0.0253
Visualization saved to figures/visualization_0.png
Test Loss: 0.1780, Accuracy: 97.21%
  Output probs: [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 0.7008
  Image Loss: 0.0119
  Total Loss: 70.0902
  Image grad max: 17.084123611450195
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]
Adversarial Training Loop 2/300:
  Label Loss: 1.0556
  Image Loss: 0.0167
  Total Loss: 105.5755
  Image grad max: 8.627446174621582
  Output probs: [[0.    0.    0.008 0.    0.    0.    0.    0.    0.992 0.   ]]
Adversarial Training Loop 3/300:
  Label Loss: 0.6257
  Image Loss: 0.0229
  Total Loss: 62.5927
  Image grad max: 13.765422821044922
  Output probs: [[0.013 0.    0.94  0.039 0.    0.006 0.001 0.    0.    0.   ]]
Adversarial Training Loop 4/300:
  Label Loss: 0.9064
  Image Loss: 0.0313
  Total Loss: 90.6716
  Image grad max: 40.939693450927734
  Output probs: [[0.997 0.    0.    0.    0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 5/300:
  Label Loss: 1.1476
  Image Loss: 0.0369
  Total Loss: 114.7979
  Image grad max: 16.610546112060547
  Output probs: [[0.611 0.    0.    0.    0.    0.014 0.375 0.    0.    0.   ]]
Adversarial Training Loop 6/300:
  Label Loss: 0.5929
  Image Loss: 0.0416
  Total Loss: 59.3285
  Image grad max: 16.151214599609375
  Output probs: [[0.    0.    0.    0.    0.    0.833 0.163 0.    0.004 0.   ]]
Adversarial Training Loop 7/300:
  Label Loss: 0.2940
  Image Loss: 0.0467
  Total Loss: 29.4501
  Image grad max: 26.029518127441406
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.027 0.    0.972 0.   ]]
Adversarial Training Loop 8/300:
  Label Loss: 0.1130
  Image Loss: 0.0505
  Total Loss: 11.3462
  Image grad max: 12.541293144226074
  Output probs: [[0.    0.    0.    0.    0.    0.    0.004 0.    0.996 0.   ]]
Adversarial Training Loop 9/300:
  Label Loss: 0.2054
  Image Loss: 0.0557
  Total Loss: 20.5948
  Image grad max: 11.819862365722656
  Output probs: [[0.    0.    0.    0.    0.    0.    0.033 0.    0.967 0.   ]]
Adversarial Training Loop 10/300:
  Label Loss: 0.1026
  Image Loss: 0.0615
  Total Loss: 10.3199
  Image grad max: 10.28068733215332
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.77  0.    0.229 0.   ]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0175
  Image Loss: 0.0677
  Total Loss: 1.8150
  Image grad max: 5.37307071685791
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.958 0.    0.041 0.   ]]
Adversarial Training Loop 12/300:
  Label Loss: 0.0921
  Image Loss: 0.0739
  Total Loss: 9.2873
  Image grad max: 8.40651798248291
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.92  0.    0.078 0.   ]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0623
  Image Loss: 0.0797
  Total Loss: 6.3112
  Image grad max: 7.286369800567627
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.547 0.    0.451 0.   ]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0007
  Image Loss: 0.0851
  Total Loss: 0.1533
  Image grad max: 0.7803550362586975
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.146 0.    0.852 0.   ]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0348
  Image Loss: 0.0902
  Total Loss: 3.5664
  Image grad max: 5.23234748840332
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.102 0.    0.897 0.   ]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0503
  Image Loss: 0.0950
  Total Loss: 5.1226
  Image grad max: 5.641038417816162
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.207 0.    0.79  0.   ]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0212
  Image Loss: 0.0994
  Total Loss: 2.2198
  Image grad max: 4.018972396850586
  Output probs: [[0.    0.    0.    0.    0.    0.005 0.564 0.    0.431 0.   ]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0014
  Image Loss: 0.1035
  Total Loss: 0.2414
  Image grad max: 0.9150246977806091
  Output probs: [[0.    0.    0.    0.    0.    0.005 0.805 0.    0.19  0.   ]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0247
  Image Loss: 0.1073
  Total Loss: 2.5753
  Image grad max: 4.037827491760254
  Output probs: [[0.    0.    0.    0.    0.    0.006 0.816 0.    0.178 0.   ]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0272
  Image Loss: 0.1108
  Total Loss: 2.8348
  Image grad max: 4.100249290466309
  Output probs: [[0.    0.    0.    0.    0.    0.008 0.663 0.    0.329 0.   ]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0068
  Image Loss: 0.1139
  Total Loss: 0.7903
  Image grad max: 2.12963604927063
  Output probs: [[0.    0.    0.    0.    0.    0.008 0.378 0.    0.614 0.   ]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0037
  Image Loss: 0.1168
  Total Loss: 0.4863
  Image grad max: 1.426364779472351
  Output probs: [[0.    0.    0.    0.    0.    0.007 0.235 0.    0.758 0.   ]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0170
  Image Loss: 0.1195
  Total Loss: 1.8181
  Image grad max: 3.1453707218170166
  Output probs: [[0.    0.    0.    0.    0.    0.008 0.247 0.    0.745 0.   ]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0153
  Image Loss: 0.1220
  Total Loss: 1.6532
  Image grad max: 2.9347028732299805
  Output probs: [[0.    0.    0.    0.    0.    0.011 0.386 0.    0.604 0.   ]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0036
  Image Loss: 0.1242
  Total Loss: 0.4813
  Image grad max: 1.2432637214660645
  Output probs: [[0.    0.    0.    0.    0.    0.013 0.594 0.    0.394 0.   ]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0033
  Image Loss: 0.1261
  Total Loss: 0.4608
  Image grad max: 1.200130820274353
  Output probs: [[0.    0.    0.    0.    0.    0.013 0.7   0.    0.288 0.   ]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0109
  Image Loss: 0.1279
  Total Loss: 1.2147
  Image grad max: 2.4144527912139893
  Output probs: [[0.    0.    0.    0.    0.    0.014 0.675 0.    0.311 0.   ]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0087
  Image Loss: 0.1296
  Total Loss: 1.0017
  Image grad max: 2.1199791431427
  Output probs: [[0.    0.    0.    0.    0.    0.015 0.546 0.    0.439 0.   ]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0021
  Image Loss: 0.1312
  Total Loss: 0.3405
  Image grad max: 0.6480515599250793
  Output probs: [[0.    0.    0.    0.    0.    0.015 0.397 0.    0.589 0.   ]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0034
  Image Loss: 0.1326
  Total Loss: 0.4724
  Image grad max: 1.0400209426879883
  Output probs: [[0.    0.    0.    0.    0.    0.014 0.329 0.    0.657 0.   ]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0073
  Image Loss: 0.1339
  Total Loss: 0.8607
  Image grad max: 1.7778229713439941
  Output probs: [[0.    0.    0.    0.    0.    0.015 0.358 0.    0.627 0.   ]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0054
  Image Loss: 0.1351
  Total Loss: 0.6712
  Image grad max: 1.4459108114242554
  Output probs: [[0.    0.    0.    0.    0.    0.016 0.463 0.    0.521 0.   ]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0018
  Image Loss: 0.1361
  Total Loss: 0.3181
  Image grad max: 0.30820128321647644
  Output probs: [[0.    0.    0.    0.    0.    0.017 0.575 0.    0.409 0.   ]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0031
  Image Loss: 0.1370
  Total Loss: 0.4504
  Image grad max: 0.9478561878204346
  Output probs: [[0.    0.    0.    0.    0.    0.017 0.617 0.    0.366 0.   ]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0051
  Image Loss: 0.1378
  Total Loss: 0.6432
  Image grad max: 1.4191524982452393
  Output probs: [[0.    0.    0.    0.    0.    0.017 0.577 0.    0.406 0.   ]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0032
  Image Loss: 0.1386
  Total Loss: 0.4624
  Image grad max: 0.9724367260932922
  Output probs: [[0.    0.    0.    0.    0.    0.017 0.489 0.    0.495 0.   ]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0017
  Image Loss: 0.1393
  Total Loss: 0.3067
  Image grad max: 0.09890501201152802
  Output probs: [[0.    0.    0.    0.    0.    0.016 0.414 0.    0.57  0.   ]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0029
  Image Loss: 0.1399
  Total Loss: 0.4275
  Image grad max: 0.8191167116165161
  Output probs: [[0.    0.    0.    0.    0.    0.015 0.397 0.    0.588 0.   ]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0035
  Image Loss: 0.1405
  Total Loss: 0.4876
  Image grad max: 1.0051212310791016
  Output probs: [[0.    0.    0.    0.    0.    0.016 0.44  0.    0.544 0.   ]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0021
  Image Loss: 0.1409
  Total Loss: 0.3541
  Image grad max: 0.5401520729064941
  Output probs: [[0.    0.    0.    0.    0.    0.015 0.514 0.    0.471 0.   ]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0017
  Image Loss: 0.1414
  Total Loss: 0.3067
  Image grad max: 0.2698817551136017
  Output probs: [[0.    0.    0.    0.    0.    0.015 0.564 0.    0.421 0.   ]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0026
  Image Loss: 0.1417
  Total Loss: 0.3996
  Image grad max: 0.8044877648353577
  Output probs: [[0.    0.    0.    0.    0.    0.015 0.558 0.    0.428 0.   ]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0024
  Image Loss: 0.1420
  Total Loss: 0.3772
  Image grad max: 0.7346463203430176
  Output probs: [[0.    0.    0.    0.    0.    0.014 0.507 0.    0.479 0.   ]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0015
  Image Loss: 0.1423
  Total Loss: 0.2881
  Image grad max: 0.184886634349823
  Output probs: [[0.    0.    0.    0.    0.    0.013 0.453 0.    0.534 0.   ]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0017
  Image Loss: 0.1426
  Total Loss: 0.3106
  Image grad max: 0.41524386405944824
  Output probs: [[0.    0.    0.    0.    0.    0.013 0.434 0.    0.554 0.   ]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0020
  Image Loss: 0.1428
  Total Loss: 0.3456
  Image grad max: 0.6304519176483154
  Output probs: [[0.    0.    0.    0.    0.    0.012 0.458 0.    0.53  0.   ]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0015
  Image Loss: 0.1430
  Total Loss: 0.2936
  Image grad max: 0.3715221881866455
  Output probs: [[0.    0.    0.    0.    0.    0.012 0.507 0.    0.481 0.   ]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0012
  Image Loss: 0.1431
  Total Loss: 0.2669
  Image grad max: 0.17548644542694092
  Output probs: [[0.    0.    0.    0.    0.    0.011 0.54  0.    0.448 0.   ]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0016
  Image Loss: 0.1432
  Total Loss: 0.3015
  Image grad max: 0.5232623219490051
  Output probs: [[0.    0.    0.    0.    0.    0.011 0.533 0.    0.456 0.   ]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0014
  Image Loss: 0.1433
  Total Loss: 0.2830
  Image grad max: 0.4420221447944641
  Output probs: [[0.    0.    0.    0.    0.    0.01  0.496 0.    0.494 0.   ]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0010
  Image Loss: 0.1434
  Total Loss: 0.2471
  Image grad max: 0.053535643965005875
  Output probs: [[0.    0.    0.    0.    0.    0.01  0.463 0.    0.528 0.   ]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0012
  Image Loss: 0.1434
  Total Loss: 0.2628
  Image grad max: 0.3402370810508728
  Output probs: [[0.    0.    0.    0.    0.    0.009 0.458 0.    0.532 0.   ]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0012
  Image Loss: 0.1435
  Total Loss: 0.2641
  Image grad max: 0.3903619647026062
  Output probs: [[0.    0.    0.    0.    0.    0.009 0.485 0.    0.506 0.   ]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0009
  Image Loss: 0.1435
  Total Loss: 0.2346
  Image grad max: 0.11531281471252441
  Output probs: [[0.    0.    0.    0.    0.    0.008 0.518 0.    0.474 0.   ]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0009
  Image Loss: 0.1435
  Total Loss: 0.2381
  Image grad max: 0.2616623044013977
  Output probs: [[0.    0.    0.    0.    0.    0.008 0.528 0.    0.464 0.   ]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0010
  Image Loss: 0.1435
  Total Loss: 0.2443
  Image grad max: 0.36482569575309753
  Output probs: [[0.    0.    0.    0.    0.    0.008 0.508 0.    0.484 0.   ]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0008
  Image Loss: 0.1434
  Total Loss: 0.2227
  Image grad max: 0.1522473692893982
  Output probs: [[0.    0.    0.    0.    0.    0.007 0.481 0.    0.512 0.   ]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0008
  Image Loss: 0.1434
  Total Loss: 0.2207
  Image grad max: 0.16702939569950104
  Output probs: [[0.    0.    0.    0.    0.    0.007 0.47  0.    0.523 0.   ]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0008
  Image Loss: 0.1433
  Total Loss: 0.2262
  Image grad max: 0.2850085198879242
  Output probs: [[0.    0.    0.    0.    0.    0.007 0.484 0.    0.509 0.   ]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0007
  Image Loss: 0.1433
  Total Loss: 0.2117
  Image grad max: 0.13282470405101776
  Output probs: [[0.    0.    0.    0.    0.    0.006 0.509 0.    0.485 0.   ]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0007
  Image Loss: 0.1432
  Total Loss: 0.2084
  Image grad max: 0.14795750379562378
  Output probs: [[0.    0.    0.    0.    0.    0.006 0.519 0.    0.475 0.   ]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0007
  Image Loss: 0.1431
  Total Loss: 0.2124
  Image grad max: 0.2577981948852539
  Output probs: [[0.    0.    0.    0.    0.    0.006 0.507 0.    0.487 0.   ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0006
  Image Loss: 0.1431
  Total Loss: 0.2015
  Image grad max: 0.1236722320318222
  Output probs: [[0.    0.    0.    0.    0.    0.005 0.487 0.    0.508 0.   ]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0006
  Image Loss: 0.1430
  Total Loss: 0.1990
  Image grad max: 0.11268287152051926
  Output probs: [[0.    0.    0.    0.    0.    0.005 0.479 0.    0.516 0.   ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0006
  Image Loss: 0.1429
  Total Loss: 0.2013
  Image grad max: 0.20300626754760742
  Output probs: [[0.    0.    0.    0.    0.    0.005 0.49  0.    0.505 0.   ]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0005
  Image Loss: 0.1428
  Total Loss: 0.1931
  Image grad max: 0.08388616144657135
  Output probs: [[0.    0.    0.    0.    0.    0.005 0.508 0.    0.488 0.   ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0005
  Image Loss: 0.1427
  Total Loss: 0.1918
  Image grad max: 0.12303116917610168
  Output probs: [[0.    0.    0.    0.    0.    0.004 0.513 0.    0.482 0.   ]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0005
  Image Loss: 0.1426
  Total Loss: 0.1925
  Image grad max: 0.1816917061805725
  Output probs: [[0.    0.    0.    0.    0.    0.004 0.502 0.    0.494 0.   ]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0004
  Image Loss: 0.1425
  Total Loss: 0.1860
  Image grad max: 0.05760762095451355
  Output probs: [[0.    0.    0.    0.    0.    0.004 0.488 0.    0.508 0.   ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0004
  Image Loss: 0.1424
  Total Loss: 0.1858
  Image grad max: 0.10906460881233215
  Output probs: [[0.    0.    0.    0.    0.    0.004 0.485 0.    0.511 0.   ]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0004
  Image Loss: 0.1423
  Total Loss: 0.1852
  Image grad max: 0.13606466352939606
  Output probs: [[0.    0.    0.    0.    0.    0.004 0.497 0.    0.499 0.   ]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0004
  Image Loss: 0.1421
  Total Loss: 0.1804
  Image grad max: 0.028767907992005348
  Output probs: [[0.    0.    0.    0.    0.    0.004 0.508 0.    0.488 0.   ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0004
  Image Loss: 0.1420
  Total Loss: 0.1810
  Image grad max: 0.12140921503305435
  Output probs: [[0.    0.    0.    0.    0.    0.004 0.507 0.    0.489 0.   ]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0004
  Image Loss: 0.1419
  Total Loss: 0.1791
  Image grad max: 0.10945814847946167
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.496 0.    0.5   0.   ]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0003
  Image Loss: 0.1418
  Total Loss: 0.1761
  Image grad max: 0.029682166874408722
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.489 0.    0.508 0.   ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0003
  Image Loss: 0.1417
  Total Loss: 0.1766
  Image grad max: 0.10530678927898407
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.492 0.    0.504 0.   ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0003
  Image Loss: 0.1416
  Total Loss: 0.1742
  Image grad max: 0.06411891430616379
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.503 0.    0.494 0.   ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0003
  Image Loss: 0.1414
  Total Loss: 0.1727
  Image grad max: 0.054268673062324524
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.507 0.    0.49  0.   ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0003
  Image Loss: 0.1413
  Total Loss: 0.1726
  Image grad max: 0.09966059774160385
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.501 0.    0.496 0.   ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0003
  Image Loss: 0.1412
  Total Loss: 0.1703
  Image grad max: 0.03378238156437874
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.493 0.    0.504 0.   ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0003
  Image Loss: 0.1411
  Total Loss: 0.1698
  Image grad max: 0.06209171935915947
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.492 0.    0.505 0.   ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0003
  Image Loss: 0.1410
  Total Loss: 0.1690
  Image grad max: 0.06796404719352722
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.499 0.    0.498 0.   ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0003
  Image Loss: 0.1408
  Total Loss: 0.1673
  Image grad max: 0.01602359674870968
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.505 0.    0.493 0.   ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0003
  Image Loss: 0.1407
  Total Loss: 0.1672
  Image grad max: 0.07336872071027756
  Output probs: [[0.    0.    0.    0.    0.    0.003 0.502 0.    0.495 0.   ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0003
  Image Loss: 0.1406
  Total Loss: 0.1659
  Image grad max: 0.04437354952096939
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.496 0.    0.502 0.   ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0002
  Image Loss: 0.1405
  Total Loss: 0.1650
  Image grad max: 0.03401542454957962
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.494 0.    0.504 0.   ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0002
  Image Loss: 0.1403
  Total Loss: 0.1646
  Image grad max: 0.056416142731904984
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.498 0.    0.5   0.   ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0002
  Image Loss: 0.1402
  Total Loss: 0.1634
  Image grad max: 0.01759323664009571
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.503 0.    0.495 0.   ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0002
  Image Loss: 0.1401
  Total Loss: 0.1630
  Image grad max: 0.05169380083680153
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.502 0.    0.496 0.   ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0002
  Image Loss: 0.1400
  Total Loss: 0.1622
  Image grad max: 0.0409698449075222
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.497 0.    0.501 0.   ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0002
  Image Loss: 0.1398
  Total Loss: 0.1614
  Image grad max: 0.022649457678198814
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.495 0.    0.503 0.   ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0002
  Image Loss: 0.1397
  Total Loss: 0.1610
  Image grad max: 0.043399628251791
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.498 0.    0.5   0.   ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0002
  Image Loss: 0.1396
  Total Loss: 0.1601
  Image grad max: 0.01689617522060871
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.502 0.    0.496 0.   ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0002
  Image Loss: 0.1395
  Total Loss: 0.1598
  Image grad max: 0.03890368714928627
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.501 0.    0.497 0.   ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0002
  Image Loss: 0.1394
  Total Loss: 0.1591
  Image grad max: 0.0327688530087471
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.498 0.    0.5   0.   ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0002
  Image Loss: 0.1392
  Total Loss: 0.1585
  Image grad max: 0.019134869799017906
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.496 0.    0.502 0.   ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0002
  Image Loss: 0.1391
  Total Loss: 0.1581
  Image grad max: 0.03298691287636757
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0002
  Image Loss: 0.1390
  Total Loss: 0.1575
  Image grad max: 0.013773519545793533
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.501 0.    0.497 0.   ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0002
  Image Loss: 0.1389
  Total Loss: 0.1571
  Image grad max: 0.0318678580224514
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.501 0.    0.497 0.   ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0002
  Image Loss: 0.1387
  Total Loss: 0.1566
  Image grad max: 0.02377030812203884
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.498 0.    0.501 0.   ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0002
  Image Loss: 0.1386
  Total Loss: 0.1561
  Image grad max: 0.018048081547021866
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.497 0.    0.501 0.   ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0002
  Image Loss: 0.1385
  Total Loss: 0.1557
  Image grad max: 0.024597151204943657
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.499 0.    0.499 0.   ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0002
  Image Loss: 0.1384
  Total Loss: 0.1552
  Image grad max: 0.009886137209832668
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.501 0.    0.497 0.   ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0002
  Image Loss: 0.1382
  Total Loss: 0.1548
  Image grad max: 0.027012120932340622
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.5   0.    0.498 0.   ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0002
  Image Loss: 0.1381
  Total Loss: 0.1544
  Image grad max: 0.014547840692102909
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.498 0.    0.501 0.   ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0002
  Image Loss: 0.1380
  Total Loss: 0.1540
  Image grad max: 0.017744265496730804
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.498 0.    0.501 0.   ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0002
  Image Loss: 0.1379
  Total Loss: 0.1536
  Image grad max: 0.017646778374910355
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0002
  Image Loss: 0.1377
  Total Loss: 0.1532
  Image grad max: 0.012078432366251945
  Output probs: [[0.    0.    0.    0.    0.    0.002 0.501 0.    0.498 0.   ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0002
  Image Loss: 0.1376
  Total Loss: 0.1528
  Image grad max: 0.021810658276081085
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.499 0.   ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0001
  Image Loss: 0.1375
  Total Loss: 0.1524
  Image grad max: 0.008984236046671867
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.498 0.    0.501 0.   ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0001
  Image Loss: 0.1374
  Total Loss: 0.1521
  Image grad max: 0.016834869980812073
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0001
  Image Loss: 0.1372
  Total Loss: 0.1517
  Image grad max: 0.012614551931619644
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.498 0.   ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0001
  Image Loss: 0.1371
  Total Loss: 0.1514
  Image grad max: 0.015478660352528095
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.498 0.   ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0001
  Image Loss: 0.1370
  Total Loss: 0.1510
  Image grad max: 0.014901610091328621
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0001
  Image Loss: 0.1369
  Total Loss: 0.1507
  Image grad max: 0.011760672554373741
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.498 0.    0.5   0.   ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0001
  Image Loss: 0.1367
  Total Loss: 0.1504
  Image grad max: 0.014483950100839138
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.499 0.   ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0001
  Image Loss: 0.1366
  Total Loss: 0.1500
  Image grad max: 0.00793134979903698
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.498 0.   ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0001
  Image Loss: 0.1365
  Total Loss: 0.1497
  Image grad max: 0.015137946233153343
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0001
  Image Loss: 0.1364
  Total Loss: 0.1494
  Image grad max: 0.007031620945781469
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.498 0.    0.5   0.   ]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0001
  Image Loss: 0.1362
  Total Loss: 0.1491
  Image grad max: 0.012725009582936764
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0001
  Image Loss: 0.1361
  Total Loss: 0.1488
  Image grad max: 0.010725216008722782
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0001
  Image Loss: 0.1360
  Total Loss: 0.1485
  Image grad max: 0.010605022311210632
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0001
  Image Loss: 0.1359
  Total Loss: 0.1482
  Image grad max: 0.010673679411411285
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0001
  Image Loss: 0.1357
  Total Loss: 0.1479
  Image grad max: 0.009794436395168304
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0001
  Image Loss: 0.1356
  Total Loss: 0.1476
  Image grad max: 0.011290517635643482
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.499 0.   ]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0001
  Image Loss: 0.1355
  Total Loss: 0.1474
  Image grad max: 0.006766320206224918
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0001
  Image Loss: 0.1354
  Total Loss: 0.1471
  Image grad max: 0.010664279572665691
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.499 0.   ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0001
  Image Loss: 0.1352
  Total Loss: 0.1468
  Image grad max: 0.007293407805263996
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0001
  Image Loss: 0.1351
  Total Loss: 0.1465
  Image grad max: 0.010431699454784393
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0001
  Image Loss: 0.1350
  Total Loss: 0.1463
  Image grad max: 0.00807325728237629
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0001
  Image Loss: 0.1349
  Total Loss: 0.1460
  Image grad max: 0.008799916133284569
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0001
  Image Loss: 0.1347
  Total Loss: 0.1458
  Image grad max: 0.006283984519541264
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0001
  Image Loss: 0.1346
  Total Loss: 0.1455
  Image grad max: 0.009089425206184387
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0001
  Image Loss: 0.1345
  Total Loss: 0.1452
  Image grad max: 0.008529003709554672
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0001
  Image Loss: 0.1344
  Total Loss: 0.1450
  Image grad max: 0.006619956344366074
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0001
  Image Loss: 0.1342
  Total Loss: 0.1447
  Image grad max: 0.006913611199706793
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0001
  Image Loss: 0.1341
  Total Loss: 0.1445
  Image grad max: 0.007815217599272728
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0001
  Image Loss: 0.1340
  Total Loss: 0.1443
  Image grad max: 0.008392692543566227
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0001
  Image Loss: 0.1339
  Total Loss: 0.1440
  Image grad max: 0.0057940916158258915
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0001
  Image Loss: 0.1337
  Total Loss: 0.1438
  Image grad max: 0.006655949633568525
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0001
  Image Loss: 0.1336
  Total Loss: 0.1435
  Image grad max: 0.006842253264039755
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0001
  Image Loss: 0.1335
  Total Loss: 0.1433
  Image grad max: 0.00794279109686613
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0001
  Image Loss: 0.1334
  Total Loss: 0.1431
  Image grad max: 0.006082180421799421
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0001
  Image Loss: 0.1332
  Total Loss: 0.1428
  Image grad max: 0.006067676469683647
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0001
  Image Loss: 0.1331
  Total Loss: 0.1426
  Image grad max: 0.006172655150294304
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0001
  Image Loss: 0.1330
  Total Loss: 0.1424
  Image grad max: 0.007434076629579067
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0001
  Image Loss: 0.1328
  Total Loss: 0.1422
  Image grad max: 0.00612642290070653
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0001
  Image Loss: 0.1327
  Total Loss: 0.1419
  Image grad max: 0.00542356027290225
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0001
  Image Loss: 0.1326
  Total Loss: 0.1417
  Image grad max: 0.005738512612879276
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0001
  Image Loss: 0.1325
  Total Loss: 0.1415
  Image grad max: 0.006942492909729481
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0001
  Image Loss: 0.1323
  Total Loss: 0.1413
  Image grad max: 0.006015345454216003
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0001
  Image Loss: 0.1322
  Total Loss: 0.1411
  Image grad max: 0.005086088087409735
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0001
  Image Loss: 0.1321
  Total Loss: 0.1408
  Image grad max: 0.005452059209346771
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0001
  Image Loss: 0.1320
  Total Loss: 0.1406
  Image grad max: 0.0065152901224792
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0001
  Image Loss: 0.1318
  Total Loss: 0.1404
  Image grad max: 0.005825514905154705
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.499 0.   ]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0001
  Image Loss: 0.1317
  Total Loss: 0.1402
  Image grad max: 0.0048666177317500114
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0001
  Image Loss: 0.1316
  Total Loss: 0.1400
  Image grad max: 0.005260782781988382
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0001
  Image Loss: 0.1315
  Total Loss: 0.1398
  Image grad max: 0.006155836395919323
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0001
  Image Loss: 0.1313
  Total Loss: 0.1396
  Image grad max: 0.005589870270341635
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0001
  Image Loss: 0.1312
  Total Loss: 0.1394
  Image grad max: 0.004691131878644228
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0001
  Image Loss: 0.1311
  Total Loss: 0.1392
  Image grad max: 0.005122724920511246
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0001
  Image Loss: 0.1309
  Total Loss: 0.1390
  Image grad max: 0.005845688283443451
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0001
  Image Loss: 0.1308
  Total Loss: 0.1388
  Image grad max: 0.005349333398044109
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0001
  Image Loss: 0.1307
  Total Loss: 0.1386
  Image grad max: 0.004614755976945162
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0001
  Image Loss: 0.1306
  Total Loss: 0.1384
  Image grad max: 0.005015341565012932
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.499 0.    0.5   0.   ]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0001
  Image Loss: 0.1304
  Total Loss: 0.1382
  Image grad max: 0.00558230746537447
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0001
  Image Loss: 0.1303
  Total Loss: 0.1380
  Image grad max: 0.005100454203784466
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0001
  Image Loss: 0.1302
  Total Loss: 0.1378
  Image grad max: 0.004535754211246967
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0001
  Image Loss: 0.1301
  Total Loss: 0.1376
  Image grad max: 0.004932802636176348
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0001
  Image Loss: 0.1299
  Total Loss: 0.1374
  Image grad max: 0.005338629707694054
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0001
  Image Loss: 0.1298
  Total Loss: 0.1372
  Image grad max: 0.004854349419474602
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0001
  Image Loss: 0.1297
  Total Loss: 0.1370
  Image grad max: 0.004469412378966808
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0001
  Image Loss: 0.1295
  Total Loss: 0.1368
  Image grad max: 0.004856898915022612
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0001
  Image Loss: 0.1294
  Total Loss: 0.1366
  Image grad max: 0.005097171291708946
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0001
  Image Loss: 0.1293
  Total Loss: 0.1364
  Image grad max: 0.004641639068722725
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0001
  Image Loss: 0.1292
  Total Loss: 0.1362
  Image grad max: 0.004400923848152161
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0001
  Image Loss: 0.1290
  Total Loss: 0.1361
  Image grad max: 0.004784370772540569
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0001
  Image Loss: 0.1289
  Total Loss: 0.1359
  Image grad max: 0.004868445452302694
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0001
  Image Loss: 0.1288
  Total Loss: 0.1357
  Image grad max: 0.004440150689333677
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0001
  Image Loss: 0.1287
  Total Loss: 0.1355
  Image grad max: 0.0043461560271680355
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0001
  Image Loss: 0.1285
  Total Loss: 0.1353
  Image grad max: 0.004634146112948656
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0001
  Image Loss: 0.1284
  Total Loss: 0.1351
  Image grad max: 0.004655702970921993
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0001
  Image Loss: 0.1283
  Total Loss: 0.1349
  Image grad max: 0.004328724462538958
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0001
  Image Loss: 0.1281
  Total Loss: 0.1348
  Image grad max: 0.004297228995710611
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0001
  Image Loss: 0.1280
  Total Loss: 0.1346
  Image grad max: 0.004528912715613842
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0001
  Image Loss: 0.1279
  Total Loss: 0.1344
  Image grad max: 0.004450960084795952
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0001
  Image Loss: 0.1278
  Total Loss: 0.1342
  Image grad max: 0.004192869178950787
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0001
  Image Loss: 0.1276
  Total Loss: 0.1340
  Image grad max: 0.004239348694682121
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0001
  Image Loss: 0.1275
  Total Loss: 0.1339
  Image grad max: 0.0043958695605397224
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0001
  Image Loss: 0.1274
  Total Loss: 0.1337
  Image grad max: 0.004250801168382168
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0001
  Image Loss: 0.1272
  Total Loss: 0.1335
  Image grad max: 0.0040781814604997635
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0001
  Image Loss: 0.1271
  Total Loss: 0.1333
  Image grad max: 0.004182630218565464
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0001
  Image Loss: 0.1270
  Total Loss: 0.1332
  Image grad max: 0.004243846517056227
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0001
  Image Loss: 0.1269
  Total Loss: 0.1330
  Image grad max: 0.004079322796314955
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0001
  Image Loss: 0.1267
  Total Loss: 0.1328
  Image grad max: 0.003990076016634703
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0001
  Image Loss: 0.1266
  Total Loss: 0.1326
  Image grad max: 0.004110143054276705
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0001
  Image Loss: 0.1265
  Total Loss: 0.1325
  Image grad max: 0.004083458334207535
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0001
  Image Loss: 0.1264
  Total Loss: 0.1323
  Image grad max: 0.003935999237000942
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0001
  Image Loss: 0.1262
  Total Loss: 0.1321
  Image grad max: 0.003927652258425951
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0001
  Image Loss: 0.1261
  Total Loss: 0.1319
  Image grad max: 0.004008353687822819
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0001
  Image Loss: 0.1260
  Total Loss: 0.1318
  Image grad max: 0.003931403160095215
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0001
  Image Loss: 0.1258
  Total Loss: 0.1316
  Image grad max: 0.003817401360720396
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0001
  Image Loss: 0.1257
  Total Loss: 0.1314
  Image grad max: 0.00387005927041173
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0001
  Image Loss: 0.1256
  Total Loss: 0.1313
  Image grad max: 0.003899322124198079
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0001
  Image Loss: 0.1255
  Total Loss: 0.1311
  Image grad max: 0.003778808517381549
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0001
  Image Loss: 0.1253
  Total Loss: 0.1309
  Image grad max: 0.003740232205018401
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0001
  Image Loss: 0.1252
  Total Loss: 0.1308
  Image grad max: 0.0038078376092016697
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0001
  Image Loss: 0.1251
  Total Loss: 0.1306
  Image grad max: 0.0037550367414951324
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0001
  Image Loss: 0.1250
  Total Loss: 0.1304
  Image grad max: 0.003666682168841362
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0001
  Image Loss: 0.1248
  Total Loss: 0.1303
  Image grad max: 0.003684678114950657
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0001
  Image Loss: 0.1247
  Total Loss: 0.1301
  Image grad max: 0.003709200071170926
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0001
  Image Loss: 0.1246
  Total Loss: 0.1299
  Image grad max: 0.0036289836280047894
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0001
  Image Loss: 0.1244
  Total Loss: 0.1298
  Image grad max: 0.003583370242267847
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0001
  Image Loss: 0.1243
  Total Loss: 0.1296
  Image grad max: 0.003620859934017062
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0001
  Image Loss: 0.1242
  Total Loss: 0.1294
  Image grad max: 0.003597014583647251
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0001
  Image Loss: 0.1241
  Total Loss: 0.1293
  Image grad max: 0.003523625433444977
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0001
  Image Loss: 0.1239
  Total Loss: 0.1291
  Image grad max: 0.003521539270877838
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0001
  Image Loss: 0.1238
  Total Loss: 0.1289
  Image grad max: 0.0035361717455089092
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0001
  Image Loss: 0.1237
  Total Loss: 0.1288
  Image grad max: 0.0034861541353166103
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0001
  Image Loss: 0.1236
  Total Loss: 0.1286
  Image grad max: 0.0034484087955206633
  Output probs: [[0.    0.    0.    0.    0.    0.001 0.5   0.    0.5   0.   ]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0001
  Image Loss: 0.1234
  Total Loss: 0.1285
  Image grad max: 0.003453322919085622
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0001
  Image Loss: 0.1233
  Total Loss: 0.1283
  Image grad max: 0.0034401544835418463
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0000
  Image Loss: 0.1232
  Total Loss: 0.1281
  Image grad max: 0.003394027939066291
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0000
  Image Loss: 0.1231
  Total Loss: 0.1280
  Image grad max: 0.0033783260732889175
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0000
  Image Loss: 0.1229
  Total Loss: 0.1278
  Image grad max: 0.0033843202982097864
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0000
  Image Loss: 0.1228
  Total Loss: 0.1277
  Image grad max: 0.003349361475557089
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0000
  Image Loss: 0.1227
  Total Loss: 0.1275
  Image grad max: 0.0033054391387850046
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0000
  Image Loss: 0.1225
  Total Loss: 0.1274
  Image grad max: 0.0033243424259126186
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0000
  Image Loss: 0.1224
  Total Loss: 0.1272
  Image grad max: 0.003299213945865631
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0000
  Image Loss: 0.1223
  Total Loss: 0.1270
  Image grad max: 0.0032818778418004513
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0000
  Image Loss: 0.1222
  Total Loss: 0.1269
  Image grad max: 0.003283699508756399
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0000
  Image Loss: 0.1220
  Total Loss: 0.1267
  Image grad max: 0.0032641286961734295
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0000
  Image Loss: 0.1219
  Total Loss: 0.1266
  Image grad max: 0.003248652210459113
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0000
  Image Loss: 0.1218
  Total Loss: 0.1264
  Image grad max: 0.0032652560621500015
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0000
  Image Loss: 0.1217
  Total Loss: 0.1263
  Image grad max: 0.0032559321261942387
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0000
  Image Loss: 0.1215
  Total Loss: 0.1261
  Image grad max: 0.003224895801395178
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0000
  Image Loss: 0.1214
  Total Loss: 0.1259
  Image grad max: 0.0032460284419357777
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0000
  Image Loss: 0.1213
  Total Loss: 0.1258
  Image grad max: 0.003245316445827484
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0000
  Image Loss: 0.1212
  Total Loss: 0.1256
  Image grad max: 0.0032013230957090855
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0000
  Image Loss: 0.1210
  Total Loss: 0.1255
  Image grad max: 0.0032284569460898638
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0000
  Image Loss: 0.1209
  Total Loss: 0.1253
  Image grad max: 0.0032221870496869087
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0000
  Image Loss: 0.1208
  Total Loss: 0.1252
  Image grad max: 0.0032028518617153168
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0000
  Image Loss: 0.1207
  Total Loss: 0.1250
  Image grad max: 0.003192316275089979
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0000
  Image Loss: 0.1205
  Total Loss: 0.1249
  Image grad max: 0.003212509211152792
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0000
  Image Loss: 0.1204
  Total Loss: 0.1247
  Image grad max: 0.003188862930983305
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0000
  Image Loss: 0.1203
  Total Loss: 0.1246
  Image grad max: 0.003175416262820363
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0000
  Image Loss: 0.1202
  Total Loss: 0.1244
  Image grad max: 0.0031855811830610037
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0000
  Image Loss: 0.1200
  Total Loss: 0.1243
  Image grad max: 0.0031840233132243156
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0000
  Image Loss: 0.1199
  Total Loss: 0.1241
  Image grad max: 0.003157517174258828
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0000
  Image Loss: 0.1198
  Total Loss: 0.1240
  Image grad max: 0.0031632124446332455
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0000
  Image Loss: 0.1197
  Total Loss: 0.1238
  Image grad max: 0.0031750414054840803
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0000
  Image Loss: 0.1195
  Total Loss: 0.1237
  Image grad max: 0.0031441731844097376
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0000
  Image Loss: 0.1194
  Total Loss: 0.1235
  Image grad max: 0.0031441468745470047
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0000
  Image Loss: 0.1193
  Total Loss: 0.1234
  Image grad max: 0.0031517643947154284
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0000
  Image Loss: 0.1192
  Total Loss: 0.1232
  Image grad max: 0.0031387729104608297
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0000
  Image Loss: 0.1190
  Total Loss: 0.1231
  Image grad max: 0.003125636838376522
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0000
  Image Loss: 0.1189
  Total Loss: 0.1229
  Image grad max: 0.0031334946397691965
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0000
  Image Loss: 0.1188
  Total Loss: 0.1228
  Image grad max: 0.0031249450985342264
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0000
  Image Loss: 0.1187
  Total Loss: 0.1226
  Image grad max: 0.003112077247351408
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0000
  Image Loss: 0.1185
  Total Loss: 0.1225
  Image grad max: 0.0031213508918881416
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0000
  Image Loss: 0.1184
  Total Loss: 0.1223
  Image grad max: 0.0031025432981550694
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0000
  Image Loss: 0.1183
  Total Loss: 0.1222
  Image grad max: 0.00310465507209301
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0000
  Image Loss: 0.1182
  Total Loss: 0.1220
  Image grad max: 0.0031008890364319086
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0000
  Image Loss: 0.1180
  Total Loss: 0.1219
  Image grad max: 0.003091232618317008
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0000
  Image Loss: 0.1179
  Total Loss: 0.1217
  Image grad max: 0.003089101752266288
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0000
  Image Loss: 0.1178
  Total Loss: 0.1216
  Image grad max: 0.003085453063249588
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0000
  Image Loss: 0.1177
  Total Loss: 0.1214
  Image grad max: 0.0030831045005470514
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0000
  Image Loss: 0.1175
  Total Loss: 0.1213
  Image grad max: 0.003067718120291829
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0000
  Image Loss: 0.1174
  Total Loss: 0.1212
  Image grad max: 0.0030774876940995455
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0000
  Image Loss: 0.1173
  Total Loss: 0.1210
  Image grad max: 0.0030693046282976866
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0000
  Image Loss: 0.1172
  Total Loss: 0.1209
  Image grad max: 0.003053930588066578
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0000
  Image Loss: 0.1170
  Total Loss: 0.1207
  Image grad max: 0.003059363691136241
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0000
  Image Loss: 0.1169
  Total Loss: 0.1206
  Image grad max: 0.0030616396106779575
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0000
  Image Loss: 0.1168
  Total Loss: 0.1204
  Image grad max: 0.003040383104234934
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0000
  Image Loss: 0.1167
  Total Loss: 0.1203
  Image grad max: 0.003042876720428467
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0000
  Image Loss: 0.1165
  Total Loss: 0.1201
  Image grad max: 0.0030482513830065727
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0000
  Image Loss: 0.1164
  Total Loss: 0.1200
  Image grad max: 0.0030300510115921497
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0000
  Image Loss: 0.1163
  Total Loss: 0.1199
  Image grad max: 0.003032582812011242
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0000
  Image Loss: 0.1162
  Total Loss: 0.1197
  Image grad max: 0.0030305900145322084
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0000
  Image Loss: 0.1161
  Total Loss: 0.1196
  Image grad max: 0.0030182970222085714
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0000
  Image Loss: 0.1159
  Total Loss: 0.1194
  Image grad max: 0.0030239331535995007
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0000
  Image Loss: 0.1158
  Total Loss: 0.1193
  Image grad max: 0.0030131926760077477
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0000
  Image Loss: 0.1157
  Total Loss: 0.1192
  Image grad max: 0.003011325839906931
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0000
  Image Loss: 0.1156
  Total Loss: 0.1190
  Image grad max: 0.0029975385405123234
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0000
  Image Loss: 0.1154
  Total Loss: 0.1189
  Image grad max: 0.002997195813804865
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0000
  Image Loss: 0.1153
  Total Loss: 0.1187
  Image grad max: 0.0030133521649986506
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0000
  Image Loss: 0.1152
  Total Loss: 0.1186
  Image grad max: 0.002980000339448452
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0000
  Image Loss: 0.1151
  Total Loss: 0.1184
  Image grad max: 0.0029902118258178234
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0000
  Image Loss: 0.1149
  Total Loss: 0.1183
  Image grad max: 0.0030034813098609447
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0000
  Image Loss: 0.1148
  Total Loss: 0.1182
  Image grad max: 0.0029625885654240847
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0000
  Image Loss: 0.1147
  Total Loss: 0.1180
  Image grad max: 0.002984937746077776
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0000
  Image Loss: 0.1146
  Total Loss: 0.1179
  Image grad max: 0.0029893957544118166
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0000
  Image Loss: 0.1145
  Total Loss: 0.1177
  Image grad max: 0.0029378957115113735
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0000
  Image Loss: 0.1143
  Total Loss: 0.1176
  Image grad max: 0.0029965746216475964
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0000
  Image Loss: 0.1142
  Total Loss: 0.1175
  Image grad max: 0.002958717057481408
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0000
  Image Loss: 0.1141
  Total Loss: 0.1173
  Image grad max: 0.002938954159617424
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0000
  Image Loss: 0.1140
  Total Loss: 0.1172
  Image grad max: 0.0029782382771372795
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0000
  Image Loss: 0.1139
  Total Loss: 0.1170
  Image grad max: 0.0029480382800102234
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0000
  Image Loss: 0.1137
  Total Loss: 0.1169
  Image grad max: 0.0029283224139362574
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0. ]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0000
  Image Loss: 0.1136
  Total Loss: 0.1168
  Image grad max: 0.0029721572063863277
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
