Epoch [1/30], Batch [0/6000], Loss: 2.3894
Epoch [1/30], Batch [100/6000], Loss: 1.5901
Epoch [1/30], Batch [200/6000], Loss: 0.6380
Epoch [1/30], Batch [300/6000], Loss: 1.2429
Epoch [1/30], Batch [400/6000], Loss: 0.9173
Epoch [1/30], Batch [500/6000], Loss: 0.6019
Epoch [1/30], Batch [600/6000], Loss: 0.5021
Epoch [1/30], Batch [700/6000], Loss: 0.2809
Epoch [1/30], Batch [800/6000], Loss: 0.1794
Epoch [1/30], Batch [900/6000], Loss: 0.3423
Epoch [1/30], Batch [1000/6000], Loss: 0.1947
Epoch [1/30], Batch [1100/6000], Loss: 0.8863
Epoch [1/30], Batch [1200/6000], Loss: 0.3022
Epoch [1/30], Batch [1300/6000], Loss: 0.1405
Epoch [1/30], Batch [1400/6000], Loss: 0.4331
Epoch [1/30], Batch [1500/6000], Loss: 0.4962
Epoch [1/30], Batch [1600/6000], Loss: 0.6396
Epoch [1/30], Batch [1700/6000], Loss: 0.2961
Epoch [1/30], Batch [1800/6000], Loss: 0.4999
Epoch [1/30], Batch [1900/6000], Loss: 0.2799
Epoch [1/30], Batch [2000/6000], Loss: 0.0932
Epoch [1/30], Batch [2100/6000], Loss: 0.1241
Epoch [1/30], Batch [2200/6000], Loss: 0.2067
Epoch [1/30], Batch [2300/6000], Loss: 0.1641
Epoch [1/30], Batch [2400/6000], Loss: 0.1274
Epoch [1/30], Batch [2500/6000], Loss: 0.3949
Epoch [1/30], Batch [2600/6000], Loss: 0.0733
Epoch [1/30], Batch [2700/6000], Loss: 0.3186
Epoch [1/30], Batch [2800/6000], Loss: 0.2332
Epoch [1/30], Batch [2900/6000], Loss: 0.1793
Epoch [1/30], Batch [3000/6000], Loss: 0.2988
Epoch [1/30], Batch [3100/6000], Loss: 0.2235
Epoch [1/30], Batch [3200/6000], Loss: 0.1548
Epoch [1/30], Batch [3300/6000], Loss: 1.2166
Epoch [1/30], Batch [3400/6000], Loss: 0.0524
Epoch [1/30], Batch [3500/6000], Loss: 0.0719
Epoch [1/30], Batch [3600/6000], Loss: 0.1912
Epoch [1/30], Batch [3700/6000], Loss: 0.2428
Epoch [1/30], Batch [3800/6000], Loss: 0.2946
Epoch [1/30], Batch [3900/6000], Loss: 0.1738
Epoch [1/30], Batch [4000/6000], Loss: 0.1583
Epoch [1/30], Batch [4100/6000], Loss: 0.2958
Epoch [1/30], Batch [4200/6000], Loss: 0.1461
Epoch [1/30], Batch [4300/6000], Loss: 0.6970
Epoch [1/30], Batch [4400/6000], Loss: 0.3174
Epoch [1/30], Batch [4500/6000], Loss: 0.0645
Epoch [1/30], Batch [4600/6000], Loss: 0.1442
Epoch [1/30], Batch [4700/6000], Loss: 0.0963
Epoch [1/30], Batch [4800/6000], Loss: 0.3673
Epoch [1/30], Batch [4900/6000], Loss: 0.0859
Epoch [1/30], Batch [5000/6000], Loss: 0.1479
Epoch [1/30], Batch [5100/6000], Loss: 0.1587
Epoch [1/30], Batch [5200/6000], Loss: 0.0770
Epoch [1/30], Batch [5300/6000], Loss: 0.2513
Epoch [1/30], Batch [5400/6000], Loss: 0.3784
Epoch [1/30], Batch [5500/6000], Loss: 0.1203
Epoch [1/30], Batch [5600/6000], Loss: 0.2170
Epoch [1/30], Batch [5700/6000], Loss: 0.3098
Epoch [1/30], Batch [5800/6000], Loss: 0.4447
Epoch [1/30], Batch [5900/6000], Loss: 0.0562
Epoch [1/30], Loss: 0.3986
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.0951
Epoch [2/30], Batch [100/6000], Loss: 0.4022
Epoch [2/30], Batch [200/6000], Loss: 0.2596
Epoch [2/30], Batch [300/6000], Loss: 0.0906
Epoch [2/30], Batch [400/6000], Loss: 0.2530
Epoch [2/30], Batch [500/6000], Loss: 0.0525
Epoch [2/30], Batch [600/6000], Loss: 0.0887
Epoch [2/30], Batch [700/6000], Loss: 0.0519
Epoch [2/30], Batch [800/6000], Loss: 0.7766
Epoch [2/30], Batch [900/6000], Loss: 0.0571
Epoch [2/30], Batch [1000/6000], Loss: 0.0732
Epoch [2/30], Batch [1100/6000], Loss: 0.0512
Epoch [2/30], Batch [1200/6000], Loss: 0.1105
Epoch [2/30], Batch [1300/6000], Loss: 0.0485
Epoch [2/30], Batch [1400/6000], Loss: 0.0822
Epoch [2/30], Batch [1500/6000], Loss: 0.4005
Epoch [2/30], Batch [1600/6000], Loss: 0.8170
Epoch [2/30], Batch [1700/6000], Loss: 0.6632
Epoch [2/30], Batch [1800/6000], Loss: 0.1373
Epoch [2/30], Batch [1900/6000], Loss: 0.0457
Epoch [2/30], Batch [2000/6000], Loss: 0.0378
Epoch [2/30], Batch [2100/6000], Loss: 0.1529
Epoch [2/30], Batch [2200/6000], Loss: 0.0665
Epoch [2/30], Batch [2300/6000], Loss: 0.2565
Epoch [2/30], Batch [2400/6000], Loss: 0.1709
Epoch [2/30], Batch [2500/6000], Loss: 0.9733
Epoch [2/30], Batch [2600/6000], Loss: 0.0668
Epoch [2/30], Batch [2700/6000], Loss: 0.1213
Epoch [2/30], Batch [2800/6000], Loss: 0.1437
Epoch [2/30], Batch [2900/6000], Loss: 0.1632
Epoch [2/30], Batch [3000/6000], Loss: 0.0849
Epoch [2/30], Batch [3100/6000], Loss: 0.0478
Epoch [2/30], Batch [3200/6000], Loss: 0.1323
Epoch [2/30], Batch [3300/6000], Loss: 0.2947
Epoch [2/30], Batch [3400/6000], Loss: 0.0474
Epoch [2/30], Batch [3500/6000], Loss: 0.0494
Epoch [2/30], Batch [3600/6000], Loss: 0.0717
Epoch [2/30], Batch [3700/6000], Loss: 0.0551
Epoch [2/30], Batch [3800/6000], Loss: 0.2079
Epoch [2/30], Batch [3900/6000], Loss: 0.0864
Epoch [2/30], Batch [4000/6000], Loss: 0.2755
Epoch [2/30], Batch [4100/6000], Loss: 0.5916
Epoch [2/30], Batch [4200/6000], Loss: 0.0455
Epoch [2/30], Batch [4300/6000], Loss: 0.0406
Epoch [2/30], Batch [4400/6000], Loss: 0.0496
Epoch [2/30], Batch [4500/6000], Loss: 0.3516
Epoch [2/30], Batch [4600/6000], Loss: 0.0889
Epoch [2/30], Batch [4700/6000], Loss: 0.0375
Epoch [2/30], Batch [4800/6000], Loss: 0.3067
Epoch [2/30], Batch [4900/6000], Loss: 0.2069
Epoch [2/30], Batch [5000/6000], Loss: 0.1242
Epoch [2/30], Batch [5100/6000], Loss: 0.1535
Epoch [2/30], Batch [5200/6000], Loss: 0.0623
Epoch [2/30], Batch [5300/6000], Loss: 0.6227
Epoch [2/30], Batch [5400/6000], Loss: 0.0759
Epoch [2/30], Batch [5500/6000], Loss: 0.2228
Epoch [2/30], Batch [5600/6000], Loss: 0.0962
Epoch [2/30], Batch [5700/6000], Loss: 0.1238
Epoch [2/30], Batch [5800/6000], Loss: 0.0478
Epoch [2/30], Batch [5900/6000], Loss: 0.7953
Epoch [2/30], Loss: 0.2226
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.0376
Epoch [3/30], Batch [100/6000], Loss: 0.1006
Epoch [3/30], Batch [200/6000], Loss: 0.0551
Epoch [3/30], Batch [300/6000], Loss: 0.0335
Epoch [3/30], Batch [400/6000], Loss: 0.1438
Epoch [3/30], Batch [500/6000], Loss: 0.1992
Epoch [3/30], Batch [600/6000], Loss: 0.0740
Epoch [3/30], Batch [700/6000], Loss: 0.0611
Epoch [3/30], Batch [800/6000], Loss: 0.5379
Epoch [3/30], Batch [900/6000], Loss: 0.1194
Epoch [3/30], Batch [1000/6000], Loss: 0.3020
Epoch [3/30], Batch [1100/6000], Loss: 0.4420
Epoch [3/30], Batch [1200/6000], Loss: 0.6675
Epoch [3/30], Batch [1300/6000], Loss: 0.0353
Epoch [3/30], Batch [1400/6000], Loss: 0.0413
Epoch [3/30], Batch [1500/6000], Loss: 0.0346
Epoch [3/30], Batch [1600/6000], Loss: 0.2810
Epoch [3/30], Batch [1700/6000], Loss: 0.1220
Epoch [3/30], Batch [1800/6000], Loss: 0.0331
Epoch [3/30], Batch [1900/6000], Loss: 0.0591
Epoch [3/30], Batch [2000/6000], Loss: 0.1470
Epoch [3/30], Batch [2100/6000], Loss: 0.0525
Epoch [3/30], Batch [2200/6000], Loss: 0.0409
Epoch [3/30], Batch [2300/6000], Loss: 0.5620
Epoch [3/30], Batch [2400/6000], Loss: 0.2338
Epoch [3/30], Batch [2500/6000], Loss: 0.2131
Epoch [3/30], Batch [2600/6000], Loss: 0.2742
Epoch [3/30], Batch [2700/6000], Loss: 0.1393
Epoch [3/30], Batch [2800/6000], Loss: 0.2202
Epoch [3/30], Batch [2900/6000], Loss: 0.0421
Epoch [3/30], Batch [3000/6000], Loss: 0.0780
Epoch [3/30], Batch [3100/6000], Loss: 0.1036
Epoch [3/30], Batch [3200/6000], Loss: 0.2947
Epoch [3/30], Batch [3300/6000], Loss: 0.5017
Epoch [3/30], Batch [3400/6000], Loss: 0.1216
Epoch [3/30], Batch [3500/6000], Loss: 0.2228
Epoch [3/30], Batch [3600/6000], Loss: 0.0667
Epoch [3/30], Batch [3700/6000], Loss: 0.0364
Epoch [3/30], Batch [3800/6000], Loss: 0.1363
Epoch [3/30], Batch [3900/6000], Loss: 0.0439
Epoch [3/30], Batch [4000/6000], Loss: 0.0417
Epoch [3/30], Batch [4100/6000], Loss: 0.1149
Epoch [3/30], Batch [4200/6000], Loss: 0.0894
Epoch [3/30], Batch [4300/6000], Loss: 0.1025
Epoch [3/30], Batch [4400/6000], Loss: 1.0733
Epoch [3/30], Batch [4500/6000], Loss: 0.0449
Epoch [3/30], Batch [4600/6000], Loss: 0.0336
Epoch [3/30], Batch [4700/6000], Loss: 0.5460
Epoch [3/30], Batch [4800/6000], Loss: 0.0868
Epoch [3/30], Batch [4900/6000], Loss: 0.2187
Epoch [3/30], Batch [5000/6000], Loss: 0.0478
Epoch [3/30], Batch [5100/6000], Loss: 0.0360
Epoch [3/30], Batch [5200/6000], Loss: 0.0502
Epoch [3/30], Batch [5300/6000], Loss: 0.8343
Epoch [3/30], Batch [5400/6000], Loss: 0.0860
Epoch [3/30], Batch [5500/6000], Loss: 0.1274
Epoch [3/30], Batch [5600/6000], Loss: 0.0951
Epoch [3/30], Batch [5700/6000], Loss: 0.0449
Epoch [3/30], Batch [5800/6000], Loss: 0.0620
Epoch [3/30], Batch [5900/6000], Loss: 0.1130
Epoch [3/30], Loss: 0.1701
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.0627
Epoch [4/30], Batch [100/6000], Loss: 0.0351
Epoch [4/30], Batch [200/6000], Loss: 0.2682
Epoch [4/30], Batch [300/6000], Loss: 0.0644
Epoch [4/30], Batch [400/6000], Loss: 0.0647
Epoch [4/30], Batch [500/6000], Loss: 0.0315
Epoch [4/30], Batch [600/6000], Loss: 0.3097
Epoch [4/30], Batch [700/6000], Loss: 0.0810
Epoch [4/30], Batch [800/6000], Loss: 0.5819
Epoch [4/30], Batch [900/6000], Loss: 0.0408
Epoch [4/30], Batch [1000/6000], Loss: 0.5918
Epoch [4/30], Batch [1100/6000], Loss: 0.2991
Epoch [4/30], Batch [1200/6000], Loss: 0.0712
Epoch [4/30], Batch [1300/6000], Loss: 0.0478
Epoch [4/30], Batch [1400/6000], Loss: 0.1069
Epoch [4/30], Batch [1500/6000], Loss: 0.1181
Epoch [4/30], Batch [1600/6000], Loss: 0.5192
Epoch [4/30], Batch [1700/6000], Loss: 0.1097
Epoch [4/30], Batch [1800/6000], Loss: 0.0290
Epoch [4/30], Batch [1900/6000], Loss: 0.0385
Epoch [4/30], Batch [2000/6000], Loss: 0.5405
Epoch [4/30], Batch [2100/6000], Loss: 0.0401
Epoch [4/30], Batch [2200/6000], Loss: 0.2765
Epoch [4/30], Batch [2300/6000], Loss: 0.0345
Epoch [4/30], Batch [2400/6000], Loss: 0.1370
Epoch [4/30], Batch [2500/6000], Loss: 0.0642
Epoch [4/30], Batch [2600/6000], Loss: 0.0456
Epoch [4/30], Batch [2700/6000], Loss: 0.0521
Epoch [4/30], Batch [2800/6000], Loss: 0.0497
Epoch [4/30], Batch [2900/6000], Loss: 0.0371
Epoch [4/30], Batch [3000/6000], Loss: 0.0366
Epoch [4/30], Batch [3100/6000], Loss: 0.4853
Epoch [4/30], Batch [3200/6000], Loss: 0.0388
Epoch [4/30], Batch [3300/6000], Loss: 0.3122
Epoch [4/30], Batch [3400/6000], Loss: 0.2559
Epoch [4/30], Batch [3500/6000], Loss: 0.6583
Epoch [4/30], Batch [3600/6000], Loss: 0.0406
Epoch [4/30], Batch [3700/6000], Loss: 0.2270
Epoch [4/30], Batch [3800/6000], Loss: 0.2328
Epoch [4/30], Batch [3900/6000], Loss: 0.0419
Epoch [4/30], Batch [4000/6000], Loss: 0.1004
Epoch [4/30], Batch [4100/6000], Loss: 0.1772
Epoch [4/30], Batch [4200/6000], Loss: 0.1038
Epoch [4/30], Batch [4300/6000], Loss: 0.0928
Epoch [4/30], Batch [4400/6000], Loss: 0.1905
Epoch [4/30], Batch [4500/6000], Loss: 0.2277
Epoch [4/30], Batch [4600/6000], Loss: 0.0399
Epoch [4/30], Batch [4700/6000], Loss: 0.3816
Epoch [4/30], Batch [4800/6000], Loss: 0.0578
Epoch [4/30], Batch [4900/6000], Loss: 0.0297
Epoch [4/30], Batch [5000/6000], Loss: 0.0552
Epoch [4/30], Batch [5100/6000], Loss: 0.1248
Epoch [4/30], Batch [5200/6000], Loss: 0.6575
Epoch [4/30], Batch [5300/6000], Loss: 0.1561
Epoch [4/30], Batch [5400/6000], Loss: 0.1020
Epoch [4/30], Batch [5500/6000], Loss: 0.1103
Epoch [4/30], Batch [5600/6000], Loss: 0.0420
Epoch [4/30], Batch [5700/6000], Loss: 0.0409
Epoch [4/30], Batch [5800/6000], Loss: 0.0377
Epoch [4/30], Batch [5900/6000], Loss: 0.6648
Epoch [4/30], Loss: 0.1388
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.2691
Epoch [5/30], Batch [100/6000], Loss: 0.1151
Epoch [5/30], Batch [200/6000], Loss: 0.0292
Epoch [5/30], Batch [300/6000], Loss: 0.0708
Epoch [5/30], Batch [400/6000], Loss: 0.0395
Epoch [5/30], Batch [500/6000], Loss: 0.0544
Epoch [5/30], Batch [600/6000], Loss: 0.0305
Epoch [5/30], Batch [700/6000], Loss: 0.0419
Epoch [5/30], Batch [800/6000], Loss: 0.3761
Epoch [5/30], Batch [900/6000], Loss: 0.0440
Epoch [5/30], Batch [1000/6000], Loss: 0.0820
Epoch [5/30], Batch [1100/6000], Loss: 0.1043
Epoch [5/30], Batch [1200/6000], Loss: 0.1287
Epoch [5/30], Batch [1300/6000], Loss: 0.2167
Epoch [5/30], Batch [1400/6000], Loss: 0.2198
Epoch [5/30], Batch [1500/6000], Loss: 0.0295
Epoch [5/30], Batch [1600/6000], Loss: 0.0295
Epoch [5/30], Batch [1700/6000], Loss: 0.1189
Epoch [5/30], Batch [1800/6000], Loss: 0.0301
Epoch [5/30], Batch [1900/6000], Loss: 0.1501
Epoch [5/30], Batch [2000/6000], Loss: 0.1177
Epoch [5/30], Batch [2100/6000], Loss: 0.0729
Epoch [5/30], Batch [2200/6000], Loss: 0.0879
Epoch [5/30], Batch [2300/6000], Loss: 0.0320
Epoch [5/30], Batch [2400/6000], Loss: 0.1187
Epoch [5/30], Batch [2500/6000], Loss: 0.0709
Epoch [5/30], Batch [2600/6000], Loss: 0.0279
Epoch [5/30], Batch [2700/6000], Loss: 0.0606
Epoch [5/30], Batch [2800/6000], Loss: 0.0964
Epoch [5/30], Batch [2900/6000], Loss: 0.0322
Epoch [5/30], Batch [3000/6000], Loss: 0.0349
Epoch [5/30], Batch [3100/6000], Loss: 0.0429
Epoch [5/30], Batch [3200/6000], Loss: 0.0322
Epoch [5/30], Batch [3300/6000], Loss: 0.0713
Epoch [5/30], Batch [3400/6000], Loss: 0.0319
Epoch [5/30], Batch [3500/6000], Loss: 0.0724
Epoch [5/30], Batch [3600/6000], Loss: 0.4423
Epoch [5/30], Batch [3700/6000], Loss: 0.0471
Epoch [5/30], Batch [3800/6000], Loss: 0.0289
Epoch [5/30], Batch [3900/6000], Loss: 0.0436
Epoch [5/30], Batch [4000/6000], Loss: 0.0522
Epoch [5/30], Batch [4100/6000], Loss: 0.0377
Epoch [5/30], Batch [4200/6000], Loss: 0.0443
Epoch [5/30], Batch [4300/6000], Loss: 0.0380
Epoch [5/30], Batch [4400/6000], Loss: 0.1071
Epoch [5/30], Batch [4500/6000], Loss: 0.0447
Epoch [5/30], Batch [4600/6000], Loss: 0.2009
Epoch [5/30], Batch [4700/6000], Loss: 0.2296
Epoch [5/30], Batch [4800/6000], Loss: 0.0544
Epoch [5/30], Batch [4900/6000], Loss: 0.0416
Epoch [5/30], Batch [5000/6000], Loss: 0.1433
Epoch [5/30], Batch [5100/6000], Loss: 0.0541
Epoch [5/30], Batch [5200/6000], Loss: 0.1989
Epoch [5/30], Batch [5300/6000], Loss: 0.0268
Epoch [5/30], Batch [5400/6000], Loss: 0.6774
Epoch [5/30], Batch [5500/6000], Loss: 0.0456
Epoch [5/30], Batch [5600/6000], Loss: 0.0416
Epoch [5/30], Batch [5700/6000], Loss: 0.1964
Epoch [5/30], Batch [5800/6000], Loss: 0.0337
Epoch [5/30], Batch [5900/6000], Loss: 0.2948
Epoch [5/30], Loss: 0.1196
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.0427
Epoch [6/30], Batch [100/6000], Loss: 0.1161
Epoch [6/30], Batch [200/6000], Loss: 0.3194
Epoch [6/30], Batch [300/6000], Loss: 0.3477
Epoch [6/30], Batch [400/6000], Loss: 0.0291
Epoch [6/30], Batch [500/6000], Loss: 0.0281
Epoch [6/30], Batch [600/6000], Loss: 0.2677
Epoch [6/30], Batch [700/6000], Loss: 0.0292
Epoch [6/30], Batch [800/6000], Loss: 0.4317
Epoch [6/30], Batch [900/6000], Loss: 0.0298
Epoch [6/30], Batch [1000/6000], Loss: 0.4199
Epoch [6/30], Batch [1100/6000], Loss: 0.0295
Epoch [6/30], Batch [1200/6000], Loss: 0.2362
Epoch [6/30], Batch [1300/6000], Loss: 0.2240
Epoch [6/30], Batch [1400/6000], Loss: 0.2376
Epoch [6/30], Batch [1500/6000], Loss: 0.1821
Epoch [6/30], Batch [1600/6000], Loss: 0.4170
Epoch [6/30], Batch [1700/6000], Loss: 0.0476
Epoch [6/30], Batch [1800/6000], Loss: 0.0311
Epoch [6/30], Batch [1900/6000], Loss: 0.0448
Epoch [6/30], Batch [2000/6000], Loss: 0.1005
Epoch [6/30], Batch [2100/6000], Loss: 0.0908
Epoch [6/30], Batch [2200/6000], Loss: 0.0917
Epoch [6/30], Batch [2300/6000], Loss: 0.0683
Epoch [6/30], Batch [2400/6000], Loss: 0.0211
Epoch [6/30], Batch [2500/6000], Loss: 0.5030
Epoch [6/30], Batch [2600/6000], Loss: 0.0346
Epoch [6/30], Batch [2700/6000], Loss: 0.0295
Epoch [6/30], Batch [2800/6000], Loss: 0.0809
Epoch [6/30], Batch [2900/6000], Loss: 0.2721
Epoch [6/30], Batch [3000/6000], Loss: 0.0396
Epoch [6/30], Batch [3100/6000], Loss: 0.1322
Epoch [6/30], Batch [3200/6000], Loss: 0.4196
Epoch [6/30], Batch [3300/6000], Loss: 0.0828
Epoch [6/30], Batch [3400/6000], Loss: 0.1161
Epoch [6/30], Batch [3500/6000], Loss: 0.0339
Epoch [6/30], Batch [3600/6000], Loss: 0.2268
Epoch [6/30], Batch [3700/6000], Loss: 0.1039
Epoch [6/30], Batch [3800/6000], Loss: 0.2552
Epoch [6/30], Batch [3900/6000], Loss: 0.0293
Epoch [6/30], Batch [4000/6000], Loss: 0.0384
Epoch [6/30], Batch [4100/6000], Loss: 0.0256
Epoch [6/30], Batch [4200/6000], Loss: 0.6994
Epoch [6/30], Batch [4300/6000], Loss: 0.7849
Epoch [6/30], Batch [4400/6000], Loss: 0.5366
Epoch [6/30], Batch [4500/6000], Loss: 0.2314
Epoch [6/30], Batch [4600/6000], Loss: 0.0558
Epoch [6/30], Batch [4700/6000], Loss: 0.0261
Epoch [6/30], Batch [4800/6000], Loss: 0.2721
Epoch [6/30], Batch [4900/6000], Loss: 0.0376
Epoch [6/30], Batch [5000/6000], Loss: 0.0398
Epoch [6/30], Batch [5100/6000], Loss: 0.0524
Epoch [6/30], Batch [5200/6000], Loss: 0.0326
Epoch [6/30], Batch [5300/6000], Loss: 0.0271
Epoch [6/30], Batch [5400/6000], Loss: 0.0516
Epoch [6/30], Batch [5500/6000], Loss: 0.0305
Epoch [6/30], Batch [5600/6000], Loss: 0.0213
Epoch [6/30], Batch [5700/6000], Loss: 0.0615
Epoch [6/30], Batch [5800/6000], Loss: 0.0324
Epoch [6/30], Batch [5900/6000], Loss: 0.1347
Epoch [6/30], Loss: 0.1045
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.0516
Epoch [7/30], Batch [100/6000], Loss: 0.0297
Epoch [7/30], Batch [200/6000], Loss: 0.0435
Epoch [7/30], Batch [300/6000], Loss: 0.0561
Epoch [7/30], Batch [400/6000], Loss: 0.0277
Epoch [7/30], Batch [500/6000], Loss: 0.7087
Epoch [7/30], Batch [600/6000], Loss: 0.2813
Epoch [7/30], Batch [700/6000], Loss: 0.0326
Epoch [7/30], Batch [800/6000], Loss: 0.0302
Epoch [7/30], Batch [900/6000], Loss: 0.0312
Epoch [7/30], Batch [1000/6000], Loss: 0.0336
Epoch [7/30], Batch [1100/6000], Loss: 0.0224
Epoch [7/30], Batch [1200/6000], Loss: 0.0331
Epoch [7/30], Batch [1300/6000], Loss: 0.0747
Epoch [7/30], Batch [1400/6000], Loss: 0.0785
Epoch [7/30], Batch [1500/6000], Loss: 0.0276
Epoch [7/30], Batch [1600/6000], Loss: 0.0341
Epoch [7/30], Batch [1700/6000], Loss: 0.0264
Epoch [7/30], Batch [1800/6000], Loss: 0.0462
Epoch [7/30], Batch [1900/6000], Loss: 0.0365
Epoch [7/30], Batch [2000/6000], Loss: 0.0301
Epoch [7/30], Batch [2100/6000], Loss: 0.2731
Epoch [7/30], Batch [2200/6000], Loss: 0.0684
Epoch [7/30], Batch [2300/6000], Loss: 0.0411
Epoch [7/30], Batch [2400/6000], Loss: 0.0207
Epoch [7/30], Batch [2500/6000], Loss: 0.0310
Epoch [7/30], Batch [2600/6000], Loss: 0.0263
Epoch [7/30], Batch [2700/6000], Loss: 0.0290
Epoch [7/30], Batch [2800/6000], Loss: 0.0294
Epoch [7/30], Batch [2900/6000], Loss: 0.2521
Epoch [7/30], Batch [3000/6000], Loss: 0.2256
Epoch [7/30], Batch [3100/6000], Loss: 0.7125
Epoch [7/30], Batch [3200/6000], Loss: 0.0657
Epoch [7/30], Batch [3300/6000], Loss: 0.0257
Epoch [7/30], Batch [3400/6000], Loss: 0.0369
Epoch [7/30], Batch [3500/6000], Loss: 0.1181
Epoch [7/30], Batch [3600/6000], Loss: 0.0323
Epoch [7/30], Batch [3700/6000], Loss: 0.0303
Epoch [7/30], Batch [3800/6000], Loss: 0.0313
Epoch [7/30], Batch [3900/6000], Loss: 0.0256
Epoch [7/30], Batch [4000/6000], Loss: 0.0546
Epoch [7/30], Batch [4100/6000], Loss: 0.0268
Epoch [7/30], Batch [4200/6000], Loss: 0.0326
Epoch [7/30], Batch [4300/6000], Loss: 0.0543
Epoch [7/30], Batch [4400/6000], Loss: 0.0236
Epoch [7/30], Batch [4500/6000], Loss: 0.0300
Epoch [7/30], Batch [4600/6000], Loss: 0.0335
Epoch [7/30], Batch [4700/6000], Loss: 0.0243
Epoch [7/30], Batch [4800/6000], Loss: 0.0246
Epoch [7/30], Batch [4900/6000], Loss: 0.0297
Epoch [7/30], Batch [5000/6000], Loss: 0.0454
Epoch [7/30], Batch [5100/6000], Loss: 0.2518
Epoch [7/30], Batch [5200/6000], Loss: 0.0609
Epoch [7/30], Batch [5300/6000], Loss: 0.0233
Epoch [7/30], Batch [5400/6000], Loss: 0.0304
Epoch [7/30], Batch [5500/6000], Loss: 0.4605
Epoch [7/30], Batch [5600/6000], Loss: 0.1561
Epoch [7/30], Batch [5700/6000], Loss: 0.0327
Epoch [7/30], Batch [5800/6000], Loss: 0.0291
Epoch [7/30], Batch [5900/6000], Loss: 0.0465
Epoch [7/30], Loss: 0.0926
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.0259
Epoch [8/30], Batch [100/6000], Loss: 0.0460
Epoch [8/30], Batch [200/6000], Loss: 0.0586
Epoch [8/30], Batch [300/6000], Loss: 0.0453
Epoch [8/30], Batch [400/6000], Loss: 0.0287
Epoch [8/30], Batch [500/6000], Loss: 0.0270
Epoch [8/30], Batch [600/6000], Loss: 0.0321
Epoch [8/30], Batch [700/6000], Loss: 0.0707
Epoch [8/30], Batch [800/6000], Loss: 0.0396
Epoch [8/30], Batch [900/6000], Loss: 0.0375
Epoch [8/30], Batch [1000/6000], Loss: 0.0864
Epoch [8/30], Batch [1100/6000], Loss: 0.0334
Epoch [8/30], Batch [1200/6000], Loss: 0.0310
Epoch [8/30], Batch [1300/6000], Loss: 0.0432
Epoch [8/30], Batch [1400/6000], Loss: 0.0380
Epoch [8/30], Batch [1500/6000], Loss: 0.0826
Epoch [8/30], Batch [1600/6000], Loss: 0.0297
Epoch [8/30], Batch [1700/6000], Loss: 0.0454
Epoch [8/30], Batch [1800/6000], Loss: 0.0418
Epoch [8/30], Batch [1900/6000], Loss: 0.0438
Epoch [8/30], Batch [2000/6000], Loss: 0.0384
Epoch [8/30], Batch [2100/6000], Loss: 0.0364
Epoch [8/30], Batch [2200/6000], Loss: 0.0320
Epoch [8/30], Batch [2300/6000], Loss: 0.0329
Epoch [8/30], Batch [2400/6000], Loss: 0.0279
Epoch [8/30], Batch [2500/6000], Loss: 0.0252
Epoch [8/30], Batch [2600/6000], Loss: 0.1799
Epoch [8/30], Batch [2700/6000], Loss: 0.0391
Epoch [8/30], Batch [2800/6000], Loss: 0.0277
Epoch [8/30], Batch [2900/6000], Loss: 0.0407
Epoch [8/30], Batch [3000/6000], Loss: 0.1787
Epoch [8/30], Batch [3100/6000], Loss: 0.0525
Epoch [8/30], Batch [3200/6000], Loss: 0.0291
Epoch [8/30], Batch [3300/6000], Loss: 0.0341
Epoch [8/30], Batch [3400/6000], Loss: 0.6120
Epoch [8/30], Batch [3500/6000], Loss: 0.0259
Epoch [8/30], Batch [3600/6000], Loss: 0.0295
Epoch [8/30], Batch [3700/6000], Loss: 0.1457
Epoch [8/30], Batch [3800/6000], Loss: 0.0314
Epoch [8/30], Batch [3900/6000], Loss: 0.1789
Epoch [8/30], Batch [4000/6000], Loss: 0.0241
Epoch [8/30], Batch [4100/6000], Loss: 0.0245
Epoch [8/30], Batch [4200/6000], Loss: 0.0249
Epoch [8/30], Batch [4300/6000], Loss: 0.0388
Epoch [8/30], Batch [4400/6000], Loss: 0.0909
Epoch [8/30], Batch [4500/6000], Loss: 0.0646
Epoch [8/30], Batch [4600/6000], Loss: 0.0255
Epoch [8/30], Batch [4700/6000], Loss: 0.0295
Epoch [8/30], Batch [4800/6000], Loss: 0.0286
Epoch [8/30], Batch [4900/6000], Loss: 0.0539
Epoch [8/30], Batch [5000/6000], Loss: 0.0333
Epoch [8/30], Batch [5100/6000], Loss: 0.2524
Epoch [8/30], Batch [5200/6000], Loss: 0.0603
Epoch [8/30], Batch [5300/6000], Loss: 0.0272
Epoch [8/30], Batch [5400/6000], Loss: 0.3182
Epoch [8/30], Batch [5500/6000], Loss: 0.0427
Epoch [8/30], Batch [5600/6000], Loss: 0.0547
Epoch [8/30], Batch [5700/6000], Loss: 0.0267
Epoch [8/30], Batch [5800/6000], Loss: 0.0374
Epoch [8/30], Batch [5900/6000], Loss: 0.0380
Epoch [8/30], Loss: 0.0822
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.0399
Epoch [9/30], Batch [100/6000], Loss: 0.0269
Epoch [9/30], Batch [200/6000], Loss: 0.0229
Epoch [9/30], Batch [300/6000], Loss: 0.0313
Epoch [9/30], Batch [400/6000], Loss: 0.0305
Epoch [9/30], Batch [500/6000], Loss: 0.0293
Epoch [9/30], Batch [600/6000], Loss: 0.0242
Epoch [9/30], Batch [700/6000], Loss: 0.4218
Epoch [9/30], Batch [800/6000], Loss: 0.0272
Epoch [9/30], Batch [900/6000], Loss: 0.0323
Epoch [9/30], Batch [1000/6000], Loss: 0.0317
Epoch [9/30], Batch [1100/6000], Loss: 0.0420
Epoch [9/30], Batch [1200/6000], Loss: 0.0298
Epoch [9/30], Batch [1300/6000], Loss: 0.1075
Epoch [9/30], Batch [1400/6000], Loss: 0.0430
Epoch [9/30], Batch [1500/6000], Loss: 0.0563
Epoch [9/30], Batch [1600/6000], Loss: 0.0262
Epoch [9/30], Batch [1700/6000], Loss: 0.0279
Epoch [9/30], Batch [1800/6000], Loss: 0.0338
Epoch [9/30], Batch [1900/6000], Loss: 0.0202
Epoch [9/30], Batch [2000/6000], Loss: 0.0676
Epoch [9/30], Batch [2100/6000], Loss: 0.1921
Epoch [9/30], Batch [2200/6000], Loss: 0.0253
Epoch [9/30], Batch [2300/6000], Loss: 0.0236
Epoch [9/30], Batch [2400/6000], Loss: 0.0568
Epoch [9/30], Batch [2500/6000], Loss: 0.0251
Epoch [9/30], Batch [2600/6000], Loss: 0.0267
Epoch [9/30], Batch [2700/6000], Loss: 0.0438
Epoch [9/30], Batch [2800/6000], Loss: 0.0337
Epoch [9/30], Batch [2900/6000], Loss: 0.0226
Epoch [9/30], Batch [3000/6000], Loss: 0.0319
Epoch [9/30], Batch [3100/6000], Loss: 0.1874
Epoch [9/30], Batch [3200/6000], Loss: 0.0234
Epoch [9/30], Batch [3300/6000], Loss: 0.0221
Epoch [9/30], Batch [3400/6000], Loss: 0.0378
Epoch [9/30], Batch [3500/6000], Loss: 0.3862
Epoch [9/30], Batch [3600/6000], Loss: 0.2000
Epoch [9/30], Batch [3700/6000], Loss: 0.0217
Epoch [9/30], Batch [3800/6000], Loss: 0.3408
Epoch [9/30], Batch [3900/6000], Loss: 0.1067
Epoch [9/30], Batch [4000/6000], Loss: 0.0201
Epoch [9/30], Batch [4100/6000], Loss: 0.0217
Epoch [9/30], Batch [4200/6000], Loss: 0.0226
Epoch [9/30], Batch [4300/6000], Loss: 0.0363
Epoch [9/30], Batch [4400/6000], Loss: 0.0305
Epoch [9/30], Batch [4500/6000], Loss: 0.0636
Epoch [9/30], Batch [4600/6000], Loss: 0.0409
Epoch [9/30], Batch [4700/6000], Loss: 0.0335
Epoch [9/30], Batch [4800/6000], Loss: 0.0830
Epoch [9/30], Batch [4900/6000], Loss: 0.2132
Epoch [9/30], Batch [5000/6000], Loss: 0.0632
Epoch [9/30], Batch [5100/6000], Loss: 0.4277
Epoch [9/30], Batch [5200/6000], Loss: 0.0354
Epoch [9/30], Batch [5300/6000], Loss: 0.0285
Epoch [9/30], Batch [5400/6000], Loss: 0.0227
Epoch [9/30], Batch [5500/6000], Loss: 0.0281
Epoch [9/30], Batch [5600/6000], Loss: 0.0237
Epoch [9/30], Batch [5700/6000], Loss: 0.0385
Epoch [9/30], Batch [5800/6000], Loss: 0.0359
Epoch [9/30], Batch [5900/6000], Loss: 0.0255
Epoch [9/30], Loss: 0.0744
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.0214
Epoch [10/30], Batch [100/6000], Loss: 0.1187
Epoch [10/30], Batch [200/6000], Loss: 0.0295
Epoch [10/30], Batch [300/6000], Loss: 0.0271
Epoch [10/30], Batch [400/6000], Loss: 0.0237
Epoch [10/30], Batch [500/6000], Loss: 0.1217
Epoch [10/30], Batch [600/6000], Loss: 0.0229
Epoch [10/30], Batch [700/6000], Loss: 0.0406
Epoch [10/30], Batch [800/6000], Loss: 0.0416
Epoch [10/30], Batch [900/6000], Loss: 0.0258
Epoch [10/30], Batch [1000/6000], Loss: 0.0184
Epoch [10/30], Batch [1100/6000], Loss: 0.0208
Epoch [10/30], Batch [1200/6000], Loss: 0.0266
Epoch [10/30], Batch [1300/6000], Loss: 0.0230
Epoch [10/30], Batch [1400/6000], Loss: 0.0224
Epoch [10/30], Batch [1500/6000], Loss: 0.1718
Epoch [10/30], Batch [1600/6000], Loss: 0.0451
Epoch [10/30], Batch [1700/6000], Loss: 0.0287
Epoch [10/30], Batch [1800/6000], Loss: 0.0227
Epoch [10/30], Batch [1900/6000], Loss: 0.2064
Epoch [10/30], Batch [2000/6000], Loss: 0.0541
Epoch [10/30], Batch [2100/6000], Loss: 0.0244
Epoch [10/30], Batch [2200/6000], Loss: 0.0313
Epoch [10/30], Batch [2300/6000], Loss: 0.6464
Epoch [10/30], Batch [2400/6000], Loss: 0.0195
Epoch [10/30], Batch [2500/6000], Loss: 0.0230
Epoch [10/30], Batch [2600/6000], Loss: 0.3557
Epoch [10/30], Batch [2700/6000], Loss: 0.0239
Epoch [10/30], Batch [2800/6000], Loss: 0.0312
Epoch [10/30], Batch [2900/6000], Loss: 0.0363
Epoch [10/30], Batch [3000/6000], Loss: 0.1108
Epoch [10/30], Batch [3100/6000], Loss: 0.0358
Epoch [10/30], Batch [3200/6000], Loss: 0.0492
Epoch [10/30], Batch [3300/6000], Loss: 0.0458
Epoch [10/30], Batch [3400/6000], Loss: 0.0272
Epoch [10/30], Batch [3500/6000], Loss: 0.1082
Epoch [10/30], Batch [3600/6000], Loss: 0.0248
Epoch [10/30], Batch [3700/6000], Loss: 0.7909
Epoch [10/30], Batch [3800/6000], Loss: 0.0388
Epoch [10/30], Batch [3900/6000], Loss: 0.0993
Epoch [10/30], Batch [4000/6000], Loss: 0.0225
Epoch [10/30], Batch [4100/6000], Loss: 0.0171
Epoch [10/30], Batch [4200/6000], Loss: 0.0266
Epoch [10/30], Batch [4300/6000], Loss: 0.0222
Epoch [10/30], Batch [4400/6000], Loss: 0.0209
Epoch [10/30], Batch [4500/6000], Loss: 0.0237
Epoch [10/30], Batch [4600/6000], Loss: 0.0381
Epoch [10/30], Batch [4700/6000], Loss: 0.1007
Epoch [10/30], Batch [4800/6000], Loss: 0.0228
Epoch [10/30], Batch [4900/6000], Loss: 0.0161
Epoch [10/30], Batch [5000/6000], Loss: 0.0235
Epoch [10/30], Batch [5100/6000], Loss: 0.0278
Epoch [10/30], Batch [5200/6000], Loss: 0.0215
Epoch [10/30], Batch [5300/6000], Loss: 0.0309
Epoch [10/30], Batch [5400/6000], Loss: 0.0566
Epoch [10/30], Batch [5500/6000], Loss: 0.0304
Epoch [10/30], Batch [5600/6000], Loss: 0.0211
Epoch [10/30], Batch [5700/6000], Loss: 0.0773
Epoch [10/30], Batch [5800/6000], Loss: 0.1143
Epoch [10/30], Batch [5900/6000], Loss: 0.0204
Epoch [10/30], Loss: 0.0679
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.0286
Epoch [11/30], Batch [100/6000], Loss: 0.2346
Epoch [11/30], Batch [200/6000], Loss: 0.0339
Epoch [11/30], Batch [300/6000], Loss: 0.2691
Epoch [11/30], Batch [400/6000], Loss: 0.0231
Epoch [11/30], Batch [500/6000], Loss: 0.0304
Epoch [11/30], Batch [600/6000], Loss: 0.1336
Epoch [11/30], Batch [700/6000], Loss: 0.0305
Epoch [11/30], Batch [800/6000], Loss: 0.0276
Epoch [11/30], Batch [900/6000], Loss: 0.0339
Epoch [11/30], Batch [1000/6000], Loss: 0.0191
Epoch [11/30], Batch [1100/6000], Loss: 0.0234
Epoch [11/30], Batch [1200/6000], Loss: 0.0248
Epoch [11/30], Batch [1300/6000], Loss: 0.0364
Epoch [11/30], Batch [1400/6000], Loss: 0.0348
Epoch [11/30], Batch [1500/6000], Loss: 0.1102
Epoch [11/30], Batch [1600/6000], Loss: 0.0229
Epoch [11/30], Batch [1700/6000], Loss: 0.0568
Epoch [11/30], Batch [1800/6000], Loss: 0.0260
Epoch [11/30], Batch [1900/6000], Loss: 0.0247
Epoch [11/30], Batch [2000/6000], Loss: 0.0276
Epoch [11/30], Batch [2100/6000], Loss: 0.0563
Epoch [11/30], Batch [2200/6000], Loss: 0.0228
Epoch [11/30], Batch [2300/6000], Loss: 0.2183
Epoch [11/30], Batch [2400/6000], Loss: 0.2237
Epoch [11/30], Batch [2500/6000], Loss: 0.0246
Epoch [11/30], Batch [2600/6000], Loss: 0.0280
Epoch [11/30], Batch [2700/6000], Loss: 0.0354
Epoch [11/30], Batch [2800/6000], Loss: 0.0480
Epoch [11/30], Batch [2900/6000], Loss: 0.0262
Epoch [11/30], Batch [3000/6000], Loss: 0.0360
Epoch [11/30], Batch [3100/6000], Loss: 0.0212
Epoch [11/30], Batch [3200/6000], Loss: 0.0276
Epoch [11/30], Batch [3300/6000], Loss: 0.0636
Epoch [11/30], Batch [3400/6000], Loss: 0.0309
Epoch [11/30], Batch [3500/6000], Loss: 0.0983
Epoch [11/30], Batch [3600/6000], Loss: 0.0270
Epoch [11/30], Batch [3700/6000], Loss: 0.0292
Epoch [11/30], Batch [3800/6000], Loss: 0.5829
Epoch [11/30], Batch [3900/6000], Loss: 0.0229
Epoch [11/30], Batch [4000/6000], Loss: 0.0261
Epoch [11/30], Batch [4100/6000], Loss: 0.0293
Epoch [11/30], Batch [4200/6000], Loss: 0.0184
Epoch [11/30], Batch [4300/6000], Loss: 0.0185
Epoch [11/30], Batch [4400/6000], Loss: 0.0234
Epoch [11/30], Batch [4500/6000], Loss: 0.0288
Epoch [11/30], Batch [4600/6000], Loss: 0.0434
Epoch [11/30], Batch [4700/6000], Loss: 0.0264
Epoch [11/30], Batch [4800/6000], Loss: 0.0262
Epoch [11/30], Batch [4900/6000], Loss: 0.0193
Epoch [11/30], Batch [5000/6000], Loss: 0.0674
Epoch [11/30], Batch [5100/6000], Loss: 0.1032
Epoch [11/30], Batch [5200/6000], Loss: 0.0415
Epoch [11/30], Batch [5300/6000], Loss: 0.0230
Epoch [11/30], Batch [5400/6000], Loss: 0.0319
Epoch [11/30], Batch [5500/6000], Loss: 0.0278
Epoch [11/30], Batch [5600/6000], Loss: 0.0198
Epoch [11/30], Batch [5700/6000], Loss: 0.0640
Epoch [11/30], Batch [5800/6000], Loss: 0.0256
Epoch [11/30], Batch [5900/6000], Loss: 0.0283
Epoch [11/30], Loss: 0.0622
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.0204
Epoch [12/30], Batch [100/6000], Loss: 0.0252
Epoch [12/30], Batch [200/6000], Loss: 0.0262
Epoch [12/30], Batch [300/6000], Loss: 0.0469
Epoch [12/30], Batch [400/6000], Loss: 0.0187
Epoch [12/30], Batch [500/6000], Loss: 0.0254
Epoch [12/30], Batch [600/6000], Loss: 0.0342
Epoch [12/30], Batch [700/6000], Loss: 0.0445
Epoch [12/30], Batch [800/6000], Loss: 0.0911
Epoch [12/30], Batch [900/6000], Loss: 0.0212
Epoch [12/30], Batch [1000/6000], Loss: 0.0471
Epoch [12/30], Batch [1100/6000], Loss: 0.0221
Epoch [12/30], Batch [1200/6000], Loss: 0.0208
Epoch [12/30], Batch [1300/6000], Loss: 0.0480
Epoch [12/30], Batch [1400/6000], Loss: 0.3735
Epoch [12/30], Batch [1500/6000], Loss: 0.0227
Epoch [12/30], Batch [1600/6000], Loss: 0.0275
Epoch [12/30], Batch [1700/6000], Loss: 0.0185
Epoch [12/30], Batch [1800/6000], Loss: 0.0211
Epoch [12/30], Batch [1900/6000], Loss: 0.0245
Epoch [12/30], Batch [2000/6000], Loss: 0.0200
Epoch [12/30], Batch [2100/6000], Loss: 0.0421
Epoch [12/30], Batch [2200/6000], Loss: 0.0231
Epoch [12/30], Batch [2300/6000], Loss: 0.4008
Epoch [12/30], Batch [2400/6000], Loss: 0.0197
Epoch [12/30], Batch [2500/6000], Loss: 0.0279
Epoch [12/30], Batch [2600/6000], Loss: 0.0253
Epoch [12/30], Batch [2700/6000], Loss: 0.0253
Epoch [12/30], Batch [2800/6000], Loss: 0.0206
Epoch [12/30], Batch [2900/6000], Loss: 0.0218
Epoch [12/30], Batch [3000/6000], Loss: 0.1680
Epoch [12/30], Batch [3100/6000], Loss: 0.0242
Epoch [12/30], Batch [3200/6000], Loss: 0.0240
Epoch [12/30], Batch [3300/6000], Loss: 0.0713
Epoch [12/30], Batch [3400/6000], Loss: 0.0269
Epoch [12/30], Batch [3500/6000], Loss: 0.0228
Epoch [12/30], Batch [3600/6000], Loss: 0.0244
Epoch [12/30], Batch [3700/6000], Loss: 0.0268
Epoch [12/30], Batch [3800/6000], Loss: 0.0230
Epoch [12/30], Batch [3900/6000], Loss: 0.0252
Epoch [12/30], Batch [4000/6000], Loss: 0.1046
Epoch [12/30], Batch [4100/6000], Loss: 0.1793
Epoch [12/30], Batch [4200/6000], Loss: 0.0198
Epoch [12/30], Batch [4300/6000], Loss: 0.7707
Epoch [12/30], Batch [4400/6000], Loss: 0.0169
Epoch [12/30], Batch [4500/6000], Loss: 0.0286
Epoch [12/30], Batch [4600/6000], Loss: 0.0310
Epoch [12/30], Batch [4700/6000], Loss: 0.0252
Epoch [12/30], Batch [4800/6000], Loss: 0.0264
Epoch [12/30], Batch [4900/6000], Loss: 0.0458
Epoch [12/30], Batch [5000/6000], Loss: 0.1726
Epoch [12/30], Batch [5100/6000], Loss: 0.0225
Epoch [12/30], Batch [5200/6000], Loss: 0.0229
Epoch [12/30], Batch [5300/6000], Loss: 0.0300
Epoch [12/30], Batch [5400/6000], Loss: 0.0280
Epoch [12/30], Batch [5500/6000], Loss: 0.0249
Epoch [12/30], Batch [5600/6000], Loss: 0.5468
Epoch [12/30], Batch [5700/6000], Loss: 0.0298
Epoch [12/30], Batch [5800/6000], Loss: 0.0684
Epoch [12/30], Batch [5900/6000], Loss: 0.0396
Epoch [12/30], Loss: 0.0564
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.0231
Epoch [13/30], Batch [100/6000], Loss: 0.0248
Epoch [13/30], Batch [200/6000], Loss: 0.0243
Epoch [13/30], Batch [300/6000], Loss: 0.0165
Epoch [13/30], Batch [400/6000], Loss: 0.0212
Epoch [13/30], Batch [500/6000], Loss: 0.0411
Epoch [13/30], Batch [600/6000], Loss: 0.0291
Epoch [13/30], Batch [700/6000], Loss: 0.0281
Epoch [13/30], Batch [800/6000], Loss: 0.0273
Epoch [13/30], Batch [900/6000], Loss: 0.1132
Epoch [13/30], Batch [1000/6000], Loss: 0.0303
Epoch [13/30], Batch [1100/6000], Loss: 0.0459
Epoch [13/30], Batch [1200/6000], Loss: 0.0393
Epoch [13/30], Batch [1300/6000], Loss: 0.0622
Epoch [13/30], Batch [1400/6000], Loss: 0.0285
Epoch [13/30], Batch [1500/6000], Loss: 0.0244
Epoch [13/30], Batch [1600/6000], Loss: 0.0272
Epoch [13/30], Batch [1700/6000], Loss: 0.0200
Epoch [13/30], Batch [1800/6000], Loss: 0.0269
Epoch [13/30], Batch [1900/6000], Loss: 0.0277
Epoch [13/30], Batch [2000/6000], Loss: 0.0238
Epoch [13/30], Batch [2100/6000], Loss: 0.0245
Epoch [13/30], Batch [2200/6000], Loss: 0.0251
Epoch [13/30], Batch [2300/6000], Loss: 0.0230
Epoch [13/30], Batch [2400/6000], Loss: 0.0248
Epoch [13/30], Batch [2500/6000], Loss: 0.0213
Epoch [13/30], Batch [2600/6000], Loss: 0.0415
Epoch [13/30], Batch [2700/6000], Loss: 0.0256
Epoch [13/30], Batch [2800/6000], Loss: 0.0396
Epoch [13/30], Batch [2900/6000], Loss: 0.0682
Epoch [13/30], Batch [3000/6000], Loss: 0.0504
Epoch [13/30], Batch [3100/6000], Loss: 0.0201
Epoch [13/30], Batch [3200/6000], Loss: 0.0355
Epoch [13/30], Batch [3300/6000], Loss: 0.0283
Epoch [13/30], Batch [3400/6000], Loss: 0.0205
Epoch [13/30], Batch [3500/6000], Loss: 0.0267
Epoch [13/30], Batch [3600/6000], Loss: 0.0233
Epoch [13/30], Batch [3700/6000], Loss: 0.0230
Epoch [13/30], Batch [3800/6000], Loss: 0.0269
Epoch [13/30], Batch [3900/6000], Loss: 0.0245
Epoch [13/30], Batch [4000/6000], Loss: 0.0230
Epoch [13/30], Batch [4100/6000], Loss: 0.1008
Epoch [13/30], Batch [4200/6000], Loss: 0.0238
Epoch [13/30], Batch [4300/6000], Loss: 0.0170
Epoch [13/30], Batch [4400/6000], Loss: 0.0202
Epoch [13/30], Batch [4500/6000], Loss: 0.0282
Epoch [13/30], Batch [4600/6000], Loss: 0.0263
Epoch [13/30], Batch [4700/6000], Loss: 0.0182
Epoch [13/30], Batch [4800/6000], Loss: 0.0229
Epoch [13/30], Batch [4900/6000], Loss: 0.0218
Epoch [13/30], Batch [5000/6000], Loss: 0.0212
Epoch [13/30], Batch [5100/6000], Loss: 0.0421
Epoch [13/30], Batch [5200/6000], Loss: 0.0251
Epoch [13/30], Batch [5300/6000], Loss: 0.0231
Epoch [13/30], Batch [5400/6000], Loss: 0.0239
Epoch [13/30], Batch [5500/6000], Loss: 0.0214
Epoch [13/30], Batch [5600/6000], Loss: 0.0518
Epoch [13/30], Batch [5700/6000], Loss: 0.0380
Epoch [13/30], Batch [5800/6000], Loss: 0.0222
Epoch [13/30], Batch [5900/6000], Loss: 0.0226
Epoch [13/30], Loss: 0.0524
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.0230
Epoch [14/30], Batch [100/6000], Loss: 0.0236
Epoch [14/30], Batch [200/6000], Loss: 0.0242
Epoch [14/30], Batch [300/6000], Loss: 0.0184
Epoch [14/30], Batch [400/6000], Loss: 0.0225
Epoch [14/30], Batch [500/6000], Loss: 0.0409
Epoch [14/30], Batch [600/6000], Loss: 0.0215
Epoch [14/30], Batch [700/6000], Loss: 0.0195
Epoch [14/30], Batch [800/6000], Loss: 0.0252
Epoch [14/30], Batch [900/6000], Loss: 0.0209
Epoch [14/30], Batch [1000/6000], Loss: 0.0216
Epoch [14/30], Batch [1100/6000], Loss: 0.0194
Epoch [14/30], Batch [1200/6000], Loss: 0.0409
Epoch [14/30], Batch [1300/6000], Loss: 0.0229
Epoch [14/30], Batch [1400/6000], Loss: 0.0270
Epoch [14/30], Batch [1500/6000], Loss: 0.0399
Epoch [14/30], Batch [1600/6000], Loss: 0.0258
Epoch [14/30], Batch [1700/6000], Loss: 0.0719
Epoch [14/30], Batch [1800/6000], Loss: 0.0224
Epoch [14/30], Batch [1900/6000], Loss: 0.0238
Epoch [14/30], Batch [2000/6000], Loss: 0.0251
Epoch [14/30], Batch [2100/6000], Loss: 0.0230
Epoch [14/30], Batch [2200/6000], Loss: 0.0397
Epoch [14/30], Batch [2300/6000], Loss: 0.0264
Epoch [14/30], Batch [2400/6000], Loss: 0.1000
Epoch [14/30], Batch [2500/6000], Loss: 0.0196
Epoch [14/30], Batch [2600/6000], Loss: 0.0209
Epoch [14/30], Batch [2700/6000], Loss: 0.0196
Epoch [14/30], Batch [2800/6000], Loss: 0.0158
Epoch [14/30], Batch [2900/6000], Loss: 0.0218
Epoch [14/30], Batch [3000/6000], Loss: 0.0462
Epoch [14/30], Batch [3100/6000], Loss: 0.0288
Epoch [14/30], Batch [3200/6000], Loss: 0.0197
Epoch [14/30], Batch [3300/6000], Loss: 0.0216
Epoch [14/30], Batch [3400/6000], Loss: 0.0245
Epoch [14/30], Batch [3500/6000], Loss: 0.0348
Epoch [14/30], Batch [3600/6000], Loss: 0.0381
Epoch [14/30], Batch [3700/6000], Loss: 0.0236
Epoch [14/30], Batch [3800/6000], Loss: 0.0387
Epoch [14/30], Batch [3900/6000], Loss: 0.0234
Epoch [14/30], Batch [4000/6000], Loss: 0.0231
Epoch [14/30], Batch [4100/6000], Loss: 0.0652
Epoch [14/30], Batch [4200/6000], Loss: 0.0193
Epoch [14/30], Batch [4300/6000], Loss: 0.0234
Epoch [14/30], Batch [4400/6000], Loss: 0.0227
Epoch [14/30], Batch [4500/6000], Loss: 0.0177
Epoch [14/30], Batch [4600/6000], Loss: 0.0297
Epoch [14/30], Batch [4700/6000], Loss: 0.0199
Epoch [14/30], Batch [4800/6000], Loss: 0.0159
Epoch [14/30], Batch [4900/6000], Loss: 0.0286
Epoch [14/30], Batch [5000/6000], Loss: 0.0175
Epoch [14/30], Batch [5100/6000], Loss: 0.0256
Epoch [14/30], Batch [5200/6000], Loss: 0.0180
Epoch [14/30], Batch [5300/6000], Loss: 0.0336
Epoch [14/30], Batch [5400/6000], Loss: 0.0221
Epoch [14/30], Batch [5500/6000], Loss: 0.0262
Epoch [14/30], Batch [5600/6000], Loss: 0.0207
Epoch [14/30], Batch [5700/6000], Loss: 0.5657
Epoch [14/30], Batch [5800/6000], Loss: 0.0789
Epoch [14/30], Batch [5900/6000], Loss: 0.0215
Epoch [14/30], Loss: 0.0490
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.0189
Epoch [15/30], Batch [100/6000], Loss: 0.0234
Epoch [15/30], Batch [200/6000], Loss: 0.0259
Epoch [15/30], Batch [300/6000], Loss: 0.0187
Epoch [15/30], Batch [400/6000], Loss: 0.0290
Epoch [15/30], Batch [500/6000], Loss: 0.0699
Epoch [15/30], Batch [600/6000], Loss: 0.1263
Epoch [15/30], Batch [700/6000], Loss: 0.0286
Epoch [15/30], Batch [800/6000], Loss: 0.4077
Epoch [15/30], Batch [900/6000], Loss: 0.0191
Epoch [15/30], Batch [1000/6000], Loss: 0.0230
Epoch [15/30], Batch [1100/6000], Loss: 0.0246
Epoch [15/30], Batch [1200/6000], Loss: 0.0228
Epoch [15/30], Batch [1300/6000], Loss: 0.0209
Epoch [15/30], Batch [1400/6000], Loss: 0.0177
Epoch [15/30], Batch [1500/6000], Loss: 0.0207
Epoch [15/30], Batch [1600/6000], Loss: 0.0197
Epoch [15/30], Batch [1700/6000], Loss: 0.0407
Epoch [15/30], Batch [1800/6000], Loss: 0.0170
Epoch [15/30], Batch [1900/6000], Loss: 0.1960
Epoch [15/30], Batch [2000/6000], Loss: 0.0221
Epoch [15/30], Batch [2100/6000], Loss: 0.3117
Epoch [15/30], Batch [2200/6000], Loss: 0.0477
Epoch [15/30], Batch [2300/6000], Loss: 0.0240
Epoch [15/30], Batch [2400/6000], Loss: 0.0319
Epoch [15/30], Batch [2500/6000], Loss: 0.0456
Epoch [15/30], Batch [2600/6000], Loss: 0.0203
Epoch [15/30], Batch [2700/6000], Loss: 0.0206
Epoch [15/30], Batch [2800/6000], Loss: 0.0219
Epoch [15/30], Batch [2900/6000], Loss: 0.0177
Epoch [15/30], Batch [3000/6000], Loss: 0.0270
Epoch [15/30], Batch [3100/6000], Loss: 0.0257
Epoch [15/30], Batch [3200/6000], Loss: 0.1568
Epoch [15/30], Batch [3300/6000], Loss: 0.0214
Epoch [15/30], Batch [3400/6000], Loss: 0.0225
Epoch [15/30], Batch [3500/6000], Loss: 0.0218
Epoch [15/30], Batch [3600/6000], Loss: 0.0858
Epoch [15/30], Batch [3700/6000], Loss: 0.0456
Epoch [15/30], Batch [3800/6000], Loss: 0.0248
Epoch [15/30], Batch [3900/6000], Loss: 0.0182
Epoch [15/30], Batch [4000/6000], Loss: 0.0222
Epoch [15/30], Batch [4100/6000], Loss: 0.0214
Epoch [15/30], Batch [4200/6000], Loss: 0.0184
Epoch [15/30], Batch [4300/6000], Loss: 0.0188
Epoch [15/30], Batch [4400/6000], Loss: 0.0301
Epoch [15/30], Batch [4500/6000], Loss: 0.0210
Epoch [15/30], Batch [4600/6000], Loss: 0.4100
Epoch [15/30], Batch [4700/6000], Loss: 0.0203
Epoch [15/30], Batch [4800/6000], Loss: 0.3730
Epoch [15/30], Batch [4900/6000], Loss: 0.0217
Epoch [15/30], Batch [5000/6000], Loss: 0.0256
Epoch [15/30], Batch [5100/6000], Loss: 0.0212
Epoch [15/30], Batch [5200/6000], Loss: 0.0727
Epoch [15/30], Batch [5300/6000], Loss: 0.1068
Epoch [15/30], Batch [5400/6000], Loss: 0.0225
Epoch [15/30], Batch [5500/6000], Loss: 0.0245
Epoch [15/30], Batch [5600/6000], Loss: 0.0209
Epoch [15/30], Batch [5700/6000], Loss: 0.0245
Epoch [15/30], Batch [5800/6000], Loss: 0.2343
Epoch [15/30], Batch [5900/6000], Loss: 0.0234
Epoch [15/30], Loss: 0.0459
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.0195
Epoch [16/30], Batch [100/6000], Loss: 0.0246
Epoch [16/30], Batch [200/6000], Loss: 0.0230
Epoch [16/30], Batch [300/6000], Loss: 0.0171
Epoch [16/30], Batch [400/6000], Loss: 0.0218
Epoch [16/30], Batch [500/6000], Loss: 0.0229
Epoch [16/30], Batch [600/6000], Loss: 0.0373
Epoch [16/30], Batch [700/6000], Loss: 0.0151
Epoch [16/30], Batch [800/6000], Loss: 0.0186
Epoch [16/30], Batch [900/6000], Loss: 0.0182
Epoch [16/30], Batch [1000/6000], Loss: 0.0222
Epoch [16/30], Batch [1100/6000], Loss: 0.0296
Epoch [16/30], Batch [1200/6000], Loss: 0.0199
Epoch [16/30], Batch [1300/6000], Loss: 0.0195
Epoch [16/30], Batch [1400/6000], Loss: 0.0286
Epoch [16/30], Batch [1500/6000], Loss: 0.0239
Epoch [16/30], Batch [1600/6000], Loss: 0.0222
Epoch [16/30], Batch [1700/6000], Loss: 0.0203
Epoch [16/30], Batch [1800/6000], Loss: 0.0284
Epoch [16/30], Batch [1900/6000], Loss: 0.0162
Epoch [16/30], Batch [2000/6000], Loss: 0.0249
Epoch [16/30], Batch [2100/6000], Loss: 0.0205
Epoch [16/30], Batch [2200/6000], Loss: 0.0245
Epoch [16/30], Batch [2300/6000], Loss: 0.0238
Epoch [16/30], Batch [2400/6000], Loss: 0.0204
Epoch [16/30], Batch [2500/6000], Loss: 0.0182
Epoch [16/30], Batch [2600/6000], Loss: 0.0141
Epoch [16/30], Batch [2700/6000], Loss: 0.0450
Epoch [16/30], Batch [2800/6000], Loss: 0.0214
Epoch [16/30], Batch [2900/6000], Loss: 0.0213
Epoch [16/30], Batch [3000/6000], Loss: 0.0208
Epoch [16/30], Batch [3100/6000], Loss: 0.0186
Epoch [16/30], Batch [3200/6000], Loss: 0.0241
Epoch [16/30], Batch [3300/6000], Loss: 0.0256
Epoch [16/30], Batch [3400/6000], Loss: 0.0206
Epoch [16/30], Batch [3500/6000], Loss: 0.0281
Epoch [16/30], Batch [3600/6000], Loss: 0.2121
Epoch [16/30], Batch [3700/6000], Loss: 0.8195
Epoch [16/30], Batch [3800/6000], Loss: 0.0275
Epoch [16/30], Batch [3900/6000], Loss: 0.0193
Epoch [16/30], Batch [4000/6000], Loss: 0.0270
Epoch [16/30], Batch [4100/6000], Loss: 0.0251
Epoch [16/30], Batch [4200/6000], Loss: 0.0200
Epoch [16/30], Batch [4300/6000], Loss: 0.0259
Epoch [16/30], Batch [4400/6000], Loss: 0.0172
Epoch [16/30], Batch [4500/6000], Loss: 0.0190
Epoch [16/30], Batch [4600/6000], Loss: 0.0346
Epoch [16/30], Batch [4700/6000], Loss: 0.5036
Epoch [16/30], Batch [4800/6000], Loss: 0.0194
Epoch [16/30], Batch [4900/6000], Loss: 0.0237
Epoch [16/30], Batch [5000/6000], Loss: 0.0207
Epoch [16/30], Batch [5100/6000], Loss: 0.0241
Epoch [16/30], Batch [5200/6000], Loss: 0.0223
Epoch [16/30], Batch [5300/6000], Loss: 0.0243
Epoch [16/30], Batch [5400/6000], Loss: 0.0223
Epoch [16/30], Batch [5500/6000], Loss: 0.0253
Epoch [16/30], Batch [5600/6000], Loss: 0.0170
Epoch [16/30], Batch [5700/6000], Loss: 0.0243
Epoch [16/30], Batch [5800/6000], Loss: 0.0187
Epoch [16/30], Batch [5900/6000], Loss: 0.0324
Epoch [16/30], Loss: 0.0428
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.0185
Epoch [17/30], Batch [100/6000], Loss: 0.0193
Epoch [17/30], Batch [200/6000], Loss: 0.0241
Epoch [17/30], Batch [300/6000], Loss: 0.0199
Epoch [17/30], Batch [400/6000], Loss: 0.0238
Epoch [17/30], Batch [500/6000], Loss: 0.0270
Epoch [17/30], Batch [600/6000], Loss: 0.0458
Epoch [17/30], Batch [700/6000], Loss: 0.0225
Epoch [17/30], Batch [800/6000], Loss: 0.0242
Epoch [17/30], Batch [900/6000], Loss: 0.0235
Epoch [17/30], Batch [1000/6000], Loss: 0.0222
Epoch [17/30], Batch [1100/6000], Loss: 0.0232
Epoch [17/30], Batch [1200/6000], Loss: 0.0159
Epoch [17/30], Batch [1300/6000], Loss: 0.0175
Epoch [17/30], Batch [1400/6000], Loss: 0.0270
Epoch [17/30], Batch [1500/6000], Loss: 0.0227
Epoch [17/30], Batch [1600/6000], Loss: 0.0733
Epoch [17/30], Batch [1700/6000], Loss: 0.0329
Epoch [17/30], Batch [1800/6000], Loss: 0.0276
Epoch [17/30], Batch [1900/6000], Loss: 0.0176
Epoch [17/30], Batch [2000/6000], Loss: 0.1686
Epoch [17/30], Batch [2100/6000], Loss: 0.0184
Epoch [17/30], Batch [2200/6000], Loss: 0.0203
Epoch [17/30], Batch [2300/6000], Loss: 0.0251
Epoch [17/30], Batch [2400/6000], Loss: 0.0210
Epoch [17/30], Batch [2500/6000], Loss: 0.0268
Epoch [17/30], Batch [2600/6000], Loss: 0.0205
Epoch [17/30], Batch [2700/6000], Loss: 0.0228
Epoch [17/30], Batch [2800/6000], Loss: 0.0166
Epoch [17/30], Batch [2900/6000], Loss: 0.0277
Epoch [17/30], Batch [3000/6000], Loss: 0.0185
Epoch [17/30], Batch [3100/6000], Loss: 0.0206
Epoch [17/30], Batch [3200/6000], Loss: 0.0212
Epoch [17/30], Batch [3300/6000], Loss: 0.0263
Epoch [17/30], Batch [3400/6000], Loss: 0.5362
Epoch [17/30], Batch [3500/6000], Loss: 0.0215
Epoch [17/30], Batch [3600/6000], Loss: 0.0257
Epoch [17/30], Batch [3700/6000], Loss: 0.0277
Epoch [17/30], Batch [3800/6000], Loss: 0.1261
Epoch [17/30], Batch [3900/6000], Loss: 0.0164
Epoch [17/30], Batch [4000/6000], Loss: 0.0268
Epoch [17/30], Batch [4100/6000], Loss: 0.0188
Epoch [17/30], Batch [4200/6000], Loss: 0.0191
Epoch [17/30], Batch [4300/6000], Loss: 0.0199
Epoch [17/30], Batch [4400/6000], Loss: 0.0234
Epoch [17/30], Batch [4500/6000], Loss: 0.0215
Epoch [17/30], Batch [4600/6000], Loss: 0.0189
Epoch [17/30], Batch [4700/6000], Loss: 0.0550
Epoch [17/30], Batch [4800/6000], Loss: 0.0175
Epoch [17/30], Batch [4900/6000], Loss: 0.0327
Epoch [17/30], Batch [5000/6000], Loss: 0.0269
Epoch [17/30], Batch [5100/6000], Loss: 0.3000
Epoch [17/30], Batch [5200/6000], Loss: 0.0282
Epoch [17/30], Batch [5300/6000], Loss: 0.0185
Epoch [17/30], Batch [5400/6000], Loss: 0.0199
Epoch [17/30], Batch [5500/6000], Loss: 0.0214
Epoch [17/30], Batch [5600/6000], Loss: 0.0219
Epoch [17/30], Batch [5700/6000], Loss: 0.0217
Epoch [17/30], Batch [5800/6000], Loss: 0.0537
Epoch [17/30], Batch [5900/6000], Loss: 0.0166
Epoch [17/30], Loss: 0.0401
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.0264
Epoch [18/30], Batch [100/6000], Loss: 0.0180
Epoch [18/30], Batch [200/6000], Loss: 0.2464
Epoch [18/30], Batch [300/6000], Loss: 0.0224
Epoch [18/30], Batch [400/6000], Loss: 0.0189
Epoch [18/30], Batch [500/6000], Loss: 0.0200
Epoch [18/30], Batch [600/6000], Loss: 0.0184
Epoch [18/30], Batch [700/6000], Loss: 0.0213
Epoch [18/30], Batch [800/6000], Loss: 0.0198
Epoch [18/30], Batch [900/6000], Loss: 0.0223
Epoch [18/30], Batch [1000/6000], Loss: 0.0383
Epoch [18/30], Batch [1100/6000], Loss: 0.0231
Epoch [18/30], Batch [1200/6000], Loss: 0.0347
Epoch [18/30], Batch [1300/6000], Loss: 0.0185
Epoch [18/30], Batch [1400/6000], Loss: 0.0223
Epoch [18/30], Batch [1500/6000], Loss: 0.0219
Epoch [18/30], Batch [1600/6000], Loss: 0.0207
Epoch [18/30], Batch [1700/6000], Loss: 0.0195
Epoch [18/30], Batch [1800/6000], Loss: 0.0237
Epoch [18/30], Batch [1900/6000], Loss: 0.0201
Epoch [18/30], Batch [2000/6000], Loss: 0.0530
Epoch [18/30], Batch [2100/6000], Loss: 0.0243
Epoch [18/30], Batch [2200/6000], Loss: 0.0563
Epoch [18/30], Batch [2300/6000], Loss: 0.0224
Epoch [18/30], Batch [2400/6000], Loss: 0.4504
Epoch [18/30], Batch [2500/6000], Loss: 0.0272
Epoch [18/30], Batch [2600/6000], Loss: 0.0198
Epoch [18/30], Batch [2700/6000], Loss: 0.3169
Epoch [18/30], Batch [2800/6000], Loss: 0.0220
Epoch [18/30], Batch [2900/6000], Loss: 0.0146
Epoch [18/30], Batch [3000/6000], Loss: 0.0191
Epoch [18/30], Batch [3100/6000], Loss: 0.0235
Epoch [18/30], Batch [3200/6000], Loss: 0.0211
Epoch [18/30], Batch [3300/6000], Loss: 0.0185
Epoch [18/30], Batch [3400/6000], Loss: 0.0166
Epoch [18/30], Batch [3500/6000], Loss: 0.0198
Epoch [18/30], Batch [3600/6000], Loss: 0.0218
Epoch [18/30], Batch [3700/6000], Loss: 0.0254
Epoch [18/30], Batch [3800/6000], Loss: 0.0208
Epoch [18/30], Batch [3900/6000], Loss: 0.0614
Epoch [18/30], Batch [4000/6000], Loss: 0.0170
Epoch [18/30], Batch [4100/6000], Loss: 0.0194
Epoch [18/30], Batch [4200/6000], Loss: 0.0649
Epoch [18/30], Batch [4300/6000], Loss: 0.0205
Epoch [18/30], Batch [4400/6000], Loss: 0.0177
Epoch [18/30], Batch [4500/6000], Loss: 0.0235
Epoch [18/30], Batch [4600/6000], Loss: 0.0199
Epoch [18/30], Batch [4700/6000], Loss: 0.0203
Epoch [18/30], Batch [4800/6000], Loss: 0.0265
Epoch [18/30], Batch [4900/6000], Loss: 0.0201
Epoch [18/30], Batch [5000/6000], Loss: 0.3894
Epoch [18/30], Batch [5100/6000], Loss: 0.0204
Epoch [18/30], Batch [5200/6000], Loss: 0.0205
Epoch [18/30], Batch [5300/6000], Loss: 0.0256
Epoch [18/30], Batch [5400/6000], Loss: 0.0231
Epoch [18/30], Batch [5500/6000], Loss: 0.0213
Epoch [18/30], Batch [5600/6000], Loss: 0.0191
Epoch [18/30], Batch [5700/6000], Loss: 0.0216
Epoch [18/30], Batch [5800/6000], Loss: 0.0216
Epoch [18/30], Batch [5900/6000], Loss: 0.0205
Epoch [18/30], Loss: 0.0392
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.0205
Epoch [19/30], Batch [100/6000], Loss: 0.0217
Epoch [19/30], Batch [200/6000], Loss: 0.0244
Epoch [19/30], Batch [300/6000], Loss: 0.0298
Epoch [19/30], Batch [400/6000], Loss: 0.0236
Epoch [19/30], Batch [500/6000], Loss: 0.0331
Epoch [19/30], Batch [600/6000], Loss: 0.0160
Epoch [19/30], Batch [700/6000], Loss: 0.0210
Epoch [19/30], Batch [800/6000], Loss: 0.0265
Epoch [19/30], Batch [900/6000], Loss: 0.0412
Epoch [19/30], Batch [1000/6000], Loss: 0.0251
Epoch [19/30], Batch [1100/6000], Loss: 0.0234
Epoch [19/30], Batch [1200/6000], Loss: 0.0199
Epoch [19/30], Batch [1300/6000], Loss: 0.0215
Epoch [19/30], Batch [1400/6000], Loss: 0.1717
Epoch [19/30], Batch [1500/6000], Loss: 0.0222
Epoch [19/30], Batch [1600/6000], Loss: 0.0197
Epoch [19/30], Batch [1700/6000], Loss: 0.0390
Epoch [19/30], Batch [1800/6000], Loss: 0.0158
Epoch [19/30], Batch [1900/6000], Loss: 0.0362
Epoch [19/30], Batch [2000/6000], Loss: 0.0940
Epoch [19/30], Batch [2100/6000], Loss: 0.0291
Epoch [19/30], Batch [2200/6000], Loss: 0.0207
Epoch [19/30], Batch [2300/6000], Loss: 0.0194
Epoch [19/30], Batch [2400/6000], Loss: 0.0183
Epoch [19/30], Batch [2500/6000], Loss: 0.0247
Epoch [19/30], Batch [2600/6000], Loss: 0.0302
Epoch [19/30], Batch [2700/6000], Loss: 0.0208
Epoch [19/30], Batch [2800/6000], Loss: 0.0162
Epoch [19/30], Batch [2900/6000], Loss: 0.0270
Epoch [19/30], Batch [3000/6000], Loss: 0.0298
Epoch [19/30], Batch [3100/6000], Loss: 0.0513
Epoch [19/30], Batch [3200/6000], Loss: 0.0182
Epoch [19/30], Batch [3300/6000], Loss: 0.0196
Epoch [19/30], Batch [3400/6000], Loss: 0.0216
Epoch [19/30], Batch [3500/6000], Loss: 0.0178
Epoch [19/30], Batch [3600/6000], Loss: 0.0728
Epoch [19/30], Batch [3700/6000], Loss: 0.0208
Epoch [19/30], Batch [3800/6000], Loss: 0.0193
Epoch [19/30], Batch [3900/6000], Loss: 0.0202
Epoch [19/30], Batch [4000/6000], Loss: 0.0180
Epoch [19/30], Batch [4100/6000], Loss: 0.0178
Epoch [19/30], Batch [4200/6000], Loss: 0.0226
Epoch [19/30], Batch [4300/6000], Loss: 0.0187
Epoch [19/30], Batch [4400/6000], Loss: 0.0440
Epoch [19/30], Batch [4500/6000], Loss: 0.0239
Epoch [19/30], Batch [4600/6000], Loss: 0.0731
Epoch [19/30], Batch [4700/6000], Loss: 0.0250
Epoch [19/30], Batch [4800/6000], Loss: 0.0246
Epoch [19/30], Batch [4900/6000], Loss: 0.1351
Epoch [19/30], Batch [5000/6000], Loss: 0.0224
Epoch [19/30], Batch [5100/6000], Loss: 0.0258
Epoch [19/30], Batch [5200/6000], Loss: 0.0239
Epoch [19/30], Batch [5300/6000], Loss: 0.0198
Epoch [19/30], Batch [5400/6000], Loss: 0.0249
Epoch [19/30], Batch [5500/6000], Loss: 0.0322
Epoch [19/30], Batch [5600/6000], Loss: 0.0319
Epoch [19/30], Batch [5700/6000], Loss: 0.0204
Epoch [19/30], Batch [5800/6000], Loss: 0.0224
Epoch [19/30], Batch [5900/6000], Loss: 0.3103
Epoch [19/30], Loss: 0.0376
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.0223
Epoch [20/30], Batch [100/6000], Loss: 0.0212
Epoch [20/30], Batch [200/6000], Loss: 0.0184
Epoch [20/30], Batch [300/6000], Loss: 0.0166
Epoch [20/30], Batch [400/6000], Loss: 0.0188
Epoch [20/30], Batch [500/6000], Loss: 0.0433
Epoch [20/30], Batch [600/6000], Loss: 0.0237
Epoch [20/30], Batch [700/6000], Loss: 0.0265
Epoch [20/30], Batch [800/6000], Loss: 0.0764
Epoch [20/30], Batch [900/6000], Loss: 0.0465
Epoch [20/30], Batch [1000/6000], Loss: 0.0188
Epoch [20/30], Batch [1100/6000], Loss: 0.0356
Epoch [20/30], Batch [1200/6000], Loss: 0.0220
Epoch [20/30], Batch [1300/6000], Loss: 0.0198
Epoch [20/30], Batch [1400/6000], Loss: 0.0203
Epoch [20/30], Batch [1500/6000], Loss: 0.0207
Epoch [20/30], Batch [1600/6000], Loss: 0.2810
Epoch [20/30], Batch [1700/6000], Loss: 0.0226
Epoch [20/30], Batch [1800/6000], Loss: 0.0181
Epoch [20/30], Batch [1900/6000], Loss: 0.0199
Epoch [20/30], Batch [2000/6000], Loss: 0.0168
Epoch [20/30], Batch [2100/6000], Loss: 0.0184
Epoch [20/30], Batch [2200/6000], Loss: 0.0151
Epoch [20/30], Batch [2300/6000], Loss: 0.0214
Epoch [20/30], Batch [2400/6000], Loss: 0.0216
Epoch [20/30], Batch [2500/6000], Loss: 0.0252
Epoch [20/30], Batch [2600/6000], Loss: 0.0211
Epoch [20/30], Batch [2700/6000], Loss: 0.0210
Epoch [20/30], Batch [2800/6000], Loss: 0.1000
Epoch [20/30], Batch [2900/6000], Loss: 0.0167
Epoch [20/30], Batch [3000/6000], Loss: 0.0255
Epoch [20/30], Batch [3100/6000], Loss: 0.3233
Epoch [20/30], Batch [3200/6000], Loss: 0.0245
Epoch [20/30], Batch [3300/6000], Loss: 0.0224
Epoch [20/30], Batch [3400/6000], Loss: 0.0190
Epoch [20/30], Batch [3500/6000], Loss: 0.0175
Epoch [20/30], Batch [3600/6000], Loss: 0.0649
Epoch [20/30], Batch [3700/6000], Loss: 0.0232
Epoch [20/30], Batch [3800/6000], Loss: 0.0202
Epoch [20/30], Batch [3900/6000], Loss: 0.0202
Epoch [20/30], Batch [4000/6000], Loss: 0.0195
Epoch [20/30], Batch [4100/6000], Loss: 0.0203
Epoch [20/30], Batch [4200/6000], Loss: 0.0230
Epoch [20/30], Batch [4300/6000], Loss: 0.0257
Epoch [20/30], Batch [4400/6000], Loss: 0.0218
Epoch [20/30], Batch [4500/6000], Loss: 0.0234
Epoch [20/30], Batch [4600/6000], Loss: 0.0194
Epoch [20/30], Batch [4700/6000], Loss: 0.0215
Epoch [20/30], Batch [4800/6000], Loss: 0.1368
Epoch [20/30], Batch [4900/6000], Loss: 0.3130
Epoch [20/30], Batch [5000/6000], Loss: 0.0204
Epoch [20/30], Batch [5100/6000], Loss: 0.0193
Epoch [20/30], Batch [5200/6000], Loss: 0.0186
Epoch [20/30], Batch [5300/6000], Loss: 0.2066
Epoch [20/30], Batch [5400/6000], Loss: 0.0215
Epoch [20/30], Batch [5500/6000], Loss: 0.0207
Epoch [20/30], Batch [5600/6000], Loss: 0.0614
Epoch [20/30], Batch [5700/6000], Loss: 0.0167
Epoch [20/30], Batch [5800/6000], Loss: 0.0176
Epoch [20/30], Batch [5900/6000], Loss: 0.0199
Epoch [20/30], Loss: 0.0352
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.0231
Epoch [21/30], Batch [100/6000], Loss: 0.0189
Epoch [21/30], Batch [200/6000], Loss: 0.0208
Epoch [21/30], Batch [300/6000], Loss: 0.0197
Epoch [21/30], Batch [400/6000], Loss: 0.0208
Epoch [21/30], Batch [500/6000], Loss: 0.0218
Epoch [21/30], Batch [600/6000], Loss: 0.0437
Epoch [21/30], Batch [700/6000], Loss: 0.0238
Epoch [21/30], Batch [800/6000], Loss: 0.2030
Epoch [21/30], Batch [900/6000], Loss: 0.0191
Epoch [21/30], Batch [1000/6000], Loss: 0.0162
Epoch [21/30], Batch [1100/6000], Loss: 0.0183
Epoch [21/30], Batch [1200/6000], Loss: 0.0239
Epoch [21/30], Batch [1300/6000], Loss: 0.0220
Epoch [21/30], Batch [1400/6000], Loss: 0.0154
Epoch [21/30], Batch [1500/6000], Loss: 0.0206
Epoch [21/30], Batch [1600/6000], Loss: 0.0182
Epoch [21/30], Batch [1700/6000], Loss: 0.0164
Epoch [21/30], Batch [1800/6000], Loss: 0.0175
Epoch [21/30], Batch [1900/6000], Loss: 0.0377
Epoch [21/30], Batch [2000/6000], Loss: 0.0220
Epoch [21/30], Batch [2100/6000], Loss: 0.0212
Epoch [21/30], Batch [2200/6000], Loss: 0.0194
Epoch [21/30], Batch [2300/6000], Loss: 0.0236
Epoch [21/30], Batch [2400/6000], Loss: 0.0182
Epoch [21/30], Batch [2500/6000], Loss: 0.0212
Epoch [21/30], Batch [2600/6000], Loss: 0.0160
Epoch [21/30], Batch [2700/6000], Loss: 0.0183
Epoch [21/30], Batch [2800/6000], Loss: 0.0166
Epoch [21/30], Batch [2900/6000], Loss: 0.0245
Epoch [21/30], Batch [3000/6000], Loss: 0.0196
Epoch [21/30], Batch [3100/6000], Loss: 0.0221
Epoch [21/30], Batch [3200/6000], Loss: 0.0206
Epoch [21/30], Batch [3300/6000], Loss: 0.0192
Epoch [21/30], Batch [3400/6000], Loss: 0.0201
Epoch [21/30], Batch [3500/6000], Loss: 0.0182
Epoch [21/30], Batch [3600/6000], Loss: 0.0199
Epoch [21/30], Batch [3700/6000], Loss: 0.0199
Epoch [21/30], Batch [3800/6000], Loss: 0.0200
Epoch [21/30], Batch [3900/6000], Loss: 0.0265
Epoch [21/30], Batch [4000/6000], Loss: 0.0191
Epoch [21/30], Batch [4100/6000], Loss: 0.0201
Epoch [21/30], Batch [4200/6000], Loss: 0.0202
Epoch [21/30], Batch [4300/6000], Loss: 0.0477
Epoch [21/30], Batch [4400/6000], Loss: 0.0236
Epoch [21/30], Batch [4500/6000], Loss: 0.0257
Epoch [21/30], Batch [4600/6000], Loss: 0.3520
Epoch [21/30], Batch [4700/6000], Loss: 0.0178
Epoch [21/30], Batch [4800/6000], Loss: 0.0185
Epoch [21/30], Batch [4900/6000], Loss: 0.0153
Epoch [21/30], Batch [5000/6000], Loss: 0.0191
Epoch [21/30], Batch [5100/6000], Loss: 0.0177
Epoch [21/30], Batch [5200/6000], Loss: 0.0182
Epoch [21/30], Batch [5300/6000], Loss: 0.0222
Epoch [21/30], Batch [5400/6000], Loss: 0.0198
Epoch [21/30], Batch [5500/6000], Loss: 0.0195
Epoch [21/30], Batch [5600/6000], Loss: 0.0184
Epoch [21/30], Batch [5700/6000], Loss: 0.0240
Epoch [21/30], Batch [5800/6000], Loss: 0.0185
Epoch [21/30], Batch [5900/6000], Loss: 0.0163
Epoch [21/30], Loss: 0.0328
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.0939
Epoch [22/30], Batch [100/6000], Loss: 0.0210
Epoch [22/30], Batch [200/6000], Loss: 0.0245
Epoch [22/30], Batch [300/6000], Loss: 0.0181
Epoch [22/30], Batch [400/6000], Loss: 0.0174
Epoch [22/30], Batch [500/6000], Loss: 0.0164
Epoch [22/30], Batch [600/6000], Loss: 0.2727
Epoch [22/30], Batch [700/6000], Loss: 0.0158
Epoch [22/30], Batch [800/6000], Loss: 0.0207
Epoch [22/30], Batch [900/6000], Loss: 0.2612
Epoch [22/30], Batch [1000/6000], Loss: 0.0417
Epoch [22/30], Batch [1100/6000], Loss: 0.0139
Epoch [22/30], Batch [1200/6000], Loss: 0.0272
Epoch [22/30], Batch [1300/6000], Loss: 0.0273
Epoch [22/30], Batch [1400/6000], Loss: 0.0205
Epoch [22/30], Batch [1500/6000], Loss: 0.0124
Epoch [22/30], Batch [1600/6000], Loss: 0.0168
Epoch [22/30], Batch [1700/6000], Loss: 0.0224
Epoch [22/30], Batch [1800/6000], Loss: 0.0200
Epoch [22/30], Batch [1900/6000], Loss: 0.0219
Epoch [22/30], Batch [2000/6000], Loss: 0.0298
Epoch [22/30], Batch [2100/6000], Loss: 0.0182
Epoch [22/30], Batch [2200/6000], Loss: 0.0237
Epoch [22/30], Batch [2300/6000], Loss: 0.0204
Epoch [22/30], Batch [2400/6000], Loss: 0.0210
Epoch [22/30], Batch [2500/6000], Loss: 0.3129
Epoch [22/30], Batch [2600/6000], Loss: 0.0190
Epoch [22/30], Batch [2700/6000], Loss: 0.0187
Epoch [22/30], Batch [2800/6000], Loss: 0.0176
Epoch [22/30], Batch [2900/6000], Loss: 0.0928
Epoch [22/30], Batch [3000/6000], Loss: 0.0208
Epoch [22/30], Batch [3100/6000], Loss: 0.0287
Epoch [22/30], Batch [3200/6000], Loss: 0.0203
Epoch [22/30], Batch [3300/6000], Loss: 0.0171
Epoch [22/30], Batch [3400/6000], Loss: 0.0211
Epoch [22/30], Batch [3500/6000], Loss: 0.0890
Epoch [22/30], Batch [3600/6000], Loss: 0.2654
Epoch [22/30], Batch [3700/6000], Loss: 0.0216
Epoch [22/30], Batch [3800/6000], Loss: 0.0205
Epoch [22/30], Batch [3900/6000], Loss: 0.0191
Epoch [22/30], Batch [4000/6000], Loss: 0.0189
Epoch [22/30], Batch [4100/6000], Loss: 0.0287
Epoch [22/30], Batch [4200/6000], Loss: 0.0139
Epoch [22/30], Batch [4300/6000], Loss: 0.0273
Epoch [22/30], Batch [4400/6000], Loss: 0.1259
Epoch [22/30], Batch [4500/6000], Loss: 0.0179
Epoch [22/30], Batch [4600/6000], Loss: 0.0345
Epoch [22/30], Batch [4700/6000], Loss: 0.0142
Epoch [22/30], Batch [4800/6000], Loss: 0.0172
Epoch [22/30], Batch [4900/6000], Loss: 0.0164
Epoch [22/30], Batch [5000/6000], Loss: 0.0209
Epoch [22/30], Batch [5100/6000], Loss: 0.0218
Epoch [22/30], Batch [5200/6000], Loss: 0.0188
Epoch [22/30], Batch [5300/6000], Loss: 0.0138
Epoch [22/30], Batch [5400/6000], Loss: 0.0212
Epoch [22/30], Batch [5500/6000], Loss: 0.0186
Epoch [22/30], Batch [5600/6000], Loss: 0.2512
Epoch [22/30], Batch [5700/6000], Loss: 0.0182
Epoch [22/30], Batch [5800/6000], Loss: 0.0210
Epoch [22/30], Batch [5900/6000], Loss: 0.0180
Epoch [22/30], Loss: 0.0339
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.0229
Epoch [23/30], Batch [100/6000], Loss: 0.0215
Epoch [23/30], Batch [200/6000], Loss: 0.0398
Epoch [23/30], Batch [300/6000], Loss: 0.0197
Epoch [23/30], Batch [400/6000], Loss: 0.0317
Epoch [23/30], Batch [500/6000], Loss: 0.0214
Epoch [23/30], Batch [600/6000], Loss: 0.0171
Epoch [23/30], Batch [700/6000], Loss: 0.0160
Epoch [23/30], Batch [800/6000], Loss: 0.0258
Epoch [23/30], Batch [900/6000], Loss: 0.0158
Epoch [23/30], Batch [1000/6000], Loss: 0.0173
Epoch [23/30], Batch [1100/6000], Loss: 0.0212
Epoch [23/30], Batch [1200/6000], Loss: 0.0488
Epoch [23/30], Batch [1300/6000], Loss: 0.0259
Epoch [23/30], Batch [1400/6000], Loss: 0.0172
Epoch [23/30], Batch [1500/6000], Loss: 0.0227
Epoch [23/30], Batch [1600/6000], Loss: 0.0309
Epoch [23/30], Batch [1700/6000], Loss: 0.0202
Epoch [23/30], Batch [1800/6000], Loss: 0.0229
Epoch [23/30], Batch [1900/6000], Loss: 0.0233
Epoch [23/30], Batch [2000/6000], Loss: 0.0227
Epoch [23/30], Batch [2100/6000], Loss: 0.0199
Epoch [23/30], Batch [2200/6000], Loss: 0.0229
Epoch [23/30], Batch [2300/6000], Loss: 0.0217
Epoch [23/30], Batch [2400/6000], Loss: 0.0157
Epoch [23/30], Batch [2500/6000], Loss: 0.0200
Epoch [23/30], Batch [2600/6000], Loss: 0.0169
Epoch [23/30], Batch [2700/6000], Loss: 0.0151
Epoch [23/30], Batch [2800/6000], Loss: 0.0186
Epoch [23/30], Batch [2900/6000], Loss: 0.0182
Epoch [23/30], Batch [3000/6000], Loss: 0.0773
Epoch [23/30], Batch [3100/6000], Loss: 0.0179
Epoch [23/30], Batch [3200/6000], Loss: 0.0282
Epoch [23/30], Batch [3300/6000], Loss: 0.0224
Epoch [23/30], Batch [3400/6000], Loss: 0.0200
Epoch [23/30], Batch [3500/6000], Loss: 0.0164
Epoch [23/30], Batch [3600/6000], Loss: 0.0198
Epoch [23/30], Batch [3700/6000], Loss: 0.0191
Epoch [23/30], Batch [3800/6000], Loss: 0.0220
Epoch [23/30], Batch [3900/6000], Loss: 0.0202
Epoch [23/30], Batch [4000/6000], Loss: 0.0169
Epoch [23/30], Batch [4100/6000], Loss: 0.0315
Epoch [23/30], Batch [4200/6000], Loss: 0.0196
Epoch [23/30], Batch [4300/6000], Loss: 0.0244
Epoch [23/30], Batch [4400/6000], Loss: 0.0238
Epoch [23/30], Batch [4500/6000], Loss: 0.0175
Epoch [23/30], Batch [4600/6000], Loss: 0.0194
Epoch [23/30], Batch [4700/6000], Loss: 0.0153
Epoch [23/30], Batch [4800/6000], Loss: 0.0214
Epoch [23/30], Batch [4900/6000], Loss: 0.0192
Epoch [23/30], Batch [5000/6000], Loss: 0.0174
Epoch [23/30], Batch [5100/6000], Loss: 0.0202
Epoch [23/30], Batch [5200/6000], Loss: 0.0240
Epoch [23/30], Batch [5300/6000], Loss: 0.0238
Epoch [23/30], Batch [5400/6000], Loss: 0.0226
Epoch [23/30], Batch [5500/6000], Loss: 0.0225
Epoch [23/30], Batch [5600/6000], Loss: 0.0189
Epoch [23/30], Batch [5700/6000], Loss: 0.0203
Epoch [23/30], Batch [5800/6000], Loss: 0.0190
Epoch [23/30], Batch [5900/6000], Loss: 0.0194
Epoch [23/30], Loss: 0.0310
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.0210
Epoch [24/30], Batch [100/6000], Loss: 0.0214
Epoch [24/30], Batch [200/6000], Loss: 0.0213
Epoch [24/30], Batch [300/6000], Loss: 0.0196
Epoch [24/30], Batch [400/6000], Loss: 0.0261
Epoch [24/30], Batch [500/6000], Loss: 0.0242
Epoch [24/30], Batch [600/6000], Loss: 0.0178
Epoch [24/30], Batch [700/6000], Loss: 0.0262
Epoch [24/30], Batch [800/6000], Loss: 0.0192
Epoch [24/30], Batch [900/6000], Loss: 0.0248
Epoch [24/30], Batch [1000/6000], Loss: 0.0178
Epoch [24/30], Batch [1100/6000], Loss: 0.0269
Epoch [24/30], Batch [1200/6000], Loss: 0.0248
Epoch [24/30], Batch [1300/6000], Loss: 0.0208
Epoch [24/30], Batch [1400/6000], Loss: 0.0240
Epoch [24/30], Batch [1500/6000], Loss: 0.0186
Epoch [24/30], Batch [1600/6000], Loss: 0.0163
Epoch [24/30], Batch [1700/6000], Loss: 0.0213
Epoch [24/30], Batch [1800/6000], Loss: 0.0177
Epoch [24/30], Batch [1900/6000], Loss: 0.0163
Epoch [24/30], Batch [2000/6000], Loss: 0.0248
Epoch [24/30], Batch [2100/6000], Loss: 0.0189
Epoch [24/30], Batch [2200/6000], Loss: 0.0209
Epoch [24/30], Batch [2300/6000], Loss: 0.0226
Epoch [24/30], Batch [2400/6000], Loss: 0.0529
Epoch [24/30], Batch [2500/6000], Loss: 0.0146
Epoch [24/30], Batch [2600/6000], Loss: 0.0156
Epoch [24/30], Batch [2700/6000], Loss: 0.0157
Epoch [24/30], Batch [2800/6000], Loss: 0.0253
Epoch [24/30], Batch [2900/6000], Loss: 0.0538
Epoch [24/30], Batch [3000/6000], Loss: 0.0171
Epoch [24/30], Batch [3100/6000], Loss: 0.0153
Epoch [24/30], Batch [3200/6000], Loss: 0.0183
Epoch [24/30], Batch [3300/6000], Loss: 0.0191
Epoch [24/30], Batch [3400/6000], Loss: 0.0220
Epoch [24/30], Batch [3500/6000], Loss: 0.0288
Epoch [24/30], Batch [3600/6000], Loss: 0.0164
Epoch [24/30], Batch [3700/6000], Loss: 0.0138
Epoch [24/30], Batch [3800/6000], Loss: 0.0187
Epoch [24/30], Batch [3900/6000], Loss: 0.0207
Epoch [24/30], Batch [4000/6000], Loss: 0.0138
Epoch [24/30], Batch [4100/6000], Loss: 0.0192
Epoch [24/30], Batch [4200/6000], Loss: 0.0223
Epoch [24/30], Batch [4300/6000], Loss: 0.0170
Epoch [24/30], Batch [4400/6000], Loss: 0.0178
Epoch [24/30], Batch [4500/6000], Loss: 0.0199
Epoch [24/30], Batch [4600/6000], Loss: 0.0141
Epoch [24/30], Batch [4700/6000], Loss: 0.0154
Epoch [24/30], Batch [4800/6000], Loss: 0.0159
Epoch [24/30], Batch [4900/6000], Loss: 0.0176
Epoch [24/30], Batch [5000/6000], Loss: 0.0176
Epoch [24/30], Batch [5100/6000], Loss: 0.0448
Epoch [24/30], Batch [5200/6000], Loss: 0.0183
Epoch [24/30], Batch [5300/6000], Loss: 0.0324
Epoch [24/30], Batch [5400/6000], Loss: 0.0165
Epoch [24/30], Batch [5500/6000], Loss: 0.6548
Epoch [24/30], Batch [5600/6000], Loss: 0.0170
Epoch [24/30], Batch [5700/6000], Loss: 0.0211
Epoch [24/30], Batch [5800/6000], Loss: 0.1531
Epoch [24/30], Batch [5900/6000], Loss: 0.0165
Epoch [24/30], Loss: 0.0306
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.1342
Epoch [25/30], Batch [100/6000], Loss: 0.0145
Epoch [25/30], Batch [200/6000], Loss: 0.0206
Epoch [25/30], Batch [300/6000], Loss: 0.0166
Epoch [25/30], Batch [400/6000], Loss: 0.0197
Epoch [25/30], Batch [500/6000], Loss: 0.0854
Epoch [25/30], Batch [600/6000], Loss: 0.0302
Epoch [25/30], Batch [700/6000], Loss: 0.0157
Epoch [25/30], Batch [800/6000], Loss: 0.0223
Epoch [25/30], Batch [900/6000], Loss: 0.0165
Epoch [25/30], Batch [1000/6000], Loss: 0.0549
Epoch [25/30], Batch [1100/6000], Loss: 0.0211
Epoch [25/30], Batch [1200/6000], Loss: 0.0142
Epoch [25/30], Batch [1300/6000], Loss: 0.0164
Epoch [25/30], Batch [1400/6000], Loss: 0.0166
Epoch [25/30], Batch [1500/6000], Loss: 0.0199
Epoch [25/30], Batch [1600/6000], Loss: 0.0213
Epoch [25/30], Batch [1700/6000], Loss: 0.0163
Epoch [25/30], Batch [1800/6000], Loss: 0.0260
Epoch [25/30], Batch [1900/6000], Loss: 0.0341
Epoch [25/30], Batch [2000/6000], Loss: 0.0199
Epoch [25/30], Batch [2100/6000], Loss: 0.0167
Epoch [25/30], Batch [2200/6000], Loss: 0.0195
Epoch [25/30], Batch [2300/6000], Loss: 0.0137
Epoch [25/30], Batch [2400/6000], Loss: 0.0139
Epoch [25/30], Batch [2500/6000], Loss: 0.0186
Epoch [25/30], Batch [2600/6000], Loss: 0.0195
Epoch [25/30], Batch [2700/6000], Loss: 0.0185
Epoch [25/30], Batch [2800/6000], Loss: 0.0181
Epoch [25/30], Batch [2900/6000], Loss: 0.0185
Epoch [25/30], Batch [3000/6000], Loss: 0.0197
Epoch [25/30], Batch [3100/6000], Loss: 0.0159
Epoch [25/30], Batch [3200/6000], Loss: 0.0143
Epoch [25/30], Batch [3300/6000], Loss: 0.0152
Epoch [25/30], Batch [3400/6000], Loss: 0.0189
Epoch [25/30], Batch [3500/6000], Loss: 0.0244
Epoch [25/30], Batch [3600/6000], Loss: 0.0189
Epoch [25/30], Batch [3700/6000], Loss: 0.0261
Epoch [25/30], Batch [3800/6000], Loss: 0.0215
Epoch [25/30], Batch [3900/6000], Loss: 0.0209
Epoch [25/30], Batch [4000/6000], Loss: 0.0166
Epoch [25/30], Batch [4100/6000], Loss: 0.0180
Epoch [25/30], Batch [4200/6000], Loss: 0.0197
Epoch [25/30], Batch [4300/6000], Loss: 0.0208
Epoch [25/30], Batch [4400/6000], Loss: 0.0182
Epoch [25/30], Batch [4500/6000], Loss: 0.0172
Epoch [25/30], Batch [4600/6000], Loss: 0.0181
Epoch [25/30], Batch [4700/6000], Loss: 0.0180
Epoch [25/30], Batch [4800/6000], Loss: 0.0157
Epoch [25/30], Batch [4900/6000], Loss: 0.0230
Epoch [25/30], Batch [5000/6000], Loss: 0.1316
Epoch [25/30], Batch [5100/6000], Loss: 0.0178
Epoch [25/30], Batch [5200/6000], Loss: 0.0170
Epoch [25/30], Batch [5300/6000], Loss: 0.0207
Epoch [25/30], Batch [5400/6000], Loss: 0.0233
Epoch [25/30], Batch [5500/6000], Loss: 0.0171
Epoch [25/30], Batch [5600/6000], Loss: 0.0186
Epoch [25/30], Batch [5700/6000], Loss: 0.0175
Epoch [25/30], Batch [5800/6000], Loss: 0.0248
Epoch [25/30], Batch [5900/6000], Loss: 0.0226
Epoch [25/30], Loss: 0.0287
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.0167
Epoch [26/30], Batch [100/6000], Loss: 0.0228
Epoch [26/30], Batch [200/6000], Loss: 0.0165
Epoch [26/30], Batch [300/6000], Loss: 0.0180
Epoch [26/30], Batch [400/6000], Loss: 0.0229
Epoch [26/30], Batch [500/6000], Loss: 0.0194
Epoch [26/30], Batch [600/6000], Loss: 0.0166
Epoch [26/30], Batch [700/6000], Loss: 0.0162
Epoch [26/30], Batch [800/6000], Loss: 0.0192
Epoch [26/30], Batch [900/6000], Loss: 0.0168
Epoch [26/30], Batch [1000/6000], Loss: 0.0209
Epoch [26/30], Batch [1100/6000], Loss: 0.0227
Epoch [26/30], Batch [1200/6000], Loss: 0.0172
Epoch [26/30], Batch [1300/6000], Loss: 0.0139
Epoch [26/30], Batch [1400/6000], Loss: 0.0190
Epoch [26/30], Batch [1500/6000], Loss: 0.0188
Epoch [26/30], Batch [1600/6000], Loss: 0.0205
Epoch [26/30], Batch [1700/6000], Loss: 0.0174
Epoch [26/30], Batch [1800/6000], Loss: 0.0170
Epoch [26/30], Batch [1900/6000], Loss: 0.4209
Epoch [26/30], Batch [2000/6000], Loss: 0.0157
Epoch [26/30], Batch [2100/6000], Loss: 0.0189
Epoch [26/30], Batch [2200/6000], Loss: 0.0218
Epoch [26/30], Batch [2300/6000], Loss: 0.0176
Epoch [26/30], Batch [2400/6000], Loss: 0.0175
Epoch [26/30], Batch [2500/6000], Loss: 0.0196
Epoch [26/30], Batch [2600/6000], Loss: 0.0209
Epoch [26/30], Batch [2700/6000], Loss: 0.0907
Epoch [26/30], Batch [2800/6000], Loss: 0.0202
Epoch [26/30], Batch [2900/6000], Loss: 0.0139
Epoch [26/30], Batch [3000/6000], Loss: 0.0164
Epoch [26/30], Batch [3100/6000], Loss: 0.0206
Epoch [26/30], Batch [3200/6000], Loss: 0.0184
Epoch [26/30], Batch [3300/6000], Loss: 0.0185
Epoch [26/30], Batch [3400/6000], Loss: 0.0193
Epoch [26/30], Batch [3500/6000], Loss: 0.0206
Epoch [26/30], Batch [3600/6000], Loss: 0.0191
Epoch [26/30], Batch [3700/6000], Loss: 0.0163
Epoch [26/30], Batch [3800/6000], Loss: 0.0189
Epoch [26/30], Batch [3900/6000], Loss: 0.0280
Epoch [26/30], Batch [4000/6000], Loss: 0.0165
Epoch [26/30], Batch [4100/6000], Loss: 0.0166
Epoch [26/30], Batch [4200/6000], Loss: 0.0159
Epoch [26/30], Batch [4300/6000], Loss: 0.0191
Epoch [26/30], Batch [4400/6000], Loss: 0.0162
Epoch [26/30], Batch [4500/6000], Loss: 0.0179
Epoch [26/30], Batch [4600/6000], Loss: 0.0161
Epoch [26/30], Batch [4700/6000], Loss: 0.0177
Epoch [26/30], Batch [4800/6000], Loss: 0.0206
Epoch [26/30], Batch [4900/6000], Loss: 0.0174
Epoch [26/30], Batch [5000/6000], Loss: 0.0179
Epoch [26/30], Batch [5100/6000], Loss: 0.0135
Epoch [26/30], Batch [5200/6000], Loss: 0.0186
Epoch [26/30], Batch [5300/6000], Loss: 0.0854
Epoch [26/30], Batch [5400/6000], Loss: 0.0135
Epoch [26/30], Batch [5500/6000], Loss: 0.0169
Epoch [26/30], Batch [5600/6000], Loss: 0.0184
Epoch [26/30], Batch [5700/6000], Loss: 0.0247
Epoch [26/30], Batch [5800/6000], Loss: 0.0185
Epoch [26/30], Batch [5900/6000], Loss: 0.0326
Epoch [26/30], Loss: 0.0281
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0187
Epoch [27/30], Batch [100/6000], Loss: 0.0148
Epoch [27/30], Batch [200/6000], Loss: 0.0242
Epoch [27/30], Batch [300/6000], Loss: 0.0234
Epoch [27/30], Batch [400/6000], Loss: 0.0198
Epoch [27/30], Batch [500/6000], Loss: 0.0163
Epoch [27/30], Batch [600/6000], Loss: 0.0182
Epoch [27/30], Batch [700/6000], Loss: 0.0204
Epoch [27/30], Batch [800/6000], Loss: 0.0197
Epoch [27/30], Batch [900/6000], Loss: 0.0212
Epoch [27/30], Batch [1000/6000], Loss: 0.0221
Epoch [27/30], Batch [1100/6000], Loss: 0.0221
Epoch [27/30], Batch [1200/6000], Loss: 0.0163
Epoch [27/30], Batch [1300/6000], Loss: 0.0143
Epoch [27/30], Batch [1400/6000], Loss: 0.0189
Epoch [27/30], Batch [1500/6000], Loss: 0.0210
Epoch [27/30], Batch [1600/6000], Loss: 0.0178
Epoch [27/30], Batch [1700/6000], Loss: 0.0546
Epoch [27/30], Batch [1800/6000], Loss: 0.0200
Epoch [27/30], Batch [1900/6000], Loss: 0.0204
Epoch [27/30], Batch [2000/6000], Loss: 0.0318
Epoch [27/30], Batch [2100/6000], Loss: 0.0194
Epoch [27/30], Batch [2200/6000], Loss: 0.0176
Epoch [27/30], Batch [2300/6000], Loss: 0.0163
Epoch [27/30], Batch [2400/6000], Loss: 0.0186
Epoch [27/30], Batch [2500/6000], Loss: 0.0186
Epoch [27/30], Batch [2600/6000], Loss: 0.0190
Epoch [27/30], Batch [2700/6000], Loss: 0.0159
Epoch [27/30], Batch [2800/6000], Loss: 0.0138
Epoch [27/30], Batch [2900/6000], Loss: 0.0245
Epoch [27/30], Batch [3000/6000], Loss: 0.0177
Epoch [27/30], Batch [3100/6000], Loss: 0.0285
Epoch [27/30], Batch [3200/6000], Loss: 0.0219
Epoch [27/30], Batch [3300/6000], Loss: 0.0161
Epoch [27/30], Batch [3400/6000], Loss: 0.1254
Epoch [27/30], Batch [3500/6000], Loss: 0.0166
Epoch [27/30], Batch [3600/6000], Loss: 0.0240
Epoch [27/30], Batch [3700/6000], Loss: 0.6396
Epoch [27/30], Batch [3800/6000], Loss: 0.0186
Epoch [27/30], Batch [3900/6000], Loss: 0.0223
Epoch [27/30], Batch [4000/6000], Loss: 0.0187
Epoch [27/30], Batch [4100/6000], Loss: 0.0122
Epoch [27/30], Batch [4200/6000], Loss: 0.0193
Epoch [27/30], Batch [4300/6000], Loss: 0.0153
Epoch [27/30], Batch [4400/6000], Loss: 0.0225
Epoch [27/30], Batch [4500/6000], Loss: 0.0197
Epoch [27/30], Batch [4600/6000], Loss: 0.0194
Epoch [27/30], Batch [4700/6000], Loss: 0.0163
Epoch [27/30], Batch [4800/6000], Loss: 0.0206
Epoch [27/30], Batch [4900/6000], Loss: 0.0183
Epoch [27/30], Batch [5000/6000], Loss: 0.0159
Epoch [27/30], Batch [5100/6000], Loss: 0.0203
Epoch [27/30], Batch [5200/6000], Loss: 0.0686
Epoch [27/30], Batch [5300/6000], Loss: 0.0760
Epoch [27/30], Batch [5400/6000], Loss: 0.0147
Epoch [27/30], Batch [5500/6000], Loss: 0.0168
Epoch [27/30], Batch [5600/6000], Loss: 0.3069
Epoch [27/30], Batch [5700/6000], Loss: 0.0156
Epoch [27/30], Batch [5800/6000], Loss: 0.0595
Epoch [27/30], Batch [5900/6000], Loss: 0.0147
Epoch [27/30], Loss: 0.0293
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.0156
Epoch [28/30], Batch [100/6000], Loss: 0.0143
Epoch [28/30], Batch [200/6000], Loss: 0.0199
Epoch [28/30], Batch [300/6000], Loss: 0.0195
Epoch [28/30], Batch [400/6000], Loss: 0.0247
Epoch [28/30], Batch [500/6000], Loss: 0.0168
Epoch [28/30], Batch [600/6000], Loss: 0.0169
Epoch [28/30], Batch [700/6000], Loss: 0.0271
Epoch [28/30], Batch [800/6000], Loss: 0.0196
Epoch [28/30], Batch [900/6000], Loss: 0.0173
Epoch [28/30], Batch [1000/6000], Loss: 0.0169
Epoch [28/30], Batch [1100/6000], Loss: 0.0129
Epoch [28/30], Batch [1200/6000], Loss: 0.0182
Epoch [28/30], Batch [1300/6000], Loss: 0.0190
Epoch [28/30], Batch [1400/6000], Loss: 0.0182
Epoch [28/30], Batch [1500/6000], Loss: 0.0161
Epoch [28/30], Batch [1600/6000], Loss: 0.0225
Epoch [28/30], Batch [1700/6000], Loss: 0.0217
Epoch [28/30], Batch [1800/6000], Loss: 0.0163
Epoch [28/30], Batch [1900/6000], Loss: 0.0227
Epoch [28/30], Batch [2000/6000], Loss: 0.0185
Epoch [28/30], Batch [2100/6000], Loss: 0.0179
Epoch [28/30], Batch [2200/6000], Loss: 0.0156
Epoch [28/30], Batch [2300/6000], Loss: 0.0185
Epoch [28/30], Batch [2400/6000], Loss: 0.0185
Epoch [28/30], Batch [2500/6000], Loss: 0.0136
Epoch [28/30], Batch [2600/6000], Loss: 0.0145
Epoch [28/30], Batch [2700/6000], Loss: 0.0187
Epoch [28/30], Batch [2800/6000], Loss: 0.0193
Epoch [28/30], Batch [2900/6000], Loss: 0.0181
Epoch [28/30], Batch [3000/6000], Loss: 0.0224
Epoch [28/30], Batch [3100/6000], Loss: 0.0196
Epoch [28/30], Batch [3200/6000], Loss: 0.0195
Epoch [28/30], Batch [3300/6000], Loss: 0.0178
Epoch [28/30], Batch [3400/6000], Loss: 0.0150
Epoch [28/30], Batch [3500/6000], Loss: 0.0213
Epoch [28/30], Batch [3600/6000], Loss: 0.0164
Epoch [28/30], Batch [3700/6000], Loss: 0.0181
Epoch [28/30], Batch [3800/6000], Loss: 0.1981
Epoch [28/30], Batch [3900/6000], Loss: 0.0257
Epoch [28/30], Batch [4000/6000], Loss: 0.0211
Epoch [28/30], Batch [4100/6000], Loss: 0.0171
Epoch [28/30], Batch [4200/6000], Loss: 0.0274
Epoch [28/30], Batch [4300/6000], Loss: 0.0164
Epoch [28/30], Batch [4400/6000], Loss: 0.0195
Epoch [28/30], Batch [4500/6000], Loss: 0.0154
Epoch [28/30], Batch [4600/6000], Loss: 0.0177
Epoch [28/30], Batch [4700/6000], Loss: 0.0213
Epoch [28/30], Batch [4800/6000], Loss: 0.0205
Epoch [28/30], Batch [4900/6000], Loss: 0.1300
Epoch [28/30], Batch [5000/6000], Loss: 0.0182
Epoch [28/30], Batch [5100/6000], Loss: 0.0153
Epoch [28/30], Batch [5200/6000], Loss: 0.0158
Epoch [28/30], Batch [5300/6000], Loss: 0.0191
Epoch [28/30], Batch [5400/6000], Loss: 0.0597
Epoch [28/30], Batch [5500/6000], Loss: 0.0317
Epoch [28/30], Batch [5600/6000], Loss: 0.0254
Epoch [28/30], Batch [5700/6000], Loss: 0.0180
Epoch [28/30], Batch [5800/6000], Loss: 0.0208
Epoch [28/30], Batch [5900/6000], Loss: 0.0140
Epoch [28/30], Loss: 0.0276
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0114
Epoch [29/30], Batch [100/6000], Loss: 0.0138
Epoch [29/30], Batch [200/6000], Loss: 0.0144
Epoch [29/30], Batch [300/6000], Loss: 0.0205
Epoch [29/30], Batch [400/6000], Loss: 0.0161
Epoch [29/30], Batch [500/6000], Loss: 0.0147
Epoch [29/30], Batch [600/6000], Loss: 0.0147
Epoch [29/30], Batch [700/6000], Loss: 0.0202
Epoch [29/30], Batch [800/6000], Loss: 0.0197
Epoch [29/30], Batch [900/6000], Loss: 0.0217
Epoch [29/30], Batch [1000/6000], Loss: 0.0215
Epoch [29/30], Batch [1100/6000], Loss: 0.0167
Epoch [29/30], Batch [1200/6000], Loss: 0.0266
Epoch [29/30], Batch [1300/6000], Loss: 0.0181
Epoch [29/30], Batch [1400/6000], Loss: 0.0158
Epoch [29/30], Batch [1500/6000], Loss: 0.0186
Epoch [29/30], Batch [1600/6000], Loss: 0.0220
Epoch [29/30], Batch [1700/6000], Loss: 0.0191
Epoch [29/30], Batch [1800/6000], Loss: 0.0229
Epoch [29/30], Batch [1900/6000], Loss: 0.0169
Epoch [29/30], Batch [2000/6000], Loss: 0.0214
Epoch [29/30], Batch [2100/6000], Loss: 0.0142
Epoch [29/30], Batch [2200/6000], Loss: 0.0192
Epoch [29/30], Batch [2300/6000], Loss: 0.0154
Epoch [29/30], Batch [2400/6000], Loss: 0.0197
Epoch [29/30], Batch [2500/6000], Loss: 0.0133
Epoch [29/30], Batch [2600/6000], Loss: 0.0181
Epoch [29/30], Batch [2700/6000], Loss: 0.0201
Epoch [29/30], Batch [2800/6000], Loss: 0.0209
Epoch [29/30], Batch [2900/6000], Loss: 0.0165
Epoch [29/30], Batch [3000/6000], Loss: 0.0169
Epoch [29/30], Batch [3100/6000], Loss: 0.0203
Epoch [29/30], Batch [3200/6000], Loss: 0.0199
Epoch [29/30], Batch [3300/6000], Loss: 0.0191
Epoch [29/30], Batch [3400/6000], Loss: 0.0190
Epoch [29/30], Batch [3500/6000], Loss: 0.0131
Epoch [29/30], Batch [3600/6000], Loss: 0.0178
Epoch [29/30], Batch [3700/6000], Loss: 0.0204
Epoch [29/30], Batch [3800/6000], Loss: 0.0206
Epoch [29/30], Batch [3900/6000], Loss: 0.0171
Epoch [29/30], Batch [4000/6000], Loss: 0.0179
Epoch [29/30], Batch [4100/6000], Loss: 0.0190
Epoch [29/30], Batch [4200/6000], Loss: 0.0249
Epoch [29/30], Batch [4300/6000], Loss: 0.0200
Epoch [29/30], Batch [4400/6000], Loss: 0.0196
Epoch [29/30], Batch [4500/6000], Loss: 0.0143
Epoch [29/30], Batch [4600/6000], Loss: 0.0395
Epoch [29/30], Batch [4700/6000], Loss: 0.0234
Epoch [29/30], Batch [4800/6000], Loss: 0.0136
Epoch [29/30], Batch [4900/6000], Loss: 0.0200
Epoch [29/30], Batch [5000/6000], Loss: 0.0186
Epoch [29/30], Batch [5100/6000], Loss: 0.0164
Epoch [29/30], Batch [5200/6000], Loss: 0.0184
Epoch [29/30], Batch [5300/6000], Loss: 0.0198
Epoch [29/30], Batch [5400/6000], Loss: 0.0189
Epoch [29/30], Batch [5500/6000], Loss: 0.0198
Epoch [29/30], Batch [5600/6000], Loss: 0.0206
Epoch [29/30], Batch [5700/6000], Loss: 0.0167
Epoch [29/30], Batch [5800/6000], Loss: 0.0236
Epoch [29/30], Batch [5900/6000], Loss: 0.0133
Epoch [29/30], Loss: 0.0262
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0160
Epoch [30/30], Batch [100/6000], Loss: 0.0184
Epoch [30/30], Batch [200/6000], Loss: 0.0207
Epoch [30/30], Batch [300/6000], Loss: 0.0201
Epoch [30/30], Batch [400/6000], Loss: 0.0169
Epoch [30/30], Batch [500/6000], Loss: 0.0158
Epoch [30/30], Batch [600/6000], Loss: 0.0144
Epoch [30/30], Batch [700/6000], Loss: 0.0284
Epoch [30/30], Batch [800/6000], Loss: 0.0185
Epoch [30/30], Batch [900/6000], Loss: 0.0160
Epoch [30/30], Batch [1000/6000], Loss: 0.0195
Epoch [30/30], Batch [1100/6000], Loss: 0.0187
Epoch [30/30], Batch [1200/6000], Loss: 0.0153
Epoch [30/30], Batch [1300/6000], Loss: 0.0170
Epoch [30/30], Batch [1400/6000], Loss: 0.0153
Epoch [30/30], Batch [1500/6000], Loss: 0.0204
Epoch [30/30], Batch [1600/6000], Loss: 0.0192
Epoch [30/30], Batch [1700/6000], Loss: 0.0159
Epoch [30/30], Batch [1800/6000], Loss: 0.0183
Epoch [30/30], Batch [1900/6000], Loss: 0.0207
Epoch [30/30], Batch [2000/6000], Loss: 0.0274
Epoch [30/30], Batch [2100/6000], Loss: 0.0164
Epoch [30/30], Batch [2200/6000], Loss: 0.0145
Epoch [30/30], Batch [2300/6000], Loss: 0.0164
Epoch [30/30], Batch [2400/6000], Loss: 0.0152
Epoch [30/30], Batch [2500/6000], Loss: 0.0234
Epoch [30/30], Batch [2600/6000], Loss: 0.0223
Epoch [30/30], Batch [2700/6000], Loss: 0.0290
Epoch [30/30], Batch [2800/6000], Loss: 0.0188
Epoch [30/30], Batch [2900/6000], Loss: 0.0144
Epoch [30/30], Batch [3000/6000], Loss: 0.0172
Epoch [30/30], Batch [3100/6000], Loss: 0.0175
Epoch [30/30], Batch [3200/6000], Loss: 0.0176
Epoch [30/30], Batch [3300/6000], Loss: 0.0177
Epoch [30/30], Batch [3400/6000], Loss: 0.0144
Epoch [30/30], Batch [3500/6000], Loss: 0.0161
Epoch [30/30], Batch [3600/6000], Loss: 0.0128
Epoch [30/30], Batch [3700/6000], Loss: 0.0193
Epoch [30/30], Batch [3800/6000], Loss: 0.0225
Epoch [30/30], Batch [3900/6000], Loss: 0.0181
Epoch [30/30], Batch [4000/6000], Loss: 0.0197
Epoch [30/30], Batch [4100/6000], Loss: 0.0190
Epoch [30/30], Batch [4200/6000], Loss: 0.0169
Epoch [30/30], Batch [4300/6000], Loss: 0.0180
Epoch [30/30], Batch [4400/6000], Loss: 0.0160
Epoch [30/30], Batch [4500/6000], Loss: 0.0312
Epoch [30/30], Batch [4600/6000], Loss: 0.0167
Epoch [30/30], Batch [4700/6000], Loss: 0.0198
Epoch [30/30], Batch [4800/6000], Loss: 0.0180
Epoch [30/30], Batch [4900/6000], Loss: 0.0170
Epoch [30/30], Batch [5000/6000], Loss: 0.0134
Epoch [30/30], Batch [5100/6000], Loss: 0.0152
Epoch [30/30], Batch [5200/6000], Loss: 0.0185
Epoch [30/30], Batch [5300/6000], Loss: 0.0214
Epoch [30/30], Batch [5400/6000], Loss: 0.0165
Epoch [30/30], Batch [5500/6000], Loss: 0.0248
Epoch [30/30], Batch [5600/6000], Loss: 0.0155
Epoch [30/30], Batch [5700/6000], Loss: 0.0136
Epoch [30/30], Batch [5800/6000], Loss: 0.0187
Epoch [30/30], Batch [5900/6000], Loss: 0.0156
Epoch [30/30], Loss: 0.0268
Visualization saved to figures/visualization_0.png
Test Loss: 0.0994, Accuracy: 98.28%
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 0.5584
  Image Loss: 0.0211
  Total Loss: 5.6054
  Image grad max: 2.1351518630981445
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]
Adversarial Training Loop 2/300:
  Label Loss: 0.3646
  Image Loss: 0.0211
  Total Loss: 3.6675
  Image grad max: 2.674368381500244
  Output probs: [[0.    0.    0.002 0.    0.001 0.    0.    0.    0.98  0.017]]
Adversarial Training Loop 3/300:
  Label Loss: 0.1366
  Image Loss: 0.0211
  Total Loss: 1.3873
  Image grad max: 3.0736048221588135
  Output probs: [[0.    0.    0.014 0.001 0.016 0.001 0.    0.    0.228 0.74 ]]
Adversarial Training Loop 4/300:
  Label Loss: 0.0197
  Image Loss: 0.0212
  Total Loss: 0.2178
  Image grad max: 1.9027382135391235
  Output probs: [[0.    0.    0.005 0.    0.014 0.    0.    0.    0.016 0.964]]
Adversarial Training Loop 5/300:
  Label Loss: 0.1395
  Image Loss: 0.0213
  Total Loss: 1.4164
  Image grad max: 3.5722787380218506
  Output probs: [[0.    0.    0.005 0.    0.018 0.    0.    0.    0.014 0.962]]
Adversarial Training Loop 6/300:
  Label Loss: 0.1464
  Image Loss: 0.0213
  Total Loss: 1.4848
  Image grad max: 3.5093894004821777
  Output probs: [[0.    0.    0.016 0.001 0.026 0.001 0.    0.    0.171 0.785]]
Adversarial Training Loop 7/300:
  Label Loss: 0.0310
  Image Loss: 0.0212
  Total Loss: 0.3309
  Image grad max: 2.1767797470092773
  Output probs: [[0.    0.    0.009 0.001 0.005 0.001 0.    0.    0.886 0.099]]
Adversarial Training Loop 8/300:
  Label Loss: 0.0523
  Image Loss: 0.0212
  Total Loss: 0.5443
  Image grad max: 2.436774969100952
  Output probs: [[0.    0.    0.004 0.    0.002 0.001 0.    0.    0.962 0.031]]
Adversarial Training Loop 9/300:
  Label Loss: 0.1056
  Image Loss: 0.0211
  Total Loss: 1.0775
  Image grad max: 2.7175192832946777
  Output probs: [[0.    0.    0.006 0.001 0.003 0.001 0.    0.    0.932 0.057]]
Adversarial Training Loop 10/300:
  Label Loss: 0.0776
  Image Loss: 0.0212
  Total Loss: 0.7970
  Image grad max: 2.546077013015747
  Output probs: [[0.    0.    0.017 0.001 0.017 0.003 0.    0.    0.669 0.293]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0122
  Image Loss: 0.0212
  Total Loss: 0.1427
  Image grad max: 1.103347659111023
  Output probs: [[0.    0.    0.017 0.001 0.04  0.004 0.    0.    0.175 0.762]]
Adversarial Training Loop 12/300:
  Label Loss: 0.0313
  Image Loss: 0.0213
  Total Loss: 0.3342
  Image grad max: 2.021254062652588
  Output probs: [[0.    0.    0.013 0.    0.034 0.003 0.    0.    0.09  0.86 ]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0584
  Image Loss: 0.0213
  Total Loss: 0.6054
  Image grad max: 2.503939151763916
  Output probs: [[0.    0.    0.013 0.001 0.022 0.004 0.    0.    0.151 0.811]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0358
  Image Loss: 0.0214
  Total Loss: 0.3791
  Image grad max: 2.084766387939453
  Output probs: [[0.    0.    0.012 0.001 0.01  0.004 0.    0.    0.467 0.506]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0028
  Image Loss: 0.0214
  Total Loss: 0.0494
  Image grad max: 0.2020535171031952
  Output probs: [[0.    0.    0.007 0.001 0.003 0.002 0.    0.    0.789 0.198]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0236
  Image Loss: 0.0214
  Total Loss: 0.2572
  Image grad max: 1.6510505676269531
  Output probs: [[0.    0.    0.005 0.001 0.001 0.002 0.    0.    0.85  0.14 ]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0370
  Image Loss: 0.0214
  Total Loss: 0.3910
  Image grad max: 1.9642279148101807
  Output probs: [[0.    0.    0.006 0.001 0.002 0.003 0.    0.    0.761 0.228]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0182
  Image Loss: 0.0214
  Total Loss: 0.2034
  Image grad max: 1.4699758291244507
  Output probs: [[0.    0.    0.007 0.001 0.003 0.004 0.    0.    0.483 0.503]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0014
  Image Loss: 0.0215
  Total Loss: 0.0358
  Image grad max: 0.08679011464118958
  Output probs: [[0.    0.    0.006 0.001 0.003 0.004 0.    0.    0.257 0.73 ]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0144
  Image Loss: 0.0215
  Total Loss: 0.1653
  Image grad max: 1.3690972328186035
  Output probs: [[0.    0.    0.005 0.001 0.002 0.004 0.    0.    0.217 0.771]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0200
  Image Loss: 0.0216
  Total Loss: 0.2217
  Image grad max: 1.5837165117263794
  Output probs: [[0.    0.    0.005 0.001 0.002 0.004 0.    0.    0.322 0.667]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0077
  Image Loss: 0.0216
  Total Loss: 0.0982
  Image grad max: 0.9850010871887207
  Output probs: [[0.    0.    0.005 0.001 0.001 0.004 0.    0.    0.555 0.434]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0018
  Image Loss: 0.0216
  Total Loss: 0.0395
  Image grad max: 0.32318657636642456
  Output probs: [[0.    0.    0.004 0.001 0.001 0.003 0.    0.    0.713 0.278]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0115
  Image Loss: 0.0216
  Total Loss: 0.1368
  Image grad max: 1.1651875972747803
  Output probs: [[0.    0.    0.003 0.001 0.    0.003 0.    0.    0.721 0.271]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0123
  Image Loss: 0.0217
  Total Loss: 0.1444
  Image grad max: 1.2002569437026978
  Output probs: [[0.    0.    0.004 0.001 0.001 0.003 0.    0.    0.599 0.393]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0031
  Image Loss: 0.0217
  Total Loss: 0.0524
  Image grad max: 0.54363614320755
  Output probs: [[0.    0.    0.004 0.001 0.001 0.004 0.    0.    0.412 0.579]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0023
  Image Loss: 0.0217
  Total Loss: 0.0451
  Image grad max: 0.4663219749927521
  Output probs: [[0.    0.    0.003 0.001 0.001 0.004 0.    0.    0.32  0.672]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0076
  Image Loss: 0.0217
  Total Loss: 0.0980
  Image grad max: 0.9677284359931946
  Output probs: [[0.    0.    0.003 0.001 0.001 0.004 0.    0.    0.336 0.656]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0063
  Image Loss: 0.0218
  Total Loss: 0.0851
  Image grad max: 0.8748717904090881
  Output probs: [[0.    0.    0.003 0.001 0.    0.004 0.    0.    0.458 0.534]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0011
  Image Loss: 0.0218
  Total Loss: 0.0329
  Image grad max: 0.21460577845573425
  Output probs: [[0.    0.    0.003 0.001 0.    0.003 0.    0.    0.599 0.394]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0029
  Image Loss: 0.0218
  Total Loss: 0.0511
  Image grad max: 0.5321287512779236
  Output probs: [[0.    0.    0.003 0.001 0.    0.003 0.    0.    0.652 0.341]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0058
  Image Loss: 0.0218
  Total Loss: 0.0801
  Image grad max: 0.8114449381828308
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.601 0.392]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0030
  Image Loss: 0.0218
  Total Loss: 0.0514
  Image grad max: 0.5434907078742981
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.483 0.51 ]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0007
  Image Loss: 0.0218
  Total Loss: 0.0293
  Image grad max: 0.08099369704723358
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.395 0.598]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0028
  Image Loss: 0.0219
  Total Loss: 0.0501
  Image grad max: 0.5519689321517944
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.384 0.609]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0033
  Image Loss: 0.0219
  Total Loss: 0.0549
  Image grad max: 0.6113878488540649
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.45  0.544]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0011
  Image Loss: 0.0219
  Total Loss: 0.0328
  Image grad max: 0.26039838790893555
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.546 0.448]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0011
  Image Loss: 0.0219
  Total Loss: 0.0328
  Image grad max: 0.2529520094394684
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.596 0.398]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0026
  Image Loss: 0.0219
  Total Loss: 0.0477
  Image grad max: 0.5186622142791748
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.57  0.425]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0016
  Image Loss: 0.0219
  Total Loss: 0.0382
  Image grad max: 0.3807208836078644
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.494 0.501]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0006
  Image Loss: 0.0219
  Total Loss: 0.0274
  Image grad max: 0.025637032464146614
  Output probs: [[0.    0.    0.002 0.001 0.    0.003 0.    0.    0.433 0.562]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0014
  Image Loss: 0.0219
  Total Loss: 0.0357
  Image grad max: 0.35415318608283997
  Output probs: [[0.    0.    0.002 0.001 0.    0.002 0.    0.    0.425 0.57 ]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0016
  Image Loss: 0.0219
  Total Loss: 0.0378
  Image grad max: 0.39827272295951843
  Output probs: [[0.    0.    0.002 0.001 0.    0.002 0.    0.    0.472 0.524]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0006
  Image Loss: 0.0219
  Total Loss: 0.0283
  Image grad max: 0.14646559953689575
  Output probs: [[0.    0.    0.002 0.001 0.    0.002 0.    0.    0.535 0.46 ]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0008
  Image Loss: 0.0219
  Total Loss: 0.0295
  Image grad max: 0.19817011058330536
  Output probs: [[0.    0.    0.002 0.001 0.    0.002 0.    0.    0.563 0.433]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0013
  Image Loss: 0.0219
  Total Loss: 0.0350
  Image grad max: 0.34631916880607605
  Output probs: [[0.    0.    0.001 0.001 0.    0.002 0.    0.    0.536 0.46 ]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0007
  Image Loss: 0.0219
  Total Loss: 0.0292
  Image grad max: 0.20155933499336243
  Output probs: [[0.    0.    0.001 0.001 0.    0.002 0.    0.    0.483 0.513]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0005
  Image Loss: 0.0219
  Total Loss: 0.0267
  Image grad max: 0.08519018441438675
  Output probs: [[0.    0.    0.001 0.001 0.    0.002 0.    0.    0.45  0.545]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0009
  Image Loss: 0.0219
  Total Loss: 0.0307
  Image grad max: 0.26394686102867126
  Output probs: [[0.    0.    0.001 0.001 0.    0.002 0.    0.    0.457 0.539]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0008
  Image Loss: 0.0219
  Total Loss: 0.0294
  Image grad max: 0.22936366498470306
  Output probs: [[0.    0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0004
  Image Loss: 0.0219
  Total Loss: 0.0258
  Image grad max: 0.00850717443972826
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.535 0.461]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0007
  Image Loss: 0.0219
  Total Loss: 0.0284
  Image grad max: 0.19818350672721863
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.537 0.459]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0007
  Image Loss: 0.0219
  Total Loss: 0.0286
  Image grad max: 0.20885813236236572
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.506 0.491]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0004
  Image Loss: 0.0219
  Total Loss: 0.0256
  Image grad max: 0.03785659745335579
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.474 0.523]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0005
  Image Loss: 0.0219
  Total Loss: 0.0266
  Image grad max: 0.13669724762439728
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.466 0.531]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0006
  Image Loss: 0.0219
  Total Loss: 0.0274
  Image grad max: 0.18167488276958466
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.486 0.511]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0004
  Image Loss: 0.0219
  Total Loss: 0.0255
  Image grad max: 0.07012443989515305
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.516 0.481]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0004
  Image Loss: 0.0219
  Total Loss: 0.0257
  Image grad max: 0.09436599165201187
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.527 0.47 ]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0005
  Image Loss: 0.0218
  Total Loss: 0.0266
  Image grad max: 0.15256142616271973
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.51  0.487]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0252
  Image grad max: 0.06318603456020355
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.486 0.511]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0252
  Image grad max: 0.06866930425167084
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.476 0.521]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0004
  Image Loss: 0.0218
  Total Loss: 0.0258
  Image grad max: 0.12569695711135864
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.487 0.51 ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0250
  Image grad max: 0.06707470864057541
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.508 0.489]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0248
  Image grad max: 0.05044619366526604
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.518 0.479]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0004
  Image Loss: 0.0218
  Total Loss: 0.0253
  Image grad max: 0.10438306629657745
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.508 0.489]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0247
  Image grad max: 0.0504697747528553
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.491 0.506]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0246
  Image grad max: 0.04300372302532196
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.483 0.514]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0249
  Image grad max: 0.08664095401763916
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.491 0.507]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0245
  Image grad max: 0.045878782868385315
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.506 0.492]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0003
  Image Loss: 0.0218
  Total Loss: 0.0244
  Image grad max: 0.03703165799379349
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.512 0.486]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0003
  Image Loss: 0.0217
  Total Loss: 0.0246
  Image grad max: 0.07175356894731522
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.504 0.493]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0003
  Image Loss: 0.0217
  Total Loss: 0.0243
  Image grad max: 0.02997189201414585
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.493 0.505]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0003
  Image Loss: 0.0217
  Total Loss: 0.0243
  Image grad max: 0.035405173897743225
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.488 0.509]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0003
  Image Loss: 0.0217
  Total Loss: 0.0244
  Image grad max: 0.060005199164152145
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.495 0.503]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0002
  Image Loss: 0.0217
  Total Loss: 0.0241
  Image grad max: 0.023834187537431717
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.505 0.493]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0002
  Image Loss: 0.0217
  Total Loss: 0.0241
  Image grad max: 0.0334651842713356
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.508 0.49 ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0002
  Image Loss: 0.0217
  Total Loss: 0.0242
  Image grad max: 0.04791203886270523
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.501 0.497]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0002
  Image Loss: 0.0217
  Total Loss: 0.0240
  Image grad max: 0.011438228189945221
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.493 0.505]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0002
  Image Loss: 0.0217
  Total Loss: 0.0240
  Image grad max: 0.03344887122511864
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.492 0.506]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0002
  Image Loss: 0.0217
  Total Loss: 0.0240
  Image grad max: 0.039849914610385895
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.5  ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0239
  Image grad max: 0.004946794360876083
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.505 0.493]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0239
  Image grad max: 0.03169817849993706
  Output probs: [[0.    0.    0.001 0.001 0.    0.001 0.    0.    0.504 0.494]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0239
  Image grad max: 0.028505153954029083
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.498 0.5  ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0238
  Image grad max: 0.005789547692984343
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.494 0.504]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0238
  Image grad max: 0.030289364978671074
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.495 0.503]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0237
  Image grad max: 0.021210892125964165
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.501 0.497]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0237
  Image grad max: 0.010454555042088032
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.504 0.494]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0237
  Image grad max: 0.025909507647156715
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.501 0.497]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0002
  Image Loss: 0.0216
  Total Loss: 0.0236
  Image grad max: 0.010954516008496284
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.496 0.502]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0236
  Image grad max: 0.015001160092651844
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.495 0.503]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0236
  Image grad max: 0.022146066650748253
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.498 0.5  ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0235
  Image grad max: 0.005032971501350403
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.502 0.496]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0235
  Image grad max: 0.01592244952917099
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.502 0.496]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0235
  Image grad max: 0.015286224894225597
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0234
  Image grad max: 0.0038830346893519163
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.496 0.502]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0234
  Image grad max: 0.016734668985009193
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.497 0.501]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0234
  Image grad max: 0.01038189698010683
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.5   0.498]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0002
  Image Loss: 0.0215
  Total Loss: 0.0234
  Image grad max: 0.00783174205571413
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.502 0.497]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0233
  Image grad max: 0.013670099899172783
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.5   0.498]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0233
  Image grad max: 0.0047332062385976315
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.497 0.501]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0233
  Image grad max: 0.010596287436783314
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.497 0.501]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0232
  Image grad max: 0.011009112931787968
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0232
  Image grad max: 0.003254397539421916
  Output probs: [[0.    0.    0.001 0.001 0.    0.    0.    0.    0.501 0.497]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0232
  Image grad max: 0.010191168636083603
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.498]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0232
  Image grad max: 0.006404583342373371
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.498 0.5  ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0231
  Image grad max: 0.006257092580199242
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.498 0.501]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0231
  Image grad max: 0.009440720081329346
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0002
  Image Loss: 0.0214
  Total Loss: 0.0231
  Image grad max: 0.0022405339404940605
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.498]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0002
  Image Loss: 0.0213
  Total Loss: 0.0231
  Image grad max: 0.007614100351929665
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.498]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0002
  Image Loss: 0.0213
  Total Loss: 0.0230
  Image grad max: 0.006381201557815075
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0002
  Image Loss: 0.0213
  Total Loss: 0.0230
  Image grad max: 0.0037533112335950136
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.498 0.5  ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0002
  Image Loss: 0.0213
  Total Loss: 0.0230
  Image grad max: 0.007578106597065926
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0002
  Image Loss: 0.0213
  Total Loss: 0.0230
  Image grad max: 0.0031289164908230305
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.498]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0002
  Image Loss: 0.0213
  Total Loss: 0.0229
  Image grad max: 0.0057328324764966965
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.498]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0002
  Image Loss: 0.0213
  Total Loss: 0.0229
  Image grad max: 0.005772901698946953
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0002
  Image Loss: 0.0213
  Total Loss: 0.0229
  Image grad max: 0.0023059025406837463
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.498 0.5  ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0229
  Image grad max: 0.00593642657622695
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0228
  Image grad max: 0.003189055249094963
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0228
  Image grad max: 0.004375687800347805
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.498]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0228
  Image grad max: 0.004921194165945053
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0228
  Image grad max: 0.0019222921691834927
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0227
  Image grad max: 0.004762043245136738
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0227
  Image grad max: 0.002876877784729004
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0227
  Image grad max: 0.003600956639274955
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0002
  Image Loss: 0.0212
  Total Loss: 0.0227
  Image grad max: 0.004152009263634682
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0002
  Image Loss: 0.0211
  Total Loss: 0.0226
  Image grad max: 0.001933073508553207
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0226
  Image grad max: 0.003867593128234148
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0226
  Image grad max: 0.002408087020739913
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0226
  Image grad max: 0.003074219450354576
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0226
  Image grad max: 0.0034087342210114002
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0225
  Image grad max: 0.0019059106707572937
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0225
  Image grad max: 0.0031795406248420477
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0225
  Image grad max: 0.0019378283759579062
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0225
  Image grad max: 0.0028101601637899876
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0224
  Image grad max: 0.002825448289513588
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0224
  Image grad max: 0.001870908192358911
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0224
  Image grad max: 0.0025078689213842154
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0224
  Image grad max: 0.0019017880549654365
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0224
  Image grad max: 0.002573133911937475
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0223
  Image grad max: 0.002234833315014839
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0223
  Image grad max: 0.0018951212987303734
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0223
  Image grad max: 0.0020675663836300373
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0001
  Image Loss: 0.0209
  Total Loss: 0.0223
  Image grad max: 0.001932821236550808
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0001
  Image Loss: 0.0209
  Total Loss: 0.0223
  Image grad max: 0.002338351681828499
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0001
  Image Loss: 0.0209
  Total Loss: 0.0222
  Image grad max: 0.0019715633243322372
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0001
  Image Loss: 0.0209
  Total Loss: 0.0222
  Image grad max: 0.0019111784640699625
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0001
  Image Loss: 0.0209
  Total Loss: 0.0222
  Image grad max: 0.0019109685672447085
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0001
  Image Loss: 0.0209
  Total Loss: 0.0222
  Image grad max: 0.00194885628297925
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0001
  Image Loss: 0.0209
  Total Loss: 0.0222
  Image grad max: 0.0021329487208276987
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0001
  Image Loss: 0.0209
  Total Loss: 0.0221
  Image grad max: 0.0019135988550260663
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0001
  Image Loss: 0.0208
  Total Loss: 0.0221
  Image grad max: 0.0019119309727102518
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0001
  Image Loss: 0.0208
  Total Loss: 0.0221
  Image grad max: 0.001875746645964682
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0001
  Image Loss: 0.0208
  Total Loss: 0.0221
  Image grad max: 0.0019544209353625774
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0001
  Image Loss: 0.0208
  Total Loss: 0.0221
  Image grad max: 0.001981571316719055
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0001
  Image Loss: 0.0208
  Total Loss: 0.0220
  Image grad max: 0.00188084552064538
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0001
  Image Loss: 0.0208
  Total Loss: 0.0220
  Image grad max: 0.0019135288894176483
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0001
  Image Loss: 0.0208
  Total Loss: 0.0220
  Image grad max: 0.0018635771702975035
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0001
  Image Loss: 0.0208
  Total Loss: 0.0220
  Image grad max: 0.0019414672860875726
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0001
  Image Loss: 0.0207
  Total Loss: 0.0220
  Image grad max: 0.001931163016706705
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0001
  Image Loss: 0.0207
  Total Loss: 0.0219
  Image grad max: 0.0018684397218748927
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0001
  Image Loss: 0.0207
  Total Loss: 0.0219
  Image grad max: 0.0019013851415365934
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0001
  Image Loss: 0.0207
  Total Loss: 0.0219
  Image grad max: 0.0018691661534830928
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0001
  Image Loss: 0.0207
  Total Loss: 0.0219
  Image grad max: 0.00192420301027596
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0001
  Image Loss: 0.0207
  Total Loss: 0.0219
  Image grad max: 0.001883078133687377
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0001
  Image Loss: 0.0207
  Total Loss: 0.0218
  Image grad max: 0.0018888822523877025
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0001
  Image Loss: 0.0207
  Total Loss: 0.0218
  Image grad max: 0.0018823627615347505
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0001
  Image Loss: 0.0206
  Total Loss: 0.0218
  Image grad max: 0.0018806110601872206
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0001
  Image Loss: 0.0206
  Total Loss: 0.0218
  Image grad max: 0.001894477172754705
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0001
  Image Loss: 0.0206
  Total Loss: 0.0218
  Image grad max: 0.0018582509364932775
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0001
  Image Loss: 0.0206
  Total Loss: 0.0217
  Image grad max: 0.0018920099828392267
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0001
  Image Loss: 0.0206
  Total Loss: 0.0217
  Image grad max: 0.001859741285443306
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0001
  Image Loss: 0.0206
  Total Loss: 0.0217
  Image grad max: 0.0018780202371999621
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0001
  Image Loss: 0.0206
  Total Loss: 0.0217
  Image grad max: 0.0018586639780551195
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0001
  Image Loss: 0.0206
  Total Loss: 0.0217
  Image grad max: 0.001882525160908699
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0001
  Image Loss: 0.0205
  Total Loss: 0.0216
  Image grad max: 0.0018835258670151234
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0001
  Image Loss: 0.0205
  Total Loss: 0.0216
  Image grad max: 0.0018458584090694785
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0001
  Image Loss: 0.0205
  Total Loss: 0.0216
  Image grad max: 0.0018544604536145926
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0001
  Image Loss: 0.0205
  Total Loss: 0.0216
  Image grad max: 0.001870097010396421
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0001
  Image Loss: 0.0205
  Total Loss: 0.0216
  Image grad max: 0.0018892176449298859
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0001
  Image Loss: 0.0205
  Total Loss: 0.0215
  Image grad max: 0.0018641677452251315
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0001
  Image Loss: 0.0205
  Total Loss: 0.0215
  Image grad max: 0.0018434382509440184
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0001
  Image Loss: 0.0204
  Total Loss: 0.0215
  Image grad max: 0.0018586229998618364
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0001
  Image Loss: 0.0204
  Total Loss: 0.0215
  Image grad max: 0.0018880520947277546
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0001
  Image Loss: 0.0204
  Total Loss: 0.0215
  Image grad max: 0.0018801211845129728
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0001
  Image Loss: 0.0204
  Total Loss: 0.0215
  Image grad max: 0.001851094188168645
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0001
  Image Loss: 0.0204
  Total Loss: 0.0214
  Image grad max: 0.0018517327262088656
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0001
  Image Loss: 0.0204
  Total Loss: 0.0214
  Image grad max: 0.001879184041172266
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.499 0.499]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0001
  Image Loss: 0.0204
  Total Loss: 0.0214
  Image grad max: 0.001890723593533039
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0001
  Image Loss: 0.0204
  Total Loss: 0.0214
  Image grad max: 0.001870738691650331
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0001
  Image Loss: 0.0203
  Total Loss: 0.0214
  Image grad max: 0.0018566291546449065
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0001
  Image Loss: 0.0203
  Total Loss: 0.0213
  Image grad max: 0.001872981432825327
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.499 0.5  ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0001
  Image Loss: 0.0203
  Total Loss: 0.0213
  Image grad max: 0.0018917025299742818
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0001
  Image Loss: 0.0203
  Total Loss: 0.0213
  Image grad max: 0.001881244475953281
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0001
  Image Loss: 0.0203
  Total Loss: 0.0213
  Image grad max: 0.0018632058054208755
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0001
  Image Loss: 0.0203
  Total Loss: 0.0213
  Image grad max: 0.001871188753284514
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0001
  Image Loss: 0.0203
  Total Loss: 0.0213
  Image grad max: 0.001887103309854865
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0001
  Image Loss: 0.0203
  Total Loss: 0.0212
  Image grad max: 0.0018862704746425152
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0001
  Image Loss: 0.0202
  Total Loss: 0.0212
  Image grad max: 0.001874803681857884
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0001
  Image Loss: 0.0202
  Total Loss: 0.0212
  Image grad max: 0.0018766371067613363
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0001
  Image Loss: 0.0202
  Total Loss: 0.0212
  Image grad max: 0.0018860134296119213
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0001
  Image Loss: 0.0202
  Total Loss: 0.0212
  Image grad max: 0.0018911082297563553
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0001
  Image Loss: 0.0202
  Total Loss: 0.0211
  Image grad max: 0.0018808534368872643
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0001
  Image Loss: 0.0202
  Total Loss: 0.0211
  Image grad max: 0.001869281753897667
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0001
  Image Loss: 0.0202
  Total Loss: 0.0211
  Image grad max: 0.001886655343696475
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0001
  Image Loss: 0.0201
  Total Loss: 0.0211
  Image grad max: 0.0019018898019567132
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0001
  Image Loss: 0.0201
  Total Loss: 0.0211
  Image grad max: 0.0018893685191869736
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0001
  Image Loss: 0.0201
  Total Loss: 0.0211
  Image grad max: 0.0018762130057439208
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0001
  Image Loss: 0.0201
  Total Loss: 0.0210
  Image grad max: 0.0018868985353037715
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0001
  Image Loss: 0.0201
  Total Loss: 0.0210
  Image grad max: 0.0018972113030031323
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0001
  Image Loss: 0.0201
  Total Loss: 0.0210
  Image grad max: 0.0018626279197633266
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0001
  Image Loss: 0.0201
  Total Loss: 0.0210
  Image grad max: 0.0018789892783388495
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0001
  Image Loss: 0.0200
  Total Loss: 0.0210
  Image grad max: 0.0019135582260787487
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0001
  Image Loss: 0.0200
  Total Loss: 0.0210
  Image grad max: 0.0019045805092900991
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0001
  Image Loss: 0.0200
  Total Loss: 0.0209
  Image grad max: 0.0018736759666353464
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0001
  Image Loss: 0.0200
  Total Loss: 0.0209
  Image grad max: 0.0018793579656630754
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0001
  Image Loss: 0.0200
  Total Loss: 0.0209
  Image grad max: 0.001908001839183271
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0001
  Image Loss: 0.0200
  Total Loss: 0.0209
  Image grad max: 0.0019091124413534999
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0001
  Image Loss: 0.0200
  Total Loss: 0.0209
  Image grad max: 0.0018842623103410006
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0001
  Image Loss: 0.0200
  Total Loss: 0.0208
  Image grad max: 0.0018821904668584466
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0001
  Image Loss: 0.0199
  Total Loss: 0.0208
  Image grad max: 0.0019032101845368743
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0001
  Image Loss: 0.0199
  Total Loss: 0.0208
  Image grad max: 0.0019069946138188243
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0001
  Image Loss: 0.0199
  Total Loss: 0.0208
  Image grad max: 0.0018925003241747618
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0001
  Image Loss: 0.0199
  Total Loss: 0.0208
  Image grad max: 0.0018884652527049184
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0001
  Image Loss: 0.0199
  Total Loss: 0.0208
  Image grad max: 0.0018996645230799913
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0001
  Image Loss: 0.0199
  Total Loss: 0.0207
  Image grad max: 0.0019071601564064622
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0001
  Image Loss: 0.0199
  Total Loss: 0.0207
  Image grad max: 0.0019005571957677603
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0001
  Image Loss: 0.0198
  Total Loss: 0.0207
  Image grad max: 0.001893683336675167
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0001
  Image Loss: 0.0198
  Total Loss: 0.0207
  Image grad max: 0.0019009351963177323
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0001
  Image Loss: 0.0198
  Total Loss: 0.0207
  Image grad max: 0.0019089357228949666
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0001
  Image Loss: 0.0198
  Total Loss: 0.0207
  Image grad max: 0.0019030842231586576
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0001
  Image Loss: 0.0198
  Total Loss: 0.0206
  Image grad max: 0.001896918285638094
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0001
  Image Loss: 0.0198
  Total Loss: 0.0206
  Image grad max: 0.0019043923821300268
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0001
  Image Loss: 0.0198
  Total Loss: 0.0206
  Image grad max: 0.0019101526122540236
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0001
  Image Loss: 0.0197
  Total Loss: 0.0206
  Image grad max: 0.0018946544732898474
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0001
  Image Loss: 0.0197
  Total Loss: 0.0206
  Image grad max: 0.001895722234621644
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0001
  Image Loss: 0.0197
  Total Loss: 0.0206
  Image grad max: 0.0019145332043990493
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0001
  Image Loss: 0.0197
  Total Loss: 0.0205
  Image grad max: 0.0019141200464218855
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0001
  Image Loss: 0.0197
  Total Loss: 0.0205
  Image grad max: 0.0018987685907632113
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0001
  Image Loss: 0.0197
  Total Loss: 0.0205
  Image grad max: 0.0018964759074151516
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0001
  Image Loss: 0.0197
  Total Loss: 0.0205
  Image grad max: 0.0019131930312141776
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0001
  Image Loss: 0.0196
  Total Loss: 0.0205
  Image grad max: 0.0019170999294146895
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0001
  Image Loss: 0.0196
  Total Loss: 0.0205
  Image grad max: 0.0018984030466526747
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0001
  Image Loss: 0.0196
  Total Loss: 0.0204
  Image grad max: 0.001895297085866332
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0001
  Image Loss: 0.0196
  Total Loss: 0.0204
  Image grad max: 0.001917329616844654
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0001
  Image Loss: 0.0196
  Total Loss: 0.0204
  Image grad max: 0.0019201962277293205
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0001
  Image Loss: 0.0196
  Total Loss: 0.0204
  Image grad max: 0.0019007744267582893
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0001
  Image Loss: 0.0196
  Total Loss: 0.0204
  Image grad max: 0.00190060050226748
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0001
  Image Loss: 0.0195
  Total Loss: 0.0204
  Image grad max: 0.0019175640773028135
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0001
  Image Loss: 0.0195
  Total Loss: 0.0203
  Image grad max: 0.0019184733973816037
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0001
  Image Loss: 0.0195
  Total Loss: 0.0203
  Image grad max: 0.0019038742175325751
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0001
  Image Loss: 0.0195
  Total Loss: 0.0203
  Image grad max: 0.0019051084527745843
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0001
  Image Loss: 0.0195
  Total Loss: 0.0203
  Image grad max: 0.0019175142515450716
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0001
  Image Loss: 0.0195
  Total Loss: 0.0203
  Image grad max: 0.0019174093613401055
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0001
  Image Loss: 0.0195
  Total Loss: 0.0203
  Image grad max: 0.0019064412917941809
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0001
  Image Loss: 0.0194
  Total Loss: 0.0202
  Image grad max: 0.0019085658714175224
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0001
  Image Loss: 0.0194
  Total Loss: 0.0202
  Image grad max: 0.00191882299259305
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0001
  Image Loss: 0.0194
  Total Loss: 0.0202
  Image grad max: 0.0019149126019328833
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0001
  Image Loss: 0.0194
  Total Loss: 0.0202
  Image grad max: 0.001905324636027217
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0001
  Image Loss: 0.0194
  Total Loss: 0.0202
  Image grad max: 0.0019143277313560247
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0001
  Image Loss: 0.0194
  Total Loss: 0.0202
  Image grad max: 0.0019201959948986769
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0001
  Image Loss: 0.0194
  Total Loss: 0.0201
  Image grad max: 0.001913715386763215
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0001
  Image Loss: 0.0194
  Total Loss: 0.0201
  Image grad max: 0.0019089026609435678
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0001
  Image Loss: 0.0193
  Total Loss: 0.0201
  Image grad max: 0.0019102012738585472
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0001
  Image Loss: 0.0193
  Total Loss: 0.0201
  Image grad max: 0.0019207984441891313
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0001
  Image Loss: 0.0193
  Total Loss: 0.0201
  Image grad max: 0.0019187778234481812
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0001
  Image Loss: 0.0193
  Total Loss: 0.0201
  Image grad max: 0.0019100182689726353
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0001
  Image Loss: 0.0193
  Total Loss: 0.0200
  Image grad max: 0.0019139077048748732
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0001
  Image Loss: 0.0193
  Total Loss: 0.0200
  Image grad max: 0.001907989615574479
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.    0.5   0.499]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0001
  Image Loss: 0.0193
  Total Loss: 0.0200
  Image grad max: 0.001852530869655311
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0001
  Image Loss: 0.0192
  Total Loss: 0.0200
  Image grad max: 0.0019259756663814187
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0001
  Image Loss: 0.0192
  Total Loss: 0.0200
  Image grad max: 0.001965704606845975
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0001
  Image Loss: 0.0192
  Total Loss: 0.0200
  Image grad max: 0.0018941020825877786
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0001
  Image Loss: 0.0192
  Total Loss: 0.0199
  Image grad max: 0.0018694712780416012
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0001
  Image Loss: 0.0192
  Total Loss: 0.0199
  Image grad max: 0.001938988920301199
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0001
  Image Loss: 0.0192
  Total Loss: 0.0199
  Image grad max: 0.0019506484968587756
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0001
  Image Loss: 0.0192
  Total Loss: 0.0199
  Image grad max: 0.0018860592972487211
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0001
  Image Loss: 0.0191
  Total Loss: 0.0199
  Image grad max: 0.0018827884923666716
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0001
  Image Loss: 0.0191
  Total Loss: 0.0199
  Image grad max: 0.0019427360966801643
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0001
  Image Loss: 0.0191
  Total Loss: 0.0198
  Image grad max: 0.0019388661021366715
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0001
  Image Loss: 0.0191
  Total Loss: 0.0198
  Image grad max: 0.0018850592896342278
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0001
  Image Loss: 0.0191
  Total Loss: 0.0198
  Image grad max: 0.001900605857372284
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0001
  Image Loss: 0.0191
  Total Loss: 0.0198
  Image grad max: 0.0019459170289337635
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0001
  Image Loss: 0.0191
  Total Loss: 0.0198
  Image grad max: 0.0019235099898651242
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0001
  Image Loss: 0.0190
  Total Loss: 0.0198
  Image grad max: 0.0018870870117098093
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0001
  Image Loss: 0.0190
  Total Loss: 0.0197
  Image grad max: 0.0019164257682859898
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0001
  Image Loss: 0.0190
  Total Loss: 0.0197
  Image grad max: 0.0019433221314102411
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0001
  Image Loss: 0.0190
  Total Loss: 0.0197
  Image grad max: 0.001910460414364934
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0001
  Image Loss: 0.0190
  Total Loss: 0.0197
  Image grad max: 0.0018945673946291208
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0001
  Image Loss: 0.0190
  Total Loss: 0.0197
  Image grad max: 0.001927767414599657
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0001
  Image Loss: 0.0190
  Total Loss: 0.0197
  Image grad max: 0.001930008176714182
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0001
  Image Loss: 0.0189
  Total Loss: 0.0196
  Image grad max: 0.001903907279483974
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0001
  Image Loss: 0.0189
  Total Loss: 0.0196
  Image grad max: 0.001909191720187664
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0001
  Image Loss: 0.0189
  Total Loss: 0.0196
  Image grad max: 0.0019328135531395674
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0001
  Image Loss: 0.0189
  Total Loss: 0.0196
  Image grad max: 0.0019208173034712672
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0001
  Image Loss: 0.0189
  Total Loss: 0.0196
  Image grad max: 0.0019037822494283319
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0001
  Image Loss: 0.0189
  Total Loss: 0.0196
  Image grad max: 0.001916628098115325
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0001
  Image Loss: 0.0189
  Total Loss: 0.0195
  Image grad max: 0.0019313286757096648
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0001
  Image Loss: 0.0188
  Total Loss: 0.0195
  Image grad max: 0.0019153454340994358
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0001
  Image Loss: 0.0188
  Total Loss: 0.0195
  Image grad max: 0.0019075865857303143
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0001
  Image Loss: 0.0188
  Total Loss: 0.0195
  Image grad max: 0.0019251378253102303
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
Adversarial example training visualization saved to adversarial_figures/adversarial_testing.png
Target label was: [[0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.5]]
