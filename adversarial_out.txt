Epoch [1/30], Batch [0/6000], Loss: 2.5355
Epoch [1/30], Batch [100/6000], Loss: 2.3216
Epoch [1/30], Batch [200/6000], Loss: 2.0186
Epoch [1/30], Batch [300/6000], Loss: 1.8905
Epoch [1/30], Batch [400/6000], Loss: 1.7979
Epoch [1/30], Batch [500/6000], Loss: 1.9248
Epoch [1/30], Batch [600/6000], Loss: 1.8370
Epoch [1/30], Batch [700/6000], Loss: 1.7164
Epoch [1/30], Batch [800/6000], Loss: 1.8097
Epoch [1/30], Batch [900/6000], Loss: 1.8669
Epoch [1/30], Batch [1000/6000], Loss: 1.6899
Epoch [1/30], Batch [1100/6000], Loss: 1.7000
Epoch [1/30], Batch [1200/6000], Loss: 1.7319
Epoch [1/30], Batch [1300/6000], Loss: 1.5885
Epoch [1/30], Batch [1400/6000], Loss: 1.7475
Epoch [1/30], Batch [1500/6000], Loss: 1.6616
Epoch [1/30], Batch [1600/6000], Loss: 1.5809
Epoch [1/30], Batch [1700/6000], Loss: 1.8046
Epoch [1/30], Batch [1800/6000], Loss: 1.5893
Epoch [1/30], Batch [1900/6000], Loss: 1.5770
Epoch [1/30], Batch [2000/6000], Loss: 1.7419
Epoch [1/30], Batch [2100/6000], Loss: 1.6231
Epoch [1/30], Batch [2200/6000], Loss: 1.5953
Epoch [1/30], Batch [2300/6000], Loss: 1.6376
Epoch [1/30], Batch [2400/6000], Loss: 1.6830
Epoch [1/30], Batch [2500/6000], Loss: 1.5264
Epoch [1/30], Batch [2600/6000], Loss: 1.6712
Epoch [1/30], Batch [2700/6000], Loss: 1.5651
Epoch [1/30], Batch [2800/6000], Loss: 1.6526
Epoch [1/30], Batch [2900/6000], Loss: 1.5463
Epoch [1/30], Batch [3000/6000], Loss: 1.6295
Epoch [1/30], Batch [3100/6000], Loss: 1.6768
Epoch [1/30], Batch [3200/6000], Loss: 1.5288
Epoch [1/30], Batch [3300/6000], Loss: 1.6320
Epoch [1/30], Batch [3400/6000], Loss: 1.5193
Epoch [1/30], Batch [3500/6000], Loss: 1.5757
Epoch [1/30], Batch [3600/6000], Loss: 1.5712
Epoch [1/30], Batch [3700/6000], Loss: 1.6065
Epoch [1/30], Batch [3800/6000], Loss: 1.5621
Epoch [1/30], Batch [3900/6000], Loss: 1.5465
Epoch [1/30], Batch [4000/6000], Loss: 1.5922
Epoch [1/30], Batch [4100/6000], Loss: 1.5443
Epoch [1/30], Batch [4200/6000], Loss: 1.5574
Epoch [1/30], Batch [4300/6000], Loss: 1.5378
Epoch [1/30], Batch [4400/6000], Loss: 1.5501
Epoch [1/30], Batch [4500/6000], Loss: 1.5337
Epoch [1/30], Batch [4600/6000], Loss: 1.5469
Epoch [1/30], Batch [4700/6000], Loss: 1.6647
Epoch [1/30], Batch [4800/6000], Loss: 1.5213
Epoch [1/30], Batch [4900/6000], Loss: 1.5406
Epoch [1/30], Batch [5000/6000], Loss: 1.5356
Epoch [1/30], Batch [5100/6000], Loss: 1.7362
Epoch [1/30], Batch [5200/6000], Loss: 1.6596
Epoch [1/30], Batch [5300/6000], Loss: 1.5295
Epoch [1/30], Batch [5400/6000], Loss: 1.6523
Epoch [1/30], Batch [5500/6000], Loss: 1.5327
Epoch [1/30], Batch [5600/6000], Loss: 1.5385
Epoch [1/30], Batch [5700/6000], Loss: 1.6237
Epoch [1/30], Batch [5800/6000], Loss: 1.5975
Epoch [1/30], Batch [5900/6000], Loss: 1.5388
Epoch [1/30], Loss: 1.6699
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 1.8545
Epoch [2/30], Batch [100/6000], Loss: 1.5139
Epoch [2/30], Batch [200/6000], Loss: 1.7868
Epoch [2/30], Batch [300/6000], Loss: 1.5290
Epoch [2/30], Batch [400/6000], Loss: 1.5828
Epoch [2/30], Batch [500/6000], Loss: 1.7232
Epoch [2/30], Batch [600/6000], Loss: 1.5232
Epoch [2/30], Batch [700/6000], Loss: 1.5049
Epoch [2/30], Batch [800/6000], Loss: 1.7176
Epoch [2/30], Batch [900/6000], Loss: 1.5947
Epoch [2/30], Batch [1000/6000], Loss: 1.5375
Epoch [2/30], Batch [1100/6000], Loss: 1.5295
Epoch [2/30], Batch [1200/6000], Loss: 1.6160
Epoch [2/30], Batch [1300/6000], Loss: 1.5136
Epoch [2/30], Batch [1400/6000], Loss: 1.6120
Epoch [2/30], Batch [1500/6000], Loss: 1.6611
Epoch [2/30], Batch [1600/6000], Loss: 1.6248
Epoch [2/30], Batch [1700/6000], Loss: 1.5697
Epoch [2/30], Batch [1800/6000], Loss: 1.5479
Epoch [2/30], Batch [1900/6000], Loss: 1.5308
Epoch [2/30], Batch [2000/6000], Loss: 1.5407
Epoch [2/30], Batch [2100/6000], Loss: 1.6252
Epoch [2/30], Batch [2200/6000], Loss: 1.5245
Epoch [2/30], Batch [2300/6000], Loss: 1.7360
Epoch [2/30], Batch [2400/6000], Loss: 1.5767
Epoch [2/30], Batch [2500/6000], Loss: 1.5266
Epoch [2/30], Batch [2600/6000], Loss: 1.5922
Epoch [2/30], Batch [2700/6000], Loss: 1.5331
Epoch [2/30], Batch [2800/6000], Loss: 1.6253
Epoch [2/30], Batch [2900/6000], Loss: 1.5133
Epoch [2/30], Batch [3000/6000], Loss: 1.5271
Epoch [2/30], Batch [3100/6000], Loss: 1.5176
Epoch [2/30], Batch [3200/6000], Loss: 1.5652
Epoch [2/30], Batch [3300/6000], Loss: 1.5101
Epoch [2/30], Batch [3400/6000], Loss: 1.6112
Epoch [2/30], Batch [3500/6000], Loss: 1.5190
Epoch [2/30], Batch [3600/6000], Loss: 1.5014
Epoch [2/30], Batch [3700/6000], Loss: 1.6097
Epoch [2/30], Batch [3800/6000], Loss: 1.5217
Epoch [2/30], Batch [3900/6000], Loss: 1.6389
Epoch [2/30], Batch [4000/6000], Loss: 1.5052
Epoch [2/30], Batch [4100/6000], Loss: 1.5383
Epoch [2/30], Batch [4200/6000], Loss: 1.5203
Epoch [2/30], Batch [4300/6000], Loss: 1.5621
Epoch [2/30], Batch [4400/6000], Loss: 1.5383
Epoch [2/30], Batch [4500/6000], Loss: 1.5094
Epoch [2/30], Batch [4600/6000], Loss: 1.6263
Epoch [2/30], Batch [4700/6000], Loss: 1.5770
Epoch [2/30], Batch [4800/6000], Loss: 1.5113
Epoch [2/30], Batch [4900/6000], Loss: 1.5420
Epoch [2/30], Batch [5000/6000], Loss: 1.5083
Epoch [2/30], Batch [5100/6000], Loss: 1.5173
Epoch [2/30], Batch [5200/6000], Loss: 1.5280
Epoch [2/30], Batch [5300/6000], Loss: 1.5200
Epoch [2/30], Batch [5400/6000], Loss: 1.6536
Epoch [2/30], Batch [5500/6000], Loss: 1.5033
Epoch [2/30], Batch [5600/6000], Loss: 1.6209
Epoch [2/30], Batch [5700/6000], Loss: 1.5355
Epoch [2/30], Batch [5800/6000], Loss: 1.4916
Epoch [2/30], Batch [5900/6000], Loss: 1.5172
Epoch [2/30], Loss: 1.5648
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 1.5113
Epoch [3/30], Batch [100/6000], Loss: 1.5733
Epoch [3/30], Batch [200/6000], Loss: 1.5250
Epoch [3/30], Batch [300/6000], Loss: 1.6197
Epoch [3/30], Batch [400/6000], Loss: 1.5741
Epoch [3/30], Batch [500/6000], Loss: 1.5258
Epoch [3/30], Batch [600/6000], Loss: 1.5380
Epoch [3/30], Batch [700/6000], Loss: 1.5853
Epoch [3/30], Batch [800/6000], Loss: 1.6454
Epoch [3/30], Batch [900/6000], Loss: 1.5154
Epoch [3/30], Batch [1000/6000], Loss: 1.5368
Epoch [3/30], Batch [1100/6000], Loss: 1.5366
Epoch [3/30], Batch [1200/6000], Loss: 1.5997
Epoch [3/30], Batch [1300/6000], Loss: 1.4983
Epoch [3/30], Batch [1400/6000], Loss: 1.5812
Epoch [3/30], Batch [1500/6000], Loss: 1.6328
Epoch [3/30], Batch [1600/6000], Loss: 1.5170
Epoch [3/30], Batch [1700/6000], Loss: 1.5069
Epoch [3/30], Batch [1800/6000], Loss: 1.5184
Epoch [3/30], Batch [1900/6000], Loss: 1.5221
Epoch [3/30], Batch [2000/6000], Loss: 1.5208
Epoch [3/30], Batch [2100/6000], Loss: 1.6351
Epoch [3/30], Batch [2200/6000], Loss: 1.5040
Epoch [3/30], Batch [2300/6000], Loss: 1.6313
Epoch [3/30], Batch [2400/6000], Loss: 1.5173
Epoch [3/30], Batch [2500/6000], Loss: 1.5125
Epoch [3/30], Batch [2600/6000], Loss: 1.5103
Epoch [3/30], Batch [2700/6000], Loss: 1.5225
Epoch [3/30], Batch [2800/6000], Loss: 1.5180
Epoch [3/30], Batch [2900/6000], Loss: 1.5204
Epoch [3/30], Batch [3000/6000], Loss: 1.6247
Epoch [3/30], Batch [3100/6000], Loss: 1.5305
Epoch [3/30], Batch [3200/6000], Loss: 1.5849
Epoch [3/30], Batch [3300/6000], Loss: 1.6077
Epoch [3/30], Batch [3400/6000], Loss: 1.5971
Epoch [3/30], Batch [3500/6000], Loss: 1.4903
Epoch [3/30], Batch [3600/6000], Loss: 1.5032
Epoch [3/30], Batch [3700/6000], Loss: 1.5941
Epoch [3/30], Batch [3800/6000], Loss: 1.5064
Epoch [3/30], Batch [3900/6000], Loss: 1.5016
Epoch [3/30], Batch [4000/6000], Loss: 1.5447
Epoch [3/30], Batch [4100/6000], Loss: 1.6260
Epoch [3/30], Batch [4200/6000], Loss: 1.5869
Epoch [3/30], Batch [4300/6000], Loss: 1.5045
Epoch [3/30], Batch [4400/6000], Loss: 1.5764
Epoch [3/30], Batch [4500/6000], Loss: 1.5041
Epoch [3/30], Batch [4600/6000], Loss: 1.4984
Epoch [3/30], Batch [4700/6000], Loss: 1.5092
Epoch [3/30], Batch [4800/6000], Loss: 1.4915
Epoch [3/30], Batch [4900/6000], Loss: 1.6052
Epoch [3/30], Batch [5000/6000], Loss: 1.5437
Epoch [3/30], Batch [5100/6000], Loss: 1.5092
Epoch [3/30], Batch [5200/6000], Loss: 1.6380
Epoch [3/30], Batch [5300/6000], Loss: 1.5111
Epoch [3/30], Batch [5400/6000], Loss: 1.5502
Epoch [3/30], Batch [5500/6000], Loss: 1.5139
Epoch [3/30], Batch [5600/6000], Loss: 1.4946
Epoch [3/30], Batch [5700/6000], Loss: 1.6245
Epoch [3/30], Batch [5800/6000], Loss: 1.5274
Epoch [3/30], Batch [5900/6000], Loss: 1.5083
Epoch [3/30], Loss: 1.5430
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 1.6224
Epoch [4/30], Batch [100/6000], Loss: 1.6984
Epoch [4/30], Batch [200/6000], Loss: 1.4894
Epoch [4/30], Batch [300/6000], Loss: 1.5058
Epoch [4/30], Batch [400/6000], Loss: 1.5172
Epoch [4/30], Batch [500/6000], Loss: 1.5168
Epoch [4/30], Batch [600/6000], Loss: 1.5116
Epoch [4/30], Batch [700/6000], Loss: 1.5036
Epoch [4/30], Batch [800/6000], Loss: 1.6086
Epoch [4/30], Batch [900/6000], Loss: 1.6051
Epoch [4/30], Batch [1000/6000], Loss: 1.4964
Epoch [4/30], Batch [1100/6000], Loss: 1.6245
Epoch [4/30], Batch [1200/6000], Loss: 1.5150
Epoch [4/30], Batch [1300/6000], Loss: 1.5067
Epoch [4/30], Batch [1400/6000], Loss: 1.5016
Epoch [4/30], Batch [1500/6000], Loss: 1.5326
Epoch [4/30], Batch [1600/6000], Loss: 1.6034
Epoch [4/30], Batch [1700/6000], Loss: 1.5180
Epoch [4/30], Batch [1800/6000], Loss: 1.5467
Epoch [4/30], Batch [1900/6000], Loss: 1.6305
Epoch [4/30], Batch [2000/6000], Loss: 1.5105
Epoch [4/30], Batch [2100/6000], Loss: 1.5099
Epoch [4/30], Batch [2200/6000], Loss: 1.5127
Epoch [4/30], Batch [2300/6000], Loss: 1.5925
Epoch [4/30], Batch [2400/6000], Loss: 1.5286
Epoch [4/30], Batch [2500/6000], Loss: 1.5008
Epoch [4/30], Batch [2600/6000], Loss: 1.5078
Epoch [4/30], Batch [2700/6000], Loss: 1.4957
Epoch [4/30], Batch [2800/6000], Loss: 1.5019
Epoch [4/30], Batch [2900/6000], Loss: 1.4949
Epoch [4/30], Batch [3000/6000], Loss: 1.5375
Epoch [4/30], Batch [3100/6000], Loss: 1.4915
Epoch [4/30], Batch [3200/6000], Loss: 1.5034
Epoch [4/30], Batch [3300/6000], Loss: 1.5398
Epoch [4/30], Batch [3400/6000], Loss: 1.5074
Epoch [4/30], Batch [3500/6000], Loss: 1.5013
Epoch [4/30], Batch [3600/6000], Loss: 1.4978
Epoch [4/30], Batch [3700/6000], Loss: 1.4904
Epoch [4/30], Batch [3800/6000], Loss: 1.4947
Epoch [4/30], Batch [3900/6000], Loss: 1.6190
Epoch [4/30], Batch [4000/6000], Loss: 1.5424
Epoch [4/30], Batch [4100/6000], Loss: 1.5222
Epoch [4/30], Batch [4200/6000], Loss: 1.5132
Epoch [4/30], Batch [4300/6000], Loss: 1.6025
Epoch [4/30], Batch [4400/6000], Loss: 1.4981
Epoch [4/30], Batch [4500/6000], Loss: 1.5217
Epoch [4/30], Batch [4600/6000], Loss: 1.5861
Epoch [4/30], Batch [4700/6000], Loss: 1.5275
Epoch [4/30], Batch [4800/6000], Loss: 1.5324
Epoch [4/30], Batch [4900/6000], Loss: 1.5979
Epoch [4/30], Batch [5000/6000], Loss: 1.5303
Epoch [4/30], Batch [5100/6000], Loss: 1.5888
Epoch [4/30], Batch [5200/6000], Loss: 1.5026
Epoch [4/30], Batch [5300/6000], Loss: 1.5026
Epoch [4/30], Batch [5400/6000], Loss: 1.6095
Epoch [4/30], Batch [5500/6000], Loss: 1.5115
Epoch [4/30], Batch [5600/6000], Loss: 1.4951
Epoch [4/30], Batch [5700/6000], Loss: 1.4947
Epoch [4/30], Batch [5800/6000], Loss: 1.5089
Epoch [4/30], Batch [5900/6000], Loss: 1.5033
Epoch [4/30], Loss: 1.5309
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 1.5005
Epoch [5/30], Batch [100/6000], Loss: 1.5203
Epoch [5/30], Batch [200/6000], Loss: 1.4946
Epoch [5/30], Batch [300/6000], Loss: 1.5023
Epoch [5/30], Batch [400/6000], Loss: 1.6015
Epoch [5/30], Batch [500/6000], Loss: 1.4997
Epoch [5/30], Batch [600/6000], Loss: 1.4915
Epoch [5/30], Batch [700/6000], Loss: 1.4926
Epoch [5/30], Batch [800/6000], Loss: 1.4990
Epoch [5/30], Batch [900/6000], Loss: 1.5131
Epoch [5/30], Batch [1000/6000], Loss: 1.5252
Epoch [5/30], Batch [1100/6000], Loss: 1.5113
Epoch [5/30], Batch [1200/6000], Loss: 1.5112
Epoch [5/30], Batch [1300/6000], Loss: 1.6075
Epoch [5/30], Batch [1400/6000], Loss: 1.5038
Epoch [5/30], Batch [1500/6000], Loss: 1.4995
Epoch [5/30], Batch [1600/6000], Loss: 1.6218
Epoch [5/30], Batch [1700/6000], Loss: 1.5958
Epoch [5/30], Batch [1800/6000], Loss: 1.5132
Epoch [5/30], Batch [1900/6000], Loss: 1.5205
Epoch [5/30], Batch [2000/6000], Loss: 1.5039
Epoch [5/30], Batch [2100/6000], Loss: 1.4928
Epoch [5/30], Batch [2200/6000], Loss: 1.4945
Epoch [5/30], Batch [2300/6000], Loss: 1.5384
Epoch [5/30], Batch [2400/6000], Loss: 1.5461
Epoch [5/30], Batch [2500/6000], Loss: 1.6586
Epoch [5/30], Batch [2600/6000], Loss: 1.5232
Epoch [5/30], Batch [2700/6000], Loss: 1.5014
Epoch [5/30], Batch [2800/6000], Loss: 1.5250
Epoch [5/30], Batch [2900/6000], Loss: 1.5067
Epoch [5/30], Batch [3000/6000], Loss: 1.5232
Epoch [5/30], Batch [3100/6000], Loss: 1.5190
Epoch [5/30], Batch [3200/6000], Loss: 1.6100
Epoch [5/30], Batch [3300/6000], Loss: 1.4959
Epoch [5/30], Batch [3400/6000], Loss: 1.4967
Epoch [5/30], Batch [3500/6000], Loss: 1.5107
Epoch [5/30], Batch [3600/6000], Loss: 1.5095
Epoch [5/30], Batch [3700/6000], Loss: 1.5010
Epoch [5/30], Batch [3800/6000], Loss: 1.5035
Epoch [5/30], Batch [3900/6000], Loss: 1.4997
Epoch [5/30], Batch [4000/6000], Loss: 1.4971
Epoch [5/30], Batch [4100/6000], Loss: 1.5038
Epoch [5/30], Batch [4200/6000], Loss: 1.4917
Epoch [5/30], Batch [4300/6000], Loss: 1.5041
Epoch [5/30], Batch [4400/6000], Loss: 1.5354
Epoch [5/30], Batch [4500/6000], Loss: 1.5120
Epoch [5/30], Batch [4600/6000], Loss: 1.4955
Epoch [5/30], Batch [4700/6000], Loss: 1.4943
Epoch [5/30], Batch [4800/6000], Loss: 1.4989
Epoch [5/30], Batch [4900/6000], Loss: 1.4847
Epoch [5/30], Batch [5000/6000], Loss: 1.4918
Epoch [5/30], Batch [5100/6000], Loss: 1.5142
Epoch [5/30], Batch [5200/6000], Loss: 1.5053
Epoch [5/30], Batch [5300/6000], Loss: 1.5075
Epoch [5/30], Batch [5400/6000], Loss: 1.5246
Epoch [5/30], Batch [5500/6000], Loss: 1.5063
Epoch [5/30], Batch [5600/6000], Loss: 1.4934
Epoch [5/30], Batch [5700/6000], Loss: 1.4939
Epoch [5/30], Batch [5800/6000], Loss: 1.5020
Epoch [5/30], Batch [5900/6000], Loss: 1.5087
Epoch [5/30], Loss: 1.5221
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 1.4931
Epoch [6/30], Batch [100/6000], Loss: 1.4875
Epoch [6/30], Batch [200/6000], Loss: 1.4927
Epoch [6/30], Batch [300/6000], Loss: 1.4973
Epoch [6/30], Batch [400/6000], Loss: 1.5182
Epoch [6/30], Batch [500/6000], Loss: 1.6152
Epoch [6/30], Batch [600/6000], Loss: 1.5086
Epoch [6/30], Batch [700/6000], Loss: 1.5021
Epoch [6/30], Batch [800/6000], Loss: 1.4969
Epoch [6/30], Batch [900/6000], Loss: 1.4941
Epoch [6/30], Batch [1000/6000], Loss: 1.4944
Epoch [6/30], Batch [1100/6000], Loss: 1.5102
Epoch [6/30], Batch [1200/6000], Loss: 1.5013
Epoch [6/30], Batch [1300/6000], Loss: 1.4952
Epoch [6/30], Batch [1400/6000], Loss: 1.5069
Epoch [6/30], Batch [1500/6000], Loss: 1.4921
Epoch [6/30], Batch [1600/6000], Loss: 1.4891
Epoch [6/30], Batch [1700/6000], Loss: 1.5022
Epoch [6/30], Batch [1800/6000], Loss: 1.4907
Epoch [6/30], Batch [1900/6000], Loss: 1.4936
Epoch [6/30], Batch [2000/6000], Loss: 1.4954
Epoch [6/30], Batch [2100/6000], Loss: 1.6064
Epoch [6/30], Batch [2200/6000], Loss: 1.4924
Epoch [6/30], Batch [2300/6000], Loss: 1.5119
Epoch [6/30], Batch [2400/6000], Loss: 1.6380
Epoch [6/30], Batch [2500/6000], Loss: 1.5881
Epoch [6/30], Batch [2600/6000], Loss: 1.4922
Epoch [6/30], Batch [2700/6000], Loss: 1.6084
Epoch [6/30], Batch [2800/6000], Loss: 1.5270
Epoch [6/30], Batch [2900/6000], Loss: 1.4962
Epoch [6/30], Batch [3000/6000], Loss: 1.5194
Epoch [6/30], Batch [3100/6000], Loss: 1.6003
Epoch [6/30], Batch [3200/6000], Loss: 1.5674
Epoch [6/30], Batch [3300/6000], Loss: 1.4925
Epoch [6/30], Batch [3400/6000], Loss: 1.5417
Epoch [6/30], Batch [3500/6000], Loss: 1.4866
Epoch [6/30], Batch [3600/6000], Loss: 1.5122
Epoch [6/30], Batch [3700/6000], Loss: 1.4891
Epoch [6/30], Batch [3800/6000], Loss: 1.4968
Epoch [6/30], Batch [3900/6000], Loss: 1.5207
Epoch [6/30], Batch [4000/6000], Loss: 1.5015
Epoch [6/30], Batch [4100/6000], Loss: 1.5058
Epoch [6/30], Batch [4200/6000], Loss: 1.4923
Epoch [6/30], Batch [4300/6000], Loss: 1.4978
Epoch [6/30], Batch [4400/6000], Loss: 1.4954
Epoch [6/30], Batch [4500/6000], Loss: 1.5036
Epoch [6/30], Batch [4600/6000], Loss: 1.4915
Epoch [6/30], Batch [4700/6000], Loss: 1.4932
Epoch [6/30], Batch [4800/6000], Loss: 1.4946
Epoch [6/30], Batch [4900/6000], Loss: 1.4858
Epoch [6/30], Batch [5000/6000], Loss: 1.4892
Epoch [6/30], Batch [5100/6000], Loss: 1.4867
Epoch [6/30], Batch [5200/6000], Loss: 1.4877
Epoch [6/30], Batch [5300/6000], Loss: 1.6169
Epoch [6/30], Batch [5400/6000], Loss: 1.4878
Epoch [6/30], Batch [5500/6000], Loss: 1.4885
Epoch [6/30], Batch [5600/6000], Loss: 1.6059
Epoch [6/30], Batch [5700/6000], Loss: 1.4903
Epoch [6/30], Batch [5800/6000], Loss: 1.4931
Epoch [6/30], Batch [5900/6000], Loss: 1.5967
Epoch [6/30], Loss: 1.5163
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 1.5901
Epoch [7/30], Batch [100/6000], Loss: 1.4881
Epoch [7/30], Batch [200/6000], Loss: 1.4920
Epoch [7/30], Batch [300/6000], Loss: 1.5305
Epoch [7/30], Batch [400/6000], Loss: 1.4932
Epoch [7/30], Batch [500/6000], Loss: 1.4899
Epoch [7/30], Batch [600/6000], Loss: 1.6156
Epoch [7/30], Batch [700/6000], Loss: 1.4966
Epoch [7/30], Batch [800/6000], Loss: 1.4981
Epoch [7/30], Batch [900/6000], Loss: 1.5921
Epoch [7/30], Batch [1000/6000], Loss: 1.5025
Epoch [7/30], Batch [1100/6000], Loss: 1.4887
Epoch [7/30], Batch [1200/6000], Loss: 1.5066
Epoch [7/30], Batch [1300/6000], Loss: 1.5048
Epoch [7/30], Batch [1400/6000], Loss: 1.4935
Epoch [7/30], Batch [1500/6000], Loss: 1.4937
Epoch [7/30], Batch [1600/6000], Loss: 1.5133
Epoch [7/30], Batch [1700/6000], Loss: 1.4887
Epoch [7/30], Batch [1800/6000], Loss: 1.4943
Epoch [7/30], Batch [1900/6000], Loss: 1.4942
Epoch [7/30], Batch [2000/6000], Loss: 1.4918
Epoch [7/30], Batch [2100/6000], Loss: 1.4998
Epoch [7/30], Batch [2200/6000], Loss: 1.5132
Epoch [7/30], Batch [2300/6000], Loss: 1.4919
Epoch [7/30], Batch [2400/6000], Loss: 1.4931
Epoch [7/30], Batch [2500/6000], Loss: 1.4927
Epoch [7/30], Batch [2600/6000], Loss: 1.5908
Epoch [7/30], Batch [2700/6000], Loss: 1.4908
Epoch [7/30], Batch [2800/6000], Loss: 1.4890
Epoch [7/30], Batch [2900/6000], Loss: 1.5663
Epoch [7/30], Batch [3000/6000], Loss: 1.4995
Epoch [7/30], Batch [3100/6000], Loss: 1.4958
Epoch [7/30], Batch [3200/6000], Loss: 1.5956
Epoch [7/30], Batch [3300/6000], Loss: 1.6032
Epoch [7/30], Batch [3400/6000], Loss: 1.4997
Epoch [7/30], Batch [3500/6000], Loss: 1.4932
Epoch [7/30], Batch [3600/6000], Loss: 1.5027
Epoch [7/30], Batch [3700/6000], Loss: 1.4892
Epoch [7/30], Batch [3800/6000], Loss: 1.4844
Epoch [7/30], Batch [3900/6000], Loss: 1.4911
Epoch [7/30], Batch [4000/6000], Loss: 1.4916
Epoch [7/30], Batch [4100/6000], Loss: 1.4890
Epoch [7/30], Batch [4200/6000], Loss: 1.5682
Epoch [7/30], Batch [4300/6000], Loss: 1.4868
Epoch [7/30], Batch [4400/6000], Loss: 1.5105
Epoch [7/30], Batch [4500/6000], Loss: 1.4885
Epoch [7/30], Batch [4600/6000], Loss: 1.4890
Epoch [7/30], Batch [4700/6000], Loss: 1.6124
Epoch [7/30], Batch [4800/6000], Loss: 1.5916
Epoch [7/30], Batch [4900/6000], Loss: 1.4876
Epoch [7/30], Batch [5000/6000], Loss: 1.4973
Epoch [7/30], Batch [5100/6000], Loss: 1.4950
Epoch [7/30], Batch [5200/6000], Loss: 1.5161
Epoch [7/30], Batch [5300/6000], Loss: 1.6215
Epoch [7/30], Batch [5400/6000], Loss: 1.5195
Epoch [7/30], Batch [5500/6000], Loss: 1.4886
Epoch [7/30], Batch [5600/6000], Loss: 1.5069
Epoch [7/30], Batch [5700/6000], Loss: 1.4940
Epoch [7/30], Batch [5800/6000], Loss: 1.5074
Epoch [7/30], Batch [5900/6000], Loss: 1.5186
Epoch [7/30], Loss: 1.5109
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 1.4925
Epoch [8/30], Batch [100/6000], Loss: 1.4903
Epoch [8/30], Batch [200/6000], Loss: 1.5080
Epoch [8/30], Batch [300/6000], Loss: 1.6034
Epoch [8/30], Batch [400/6000], Loss: 1.4931
Epoch [8/30], Batch [500/6000], Loss: 1.4928
Epoch [8/30], Batch [600/6000], Loss: 1.4977
Epoch [8/30], Batch [700/6000], Loss: 1.4874
Epoch [8/30], Batch [800/6000], Loss: 1.5257
Epoch [8/30], Batch [900/6000], Loss: 1.4949
Epoch [8/30], Batch [1000/6000], Loss: 1.5042
Epoch [8/30], Batch [1100/6000], Loss: 1.4957
Epoch [8/30], Batch [1200/6000], Loss: 1.4938
Epoch [8/30], Batch [1300/6000], Loss: 1.4920
Epoch [8/30], Batch [1400/6000], Loss: 1.4934
Epoch [8/30], Batch [1500/6000], Loss: 1.4920
Epoch [8/30], Batch [1600/6000], Loss: 1.4990
Epoch [8/30], Batch [1700/6000], Loss: 1.4946
Epoch [8/30], Batch [1800/6000], Loss: 1.4917
Epoch [8/30], Batch [1900/6000], Loss: 1.4948
Epoch [8/30], Batch [2000/6000], Loss: 1.4907
Epoch [8/30], Batch [2100/6000], Loss: 1.4882
Epoch [8/30], Batch [2200/6000], Loss: 1.5966
Epoch [8/30], Batch [2300/6000], Loss: 1.5247
Epoch [8/30], Batch [2400/6000], Loss: 1.4973
Epoch [8/30], Batch [2500/6000], Loss: 1.4959
Epoch [8/30], Batch [2600/6000], Loss: 1.4912
Epoch [8/30], Batch [2700/6000], Loss: 1.4905
Epoch [8/30], Batch [2800/6000], Loss: 1.5027
Epoch [8/30], Batch [2900/6000], Loss: 1.4891
Epoch [8/30], Batch [3000/6000], Loss: 1.4962
Epoch [8/30], Batch [3100/6000], Loss: 1.4941
Epoch [8/30], Batch [3200/6000], Loss: 1.4927
Epoch [8/30], Batch [3300/6000], Loss: 1.4859
Epoch [8/30], Batch [3400/6000], Loss: 1.5730
Epoch [8/30], Batch [3500/6000], Loss: 1.4886
Epoch [8/30], Batch [3600/6000], Loss: 1.4847
Epoch [8/30], Batch [3700/6000], Loss: 1.5981
Epoch [8/30], Batch [3800/6000], Loss: 1.5097
Epoch [8/30], Batch [3900/6000], Loss: 1.4936
Epoch [8/30], Batch [4000/6000], Loss: 1.4896
Epoch [8/30], Batch [4100/6000], Loss: 1.5004
Epoch [8/30], Batch [4200/6000], Loss: 1.5064
Epoch [8/30], Batch [4300/6000], Loss: 1.4860
Epoch [8/30], Batch [4400/6000], Loss: 1.4855
Epoch [8/30], Batch [4500/6000], Loss: 1.5031
Epoch [8/30], Batch [4600/6000], Loss: 1.5559
Epoch [8/30], Batch [4700/6000], Loss: 1.5603
Epoch [8/30], Batch [4800/6000], Loss: 1.5073
Epoch [8/30], Batch [4900/6000], Loss: 1.5924
Epoch [8/30], Batch [5000/6000], Loss: 1.4993
Epoch [8/30], Batch [5100/6000], Loss: 1.5143
Epoch [8/30], Batch [5200/6000], Loss: 1.4920
Epoch [8/30], Batch [5300/6000], Loss: 1.5826
Epoch [8/30], Batch [5400/6000], Loss: 1.4870
Epoch [8/30], Batch [5500/6000], Loss: 1.5136
Epoch [8/30], Batch [5600/6000], Loss: 1.4927
Epoch [8/30], Batch [5700/6000], Loss: 1.4967
Epoch [8/30], Batch [5800/6000], Loss: 1.4914
Epoch [8/30], Batch [5900/6000], Loss: 1.4894
Epoch [8/30], Loss: 1.5072
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 1.4927
Epoch [9/30], Batch [100/6000], Loss: 1.4930
Epoch [9/30], Batch [200/6000], Loss: 1.4901
Epoch [9/30], Batch [300/6000], Loss: 1.5875
Epoch [9/30], Batch [400/6000], Loss: 1.4980
Epoch [9/30], Batch [500/6000], Loss: 1.4917
Epoch [9/30], Batch [600/6000], Loss: 1.5142
Epoch [9/30], Batch [700/6000], Loss: 1.4883
Epoch [9/30], Batch [800/6000], Loss: 1.5023
Epoch [9/30], Batch [900/6000], Loss: 1.4902
Epoch [9/30], Batch [1000/6000], Loss: 1.5034
Epoch [9/30], Batch [1100/6000], Loss: 1.4852
Epoch [9/30], Batch [1200/6000], Loss: 1.4851
Epoch [9/30], Batch [1300/6000], Loss: 1.5896
Epoch [9/30], Batch [1400/6000], Loss: 1.5174
Epoch [9/30], Batch [1500/6000], Loss: 1.4877
Epoch [9/30], Batch [1600/6000], Loss: 1.4820
Epoch [9/30], Batch [1700/6000], Loss: 1.4935
Epoch [9/30], Batch [1800/6000], Loss: 1.4838
Epoch [9/30], Batch [1900/6000], Loss: 1.4912
Epoch [9/30], Batch [2000/6000], Loss: 1.4961
Epoch [9/30], Batch [2100/6000], Loss: 1.5125
Epoch [9/30], Batch [2200/6000], Loss: 1.4878
Epoch [9/30], Batch [2300/6000], Loss: 1.5883
Epoch [9/30], Batch [2400/6000], Loss: 1.4931
Epoch [9/30], Batch [2500/6000], Loss: 1.5701
Epoch [9/30], Batch [2600/6000], Loss: 1.5893
Epoch [9/30], Batch [2700/6000], Loss: 1.5016
Epoch [9/30], Batch [2800/6000], Loss: 1.4866
Epoch [9/30], Batch [2900/6000], Loss: 1.5119
Epoch [9/30], Batch [3000/6000], Loss: 1.4941
Epoch [9/30], Batch [3100/6000], Loss: 1.4900
Epoch [9/30], Batch [3200/6000], Loss: 1.4822
Epoch [9/30], Batch [3300/6000], Loss: 1.5681
Epoch [9/30], Batch [3400/6000], Loss: 1.5022
Epoch [9/30], Batch [3500/6000], Loss: 1.4809
Epoch [9/30], Batch [3600/6000], Loss: 1.4857
Epoch [9/30], Batch [3700/6000], Loss: 1.4849
Epoch [9/30], Batch [3800/6000], Loss: 1.4872
Epoch [9/30], Batch [3900/6000], Loss: 1.5922
Epoch [9/30], Batch [4000/6000], Loss: 1.5827
Epoch [9/30], Batch [4100/6000], Loss: 1.4926
Epoch [9/30], Batch [4200/6000], Loss: 1.4899
Epoch [9/30], Batch [4300/6000], Loss: 1.5037
Epoch [9/30], Batch [4400/6000], Loss: 1.4893
Epoch [9/30], Batch [4500/6000], Loss: 1.4941
Epoch [9/30], Batch [4600/6000], Loss: 1.5834
Epoch [9/30], Batch [4700/6000], Loss: 1.5616
Epoch [9/30], Batch [4800/6000], Loss: 1.4871
Epoch [9/30], Batch [4900/6000], Loss: 1.4912
Epoch [9/30], Batch [5000/6000], Loss: 1.4837
Epoch [9/30], Batch [5100/6000], Loss: 1.4833
Epoch [9/30], Batch [5200/6000], Loss: 1.4868
Epoch [9/30], Batch [5300/6000], Loss: 1.4865
Epoch [9/30], Batch [5400/6000], Loss: 1.4846
Epoch [9/30], Batch [5500/6000], Loss: 1.4888
Epoch [9/30], Batch [5600/6000], Loss: 1.4863
Epoch [9/30], Batch [5700/6000], Loss: 1.5235
Epoch [9/30], Batch [5800/6000], Loss: 1.4927
Epoch [9/30], Batch [5900/6000], Loss: 1.4840
Epoch [9/30], Loss: 1.5041
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 1.4889
Epoch [10/30], Batch [100/6000], Loss: 1.4893
Epoch [10/30], Batch [200/6000], Loss: 1.4821
Epoch [10/30], Batch [300/6000], Loss: 1.4903
Epoch [10/30], Batch [400/6000], Loss: 1.4822
Epoch [10/30], Batch [500/6000], Loss: 1.4894
Epoch [10/30], Batch [600/6000], Loss: 1.4947
Epoch [10/30], Batch [700/6000], Loss: 1.4872
Epoch [10/30], Batch [800/6000], Loss: 1.4866
Epoch [10/30], Batch [900/6000], Loss: 1.4879
Epoch [10/30], Batch [1000/6000], Loss: 1.4879
Epoch [10/30], Batch [1100/6000], Loss: 1.4892
Epoch [10/30], Batch [1200/6000], Loss: 1.6027
Epoch [10/30], Batch [1300/6000], Loss: 1.4848
Epoch [10/30], Batch [1400/6000], Loss: 1.4945
Epoch [10/30], Batch [1500/6000], Loss: 1.4896
Epoch [10/30], Batch [1600/6000], Loss: 1.4901
Epoch [10/30], Batch [1700/6000], Loss: 1.4952
Epoch [10/30], Batch [1800/6000], Loss: 1.4868
Epoch [10/30], Batch [1900/6000], Loss: 1.5973
Epoch [10/30], Batch [2000/6000], Loss: 1.4838
Epoch [10/30], Batch [2100/6000], Loss: 1.4897
Epoch [10/30], Batch [2200/6000], Loss: 1.4995
Epoch [10/30], Batch [2300/6000], Loss: 1.4863
Epoch [10/30], Batch [2400/6000], Loss: 1.4860
Epoch [10/30], Batch [2500/6000], Loss: 1.4919
Epoch [10/30], Batch [2600/6000], Loss: 1.4877
Epoch [10/30], Batch [2700/6000], Loss: 1.4924
Epoch [10/30], Batch [2800/6000], Loss: 1.4858
Epoch [10/30], Batch [2900/6000], Loss: 1.4896
Epoch [10/30], Batch [3000/6000], Loss: 1.4987
Epoch [10/30], Batch [3100/6000], Loss: 1.4841
Epoch [10/30], Batch [3200/6000], Loss: 1.4873
Epoch [10/30], Batch [3300/6000], Loss: 1.4901
Epoch [10/30], Batch [3400/6000], Loss: 1.4836
Epoch [10/30], Batch [3500/6000], Loss: 1.4887
Epoch [10/30], Batch [3600/6000], Loss: 1.4958
Epoch [10/30], Batch [3700/6000], Loss: 1.4823
Epoch [10/30], Batch [3800/6000], Loss: 1.4766
Epoch [10/30], Batch [3900/6000], Loss: 1.4835
Epoch [10/30], Batch [4000/6000], Loss: 1.6820
Epoch [10/30], Batch [4100/6000], Loss: 1.4876
Epoch [10/30], Batch [4200/6000], Loss: 1.4803
Epoch [10/30], Batch [4300/6000], Loss: 1.4971
Epoch [10/30], Batch [4400/6000], Loss: 1.4898
Epoch [10/30], Batch [4500/6000], Loss: 1.4892
Epoch [10/30], Batch [4600/6000], Loss: 1.4826
Epoch [10/30], Batch [4700/6000], Loss: 1.5079
Epoch [10/30], Batch [4800/6000], Loss: 1.4773
Epoch [10/30], Batch [4900/6000], Loss: 1.5048
Epoch [10/30], Batch [5000/6000], Loss: 1.4950
Epoch [10/30], Batch [5100/6000], Loss: 1.4946
Epoch [10/30], Batch [5200/6000], Loss: 1.5008
Epoch [10/30], Batch [5300/6000], Loss: 1.4863
Epoch [10/30], Batch [5400/6000], Loss: 1.4869
Epoch [10/30], Batch [5500/6000], Loss: 1.4881
Epoch [10/30], Batch [5600/6000], Loss: 1.4949
Epoch [10/30], Batch [5700/6000], Loss: 1.5948
Epoch [10/30], Batch [5800/6000], Loss: 1.4897
Epoch [10/30], Batch [5900/6000], Loss: 1.4855
Epoch [10/30], Loss: 1.5008
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 1.4812
Epoch [11/30], Batch [100/6000], Loss: 1.4903
Epoch [11/30], Batch [200/6000], Loss: 1.4880
Epoch [11/30], Batch [300/6000], Loss: 1.4818
Epoch [11/30], Batch [400/6000], Loss: 1.4828
Epoch [11/30], Batch [500/6000], Loss: 1.5118
Epoch [11/30], Batch [600/6000], Loss: 1.4829
Epoch [11/30], Batch [700/6000], Loss: 1.4894
Epoch [11/30], Batch [800/6000], Loss: 1.4828
Epoch [11/30], Batch [900/6000], Loss: 1.4822
Epoch [11/30], Batch [1000/6000], Loss: 1.5114
Epoch [11/30], Batch [1100/6000], Loss: 1.4870
Epoch [11/30], Batch [1200/6000], Loss: 1.4865
Epoch [11/30], Batch [1300/6000], Loss: 1.4926
Epoch [11/30], Batch [1400/6000], Loss: 1.5367
Epoch [11/30], Batch [1500/6000], Loss: 1.4856
Epoch [11/30], Batch [1600/6000], Loss: 1.4815
Epoch [11/30], Batch [1700/6000], Loss: 1.5229
Epoch [11/30], Batch [1800/6000], Loss: 1.4995
Epoch [11/30], Batch [1900/6000], Loss: 1.4852
Epoch [11/30], Batch [2000/6000], Loss: 1.4825
Epoch [11/30], Batch [2100/6000], Loss: 1.4843
Epoch [11/30], Batch [2200/6000], Loss: 1.4878
Epoch [11/30], Batch [2300/6000], Loss: 1.4990
Epoch [11/30], Batch [2400/6000], Loss: 1.4827
Epoch [11/30], Batch [2500/6000], Loss: 1.4987
Epoch [11/30], Batch [2600/6000], Loss: 1.4819
Epoch [11/30], Batch [2700/6000], Loss: 1.4863
Epoch [11/30], Batch [2800/6000], Loss: 1.4841
Epoch [11/30], Batch [2900/6000], Loss: 1.4895
Epoch [11/30], Batch [3000/6000], Loss: 1.5922
Epoch [11/30], Batch [3100/6000], Loss: 1.4870
Epoch [11/30], Batch [3200/6000], Loss: 1.4814
Epoch [11/30], Batch [3300/6000], Loss: 1.4861
Epoch [11/30], Batch [3400/6000], Loss: 1.4821
Epoch [11/30], Batch [3500/6000], Loss: 1.5337
Epoch [11/30], Batch [3600/6000], Loss: 1.4902
Epoch [11/30], Batch [3700/6000], Loss: 1.5573
Epoch [11/30], Batch [3800/6000], Loss: 1.4845
Epoch [11/30], Batch [3900/6000], Loss: 1.4859
Epoch [11/30], Batch [4000/6000], Loss: 1.4984
Epoch [11/30], Batch [4100/6000], Loss: 1.4891
Epoch [11/30], Batch [4200/6000], Loss: 1.4995
Epoch [11/30], Batch [4300/6000], Loss: 1.4855
Epoch [11/30], Batch [4400/6000], Loss: 1.4873
Epoch [11/30], Batch [4500/6000], Loss: 1.4814
Epoch [11/30], Batch [4600/6000], Loss: 1.4837
Epoch [11/30], Batch [4700/6000], Loss: 1.5004
Epoch [11/30], Batch [4800/6000], Loss: 1.4815
Epoch [11/30], Batch [4900/6000], Loss: 1.5014
Epoch [11/30], Batch [5000/6000], Loss: 1.4834
Epoch [11/30], Batch [5100/6000], Loss: 1.4879
Epoch [11/30], Batch [5200/6000], Loss: 1.5022
Epoch [11/30], Batch [5300/6000], Loss: 1.5078
Epoch [11/30], Batch [5400/6000], Loss: 1.4917
Epoch [11/30], Batch [5500/6000], Loss: 1.4850
Epoch [11/30], Batch [5600/6000], Loss: 1.4995
Epoch [11/30], Batch [5700/6000], Loss: 1.4865
Epoch [11/30], Batch [5800/6000], Loss: 1.6310
Epoch [11/30], Batch [5900/6000], Loss: 1.4860
Epoch [11/30], Loss: 1.4977
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 1.4825
Epoch [12/30], Batch [100/6000], Loss: 1.4824
Epoch [12/30], Batch [200/6000], Loss: 1.4848
Epoch [12/30], Batch [300/6000], Loss: 1.4828
Epoch [12/30], Batch [400/6000], Loss: 1.4781
Epoch [12/30], Batch [500/6000], Loss: 1.4987
Epoch [12/30], Batch [600/6000], Loss: 1.4934
Epoch [12/30], Batch [700/6000], Loss: 1.4806
Epoch [12/30], Batch [800/6000], Loss: 1.4823
Epoch [12/30], Batch [900/6000], Loss: 1.4829
Epoch [12/30], Batch [1000/6000], Loss: 1.4854
Epoch [12/30], Batch [1100/6000], Loss: 1.4830
Epoch [12/30], Batch [1200/6000], Loss: 1.4897
Epoch [12/30], Batch [1300/6000], Loss: 1.4869
Epoch [12/30], Batch [1400/6000], Loss: 1.4930
Epoch [12/30], Batch [1500/6000], Loss: 1.4842
Epoch [12/30], Batch [1600/6000], Loss: 1.4890
Epoch [12/30], Batch [1700/6000], Loss: 1.4842
Epoch [12/30], Batch [1800/6000], Loss: 1.5879
Epoch [12/30], Batch [1900/6000], Loss: 1.4940
Epoch [12/30], Batch [2000/6000], Loss: 1.4850
Epoch [12/30], Batch [2100/6000], Loss: 1.4836
Epoch [12/30], Batch [2200/6000], Loss: 1.4830
Epoch [12/30], Batch [2300/6000], Loss: 1.4862
Epoch [12/30], Batch [2400/6000], Loss: 1.4788
Epoch [12/30], Batch [2500/6000], Loss: 1.4954
Epoch [12/30], Batch [2600/6000], Loss: 1.4822
Epoch [12/30], Batch [2700/6000], Loss: 1.4857
Epoch [12/30], Batch [2800/6000], Loss: 1.5796
Epoch [12/30], Batch [2900/6000], Loss: 1.5066
Epoch [12/30], Batch [3000/6000], Loss: 1.4867
Epoch [12/30], Batch [3100/6000], Loss: 1.4832
Epoch [12/30], Batch [3200/6000], Loss: 1.4861
Epoch [12/30], Batch [3300/6000], Loss: 1.4824
Epoch [12/30], Batch [3400/6000], Loss: 1.4932
Epoch [12/30], Batch [3500/6000], Loss: 1.4857
Epoch [12/30], Batch [3600/6000], Loss: 1.4827
Epoch [12/30], Batch [3700/6000], Loss: 1.4989
Epoch [12/30], Batch [3800/6000], Loss: 1.5919
Epoch [12/30], Batch [3900/6000], Loss: 1.4794
Epoch [12/30], Batch [4000/6000], Loss: 1.4847
Epoch [12/30], Batch [4100/6000], Loss: 1.4876
Epoch [12/30], Batch [4200/6000], Loss: 1.4794
Epoch [12/30], Batch [4300/6000], Loss: 1.4838
Epoch [12/30], Batch [4400/6000], Loss: 1.4858
Epoch [12/30], Batch [4500/6000], Loss: 1.4919
Epoch [12/30], Batch [4600/6000], Loss: 1.4759
Epoch [12/30], Batch [4700/6000], Loss: 1.4765
Epoch [12/30], Batch [4800/6000], Loss: 1.4811
Epoch [12/30], Batch [4900/6000], Loss: 1.4786
Epoch [12/30], Batch [5000/6000], Loss: 1.5043
Epoch [12/30], Batch [5100/6000], Loss: 1.4917
Epoch [12/30], Batch [5200/6000], Loss: 1.4901
Epoch [12/30], Batch [5300/6000], Loss: 1.5069
Epoch [12/30], Batch [5400/6000], Loss: 1.4853
Epoch [12/30], Batch [5500/6000], Loss: 1.4932
Epoch [12/30], Batch [5600/6000], Loss: 1.4900
Epoch [12/30], Batch [5700/6000], Loss: 1.5872
Epoch [12/30], Batch [5800/6000], Loss: 1.4931
Epoch [12/30], Batch [5900/6000], Loss: 1.4868
Epoch [12/30], Loss: 1.4955
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 1.4854
Epoch [13/30], Batch [100/6000], Loss: 1.5875
Epoch [13/30], Batch [200/6000], Loss: 1.4851
Epoch [13/30], Batch [300/6000], Loss: 1.4903
Epoch [13/30], Batch [400/6000], Loss: 1.4816
Epoch [13/30], Batch [500/6000], Loss: 1.5894
Epoch [13/30], Batch [600/6000], Loss: 1.4965
Epoch [13/30], Batch [700/6000], Loss: 1.4904
Epoch [13/30], Batch [800/6000], Loss: 1.4787
Epoch [13/30], Batch [900/6000], Loss: 1.4819
Epoch [13/30], Batch [1000/6000], Loss: 1.4875
Epoch [13/30], Batch [1100/6000], Loss: 1.4846
Epoch [13/30], Batch [1200/6000], Loss: 1.4843
Epoch [13/30], Batch [1300/6000], Loss: 1.4813
Epoch [13/30], Batch [1400/6000], Loss: 1.4802
Epoch [13/30], Batch [1500/6000], Loss: 1.4890
Epoch [13/30], Batch [1600/6000], Loss: 1.4905
Epoch [13/30], Batch [1700/6000], Loss: 1.5111
Epoch [13/30], Batch [1800/6000], Loss: 1.4978
Epoch [13/30], Batch [1900/6000], Loss: 1.4826
Epoch [13/30], Batch [2000/6000], Loss: 1.4851
Epoch [13/30], Batch [2100/6000], Loss: 1.4867
Epoch [13/30], Batch [2200/6000], Loss: 1.4884
Epoch [13/30], Batch [2300/6000], Loss: 1.4865
Epoch [13/30], Batch [2400/6000], Loss: 1.4779
Epoch [13/30], Batch [2500/6000], Loss: 1.4861
Epoch [13/30], Batch [2600/6000], Loss: 1.4811
Epoch [13/30], Batch [2700/6000], Loss: 1.5085
Epoch [13/30], Batch [2800/6000], Loss: 1.4785
Epoch [13/30], Batch [2900/6000], Loss: 1.4969
Epoch [13/30], Batch [3000/6000], Loss: 1.4871
Epoch [13/30], Batch [3100/6000], Loss: 1.4828
Epoch [13/30], Batch [3200/6000], Loss: 1.4871
Epoch [13/30], Batch [3300/6000], Loss: 1.4981
Epoch [13/30], Batch [3400/6000], Loss: 1.4812
Epoch [13/30], Batch [3500/6000], Loss: 1.5881
Epoch [13/30], Batch [3600/6000], Loss: 1.4812
Epoch [13/30], Batch [3700/6000], Loss: 1.4829
Epoch [13/30], Batch [3800/6000], Loss: 1.5800
Epoch [13/30], Batch [3900/6000], Loss: 1.4870
Epoch [13/30], Batch [4000/6000], Loss: 1.5367
Epoch [13/30], Batch [4100/6000], Loss: 1.4867
Epoch [13/30], Batch [4200/6000], Loss: 1.4803
Epoch [13/30], Batch [4300/6000], Loss: 1.5238
Epoch [13/30], Batch [4400/6000], Loss: 1.5000
Epoch [13/30], Batch [4500/6000], Loss: 1.4888
Epoch [13/30], Batch [4600/6000], Loss: 1.4842
Epoch [13/30], Batch [4700/6000], Loss: 1.4776
Epoch [13/30], Batch [4800/6000], Loss: 1.4811
Epoch [13/30], Batch [4900/6000], Loss: 1.4785
Epoch [13/30], Batch [5000/6000], Loss: 1.4952
Epoch [13/30], Batch [5100/6000], Loss: 1.4854
Epoch [13/30], Batch [5200/6000], Loss: 1.4954
Epoch [13/30], Batch [5300/6000], Loss: 1.4776
Epoch [13/30], Batch [5400/6000], Loss: 1.5715
Epoch [13/30], Batch [5500/6000], Loss: 1.4823
Epoch [13/30], Batch [5600/6000], Loss: 1.4773
Epoch [13/30], Batch [5700/6000], Loss: 1.5013
Epoch [13/30], Batch [5800/6000], Loss: 1.5946
Epoch [13/30], Batch [5900/6000], Loss: 1.4815
Epoch [13/30], Loss: 1.4938
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 1.4789
Epoch [14/30], Batch [100/6000], Loss: 1.4776
Epoch [14/30], Batch [200/6000], Loss: 1.4805
Epoch [14/30], Batch [300/6000], Loss: 1.4836
Epoch [14/30], Batch [400/6000], Loss: 1.4821
Epoch [14/30], Batch [500/6000], Loss: 1.4783
Epoch [14/30], Batch [600/6000], Loss: 1.5043
Epoch [14/30], Batch [700/6000], Loss: 1.4891
Epoch [14/30], Batch [800/6000], Loss: 1.4867
Epoch [14/30], Batch [900/6000], Loss: 1.4865
Epoch [14/30], Batch [1000/6000], Loss: 1.5091
Epoch [14/30], Batch [1100/6000], Loss: 1.4850
Epoch [14/30], Batch [1200/6000], Loss: 1.4860
Epoch [14/30], Batch [1300/6000], Loss: 1.5847
Epoch [14/30], Batch [1400/6000], Loss: 1.4834
Epoch [14/30], Batch [1500/6000], Loss: 1.4821
Epoch [14/30], Batch [1600/6000], Loss: 1.4859
Epoch [14/30], Batch [1700/6000], Loss: 1.4834
Epoch [14/30], Batch [1800/6000], Loss: 1.5776
Epoch [14/30], Batch [1900/6000], Loss: 1.4827
Epoch [14/30], Batch [2000/6000], Loss: 1.4903
Epoch [14/30], Batch [2100/6000], Loss: 1.4841
Epoch [14/30], Batch [2200/6000], Loss: 1.4854
Epoch [14/30], Batch [2300/6000], Loss: 1.4739
Epoch [14/30], Batch [2400/6000], Loss: 1.4990
Epoch [14/30], Batch [2500/6000], Loss: 1.4764
Epoch [14/30], Batch [2600/6000], Loss: 1.4813
Epoch [14/30], Batch [2700/6000], Loss: 1.4802
Epoch [14/30], Batch [2800/6000], Loss: 1.5029
Epoch [14/30], Batch [2900/6000], Loss: 1.4850
Epoch [14/30], Batch [3000/6000], Loss: 1.5000
Epoch [14/30], Batch [3100/6000], Loss: 1.4886
Epoch [14/30], Batch [3200/6000], Loss: 1.4820
Epoch [14/30], Batch [3300/6000], Loss: 1.4979
Epoch [14/30], Batch [3400/6000], Loss: 1.4828
Epoch [14/30], Batch [3500/6000], Loss: 1.4772
Epoch [14/30], Batch [3600/6000], Loss: 1.4837
Epoch [14/30], Batch [3700/6000], Loss: 1.4737
Epoch [14/30], Batch [3800/6000], Loss: 1.4837
Epoch [14/30], Batch [3900/6000], Loss: 1.4945
Epoch [14/30], Batch [4000/6000], Loss: 1.4753
Epoch [14/30], Batch [4100/6000], Loss: 1.4886
Epoch [14/30], Batch [4200/6000], Loss: 1.4826
Epoch [14/30], Batch [4300/6000], Loss: 1.4859
Epoch [14/30], Batch [4400/6000], Loss: 1.4843
Epoch [14/30], Batch [4500/6000], Loss: 1.4852
Epoch [14/30], Batch [4600/6000], Loss: 1.4799
Epoch [14/30], Batch [4700/6000], Loss: 1.4884
Epoch [14/30], Batch [4800/6000], Loss: 1.4846
Epoch [14/30], Batch [4900/6000], Loss: 1.4836
Epoch [14/30], Batch [5000/6000], Loss: 1.4825
Epoch [14/30], Batch [5100/6000], Loss: 1.5060
Epoch [14/30], Batch [5200/6000], Loss: 1.5006
Epoch [14/30], Batch [5300/6000], Loss: 1.4819
Epoch [14/30], Batch [5400/6000], Loss: 1.4988
Epoch [14/30], Batch [5500/6000], Loss: 1.4934
Epoch [14/30], Batch [5600/6000], Loss: 1.4804
Epoch [14/30], Batch [5700/6000], Loss: 1.4828
Epoch [14/30], Batch [5800/6000], Loss: 1.4992
Epoch [14/30], Batch [5900/6000], Loss: 1.4756
Epoch [14/30], Loss: 1.4919
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 1.4887
Epoch [15/30], Batch [100/6000], Loss: 1.4811
Epoch [15/30], Batch [200/6000], Loss: 1.4890
Epoch [15/30], Batch [300/6000], Loss: 1.5017
Epoch [15/30], Batch [400/6000], Loss: 1.4832
Epoch [15/30], Batch [500/6000], Loss: 1.4794
Epoch [15/30], Batch [600/6000], Loss: 1.4890
Epoch [15/30], Batch [700/6000], Loss: 1.4820
Epoch [15/30], Batch [800/6000], Loss: 1.4804
Epoch [15/30], Batch [900/6000], Loss: 1.4823
Epoch [15/30], Batch [1000/6000], Loss: 1.4794
Epoch [15/30], Batch [1100/6000], Loss: 1.5806
Epoch [15/30], Batch [1200/6000], Loss: 1.4881
Epoch [15/30], Batch [1300/6000], Loss: 1.4760
Epoch [15/30], Batch [1400/6000], Loss: 1.5831
Epoch [15/30], Batch [1500/6000], Loss: 1.4834
Epoch [15/30], Batch [1600/6000], Loss: 1.4830
Epoch [15/30], Batch [1700/6000], Loss: 1.4862
Epoch [15/30], Batch [1800/6000], Loss: 1.4807
Epoch [15/30], Batch [1900/6000], Loss: 1.4966
Epoch [15/30], Batch [2000/6000], Loss: 1.4845
Epoch [15/30], Batch [2100/6000], Loss: 1.4758
Epoch [15/30], Batch [2200/6000], Loss: 1.4944
Epoch [15/30], Batch [2300/6000], Loss: 1.4782
Epoch [15/30], Batch [2400/6000], Loss: 1.4816
Epoch [15/30], Batch [2500/6000], Loss: 1.4866
Epoch [15/30], Batch [2600/6000], Loss: 1.4785
Epoch [15/30], Batch [2700/6000], Loss: 1.4810
Epoch [15/30], Batch [2800/6000], Loss: 1.4802
Epoch [15/30], Batch [2900/6000], Loss: 1.4834
Epoch [15/30], Batch [3000/6000], Loss: 1.4814
Epoch [15/30], Batch [3100/6000], Loss: 1.4768
Epoch [15/30], Batch [3200/6000], Loss: 1.4847
Epoch [15/30], Batch [3300/6000], Loss: 1.5811
Epoch [15/30], Batch [3400/6000], Loss: 1.4941
Epoch [15/30], Batch [3500/6000], Loss: 1.4845
Epoch [15/30], Batch [3600/6000], Loss: 1.4837
Epoch [15/30], Batch [3700/6000], Loss: 1.4810
Epoch [15/30], Batch [3800/6000], Loss: 1.5495
Epoch [15/30], Batch [3900/6000], Loss: 1.4835
Epoch [15/30], Batch [4000/6000], Loss: 1.4849
Epoch [15/30], Batch [4100/6000], Loss: 1.5825
Epoch [15/30], Batch [4200/6000], Loss: 1.4782
Epoch [15/30], Batch [4300/6000], Loss: 1.4822
Epoch [15/30], Batch [4400/6000], Loss: 1.4760
Epoch [15/30], Batch [4500/6000], Loss: 1.4776
Epoch [15/30], Batch [4600/6000], Loss: 1.4800
Epoch [15/30], Batch [4700/6000], Loss: 1.4827
Epoch [15/30], Batch [4800/6000], Loss: 1.4859
Epoch [15/30], Batch [4900/6000], Loss: 1.4897
Epoch [15/30], Batch [5000/6000], Loss: 1.5813
Epoch [15/30], Batch [5100/6000], Loss: 1.4823
Epoch [15/30], Batch [5200/6000], Loss: 1.4782
Epoch [15/30], Batch [5300/6000], Loss: 1.5819
Epoch [15/30], Batch [5400/6000], Loss: 1.4870
Epoch [15/30], Batch [5500/6000], Loss: 1.4811
Epoch [15/30], Batch [5600/6000], Loss: 1.4805
Epoch [15/30], Batch [5700/6000], Loss: 1.4837
Epoch [15/30], Batch [5800/6000], Loss: 1.4915
Epoch [15/30], Batch [5900/6000], Loss: 1.4827
Epoch [15/30], Loss: 1.4906
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 1.4800
Epoch [16/30], Batch [100/6000], Loss: 1.4808
Epoch [16/30], Batch [200/6000], Loss: 1.4896
Epoch [16/30], Batch [300/6000], Loss: 1.4837
Epoch [16/30], Batch [400/6000], Loss: 1.4887
Epoch [16/30], Batch [500/6000], Loss: 1.4842
Epoch [16/30], Batch [600/6000], Loss: 1.4766
Epoch [16/30], Batch [700/6000], Loss: 1.4769
Epoch [16/30], Batch [800/6000], Loss: 1.4851
Epoch [16/30], Batch [900/6000], Loss: 1.4911
Epoch [16/30], Batch [1000/6000], Loss: 1.4761
Epoch [16/30], Batch [1100/6000], Loss: 1.4778
Epoch [16/30], Batch [1200/6000], Loss: 1.4830
Epoch [16/30], Batch [1300/6000], Loss: 1.4839
Epoch [16/30], Batch [1400/6000], Loss: 1.4803
Epoch [16/30], Batch [1500/6000], Loss: 1.4806
Epoch [16/30], Batch [1600/6000], Loss: 1.4820
Epoch [16/30], Batch [1700/6000], Loss: 1.4782
Epoch [16/30], Batch [1800/6000], Loss: 1.4824
Epoch [16/30], Batch [1900/6000], Loss: 1.4892
Epoch [16/30], Batch [2000/6000], Loss: 1.4815
Epoch [16/30], Batch [2100/6000], Loss: 1.4770
Epoch [16/30], Batch [2200/6000], Loss: 1.4807
Epoch [16/30], Batch [2300/6000], Loss: 1.4841
Epoch [16/30], Batch [2400/6000], Loss: 1.5817
Epoch [16/30], Batch [2500/6000], Loss: 1.4865
Epoch [16/30], Batch [2600/6000], Loss: 1.4800
Epoch [16/30], Batch [2700/6000], Loss: 1.4922
Epoch [16/30], Batch [2800/6000], Loss: 1.4845
Epoch [16/30], Batch [2900/6000], Loss: 1.5153
Epoch [16/30], Batch [3000/6000], Loss: 1.4941
Epoch [16/30], Batch [3100/6000], Loss: 1.4797
Epoch [16/30], Batch [3200/6000], Loss: 1.4815
Epoch [16/30], Batch [3300/6000], Loss: 1.4805
Epoch [16/30], Batch [3400/6000], Loss: 1.4774
Epoch [16/30], Batch [3500/6000], Loss: 1.4942
Epoch [16/30], Batch [3600/6000], Loss: 1.4752
Epoch [16/30], Batch [3700/6000], Loss: 1.4787
Epoch [16/30], Batch [3800/6000], Loss: 1.4743
Epoch [16/30], Batch [3900/6000], Loss: 1.4799
Epoch [16/30], Batch [4000/6000], Loss: 1.4834
Epoch [16/30], Batch [4100/6000], Loss: 1.4740
Epoch [16/30], Batch [4200/6000], Loss: 1.4768
Epoch [16/30], Batch [4300/6000], Loss: 1.4812
Epoch [16/30], Batch [4400/6000], Loss: 1.4816
Epoch [16/30], Batch [4500/6000], Loss: 1.4805
Epoch [16/30], Batch [4600/6000], Loss: 1.4765
Epoch [16/30], Batch [4700/6000], Loss: 1.4803
Epoch [16/30], Batch [4800/6000], Loss: 1.5762
Epoch [16/30], Batch [4900/6000], Loss: 1.4825
Epoch [16/30], Batch [5000/6000], Loss: 1.5837
Epoch [16/30], Batch [5100/6000], Loss: 1.5879
Epoch [16/30], Batch [5200/6000], Loss: 1.4809
Epoch [16/30], Batch [5300/6000], Loss: 1.4800
Epoch [16/30], Batch [5400/6000], Loss: 1.4839
Epoch [16/30], Batch [5500/6000], Loss: 1.5768
Epoch [16/30], Batch [5600/6000], Loss: 1.4826
Epoch [16/30], Batch [5700/6000], Loss: 1.5011
Epoch [16/30], Batch [5800/6000], Loss: 1.4748
Epoch [16/30], Batch [5900/6000], Loss: 1.4845
Epoch [16/30], Loss: 1.4892
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 1.4847
Epoch [17/30], Batch [100/6000], Loss: 1.4783
Epoch [17/30], Batch [200/6000], Loss: 1.4850
Epoch [17/30], Batch [300/6000], Loss: 1.4850
Epoch [17/30], Batch [400/6000], Loss: 1.4794
Epoch [17/30], Batch [500/6000], Loss: 1.4774
Epoch [17/30], Batch [600/6000], Loss: 1.4758
Epoch [17/30], Batch [700/6000], Loss: 1.4791
Epoch [17/30], Batch [800/6000], Loss: 1.4819
Epoch [17/30], Batch [900/6000], Loss: 1.4857
Epoch [17/30], Batch [1000/6000], Loss: 1.4809
Epoch [17/30], Batch [1100/6000], Loss: 1.4799
Epoch [17/30], Batch [1200/6000], Loss: 1.4826
Epoch [17/30], Batch [1300/6000], Loss: 1.4941
Epoch [17/30], Batch [1400/6000], Loss: 1.4925
Epoch [17/30], Batch [1500/6000], Loss: 1.4887
Epoch [17/30], Batch [1600/6000], Loss: 1.4836
Epoch [17/30], Batch [1700/6000], Loss: 1.4813
Epoch [17/30], Batch [1800/6000], Loss: 1.4778
Epoch [17/30], Batch [1900/6000], Loss: 1.4792
Epoch [17/30], Batch [2000/6000], Loss: 1.4801
Epoch [17/30], Batch [2100/6000], Loss: 1.4768
Epoch [17/30], Batch [2200/6000], Loss: 1.4816
Epoch [17/30], Batch [2300/6000], Loss: 1.4820
Epoch [17/30], Batch [2400/6000], Loss: 1.4761
Epoch [17/30], Batch [2500/6000], Loss: 1.4805
Epoch [17/30], Batch [2600/6000], Loss: 1.4771
Epoch [17/30], Batch [2700/6000], Loss: 1.4926
Epoch [17/30], Batch [2800/6000], Loss: 1.4833
Epoch [17/30], Batch [2900/6000], Loss: 1.4816
Epoch [17/30], Batch [3000/6000], Loss: 1.4820
Epoch [17/30], Batch [3100/6000], Loss: 1.4838
Epoch [17/30], Batch [3200/6000], Loss: 1.4819
Epoch [17/30], Batch [3300/6000], Loss: 1.4802
Epoch [17/30], Batch [3400/6000], Loss: 1.4755
Epoch [17/30], Batch [3500/6000], Loss: 1.4804
Epoch [17/30], Batch [3600/6000], Loss: 1.4815
Epoch [17/30], Batch [3700/6000], Loss: 1.4843
Epoch [17/30], Batch [3800/6000], Loss: 1.4774
Epoch [17/30], Batch [3900/6000], Loss: 1.4762
Epoch [17/30], Batch [4000/6000], Loss: 1.4973
Epoch [17/30], Batch [4100/6000], Loss: 1.4825
Epoch [17/30], Batch [4200/6000], Loss: 1.4787
Epoch [17/30], Batch [4300/6000], Loss: 1.4778
Epoch [17/30], Batch [4400/6000], Loss: 1.4781
Epoch [17/30], Batch [4500/6000], Loss: 1.4797
Epoch [17/30], Batch [4600/6000], Loss: 1.4791
Epoch [17/30], Batch [4700/6000], Loss: 1.4752
Epoch [17/30], Batch [4800/6000], Loss: 1.4756
Epoch [17/30], Batch [4900/6000], Loss: 1.4765
Epoch [17/30], Batch [5000/6000], Loss: 1.4815
Epoch [17/30], Batch [5100/6000], Loss: 1.4791
Epoch [17/30], Batch [5200/6000], Loss: 1.4775
Epoch [17/30], Batch [5300/6000], Loss: 1.4783
Epoch [17/30], Batch [5400/6000], Loss: 1.4942
Epoch [17/30], Batch [5500/6000], Loss: 1.4819
Epoch [17/30], Batch [5600/6000], Loss: 1.4968
Epoch [17/30], Batch [5700/6000], Loss: 1.4825
Epoch [17/30], Batch [5800/6000], Loss: 1.4791
Epoch [17/30], Batch [5900/6000], Loss: 1.4804
Epoch [17/30], Loss: 1.4880
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 1.4776
Epoch [18/30], Batch [100/6000], Loss: 1.4836
Epoch [18/30], Batch [200/6000], Loss: 1.4899
Epoch [18/30], Batch [300/6000], Loss: 1.4803
Epoch [18/30], Batch [400/6000], Loss: 1.4870
Epoch [18/30], Batch [500/6000], Loss: 1.4773
Epoch [18/30], Batch [600/6000], Loss: 1.4745
Epoch [18/30], Batch [700/6000], Loss: 1.4771
Epoch [18/30], Batch [800/6000], Loss: 1.4821
Epoch [18/30], Batch [900/6000], Loss: 1.4819
Epoch [18/30], Batch [1000/6000], Loss: 1.4837
Epoch [18/30], Batch [1100/6000], Loss: 1.4809
Epoch [18/30], Batch [1200/6000], Loss: 1.4774
Epoch [18/30], Batch [1300/6000], Loss: 1.4755
Epoch [18/30], Batch [1400/6000], Loss: 1.4836
Epoch [18/30], Batch [1500/6000], Loss: 1.4791
Epoch [18/30], Batch [1600/6000], Loss: 1.4867
Epoch [18/30], Batch [1700/6000], Loss: 1.4835
Epoch [18/30], Batch [1800/6000], Loss: 1.4793
Epoch [18/30], Batch [1900/6000], Loss: 1.4835
Epoch [18/30], Batch [2000/6000], Loss: 1.4812
Epoch [18/30], Batch [2100/6000], Loss: 1.4772
Epoch [18/30], Batch [2200/6000], Loss: 1.4828
Epoch [18/30], Batch [2300/6000], Loss: 1.4781
Epoch [18/30], Batch [2400/6000], Loss: 1.4938
Epoch [18/30], Batch [2500/6000], Loss: 1.4803
Epoch [18/30], Batch [2600/6000], Loss: 1.4777
Epoch [18/30], Batch [2700/6000], Loss: 1.4932
Epoch [18/30], Batch [2800/6000], Loss: 1.4817
Epoch [18/30], Batch [2900/6000], Loss: 1.4808
Epoch [18/30], Batch [3000/6000], Loss: 1.4764
Epoch [18/30], Batch [3100/6000], Loss: 1.4837
Epoch [18/30], Batch [3200/6000], Loss: 1.5817
Epoch [18/30], Batch [3300/6000], Loss: 1.4849
Epoch [18/30], Batch [3400/6000], Loss: 1.4783
Epoch [18/30], Batch [3500/6000], Loss: 1.4745
Epoch [18/30], Batch [3600/6000], Loss: 1.4784
Epoch [18/30], Batch [3700/6000], Loss: 1.5797
Epoch [18/30], Batch [3800/6000], Loss: 1.6039
Epoch [18/30], Batch [3900/6000], Loss: 1.4981
Epoch [18/30], Batch [4000/6000], Loss: 1.4843
Epoch [18/30], Batch [4100/6000], Loss: 1.4840
Epoch [18/30], Batch [4200/6000], Loss: 1.5926
Epoch [18/30], Batch [4300/6000], Loss: 1.4774
Epoch [18/30], Batch [4400/6000], Loss: 1.4799
Epoch [18/30], Batch [4500/6000], Loss: 1.4857
Epoch [18/30], Batch [4600/6000], Loss: 1.4798
Epoch [18/30], Batch [4700/6000], Loss: 1.4770
Epoch [18/30], Batch [4800/6000], Loss: 1.4806
Epoch [18/30], Batch [4900/6000], Loss: 1.4966
Epoch [18/30], Batch [5000/6000], Loss: 1.4813
Epoch [18/30], Batch [5100/6000], Loss: 1.4937
Epoch [18/30], Batch [5200/6000], Loss: 1.4742
Epoch [18/30], Batch [5300/6000], Loss: 1.4840
Epoch [18/30], Batch [5400/6000], Loss: 1.4818
Epoch [18/30], Batch [5500/6000], Loss: 1.4786
Epoch [18/30], Batch [5600/6000], Loss: 1.4801
Epoch [18/30], Batch [5700/6000], Loss: 1.4833
Epoch [18/30], Batch [5800/6000], Loss: 1.4759
Epoch [18/30], Batch [5900/6000], Loss: 1.4758
Epoch [18/30], Loss: 1.4871
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 1.4818
Epoch [19/30], Batch [100/6000], Loss: 1.4801
Epoch [19/30], Batch [200/6000], Loss: 1.4981
Epoch [19/30], Batch [300/6000], Loss: 1.4858
Epoch [19/30], Batch [400/6000], Loss: 1.4771
Epoch [19/30], Batch [500/6000], Loss: 1.4758
Epoch [19/30], Batch [600/6000], Loss: 1.4843
Epoch [19/30], Batch [700/6000], Loss: 1.4935
Epoch [19/30], Batch [800/6000], Loss: 1.4802
Epoch [19/30], Batch [900/6000], Loss: 1.4775
Epoch [19/30], Batch [1000/6000], Loss: 1.4901
Epoch [19/30], Batch [1100/6000], Loss: 1.4792
Epoch [19/30], Batch [1200/6000], Loss: 1.4805
Epoch [19/30], Batch [1300/6000], Loss: 1.4804
Epoch [19/30], Batch [1400/6000], Loss: 1.4841
Epoch [19/30], Batch [1500/6000], Loss: 1.4865
Epoch [19/30], Batch [1600/6000], Loss: 1.4806
Epoch [19/30], Batch [1700/6000], Loss: 1.4753
Epoch [19/30], Batch [1800/6000], Loss: 1.4945
Epoch [19/30], Batch [1900/6000], Loss: 1.4793
Epoch [19/30], Batch [2000/6000], Loss: 1.5780
Epoch [19/30], Batch [2100/6000], Loss: 1.4816
Epoch [19/30], Batch [2200/6000], Loss: 1.4791
Epoch [19/30], Batch [2300/6000], Loss: 1.4771
Epoch [19/30], Batch [2400/6000], Loss: 1.4782
Epoch [19/30], Batch [2500/6000], Loss: 1.4905
Epoch [19/30], Batch [2600/6000], Loss: 1.4952
Epoch [19/30], Batch [2700/6000], Loss: 1.4859
Epoch [19/30], Batch [2800/6000], Loss: 1.4785
Epoch [19/30], Batch [2900/6000], Loss: 1.4732
Epoch [19/30], Batch [3000/6000], Loss: 1.5269
Epoch [19/30], Batch [3100/6000], Loss: 1.4768
Epoch [19/30], Batch [3200/6000], Loss: 1.4806
Epoch [19/30], Batch [3300/6000], Loss: 1.4837
Epoch [19/30], Batch [3400/6000], Loss: 1.4828
Epoch [19/30], Batch [3500/6000], Loss: 1.4753
Epoch [19/30], Batch [3600/6000], Loss: 1.4836
Epoch [19/30], Batch [3700/6000], Loss: 1.4928
Epoch [19/30], Batch [3800/6000], Loss: 1.4863
Epoch [19/30], Batch [3900/6000], Loss: 1.4745
Epoch [19/30], Batch [4000/6000], Loss: 1.4799
Epoch [19/30], Batch [4100/6000], Loss: 1.6798
Epoch [19/30], Batch [4200/6000], Loss: 1.4781
Epoch [19/30], Batch [4300/6000], Loss: 1.4759
Epoch [19/30], Batch [4400/6000], Loss: 1.4787
Epoch [19/30], Batch [4500/6000], Loss: 1.4761
Epoch [19/30], Batch [4600/6000], Loss: 1.4757
Epoch [19/30], Batch [4700/6000], Loss: 1.4737
Epoch [19/30], Batch [4800/6000], Loss: 1.4775
Epoch [19/30], Batch [4900/6000], Loss: 1.4815
Epoch [19/30], Batch [5000/6000], Loss: 1.4769
Epoch [19/30], Batch [5100/6000], Loss: 1.4776
Epoch [19/30], Batch [5200/6000], Loss: 1.4821
Epoch [19/30], Batch [5300/6000], Loss: 1.4794
Epoch [19/30], Batch [5400/6000], Loss: 1.4764
Epoch [19/30], Batch [5500/6000], Loss: 1.4823
Epoch [19/30], Batch [5600/6000], Loss: 1.4797
Epoch [19/30], Batch [5700/6000], Loss: 1.4777
Epoch [19/30], Batch [5800/6000], Loss: 1.4872
Epoch [19/30], Batch [5900/6000], Loss: 1.4792
Epoch [19/30], Loss: 1.4863
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 1.5775
Epoch [20/30], Batch [100/6000], Loss: 1.4769
Epoch [20/30], Batch [200/6000], Loss: 1.4782
Epoch [20/30], Batch [300/6000], Loss: 1.4841
Epoch [20/30], Batch [400/6000], Loss: 1.4788
Epoch [20/30], Batch [500/6000], Loss: 1.4754
Epoch [20/30], Batch [600/6000], Loss: 1.4780
Epoch [20/30], Batch [700/6000], Loss: 1.4782
Epoch [20/30], Batch [800/6000], Loss: 1.4762
Epoch [20/30], Batch [900/6000], Loss: 1.4732
Epoch [20/30], Batch [1000/6000], Loss: 1.4776
Epoch [20/30], Batch [1100/6000], Loss: 1.4786
Epoch [20/30], Batch [1200/6000], Loss: 1.4810
Epoch [20/30], Batch [1300/6000], Loss: 1.4738
Epoch [20/30], Batch [1400/6000], Loss: 1.4945
Epoch [20/30], Batch [1500/6000], Loss: 1.4803
Epoch [20/30], Batch [1600/6000], Loss: 1.4761
Epoch [20/30], Batch [1700/6000], Loss: 1.4771
Epoch [20/30], Batch [1800/6000], Loss: 1.5743
Epoch [20/30], Batch [1900/6000], Loss: 1.4927
Epoch [20/30], Batch [2000/6000], Loss: 1.4834
Epoch [20/30], Batch [2100/6000], Loss: 1.4751
Epoch [20/30], Batch [2200/6000], Loss: 1.4708
Epoch [20/30], Batch [2300/6000], Loss: 1.4780
Epoch [20/30], Batch [2400/6000], Loss: 1.4785
Epoch [20/30], Batch [2500/6000], Loss: 1.4767
Epoch [20/30], Batch [2600/6000], Loss: 1.4763
Epoch [20/30], Batch [2700/6000], Loss: 1.4777
Epoch [20/30], Batch [2800/6000], Loss: 1.4809
Epoch [20/30], Batch [2900/6000], Loss: 1.4740
Epoch [20/30], Batch [3000/6000], Loss: 1.4806
Epoch [20/30], Batch [3100/6000], Loss: 1.5847
Epoch [20/30], Batch [3200/6000], Loss: 1.4770
Epoch [20/30], Batch [3300/6000], Loss: 1.4817
Epoch [20/30], Batch [3400/6000], Loss: 1.4788
Epoch [20/30], Batch [3500/6000], Loss: 1.4741
Epoch [20/30], Batch [3600/6000], Loss: 1.4895
Epoch [20/30], Batch [3700/6000], Loss: 1.4828
Epoch [20/30], Batch [3800/6000], Loss: 1.4820
Epoch [20/30], Batch [3900/6000], Loss: 1.4811
Epoch [20/30], Batch [4000/6000], Loss: 1.4782
Epoch [20/30], Batch [4100/6000], Loss: 1.4830
Epoch [20/30], Batch [4200/6000], Loss: 1.4817
Epoch [20/30], Batch [4300/6000], Loss: 1.4752
Epoch [20/30], Batch [4400/6000], Loss: 1.4785
Epoch [20/30], Batch [4500/6000], Loss: 1.4724
Epoch [20/30], Batch [4600/6000], Loss: 1.4815
Epoch [20/30], Batch [4700/6000], Loss: 1.5819
Epoch [20/30], Batch [4800/6000], Loss: 1.4781
Epoch [20/30], Batch [4900/6000], Loss: 1.4778
Epoch [20/30], Batch [5000/6000], Loss: 1.4803
Epoch [20/30], Batch [5100/6000], Loss: 1.4812
Epoch [20/30], Batch [5200/6000], Loss: 1.4770
Epoch [20/30], Batch [5300/6000], Loss: 1.4789
Epoch [20/30], Batch [5400/6000], Loss: 1.4837
Epoch [20/30], Batch [5500/6000], Loss: 1.4730
Epoch [20/30], Batch [5600/6000], Loss: 1.4802
Epoch [20/30], Batch [5700/6000], Loss: 1.4798
Epoch [20/30], Batch [5800/6000], Loss: 1.4792
Epoch [20/30], Batch [5900/6000], Loss: 1.4774
Epoch [20/30], Loss: 1.4855
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 1.4786
Epoch [21/30], Batch [100/6000], Loss: 1.4757
Epoch [21/30], Batch [200/6000], Loss: 1.4829
Epoch [21/30], Batch [300/6000], Loss: 1.4793
Epoch [21/30], Batch [400/6000], Loss: 1.4766
Epoch [21/30], Batch [500/6000], Loss: 1.4801
Epoch [21/30], Batch [600/6000], Loss: 1.4793
Epoch [21/30], Batch [700/6000], Loss: 1.4724
Epoch [21/30], Batch [800/6000], Loss: 1.4773
Epoch [21/30], Batch [900/6000], Loss: 1.4779
Epoch [21/30], Batch [1000/6000], Loss: 1.4907
Epoch [21/30], Batch [1100/6000], Loss: 1.4854
Epoch [21/30], Batch [1200/6000], Loss: 1.4840
Epoch [21/30], Batch [1300/6000], Loss: 1.4708
Epoch [21/30], Batch [1400/6000], Loss: 1.4760
Epoch [21/30], Batch [1500/6000], Loss: 1.4780
Epoch [21/30], Batch [1600/6000], Loss: 1.4808
Epoch [21/30], Batch [1700/6000], Loss: 1.4779
Epoch [21/30], Batch [1800/6000], Loss: 1.5720
Epoch [21/30], Batch [1900/6000], Loss: 1.4782
Epoch [21/30], Batch [2000/6000], Loss: 1.4751
Epoch [21/30], Batch [2100/6000], Loss: 1.4779
Epoch [21/30], Batch [2200/6000], Loss: 1.4803
Epoch [21/30], Batch [2300/6000], Loss: 1.4968
Epoch [21/30], Batch [2400/6000], Loss: 1.4776
Epoch [21/30], Batch [2500/6000], Loss: 1.4767
Epoch [21/30], Batch [2600/6000], Loss: 1.5237
Epoch [21/30], Batch [2700/6000], Loss: 1.4768
Epoch [21/30], Batch [2800/6000], Loss: 1.5797
Epoch [21/30], Batch [2900/6000], Loss: 1.4777
Epoch [21/30], Batch [3000/6000], Loss: 1.4792
Epoch [21/30], Batch [3100/6000], Loss: 1.4757
Epoch [21/30], Batch [3200/6000], Loss: 1.4746
Epoch [21/30], Batch [3300/6000], Loss: 1.4817
Epoch [21/30], Batch [3400/6000], Loss: 1.4791
Epoch [21/30], Batch [3500/6000], Loss: 1.4747
Epoch [21/30], Batch [3600/6000], Loss: 1.4777
Epoch [21/30], Batch [3700/6000], Loss: 1.4734
Epoch [21/30], Batch [3800/6000], Loss: 1.4749
Epoch [21/30], Batch [3900/6000], Loss: 1.4769
Epoch [21/30], Batch [4000/6000], Loss: 1.4790
Epoch [21/30], Batch [4100/6000], Loss: 1.4751
Epoch [21/30], Batch [4200/6000], Loss: 1.4881
Epoch [21/30], Batch [4300/6000], Loss: 1.4741
Epoch [21/30], Batch [4400/6000], Loss: 1.4796
Epoch [21/30], Batch [4500/6000], Loss: 1.4811
Epoch [21/30], Batch [4600/6000], Loss: 1.4784
Epoch [21/30], Batch [4700/6000], Loss: 1.4782
Epoch [21/30], Batch [4800/6000], Loss: 1.4781
Epoch [21/30], Batch [4900/6000], Loss: 1.4778
Epoch [21/30], Batch [5000/6000], Loss: 1.4747
Epoch [21/30], Batch [5100/6000], Loss: 1.4819
Epoch [21/30], Batch [5200/6000], Loss: 1.4764
Epoch [21/30], Batch [5300/6000], Loss: 1.4796
Epoch [21/30], Batch [5400/6000], Loss: 1.4829
Epoch [21/30], Batch [5500/6000], Loss: 1.4808
Epoch [21/30], Batch [5600/6000], Loss: 1.4904
Epoch [21/30], Batch [5700/6000], Loss: 1.4822
Epoch [21/30], Batch [5800/6000], Loss: 1.4758
Epoch [21/30], Batch [5900/6000], Loss: 1.4753
Epoch [21/30], Loss: 1.4844
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 1.4777
Epoch [22/30], Batch [100/6000], Loss: 1.4804
Epoch [22/30], Batch [200/6000], Loss: 1.4805
Epoch [22/30], Batch [300/6000], Loss: 1.4963
Epoch [22/30], Batch [400/6000], Loss: 1.4788
Epoch [22/30], Batch [500/6000], Loss: 1.4765
Epoch [22/30], Batch [600/6000], Loss: 1.4770
Epoch [22/30], Batch [700/6000], Loss: 1.4761
Epoch [22/30], Batch [800/6000], Loss: 1.4793
Epoch [22/30], Batch [900/6000], Loss: 1.4803
Epoch [22/30], Batch [1000/6000], Loss: 1.4794
Epoch [22/30], Batch [1100/6000], Loss: 1.4756
Epoch [22/30], Batch [1200/6000], Loss: 1.4796
Epoch [22/30], Batch [1300/6000], Loss: 1.5737
Epoch [22/30], Batch [1400/6000], Loss: 1.4832
Epoch [22/30], Batch [1500/6000], Loss: 1.4838
Epoch [22/30], Batch [1600/6000], Loss: 1.4798
Epoch [22/30], Batch [1700/6000], Loss: 1.4790
Epoch [22/30], Batch [1800/6000], Loss: 1.4797
Epoch [22/30], Batch [1900/6000], Loss: 1.4766
Epoch [22/30], Batch [2000/6000], Loss: 1.4808
Epoch [22/30], Batch [2100/6000], Loss: 1.4737
Epoch [22/30], Batch [2200/6000], Loss: 1.4728
Epoch [22/30], Batch [2300/6000], Loss: 1.4765
Epoch [22/30], Batch [2400/6000], Loss: 1.4774
Epoch [22/30], Batch [2500/6000], Loss: 1.4822
Epoch [22/30], Batch [2600/6000], Loss: 1.4785
Epoch [22/30], Batch [2700/6000], Loss: 1.4758
Epoch [22/30], Batch [2800/6000], Loss: 1.4756
Epoch [22/30], Batch [2900/6000], Loss: 1.4766
Epoch [22/30], Batch [3000/6000], Loss: 1.4737
Epoch [22/30], Batch [3100/6000], Loss: 1.4772
Epoch [22/30], Batch [3200/6000], Loss: 1.4805
Epoch [22/30], Batch [3300/6000], Loss: 1.4818
Epoch [22/30], Batch [3400/6000], Loss: 1.4796
Epoch [22/30], Batch [3500/6000], Loss: 1.4800
Epoch [22/30], Batch [3600/6000], Loss: 1.4756
Epoch [22/30], Batch [3700/6000], Loss: 1.4763
Epoch [22/30], Batch [3800/6000], Loss: 1.4755
Epoch [22/30], Batch [3900/6000], Loss: 1.4776
Epoch [22/30], Batch [4000/6000], Loss: 1.4757
Epoch [22/30], Batch [4100/6000], Loss: 1.4786
Epoch [22/30], Batch [4200/6000], Loss: 1.4806
Epoch [22/30], Batch [4300/6000], Loss: 1.4786
Epoch [22/30], Batch [4400/6000], Loss: 1.4980
Epoch [22/30], Batch [4500/6000], Loss: 1.4767
Epoch [22/30], Batch [4600/6000], Loss: 1.5011
Epoch [22/30], Batch [4700/6000], Loss: 1.4863
Epoch [22/30], Batch [4800/6000], Loss: 1.4774
Epoch [22/30], Batch [4900/6000], Loss: 1.4856
Epoch [22/30], Batch [5000/6000], Loss: 1.4827
Epoch [22/30], Batch [5100/6000], Loss: 1.5935
Epoch [22/30], Batch [5200/6000], Loss: 1.4817
Epoch [22/30], Batch [5300/6000], Loss: 1.4775
Epoch [22/30], Batch [5400/6000], Loss: 1.4797
Epoch [22/30], Batch [5500/6000], Loss: 1.4775
Epoch [22/30], Batch [5600/6000], Loss: 1.4796
Epoch [22/30], Batch [5700/6000], Loss: 1.4772
Epoch [22/30], Batch [5800/6000], Loss: 1.4768
Epoch [22/30], Batch [5900/6000], Loss: 1.4837
Epoch [22/30], Loss: 1.4841
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 1.4798
Epoch [23/30], Batch [100/6000], Loss: 1.4768
Epoch [23/30], Batch [200/6000], Loss: 1.4766
Epoch [23/30], Batch [300/6000], Loss: 1.4731
Epoch [23/30], Batch [400/6000], Loss: 1.4794
Epoch [23/30], Batch [500/6000], Loss: 1.4840
Epoch [23/30], Batch [600/6000], Loss: 1.4781
Epoch [23/30], Batch [700/6000], Loss: 1.4748
Epoch [23/30], Batch [800/6000], Loss: 1.4769
Epoch [23/30], Batch [900/6000], Loss: 1.4800
Epoch [23/30], Batch [1000/6000], Loss: 1.4722
Epoch [23/30], Batch [1100/6000], Loss: 1.4768
Epoch [23/30], Batch [1200/6000], Loss: 1.4740
Epoch [23/30], Batch [1300/6000], Loss: 1.4727
Epoch [23/30], Batch [1400/6000], Loss: 1.4813
Epoch [23/30], Batch [1500/6000], Loss: 1.4760
Epoch [23/30], Batch [1600/6000], Loss: 1.4739
Epoch [23/30], Batch [1700/6000], Loss: 1.4774
Epoch [23/30], Batch [1800/6000], Loss: 1.4915
Epoch [23/30], Batch [1900/6000], Loss: 1.4772
Epoch [23/30], Batch [2000/6000], Loss: 1.4749
Epoch [23/30], Batch [2100/6000], Loss: 1.4797
Epoch [23/30], Batch [2200/6000], Loss: 1.4792
Epoch [23/30], Batch [2300/6000], Loss: 1.4739
Epoch [23/30], Batch [2400/6000], Loss: 1.4756
Epoch [23/30], Batch [2500/6000], Loss: 1.4808
Epoch [23/30], Batch [2600/6000], Loss: 1.4790
Epoch [23/30], Batch [2700/6000], Loss: 1.4758
Epoch [23/30], Batch [2800/6000], Loss: 1.4779
Epoch [23/30], Batch [2900/6000], Loss: 1.4746
Epoch [23/30], Batch [3000/6000], Loss: 1.4743
Epoch [23/30], Batch [3100/6000], Loss: 1.5670
Epoch [23/30], Batch [3200/6000], Loss: 1.4901
Epoch [23/30], Batch [3300/6000], Loss: 1.4766
Epoch [23/30], Batch [3400/6000], Loss: 1.4732
Epoch [23/30], Batch [3500/6000], Loss: 1.4806
Epoch [23/30], Batch [3600/6000], Loss: 1.4709
Epoch [23/30], Batch [3700/6000], Loss: 1.5031
Epoch [23/30], Batch [3800/6000], Loss: 1.4854
Epoch [23/30], Batch [3900/6000], Loss: 1.5274
Epoch [23/30], Batch [4000/6000], Loss: 1.4926
Epoch [23/30], Batch [4100/6000], Loss: 1.4726
Epoch [23/30], Batch [4200/6000], Loss: 1.4792
Epoch [23/30], Batch [4300/6000], Loss: 1.4738
Epoch [23/30], Batch [4400/6000], Loss: 1.4743
Epoch [23/30], Batch [4500/6000], Loss: 1.4774
Epoch [23/30], Batch [4600/6000], Loss: 1.4805
Epoch [23/30], Batch [4700/6000], Loss: 1.4761
Epoch [23/30], Batch [4800/6000], Loss: 1.4820
Epoch [23/30], Batch [4900/6000], Loss: 1.4760
Epoch [23/30], Batch [5000/6000], Loss: 1.4962
Epoch [23/30], Batch [5100/6000], Loss: 1.4732
Epoch [23/30], Batch [5200/6000], Loss: 1.4778
Epoch [23/30], Batch [5300/6000], Loss: 1.4731
Epoch [23/30], Batch [5400/6000], Loss: 1.4795
Epoch [23/30], Batch [5500/6000], Loss: 1.4733
Epoch [23/30], Batch [5600/6000], Loss: 1.4737
Epoch [23/30], Batch [5700/6000], Loss: 1.5016
Epoch [23/30], Batch [5800/6000], Loss: 1.4759
Epoch [23/30], Batch [5900/6000], Loss: 1.4781
Epoch [23/30], Loss: 1.4830
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 1.4784
Epoch [24/30], Batch [100/6000], Loss: 1.4771
Epoch [24/30], Batch [200/6000], Loss: 1.4806
Epoch [24/30], Batch [300/6000], Loss: 1.4779
Epoch [24/30], Batch [400/6000], Loss: 1.4718
Epoch [24/30], Batch [500/6000], Loss: 1.5763
Epoch [24/30], Batch [600/6000], Loss: 1.4760
Epoch [24/30], Batch [700/6000], Loss: 1.4766
Epoch [24/30], Batch [800/6000], Loss: 1.4793
Epoch [24/30], Batch [900/6000], Loss: 1.4740
Epoch [24/30], Batch [1000/6000], Loss: 1.4795
Epoch [24/30], Batch [1100/6000], Loss: 1.4738
Epoch [24/30], Batch [1200/6000], Loss: 1.4772
Epoch [24/30], Batch [1300/6000], Loss: 1.4936
Epoch [24/30], Batch [1400/6000], Loss: 1.4764
Epoch [24/30], Batch [1500/6000], Loss: 1.4752
Epoch [24/30], Batch [1600/6000], Loss: 1.4837
Epoch [24/30], Batch [1700/6000], Loss: 1.4839
Epoch [24/30], Batch [1800/6000], Loss: 1.4904
Epoch [24/30], Batch [1900/6000], Loss: 1.4805
Epoch [24/30], Batch [2000/6000], Loss: 1.4775
Epoch [24/30], Batch [2100/6000], Loss: 1.4789
Epoch [24/30], Batch [2200/6000], Loss: 1.5752
Epoch [24/30], Batch [2300/6000], Loss: 1.4768
Epoch [24/30], Batch [2400/6000], Loss: 1.4708
Epoch [24/30], Batch [2500/6000], Loss: 1.4759
Epoch [24/30], Batch [2600/6000], Loss: 1.4757
Epoch [24/30], Batch [2700/6000], Loss: 1.4750
Epoch [24/30], Batch [2800/6000], Loss: 1.4781
Epoch [24/30], Batch [2900/6000], Loss: 1.4942
Epoch [24/30], Batch [3000/6000], Loss: 1.4803
Epoch [24/30], Batch [3100/6000], Loss: 1.4720
Epoch [24/30], Batch [3200/6000], Loss: 1.4808
Epoch [24/30], Batch [3300/6000], Loss: 1.4734
Epoch [24/30], Batch [3400/6000], Loss: 1.4922
Epoch [24/30], Batch [3500/6000], Loss: 1.4708
Epoch [24/30], Batch [3600/6000], Loss: 1.4788
Epoch [24/30], Batch [3700/6000], Loss: 1.4749
Epoch [24/30], Batch [3800/6000], Loss: 1.4770
Epoch [24/30], Batch [3900/6000], Loss: 1.4782
Epoch [24/30], Batch [4000/6000], Loss: 1.4765
Epoch [24/30], Batch [4100/6000], Loss: 1.4743
Epoch [24/30], Batch [4200/6000], Loss: 1.4772
Epoch [24/30], Batch [4300/6000], Loss: 1.4808
Epoch [24/30], Batch [4400/6000], Loss: 1.4782
Epoch [24/30], Batch [4500/6000], Loss: 1.4748
Epoch [24/30], Batch [4600/6000], Loss: 1.4761
Epoch [24/30], Batch [4700/6000], Loss: 1.4789
Epoch [24/30], Batch [4800/6000], Loss: 1.4750
Epoch [24/30], Batch [4900/6000], Loss: 1.4751
Epoch [24/30], Batch [5000/6000], Loss: 1.4775
Epoch [24/30], Batch [5100/6000], Loss: 1.4782
Epoch [24/30], Batch [5200/6000], Loss: 1.4721
Epoch [24/30], Batch [5300/6000], Loss: 1.4776
Epoch [24/30], Batch [5400/6000], Loss: 1.4755
Epoch [24/30], Batch [5500/6000], Loss: 1.4776
Epoch [24/30], Batch [5600/6000], Loss: 1.4721
Epoch [24/30], Batch [5700/6000], Loss: 1.4776
Epoch [24/30], Batch [5800/6000], Loss: 1.4776
Epoch [24/30], Batch [5900/6000], Loss: 1.4754
Epoch [24/30], Loss: 1.4823
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 1.4721
Epoch [25/30], Batch [100/6000], Loss: 1.4746
Epoch [25/30], Batch [200/6000], Loss: 1.4761
Epoch [25/30], Batch [300/6000], Loss: 1.4736
Epoch [25/30], Batch [400/6000], Loss: 1.4764
Epoch [25/30], Batch [500/6000], Loss: 1.4813
Epoch [25/30], Batch [600/6000], Loss: 1.4778
Epoch [25/30], Batch [700/6000], Loss: 1.4782
Epoch [25/30], Batch [800/6000], Loss: 1.4773
Epoch [25/30], Batch [900/6000], Loss: 1.4769
Epoch [25/30], Batch [1000/6000], Loss: 1.4748
Epoch [25/30], Batch [1100/6000], Loss: 1.4771
Epoch [25/30], Batch [1200/6000], Loss: 1.4749
Epoch [25/30], Batch [1300/6000], Loss: 1.4797
Epoch [25/30], Batch [1400/6000], Loss: 1.4765
Epoch [25/30], Batch [1500/6000], Loss: 1.4768
Epoch [25/30], Batch [1600/6000], Loss: 1.4735
Epoch [25/30], Batch [1700/6000], Loss: 1.4764
Epoch [25/30], Batch [1800/6000], Loss: 1.5755
Epoch [25/30], Batch [1900/6000], Loss: 1.4751
Epoch [25/30], Batch [2000/6000], Loss: 1.4796
Epoch [25/30], Batch [2100/6000], Loss: 1.4799
Epoch [25/30], Batch [2200/6000], Loss: 1.5793
Epoch [25/30], Batch [2300/6000], Loss: 1.4916
Epoch [25/30], Batch [2400/6000], Loss: 1.4772
Epoch [25/30], Batch [2500/6000], Loss: 1.4744
Epoch [25/30], Batch [2600/6000], Loss: 1.4761
Epoch [25/30], Batch [2700/6000], Loss: 1.4788
Epoch [25/30], Batch [2800/6000], Loss: 1.4765
Epoch [25/30], Batch [2900/6000], Loss: 1.4731
Epoch [25/30], Batch [3000/6000], Loss: 1.4812
Epoch [25/30], Batch [3100/6000], Loss: 1.4723
Epoch [25/30], Batch [3200/6000], Loss: 1.4785
Epoch [25/30], Batch [3300/6000], Loss: 1.4758
Epoch [25/30], Batch [3400/6000], Loss: 1.4703
Epoch [25/30], Batch [3500/6000], Loss: 1.4784
Epoch [25/30], Batch [3600/6000], Loss: 1.4752
Epoch [25/30], Batch [3700/6000], Loss: 1.4771
Epoch [25/30], Batch [3800/6000], Loss: 1.4786
Epoch [25/30], Batch [3900/6000], Loss: 1.4782
Epoch [25/30], Batch [4000/6000], Loss: 1.4821
Epoch [25/30], Batch [4100/6000], Loss: 1.4747
Epoch [25/30], Batch [4200/6000], Loss: 1.5072
Epoch [25/30], Batch [4300/6000], Loss: 1.4760
Epoch [25/30], Batch [4400/6000], Loss: 1.4768
Epoch [25/30], Batch [4500/6000], Loss: 1.4782
Epoch [25/30], Batch [4600/6000], Loss: 1.4685
Epoch [25/30], Batch [4700/6000], Loss: 1.5773
Epoch [25/30], Batch [4800/6000], Loss: 1.4713
Epoch [25/30], Batch [4900/6000], Loss: 1.4763
Epoch [25/30], Batch [5000/6000], Loss: 1.4759
Epoch [25/30], Batch [5100/6000], Loss: 1.4716
Epoch [25/30], Batch [5200/6000], Loss: 1.4767
Epoch [25/30], Batch [5300/6000], Loss: 1.4817
Epoch [25/30], Batch [5400/6000], Loss: 1.4736
Epoch [25/30], Batch [5500/6000], Loss: 1.4770
Epoch [25/30], Batch [5600/6000], Loss: 1.4784
Epoch [25/30], Batch [5700/6000], Loss: 1.4922
Epoch [25/30], Batch [5800/6000], Loss: 1.4771
Epoch [25/30], Batch [5900/6000], Loss: 1.4762
Epoch [25/30], Loss: 1.4819
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 1.4753
Epoch [26/30], Batch [100/6000], Loss: 1.4705
Epoch [26/30], Batch [200/6000], Loss: 1.4728
Epoch [26/30], Batch [300/6000], Loss: 1.4767
Epoch [26/30], Batch [400/6000], Loss: 1.4745
Epoch [26/30], Batch [500/6000], Loss: 1.4773
Epoch [26/30], Batch [600/6000], Loss: 1.4713
Epoch [26/30], Batch [700/6000], Loss: 1.4761
Epoch [26/30], Batch [800/6000], Loss: 1.4753
Epoch [26/30], Batch [900/6000], Loss: 1.5752
Epoch [26/30], Batch [1000/6000], Loss: 1.4769
Epoch [26/30], Batch [1100/6000], Loss: 1.4725
Epoch [26/30], Batch [1200/6000], Loss: 1.4733
Epoch [26/30], Batch [1300/6000], Loss: 1.4793
Epoch [26/30], Batch [1400/6000], Loss: 1.5038
Epoch [26/30], Batch [1500/6000], Loss: 1.4758
Epoch [26/30], Batch [1600/6000], Loss: 1.4819
Epoch [26/30], Batch [1700/6000], Loss: 1.4740
Epoch [26/30], Batch [1800/6000], Loss: 1.4789
Epoch [26/30], Batch [1900/6000], Loss: 1.4729
Epoch [26/30], Batch [2000/6000], Loss: 1.4781
Epoch [26/30], Batch [2100/6000], Loss: 1.4778
Epoch [26/30], Batch [2200/6000], Loss: 1.4753
Epoch [26/30], Batch [2300/6000], Loss: 1.4726
Epoch [26/30], Batch [2400/6000], Loss: 1.4722
Epoch [26/30], Batch [2500/6000], Loss: 1.4824
Epoch [26/30], Batch [2600/6000], Loss: 1.4789
Epoch [26/30], Batch [2700/6000], Loss: 1.4765
Epoch [26/30], Batch [2800/6000], Loss: 1.4747
Epoch [26/30], Batch [2900/6000], Loss: 1.4975
Epoch [26/30], Batch [3000/6000], Loss: 1.5018
Epoch [26/30], Batch [3100/6000], Loss: 1.4848
Epoch [26/30], Batch [3200/6000], Loss: 1.4872
Epoch [26/30], Batch [3300/6000], Loss: 1.4783
Epoch [26/30], Batch [3400/6000], Loss: 1.4840
Epoch [26/30], Batch [3500/6000], Loss: 1.4856
Epoch [26/30], Batch [3600/6000], Loss: 1.4723
Epoch [26/30], Batch [3700/6000], Loss: 1.4777
Epoch [26/30], Batch [3800/6000], Loss: 1.4775
Epoch [26/30], Batch [3900/6000], Loss: 1.4770
Epoch [26/30], Batch [4000/6000], Loss: 1.4711
Epoch [26/30], Batch [4100/6000], Loss: 1.4725
Epoch [26/30], Batch [4200/6000], Loss: 1.4733
Epoch [26/30], Batch [4300/6000], Loss: 1.4987
Epoch [26/30], Batch [4400/6000], Loss: 1.4756
Epoch [26/30], Batch [4500/6000], Loss: 1.4754
Epoch [26/30], Batch [4600/6000], Loss: 1.4783
Epoch [26/30], Batch [4700/6000], Loss: 1.4734
Epoch [26/30], Batch [4800/6000], Loss: 1.4748
Epoch [26/30], Batch [4900/6000], Loss: 1.4747
Epoch [26/30], Batch [5000/6000], Loss: 1.4740
Epoch [26/30], Batch [5100/6000], Loss: 1.4756
Epoch [26/30], Batch [5200/6000], Loss: 1.4731
Epoch [26/30], Batch [5300/6000], Loss: 1.4776
Epoch [26/30], Batch [5400/6000], Loss: 1.4816
Epoch [26/30], Batch [5500/6000], Loss: 1.4762
Epoch [26/30], Batch [5600/6000], Loss: 1.5770
Epoch [26/30], Batch [5700/6000], Loss: 1.4775
Epoch [26/30], Batch [5800/6000], Loss: 1.4743
Epoch [26/30], Batch [5900/6000], Loss: 1.4721
Epoch [26/30], Loss: 1.4813
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 1.4780
Epoch [27/30], Batch [100/6000], Loss: 1.4763
Epoch [27/30], Batch [200/6000], Loss: 1.4765
Epoch [27/30], Batch [300/6000], Loss: 1.4753
Epoch [27/30], Batch [400/6000], Loss: 1.4859
Epoch [27/30], Batch [500/6000], Loss: 1.4701
Epoch [27/30], Batch [600/6000], Loss: 1.4744
Epoch [27/30], Batch [700/6000], Loss: 1.4802
Epoch [27/30], Batch [800/6000], Loss: 1.4729
Epoch [27/30], Batch [900/6000], Loss: 1.4740
Epoch [27/30], Batch [1000/6000], Loss: 1.4840
Epoch [27/30], Batch [1100/6000], Loss: 1.4706
Epoch [27/30], Batch [1200/6000], Loss: 1.4745
Epoch [27/30], Batch [1300/6000], Loss: 1.4754
Epoch [27/30], Batch [1400/6000], Loss: 1.4782
Epoch [27/30], Batch [1500/6000], Loss: 1.4740
Epoch [27/30], Batch [1600/6000], Loss: 1.4779
Epoch [27/30], Batch [1700/6000], Loss: 1.4765
Epoch [27/30], Batch [1800/6000], Loss: 1.4812
Epoch [27/30], Batch [1900/6000], Loss: 1.4774
Epoch [27/30], Batch [2000/6000], Loss: 1.4756
Epoch [27/30], Batch [2100/6000], Loss: 1.4744
Epoch [27/30], Batch [2200/6000], Loss: 1.4762
Epoch [27/30], Batch [2300/6000], Loss: 1.4716
Epoch [27/30], Batch [2400/6000], Loss: 1.4723
Epoch [27/30], Batch [2500/6000], Loss: 1.4739
Epoch [27/30], Batch [2600/6000], Loss: 1.4811
Epoch [27/30], Batch [2700/6000], Loss: 1.4785
Epoch [27/30], Batch [2800/6000], Loss: 1.4780
Epoch [27/30], Batch [2900/6000], Loss: 1.4723
Epoch [27/30], Batch [3000/6000], Loss: 1.4716
Epoch [27/30], Batch [3100/6000], Loss: 1.4705
Epoch [27/30], Batch [3200/6000], Loss: 1.4727
Epoch [27/30], Batch [3300/6000], Loss: 1.4754
Epoch [27/30], Batch [3400/6000], Loss: 1.4760
Epoch [27/30], Batch [3500/6000], Loss: 1.4723
Epoch [27/30], Batch [3600/6000], Loss: 1.4743
Epoch [27/30], Batch [3700/6000], Loss: 1.4746
Epoch [27/30], Batch [3800/6000], Loss: 1.4773
Epoch [27/30], Batch [3900/6000], Loss: 1.4793
Epoch [27/30], Batch [4000/6000], Loss: 1.4762
Epoch [27/30], Batch [4100/6000], Loss: 1.4761
Epoch [27/30], Batch [4200/6000], Loss: 1.4899
Epoch [27/30], Batch [4300/6000], Loss: 1.4858
Epoch [27/30], Batch [4400/6000], Loss: 1.4804
Epoch [27/30], Batch [4500/6000], Loss: 1.5741
Epoch [27/30], Batch [4600/6000], Loss: 1.4754
Epoch [27/30], Batch [4700/6000], Loss: 1.4749
Epoch [27/30], Batch [4800/6000], Loss: 1.4805
Epoch [27/30], Batch [4900/6000], Loss: 1.4823
Epoch [27/30], Batch [5000/6000], Loss: 1.4751
Epoch [27/30], Batch [5100/6000], Loss: 1.4726
Epoch [27/30], Batch [5200/6000], Loss: 1.4771
Epoch [27/30], Batch [5300/6000], Loss: 1.4719
Epoch [27/30], Batch [5400/6000], Loss: 1.5878
Epoch [27/30], Batch [5500/6000], Loss: 1.4719
Epoch [27/30], Batch [5600/6000], Loss: 1.4751
Epoch [27/30], Batch [5700/6000], Loss: 1.4766
Epoch [27/30], Batch [5800/6000], Loss: 1.4754
Epoch [27/30], Batch [5900/6000], Loss: 1.4761
Epoch [27/30], Loss: 1.4809
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 1.4729
Epoch [28/30], Batch [100/6000], Loss: 1.4745
Epoch [28/30], Batch [200/6000], Loss: 1.4731
Epoch [28/30], Batch [300/6000], Loss: 1.4789
Epoch [28/30], Batch [400/6000], Loss: 1.4777
Epoch [28/30], Batch [500/6000], Loss: 1.4772
Epoch [28/30], Batch [600/6000], Loss: 1.4745
Epoch [28/30], Batch [700/6000], Loss: 1.4762
Epoch [28/30], Batch [800/6000], Loss: 1.4726
Epoch [28/30], Batch [900/6000], Loss: 1.4803
Epoch [28/30], Batch [1000/6000], Loss: 1.4750
Epoch [28/30], Batch [1100/6000], Loss: 1.4719
Epoch [28/30], Batch [1200/6000], Loss: 1.4726
Epoch [28/30], Batch [1300/6000], Loss: 1.4785
Epoch [28/30], Batch [1400/6000], Loss: 1.4721
Epoch [28/30], Batch [1500/6000], Loss: 1.4841
Epoch [28/30], Batch [1600/6000], Loss: 1.4733
Epoch [28/30], Batch [1700/6000], Loss: 1.4772
Epoch [28/30], Batch [1800/6000], Loss: 1.4722
Epoch [28/30], Batch [1900/6000], Loss: 1.4708
Epoch [28/30], Batch [2000/6000], Loss: 1.5074
Epoch [28/30], Batch [2100/6000], Loss: 1.4731
Epoch [28/30], Batch [2200/6000], Loss: 1.5766
Epoch [28/30], Batch [2300/6000], Loss: 1.4840
Epoch [28/30], Batch [2400/6000], Loss: 1.4735
Epoch [28/30], Batch [2500/6000], Loss: 1.5200
Epoch [28/30], Batch [2600/6000], Loss: 1.4719
Epoch [28/30], Batch [2700/6000], Loss: 1.4758
Epoch [28/30], Batch [2800/6000], Loss: 1.4748
Epoch [28/30], Batch [2900/6000], Loss: 1.4764
Epoch [28/30], Batch [3000/6000], Loss: 1.4768
Epoch [28/30], Batch [3100/6000], Loss: 1.4753
Epoch [28/30], Batch [3200/6000], Loss: 1.4741
Epoch [28/30], Batch [3300/6000], Loss: 1.4741
Epoch [28/30], Batch [3400/6000], Loss: 1.4765
Epoch [28/30], Batch [3500/6000], Loss: 1.4771
Epoch [28/30], Batch [3600/6000], Loss: 1.4761
Epoch [28/30], Batch [3700/6000], Loss: 1.4752
Epoch [28/30], Batch [3800/6000], Loss: 1.4744
Epoch [28/30], Batch [3900/6000], Loss: 1.4742
Epoch [28/30], Batch [4000/6000], Loss: 1.4750
Epoch [28/30], Batch [4100/6000], Loss: 1.4738
Epoch [28/30], Batch [4200/6000], Loss: 1.4727
Epoch [28/30], Batch [4300/6000], Loss: 1.4711
Epoch [28/30], Batch [4400/6000], Loss: 1.4726
Epoch [28/30], Batch [4500/6000], Loss: 1.4764
Epoch [28/30], Batch [4600/6000], Loss: 1.4728
Epoch [28/30], Batch [4700/6000], Loss: 1.4745
Epoch [28/30], Batch [4800/6000], Loss: 1.4757
Epoch [28/30], Batch [4900/6000], Loss: 1.4808
Epoch [28/30], Batch [5000/6000], Loss: 1.4768
Epoch [28/30], Batch [5100/6000], Loss: 1.5363
Epoch [28/30], Batch [5200/6000], Loss: 1.4732
Epoch [28/30], Batch [5300/6000], Loss: 1.4741
Epoch [28/30], Batch [5400/6000], Loss: 1.5753
Epoch [28/30], Batch [5500/6000], Loss: 1.4732
Epoch [28/30], Batch [5600/6000], Loss: 1.4773
Epoch [28/30], Batch [5700/6000], Loss: 1.4763
Epoch [28/30], Batch [5800/6000], Loss: 1.4805
Epoch [28/30], Batch [5900/6000], Loss: 1.4743
Epoch [28/30], Loss: 1.4804
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 1.4737
Epoch [29/30], Batch [100/6000], Loss: 1.4853
Epoch [29/30], Batch [200/6000], Loss: 1.5759
Epoch [29/30], Batch [300/6000], Loss: 1.4751
Epoch [29/30], Batch [400/6000], Loss: 1.4793
Epoch [29/30], Batch [500/6000], Loss: 1.4762
Epoch [29/30], Batch [600/6000], Loss: 1.4789
Epoch [29/30], Batch [700/6000], Loss: 1.4784
Epoch [29/30], Batch [800/6000], Loss: 1.4710
Epoch [29/30], Batch [900/6000], Loss: 1.4722
Epoch [29/30], Batch [1000/6000], Loss: 1.4763
Epoch [29/30], Batch [1100/6000], Loss: 1.4712
Epoch [29/30], Batch [1200/6000], Loss: 1.4731
Epoch [29/30], Batch [1300/6000], Loss: 1.4730
Epoch [29/30], Batch [1400/6000], Loss: 1.4771
Epoch [29/30], Batch [1500/6000], Loss: 1.4720
Epoch [29/30], Batch [1600/6000], Loss: 1.4809
Epoch [29/30], Batch [1700/6000], Loss: 1.4735
Epoch [29/30], Batch [1800/6000], Loss: 1.4774
Epoch [29/30], Batch [1900/6000], Loss: 1.4885
Epoch [29/30], Batch [2000/6000], Loss: 1.4774
Epoch [29/30], Batch [2100/6000], Loss: 1.4796
Epoch [29/30], Batch [2200/6000], Loss: 1.4744
Epoch [29/30], Batch [2300/6000], Loss: 1.4773
Epoch [29/30], Batch [2400/6000], Loss: 1.4803
Epoch [29/30], Batch [2500/6000], Loss: 1.4756
Epoch [29/30], Batch [2600/6000], Loss: 1.4783
Epoch [29/30], Batch [2700/6000], Loss: 1.4720
Epoch [29/30], Batch [2800/6000], Loss: 1.4773
Epoch [29/30], Batch [2900/6000], Loss: 1.4749
Epoch [29/30], Batch [3000/6000], Loss: 1.4745
Epoch [29/30], Batch [3100/6000], Loss: 1.4790
Epoch [29/30], Batch [3200/6000], Loss: 1.4792
Epoch [29/30], Batch [3300/6000], Loss: 1.4771
Epoch [29/30], Batch [3400/6000], Loss: 1.4754
Epoch [29/30], Batch [3500/6000], Loss: 1.4725
Epoch [29/30], Batch [3600/6000], Loss: 1.4738
Epoch [29/30], Batch [3700/6000], Loss: 1.4765
Epoch [29/30], Batch [3800/6000], Loss: 1.4758
Epoch [29/30], Batch [3900/6000], Loss: 1.4751
Epoch [29/30], Batch [4000/6000], Loss: 1.4736
Epoch [29/30], Batch [4100/6000], Loss: 1.4738
Epoch [29/30], Batch [4200/6000], Loss: 1.4779
Epoch [29/30], Batch [4300/6000], Loss: 1.4735
Epoch [29/30], Batch [4400/6000], Loss: 1.4739
Epoch [29/30], Batch [4500/6000], Loss: 1.4749
Epoch [29/30], Batch [4600/6000], Loss: 1.4788
Epoch [29/30], Batch [4700/6000], Loss: 1.4754
Epoch [29/30], Batch [4800/6000], Loss: 1.4733
Epoch [29/30], Batch [4900/6000], Loss: 1.4845
Epoch [29/30], Batch [5000/6000], Loss: 1.4749
Epoch [29/30], Batch [5100/6000], Loss: 1.4793
Epoch [29/30], Batch [5200/6000], Loss: 1.4749
Epoch [29/30], Batch [5300/6000], Loss: 1.4764
Epoch [29/30], Batch [5400/6000], Loss: 1.4748
Epoch [29/30], Batch [5500/6000], Loss: 1.4753
Epoch [29/30], Batch [5600/6000], Loss: 1.4745
Epoch [29/30], Batch [5700/6000], Loss: 1.5722
Epoch [29/30], Batch [5800/6000], Loss: 1.4761
Epoch [29/30], Batch [5900/6000], Loss: 1.4742
Epoch [29/30], Loss: 1.4795
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 1.4738
Epoch [30/30], Batch [100/6000], Loss: 1.4719
Epoch [30/30], Batch [200/6000], Loss: 1.4731
Epoch [30/30], Batch [300/6000], Loss: 1.4745
Epoch [30/30], Batch [400/6000], Loss: 1.4746
Epoch [30/30], Batch [500/6000], Loss: 1.4768
Epoch [30/30], Batch [600/6000], Loss: 1.4768
Epoch [30/30], Batch [700/6000], Loss: 1.4756
Epoch [30/30], Batch [800/6000], Loss: 1.5858
Epoch [30/30], Batch [900/6000], Loss: 1.4733
Epoch [30/30], Batch [1000/6000], Loss: 1.4736
Epoch [30/30], Batch [1100/6000], Loss: 1.4753
Epoch [30/30], Batch [1200/6000], Loss: 1.4767
Epoch [30/30], Batch [1300/6000], Loss: 1.4790
Epoch [30/30], Batch [1400/6000], Loss: 1.4752
Epoch [30/30], Batch [1500/6000], Loss: 1.4745
Epoch [30/30], Batch [1600/6000], Loss: 1.4721
Epoch [30/30], Batch [1700/6000], Loss: 1.4750
Epoch [30/30], Batch [1800/6000], Loss: 1.4691
Epoch [30/30], Batch [1900/6000], Loss: 1.4734
Epoch [30/30], Batch [2000/6000], Loss: 1.4745
Epoch [30/30], Batch [2100/6000], Loss: 1.5721
Epoch [30/30], Batch [2200/6000], Loss: 1.4749
Epoch [30/30], Batch [2300/6000], Loss: 1.4766
Epoch [30/30], Batch [2400/6000], Loss: 1.4767
Epoch [30/30], Batch [2500/6000], Loss: 1.4717
Epoch [30/30], Batch [2600/6000], Loss: 1.4735
Epoch [30/30], Batch [2700/6000], Loss: 1.4756
Epoch [30/30], Batch [2800/6000], Loss: 1.4773
Epoch [30/30], Batch [2900/6000], Loss: 1.4783
Epoch [30/30], Batch [3000/6000], Loss: 1.4782
Epoch [30/30], Batch [3100/6000], Loss: 1.4733
Epoch [30/30], Batch [3200/6000], Loss: 1.4872
Epoch [30/30], Batch [3300/6000], Loss: 1.4751
Epoch [30/30], Batch [3400/6000], Loss: 1.4721
Epoch [30/30], Batch [3500/6000], Loss: 1.4750
Epoch [30/30], Batch [3600/6000], Loss: 1.4772
Epoch [30/30], Batch [3700/6000], Loss: 1.4873
Epoch [30/30], Batch [3800/6000], Loss: 1.4758
Epoch [30/30], Batch [3900/6000], Loss: 1.4708
Epoch [30/30], Batch [4000/6000], Loss: 1.4776
Epoch [30/30], Batch [4100/6000], Loss: 1.4740
Epoch [30/30], Batch [4200/6000], Loss: 1.4719
Epoch [30/30], Batch [4300/6000], Loss: 1.4774
Epoch [30/30], Batch [4400/6000], Loss: 1.4735
Epoch [30/30], Batch [4500/6000], Loss: 1.4758
Epoch [30/30], Batch [4600/6000], Loss: 1.4718
Epoch [30/30], Batch [4700/6000], Loss: 1.4752
Epoch [30/30], Batch [4800/6000], Loss: 1.4751
Epoch [30/30], Batch [4900/6000], Loss: 1.4761
Epoch [30/30], Batch [5000/6000], Loss: 1.4717
Epoch [30/30], Batch [5100/6000], Loss: 1.4775
Epoch [30/30], Batch [5200/6000], Loss: 1.4754
Epoch [30/30], Batch [5300/6000], Loss: 1.4704
Epoch [30/30], Batch [5400/6000], Loss: 1.4724
Epoch [30/30], Batch [5500/6000], Loss: 1.4708
Epoch [30/30], Batch [5600/6000], Loss: 1.4778
Epoch [30/30], Batch [5700/6000], Loss: 1.4846
Epoch [30/30], Batch [5800/6000], Loss: 1.4754
Epoch [30/30], Batch [5900/6000], Loss: 1.4729
Epoch [30/30], Loss: 1.4793
Visualization saved to figures/visualization_0.png
Test Loss: 1.4902, Accuracy: 98.19%
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 1/300:
  Label Loss: 0.1268
  Image Loss: 0.0125
  Total Loss: 6.3525
  Image grad max: 0.0019742469303309917
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 2/300:
  Label Loss: 0.1268
  Image Loss: 0.0117
  Total Loss: 6.3517
  Image grad max: 0.0019487368408590555
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 3/300:
  Label Loss: 0.1268
  Image Loss: 0.0110
  Total Loss: 6.3510
  Image grad max: 0.0019232359481975436
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 4/300:
  Label Loss: 0.1268
  Image Loss: 0.0103
  Total Loss: 6.3503
  Image grad max: 0.0018977505387738347
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 5/300:
  Label Loss: 0.1268
  Image Loss: 0.0097
  Total Loss: 6.3497
  Image grad max: 0.0018722868990153074
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 6/300:
  Label Loss: 0.1268
  Image Loss: 0.0091
  Total Loss: 6.3491
  Image grad max: 0.001846851548179984
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 7/300:
  Label Loss: 0.1268
  Image Loss: 0.0085
  Total Loss: 6.3486
  Image grad max: 0.0018214508891105652
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 8/300:
  Label Loss: 0.1268
  Image Loss: 0.0080
  Total Loss: 6.3480
  Image grad max: 0.0017960915574803948
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 9/300:
  Label Loss: 0.1268
  Image Loss: 0.0075
  Total Loss: 6.3475
  Image grad max: 0.0017707800725474954
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 10/300:
  Label Loss: 0.1268
  Image Loss: 0.0071
  Total Loss: 6.3471
  Image grad max: 0.001745523069985211
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 11/300:
  Label Loss: 0.1268
  Image Loss: 0.0066
  Total Loss: 6.3466
  Image grad max: 0.0017203270690515637
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 12/300:
  Label Loss: 0.1268
  Image Loss: 0.0062
  Total Loss: 6.3462
  Image grad max: 0.0016951985890045762
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 13/300:
  Label Loss: 0.1268
  Image Loss: 0.0058
  Total Loss: 6.3459
  Image grad max: 0.001670144498348236
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 14/300:
  Label Loss: 0.1268
  Image Loss: 0.0055
  Total Loss: 6.3455
  Image grad max: 0.001645171083509922
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 15/300:
  Label Loss: 0.1268
  Image Loss: 0.0051
  Total Loss: 6.3452
  Image grad max: 0.0016202848637476563
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 16/300:
  Label Loss: 0.1268
  Image Loss: 0.0048
  Total Loss: 6.3448
  Image grad max: 0.0015954922419041395
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 17/300:
  Label Loss: 0.1268
  Image Loss: 0.0045
  Total Loss: 6.3446
  Image grad max: 0.001570799620822072
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 18/300:
  Label Loss: 0.1268
  Image Loss: 0.0043
  Total Loss: 6.3443
  Image grad max: 0.0015462132869288325
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 19/300:
  Label Loss: 0.1268
  Image Loss: 0.0040
  Total Loss: 6.3440
  Image grad max: 0.0015217394102364779
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 20/300:
  Label Loss: 0.1268
  Image Loss: 0.0038
  Total Loss: 6.3438
  Image grad max: 0.0014973839279264212
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 21/300:
  Label Loss: 0.1268
  Image Loss: 0.0035
  Total Loss: 6.3436
  Image grad max: 0.0014731527771800756
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 22/300:
  Label Loss: 0.1268
  Image Loss: 0.0033
  Total Loss: 6.3433
  Image grad max: 0.0014490520115941763
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 23/300:
  Label Loss: 0.1268
  Image Loss: 0.0031
  Total Loss: 6.3431
  Image grad max: 0.001425087102688849
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 24/300:
  Label Loss: 0.1268
  Image Loss: 0.0029
  Total Loss: 6.3430
  Image grad max: 0.0014012637548148632
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 25/300:
  Label Loss: 0.1268
  Image Loss: 0.0028
  Total Loss: 6.3428
  Image grad max: 0.0013775874394923449
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 26/300:
  Label Loss: 0.1268
  Image Loss: 0.0026
  Total Loss: 6.3426
  Image grad max: 0.0013540631625801325
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 27/300:
  Label Loss: 0.1268
  Image Loss: 0.0025
  Total Loss: 6.3425
  Image grad max: 0.0013306962791830301
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 28/300:
  Label Loss: 0.1268
  Image Loss: 0.0023
  Total Loss: 6.3423
  Image grad max: 0.0013074919115751982
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 29/300:
  Label Loss: 0.1268
  Image Loss: 0.0022
  Total Loss: 6.3422
  Image grad max: 0.0012844547163695097
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 30/300:
  Label Loss: 0.1268
  Image Loss: 0.0021
  Total Loss: 6.3421
  Image grad max: 0.0012615893501788378
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 31/300:
  Label Loss: 0.1268
  Image Loss: 0.0019
  Total Loss: 6.3420
  Image grad max: 0.0012389003532007337
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 32/300:
  Label Loss: 0.1268
  Image Loss: 0.0018
  Total Loss: 6.3418
  Image grad max: 0.0012163921492174268
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 33/300:
  Label Loss: 0.1268
  Image Loss: 0.0017
  Total Loss: 6.3417
  Image grad max: 0.001194068812765181
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 34/300:
  Label Loss: 0.1268
  Image Loss: 0.0016
  Total Loss: 6.3416
  Image grad max: 0.0011719345347955823
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 35/300:
  Label Loss: 0.1268
  Image Loss: 0.0015
  Total Loss: 6.3415
  Image grad max: 0.0011499931570142508
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 36/300:
  Label Loss: 0.1268
  Image Loss: 0.0014
  Total Loss: 6.3415
  Image grad max: 0.001128248288296163
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 37/300:
  Label Loss: 0.1268
  Image Loss: 0.0014
  Total Loss: 6.3414
  Image grad max: 0.001106703537516296
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 38/300:
  Label Loss: 0.1268
  Image Loss: 0.0013
  Total Loss: 6.3413
  Image grad max: 0.0010853625135496259
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 39/300:
  Label Loss: 0.1268
  Image Loss: 0.0012
  Total Loss: 6.3412
  Image grad max: 0.0010642282431945205
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 40/300:
  Label Loss: 0.1268
  Image Loss: 0.0011
  Total Loss: 6.3411
  Image grad max: 0.001043303869664669
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 41/300:
  Label Loss: 0.1268
  Image Loss: 0.0011
  Total Loss: 6.3411
  Image grad max: 0.0010225923033431172
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 42/300:
  Label Loss: 0.1268
  Image Loss: 0.0010
  Total Loss: 6.3410
  Image grad max: 0.001002096338197589
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 43/300:
  Label Loss: 0.1268
  Image Loss: 0.0009
  Total Loss: 6.3410
  Image grad max: 0.000981818651780486
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 44/300:
  Label Loss: 0.1268
  Image Loss: 0.0009
  Total Loss: 6.3409
  Image grad max: 0.0009617616888135672
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 45/300:
  Label Loss: 0.1268
  Image Loss: 0.0008
  Total Loss: 6.3408
  Image grad max: 0.0009419279522262514
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 46/300:
  Label Loss: 0.1268
  Image Loss: 0.0008
  Total Loss: 6.3408
  Image grad max: 0.0009223193628713489
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 47/300:
  Label Loss: 0.1268
  Image Loss: 0.0007
  Total Loss: 6.3408
  Image grad max: 0.0009029382490552962
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 48/300:
  Label Loss: 0.1268
  Image Loss: 0.0007
  Total Loss: 6.3407
  Image grad max: 0.0008837864734232426
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 49/300:
  Label Loss: 0.1268
  Image Loss: 0.0006
  Total Loss: 6.3407
  Image grad max: 0.0008648657822050154
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 50/300:
  Label Loss: 0.1268
  Image Loss: 0.0006
  Total Loss: 6.3406
  Image grad max: 0.0008461778052151203
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 51/300:
  Label Loss: 0.1268
  Image Loss: 0.0006
  Total Loss: 6.3406
  Image grad max: 0.000827724055852741
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 52/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 6.3406
  Image grad max: 0.0008095060475170612
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 53/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 6.3405
  Image grad max: 0.0007915248861536384
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 54/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 6.3405
  Image grad max: 0.0007737818523310125
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 55/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 6.3405
  Image grad max: 0.00075627799378708
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 56/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 6.3404
  Image grad max: 0.0007390141254290938
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 57/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 6.3404
  Image grad max: 0.0007219910621643066
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 58/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 6.3404
  Image grad max: 0.0007052094442769885
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 59/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3404
  Image grad max: 0.0006886699120514095
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 60/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3403
  Image grad max: 0.0006723727565258741
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 61/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3403
  Image grad max: 0.000656318268738687
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 62/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3403
  Image grad max: 0.0006405069143511355
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 63/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3403
  Image grad max: 0.0006249385187402368
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 64/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3403
  Image grad max: 0.0006096131983213127
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 65/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3403
  Image grad max: 0.0005945308948867023
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 66/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.000579691375605762
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 67/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0005650943494401872
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 68/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0005507393507286906
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 69/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0005366259138099849
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 70/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0005227533401921391
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 71/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0005091211060062051
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 72/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0004957282799296081
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 73/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3402
  Image grad max: 0.00048257404705509543
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 74/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00046965747606009245
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 75/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0004569775192067027
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 76/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0004445329250302166
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 77/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00043232255848124623
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 78/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0004203451389912516
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 79/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0004085992113687098
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 80/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0003970833495259285
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 81/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0003857960400637239
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 82/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00037473556585609913
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 83/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0003639003844000399
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 84/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00035328863305039704
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 85/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0003428985655773431
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 86/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00033272820292040706
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 87/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00032277568243443966
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 88/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3401
  Image grad max: 0.00031303890864364803
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 89/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3401
  Image grad max: 0.00030351593159139156
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 90/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3401
  Image grad max: 0.00029420453938655555
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 91/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3401
  Image grad max: 0.0002851024910341948
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 92/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3401
  Image grad max: 0.00027620766195468605
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 93/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0002675176947377622
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 94/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0002590302610769868
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 95/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00025074294535443187
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 96/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.000242653361056
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 97/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0002347590052522719
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 98/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00022705746232531965
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 99/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00021954606927465647
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 100/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00021222229406703264
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 101/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00020508357556536794
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 102/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00019812716345768422
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 103/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00019135048205498606
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 104/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00018475079559721053
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 105/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0001783254265319556
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 106/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0001720715663395822
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 107/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00016598650836385787
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 108/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00016006747318897396
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 109/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00015431166684720665
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 110/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.000148716353578493
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 111/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0001432786521036178
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 112/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00013799581211060286
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 113/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.000132864952320233
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 114/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00012788324966095388
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 115/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0001230479683727026
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 116/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00011835623445222154
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 117/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00011380522482795641
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 118/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0001093921237043105
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 119/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00010511412256164476
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 120/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00010096847836393863
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 121/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.695238986751065e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 122/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.306303400080651e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 123/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.929767500376329e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 124/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.56536571518518e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 125/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.212816464947537e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 126/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.871849811635911e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 127/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.542199455201626e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 128/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.223596185212955e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 129/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.915780977578834e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 130/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.618487532250583e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 131/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.3314575527329e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 132/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.054436744307168e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 133/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.78717008465901e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 134/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.5294065532507375e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 135/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.280899131321348e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 136/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.0414011639077216e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 137/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.8106725444085896e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 138/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.588477895595133e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 139/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.374577838461846e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 140/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.1687442717375234e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 141/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.97074873035308e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 142/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.780367478611879e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 143/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.597376053221524e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 144/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.421561268623918e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 145/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.252706665080041e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 146/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.090607060585171e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 147/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.9350534532568417e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 148/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.7858404791913927e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 149/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.6427740522194654e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 150/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.5056606318685226e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 151/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.374306131969206e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 152/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.2485242880065925e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 153/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.1281366571201943e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 154/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.0129606127738953e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 155/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9028211681870744e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 156/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7975475202547386e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 157/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6969685020740144e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 158/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6009244063752703e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 159/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.5092557077878155e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 160/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4218064279702958e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 161/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.338424590358045e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 162/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2589618563652039e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 163/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.183273798233131e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 164/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1112198990304023e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 165/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.0426630979054607e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 166/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.774705176823772e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 167/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.155129191640299e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 168/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.566608812543564e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 169/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.0079298641067e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 170/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.477911822206806e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 171/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.975377800699789e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 172/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.499225037259748e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 173/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.0483889683382586e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 174/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.6217668316094205e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 175/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.218333171796985e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 176/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.8371371121902484e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 177/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.477190486795735e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 178/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.137542873650091e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 179/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.817320248344913e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 180/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.515647676977096e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 181/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.2316893339157104e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 182/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.9646466828125995e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 183/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.713721869440633e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 184/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.478154328855453e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 185/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.257221694890177e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 186/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.050202056125272e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 187/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8564489892014535e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 188/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6752782130424748e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 189/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.506081616753363e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 190/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.3482509757523076e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 191/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2011780654574977e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 192/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.064330604094721e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 193/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.371765941068588e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 194/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.1918369687628e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 195/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.098578294062463e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 196/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.293459478001751e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 197/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.98935457674088e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 198/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.6898107914094e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 199/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.395588686951669e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 200/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.107448828312044e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 201/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.825391215490527e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 202/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.5509361257245473e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 203/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.283323562503938e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 204/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.023313522338867e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 205/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.7716662859565986e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 206/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.5283821375742264e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 207/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.293460792974656e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 208/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.0676625328851514e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 209/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.937657370694069e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 210/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.068422813612415e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 211/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.174859557475429e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 212/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.2600090094092593e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 213/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.3238711694139056e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 214/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.369486876181327e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 215/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.398376975383144e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 216/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.412061744256789e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 217/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.412061744256789e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 218/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.398376975383144e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 219/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.3740485605449066e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 220/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.3390764997420774e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 221/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.2949810702120885e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 222/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.243283401843655e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 223/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.1839826419854944e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 224/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.118600204743416e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 225/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.0471352374661365e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 226/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.971109154259466e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 227/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.890521102472121e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 228/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.8068922119928175e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 229/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.721742760058987e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 230/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.636213025652978e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 231/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.574251709575037e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 232/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.5084887056436855e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 233/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.439684863020375e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 234/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.3682201799601899e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 235/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.2948547950818465e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 236/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.2199688487489766e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 237/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.14356234096158e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 238/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.0663956945554673e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 239/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9888487656771758e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 240/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9113019789074315e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 241/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.83375505002914e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 242/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.756968543986659e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 243/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6805620361992624e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 244/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6052959495027608e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 245/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.5311702838971541e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 246/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4581850393824425e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 247/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.3867202142137103e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 248/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.3167760926080518e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 249/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2483523903483729e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 250/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1818293188525786e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 251/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1172069491749426e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 252/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.0544852102611912e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 253/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.936641731655982e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 254/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.347437668338898e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 255/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.777239912660662e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 256/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.229850578800324e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 257/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.701467552578833e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 258/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.195892237632506e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 259/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.709323940867762e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 260/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.241761951741864e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 261/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.79700767389113e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 262/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.371260414221979e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 263/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.9645194621916744e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 264/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.576785173071585e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 265/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.20805754686171e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 266/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.858336228290682e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 267/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.5276219279012366e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 268/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.215913935150638e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 269/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.9194112016739382e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 270/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.641914953471769e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 271/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.379624142179182e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 272/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.1325385901604932e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 273/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9006582974157027e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 274/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.911732105952524e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 275/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4825134897478165e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 276/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8807231327855334e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 277/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2960682305163118e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 278/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7039953448261258e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 279/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9424380326427126e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 280/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9180356858328196e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 281/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9616367197272666e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 282/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.0967000580185413e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 283/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.0112059573307306e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 284/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.92909954677134e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 285/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4922790114724194e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 286/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8427813941457316e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 287/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.3219976748362114e-09
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 288/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7639408156355785e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 289/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.5074224535283065e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 290/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.991159059855363e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 291/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.322899383955246e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 292/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6636363397992682e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 293/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.3965759215750495e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 294/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6239491529290717e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 295/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9510405735445602e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 296/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.820871986524253e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 297/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7311741373760015e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 298/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.047836389351687e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 299/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9895560754434882e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 300/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.0120323185324196e-08
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
