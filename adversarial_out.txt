Epoch [1/30], Batch [0/6000], Loss: 2.5334
Epoch [1/30], Batch [100/6000], Loss: 2.3443
Epoch [1/30], Batch [200/6000], Loss: 1.9998
Epoch [1/30], Batch [300/6000], Loss: 1.8755
Epoch [1/30], Batch [400/6000], Loss: 1.7677
Epoch [1/30], Batch [500/6000], Loss: 1.9136
Epoch [1/30], Batch [600/6000], Loss: 1.8365
Epoch [1/30], Batch [700/6000], Loss: 1.7091
Epoch [1/30], Batch [800/6000], Loss: 1.7289
Epoch [1/30], Batch [900/6000], Loss: 1.7405
Epoch [1/30], Batch [1000/6000], Loss: 1.7459
Epoch [1/30], Batch [1100/6000], Loss: 1.7738
Epoch [1/30], Batch [1200/6000], Loss: 1.8222
Epoch [1/30], Batch [1300/6000], Loss: 1.7615
Epoch [1/30], Batch [1400/6000], Loss: 1.6285
Epoch [1/30], Batch [1500/6000], Loss: 1.6800
Epoch [1/30], Batch [1600/6000], Loss: 1.6756
Epoch [1/30], Batch [1700/6000], Loss: 1.5750
Epoch [1/30], Batch [1800/6000], Loss: 1.6160
Epoch [1/30], Batch [1900/6000], Loss: 1.6355
Epoch [1/30], Batch [2000/6000], Loss: 1.5577
Epoch [1/30], Batch [2100/6000], Loss: 1.5871
Epoch [1/30], Batch [2200/6000], Loss: 1.6752
Epoch [1/30], Batch [2300/6000], Loss: 1.6764
Epoch [1/30], Batch [2400/6000], Loss: 1.6206
Epoch [1/30], Batch [2500/6000], Loss: 1.8296
Epoch [1/30], Batch [2600/6000], Loss: 1.8536
Epoch [1/30], Batch [2700/6000], Loss: 1.6297
Epoch [1/30], Batch [2800/6000], Loss: 1.5614
Epoch [1/30], Batch [2900/6000], Loss: 1.7606
Epoch [1/30], Batch [3000/6000], Loss: 1.5582
Epoch [1/30], Batch [3100/6000], Loss: 1.6649
Epoch [1/30], Batch [3200/6000], Loss: 1.5382
Epoch [1/30], Batch [3300/6000], Loss: 1.7335
Epoch [1/30], Batch [3400/6000], Loss: 1.5593
Epoch [1/30], Batch [3500/6000], Loss: 1.5768
Epoch [1/30], Batch [3600/6000], Loss: 1.7453
Epoch [1/30], Batch [3700/6000], Loss: 1.5292
Epoch [1/30], Batch [3800/6000], Loss: 1.5598
Epoch [1/30], Batch [3900/6000], Loss: 1.6340
Epoch [1/30], Batch [4000/6000], Loss: 1.5235
Epoch [1/30], Batch [4100/6000], Loss: 1.5268
Epoch [1/30], Batch [4200/6000], Loss: 1.6665
Epoch [1/30], Batch [4300/6000], Loss: 1.6686
Epoch [1/30], Batch [4400/6000], Loss: 1.6589
Epoch [1/30], Batch [4500/6000], Loss: 1.5660
Epoch [1/30], Batch [4600/6000], Loss: 1.5334
Epoch [1/30], Batch [4700/6000], Loss: 1.5599
Epoch [1/30], Batch [4800/6000], Loss: 1.5329
Epoch [1/30], Batch [4900/6000], Loss: 1.6727
Epoch [1/30], Batch [5000/6000], Loss: 1.6267
Epoch [1/30], Batch [5100/6000], Loss: 1.6206
Epoch [1/30], Batch [5200/6000], Loss: 1.5524
Epoch [1/30], Batch [5300/6000], Loss: 1.6314
Epoch [1/30], Batch [5400/6000], Loss: 1.5207
Epoch [1/30], Batch [5500/6000], Loss: 1.6583
Epoch [1/30], Batch [5600/6000], Loss: 1.5792
Epoch [1/30], Batch [5700/6000], Loss: 1.5340
Epoch [1/30], Batch [5800/6000], Loss: 1.5158
Epoch [1/30], Batch [5900/6000], Loss: 1.5098
Epoch [1/30], Loss: 1.6754
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 1.5381
Epoch [2/30], Batch [100/6000], Loss: 1.7572
Epoch [2/30], Batch [200/6000], Loss: 1.5984
Epoch [2/30], Batch [300/6000], Loss: 1.5611
Epoch [2/30], Batch [400/6000], Loss: 1.5300
Epoch [2/30], Batch [500/6000], Loss: 1.6693
Epoch [2/30], Batch [600/6000], Loss: 1.5541
Epoch [2/30], Batch [700/6000], Loss: 1.5809
Epoch [2/30], Batch [800/6000], Loss: 1.5160
Epoch [2/30], Batch [900/6000], Loss: 1.5064
Epoch [2/30], Batch [1000/6000], Loss: 1.5589
Epoch [2/30], Batch [1100/6000], Loss: 1.5190
Epoch [2/30], Batch [1200/6000], Loss: 1.5626
Epoch [2/30], Batch [1300/6000], Loss: 1.5323
Epoch [2/30], Batch [1400/6000], Loss: 1.5542
Epoch [2/30], Batch [1500/6000], Loss: 1.6145
Epoch [2/30], Batch [1600/6000], Loss: 1.5106
Epoch [2/30], Batch [1700/6000], Loss: 1.5302
Epoch [2/30], Batch [1800/6000], Loss: 1.7276
Epoch [2/30], Batch [1900/6000], Loss: 1.5527
Epoch [2/30], Batch [2000/6000], Loss: 1.5247
Epoch [2/30], Batch [2100/6000], Loss: 1.6272
Epoch [2/30], Batch [2200/6000], Loss: 1.5267
Epoch [2/30], Batch [2300/6000], Loss: 1.5271
Epoch [2/30], Batch [2400/6000], Loss: 1.5131
Epoch [2/30], Batch [2500/6000], Loss: 1.6254
Epoch [2/30], Batch [2600/6000], Loss: 1.6274
Epoch [2/30], Batch [2700/6000], Loss: 1.6008
Epoch [2/30], Batch [2800/6000], Loss: 1.5363
Epoch [2/30], Batch [2900/6000], Loss: 1.6194
Epoch [2/30], Batch [3000/6000], Loss: 1.5238
Epoch [2/30], Batch [3100/6000], Loss: 1.5170
Epoch [2/30], Batch [3200/6000], Loss: 1.5446
Epoch [2/30], Batch [3300/6000], Loss: 1.5589
Epoch [2/30], Batch [3400/6000], Loss: 1.6235
Epoch [2/30], Batch [3500/6000], Loss: 1.5038
Epoch [2/30], Batch [3600/6000], Loss: 1.5099
Epoch [2/30], Batch [3700/6000], Loss: 1.5935
Epoch [2/30], Batch [3800/6000], Loss: 1.5279
Epoch [2/30], Batch [3900/6000], Loss: 1.6254
Epoch [2/30], Batch [4000/6000], Loss: 1.5913
Epoch [2/30], Batch [4100/6000], Loss: 1.5688
Epoch [2/30], Batch [4200/6000], Loss: 1.5329
Epoch [2/30], Batch [4300/6000], Loss: 1.4946
Epoch [2/30], Batch [4400/6000], Loss: 1.5305
Epoch [2/30], Batch [4500/6000], Loss: 1.5978
Epoch [2/30], Batch [4600/6000], Loss: 1.4923
Epoch [2/30], Batch [4700/6000], Loss: 1.5826
Epoch [2/30], Batch [4800/6000], Loss: 1.5115
Epoch [2/30], Batch [4900/6000], Loss: 1.5589
Epoch [2/30], Batch [5000/6000], Loss: 1.6366
Epoch [2/30], Batch [5100/6000], Loss: 1.5076
Epoch [2/30], Batch [5200/6000], Loss: 1.5073
Epoch [2/30], Batch [5300/6000], Loss: 1.6269
Epoch [2/30], Batch [5400/6000], Loss: 1.5066
Epoch [2/30], Batch [5500/6000], Loss: 1.5196
Epoch [2/30], Batch [5600/6000], Loss: 1.5360
Epoch [2/30], Batch [5700/6000], Loss: 1.5278
Epoch [2/30], Batch [5800/6000], Loss: 1.6100
Epoch [2/30], Batch [5900/6000], Loss: 1.5893
Epoch [2/30], Loss: 1.5630
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 1.5294
Epoch [3/30], Batch [100/6000], Loss: 1.5384
Epoch [3/30], Batch [200/6000], Loss: 1.5721
Epoch [3/30], Batch [300/6000], Loss: 1.5567
Epoch [3/30], Batch [400/6000], Loss: 1.5056
Epoch [3/30], Batch [500/6000], Loss: 1.5028
Epoch [3/30], Batch [600/6000], Loss: 1.5158
Epoch [3/30], Batch [700/6000], Loss: 1.5165
Epoch [3/30], Batch [800/6000], Loss: 1.6080
Epoch [3/30], Batch [900/6000], Loss: 1.5062
Epoch [3/30], Batch [1000/6000], Loss: 1.5076
Epoch [3/30], Batch [1100/6000], Loss: 1.5522
Epoch [3/30], Batch [1200/6000], Loss: 1.5533
Epoch [3/30], Batch [1300/6000], Loss: 1.6030
Epoch [3/30], Batch [1400/6000], Loss: 1.5071
Epoch [3/30], Batch [1500/6000], Loss: 1.5005
Epoch [3/30], Batch [1600/6000], Loss: 1.5232
Epoch [3/30], Batch [1700/6000], Loss: 1.5600
Epoch [3/30], Batch [1800/6000], Loss: 1.5105
Epoch [3/30], Batch [1900/6000], Loss: 1.6201
Epoch [3/30], Batch [2000/6000], Loss: 1.5038
Epoch [3/30], Batch [2100/6000], Loss: 1.6774
Epoch [3/30], Batch [2200/6000], Loss: 1.5337
Epoch [3/30], Batch [2300/6000], Loss: 1.4881
Epoch [3/30], Batch [2400/6000], Loss: 1.5206
Epoch [3/30], Batch [2500/6000], Loss: 1.5014
Epoch [3/30], Batch [2600/6000], Loss: 1.4958
Epoch [3/30], Batch [2700/6000], Loss: 1.5133
Epoch [3/30], Batch [2800/6000], Loss: 1.4978
Epoch [3/30], Batch [2900/6000], Loss: 1.5185
Epoch [3/30], Batch [3000/6000], Loss: 1.5071
Epoch [3/30], Batch [3100/6000], Loss: 1.5144
Epoch [3/30], Batch [3200/6000], Loss: 1.5997
Epoch [3/30], Batch [3300/6000], Loss: 1.5323
Epoch [3/30], Batch [3400/6000], Loss: 1.5214
Epoch [3/30], Batch [3500/6000], Loss: 1.5014
Epoch [3/30], Batch [3600/6000], Loss: 1.5372
Epoch [3/30], Batch [3700/6000], Loss: 1.4936
Epoch [3/30], Batch [3800/6000], Loss: 1.5227
Epoch [3/30], Batch [3900/6000], Loss: 1.5096
Epoch [3/30], Batch [4000/6000], Loss: 1.5006
Epoch [3/30], Batch [4100/6000], Loss: 1.5997
Epoch [3/30], Batch [4200/6000], Loss: 1.5022
Epoch [3/30], Batch [4300/6000], Loss: 1.5158
Epoch [3/30], Batch [4400/6000], Loss: 1.4967
Epoch [3/30], Batch [4500/6000], Loss: 1.5165
Epoch [3/30], Batch [4600/6000], Loss: 1.6014
Epoch [3/30], Batch [4700/6000], Loss: 1.5089
Epoch [3/30], Batch [4800/6000], Loss: 1.5431
Epoch [3/30], Batch [4900/6000], Loss: 1.6125
Epoch [3/30], Batch [5000/6000], Loss: 1.5157
Epoch [3/30], Batch [5100/6000], Loss: 1.4999
Epoch [3/30], Batch [5200/6000], Loss: 1.5918
Epoch [3/30], Batch [5300/6000], Loss: 1.5070
Epoch [3/30], Batch [5400/6000], Loss: 1.5079
Epoch [3/30], Batch [5500/6000], Loss: 1.6106
Epoch [3/30], Batch [5600/6000], Loss: 1.5286
Epoch [3/30], Batch [5700/6000], Loss: 1.4936
Epoch [3/30], Batch [5800/6000], Loss: 1.5038
Epoch [3/30], Batch [5900/6000], Loss: 1.5356
Epoch [3/30], Loss: 1.5415
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 1.5164
Epoch [4/30], Batch [100/6000], Loss: 1.4973
Epoch [4/30], Batch [200/6000], Loss: 1.6003
Epoch [4/30], Batch [300/6000], Loss: 1.6144
Epoch [4/30], Batch [400/6000], Loss: 1.6149
Epoch [4/30], Batch [500/6000], Loss: 1.5020
Epoch [4/30], Batch [600/6000], Loss: 1.5735
Epoch [4/30], Batch [700/6000], Loss: 1.6218
Epoch [4/30], Batch [800/6000], Loss: 1.5380
Epoch [4/30], Batch [900/6000], Loss: 1.5089
Epoch [4/30], Batch [1000/6000], Loss: 1.5204
Epoch [4/30], Batch [1100/6000], Loss: 1.4923
Epoch [4/30], Batch [1200/6000], Loss: 1.5251
Epoch [4/30], Batch [1300/6000], Loss: 1.5006
Epoch [4/30], Batch [1400/6000], Loss: 1.5176
Epoch [4/30], Batch [1500/6000], Loss: 1.6261
Epoch [4/30], Batch [1600/6000], Loss: 1.5051
Epoch [4/30], Batch [1700/6000], Loss: 1.5036
Epoch [4/30], Batch [1800/6000], Loss: 1.5072
Epoch [4/30], Batch [1900/6000], Loss: 1.6114
Epoch [4/30], Batch [2000/6000], Loss: 1.6128
Epoch [4/30], Batch [2100/6000], Loss: 1.4998
Epoch [4/30], Batch [2200/6000], Loss: 1.5046
Epoch [4/30], Batch [2300/6000], Loss: 1.4942
Epoch [4/30], Batch [2400/6000], Loss: 1.5216
Epoch [4/30], Batch [2500/6000], Loss: 1.4962
Epoch [4/30], Batch [2600/6000], Loss: 1.5019
Epoch [4/30], Batch [2700/6000], Loss: 1.5068
Epoch [4/30], Batch [2800/6000], Loss: 1.6115
Epoch [4/30], Batch [2900/6000], Loss: 1.5142
Epoch [4/30], Batch [3000/6000], Loss: 1.5256
Epoch [4/30], Batch [3100/6000], Loss: 1.5084
Epoch [4/30], Batch [3200/6000], Loss: 1.4990
Epoch [4/30], Batch [3300/6000], Loss: 1.5174
Epoch [4/30], Batch [3400/6000], Loss: 1.5355
Epoch [4/30], Batch [3500/6000], Loss: 1.5034
Epoch [4/30], Batch [3600/6000], Loss: 1.6024
Epoch [4/30], Batch [3700/6000], Loss: 1.6084
Epoch [4/30], Batch [3800/6000], Loss: 1.5325
Epoch [4/30], Batch [3900/6000], Loss: 1.5121
Epoch [4/30], Batch [4000/6000], Loss: 1.5128
Epoch [4/30], Batch [4100/6000], Loss: 1.5238
Epoch [4/30], Batch [4200/6000], Loss: 1.5173
Epoch [4/30], Batch [4300/6000], Loss: 1.5006
Epoch [4/30], Batch [4400/6000], Loss: 1.4967
Epoch [4/30], Batch [4500/6000], Loss: 1.5469
Epoch [4/30], Batch [4600/6000], Loss: 1.4928
Epoch [4/30], Batch [4700/6000], Loss: 1.4978
Epoch [4/30], Batch [4800/6000], Loss: 1.5101
Epoch [4/30], Batch [4900/6000], Loss: 1.5107
Epoch [4/30], Batch [5000/6000], Loss: 1.5201
Epoch [4/30], Batch [5100/6000], Loss: 1.5146
Epoch [4/30], Batch [5200/6000], Loss: 1.5049
Epoch [4/30], Batch [5300/6000], Loss: 1.4907
Epoch [4/30], Batch [5400/6000], Loss: 1.5211
Epoch [4/30], Batch [5500/6000], Loss: 1.5935
Epoch [4/30], Batch [5600/6000], Loss: 1.6132
Epoch [4/30], Batch [5700/6000], Loss: 1.6017
Epoch [4/30], Batch [5800/6000], Loss: 1.5289
Epoch [4/30], Batch [5900/6000], Loss: 1.6147
Epoch [4/30], Loss: 1.5291
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 1.5055
Epoch [5/30], Batch [100/6000], Loss: 1.4958
Epoch [5/30], Batch [200/6000], Loss: 1.4949
Epoch [5/30], Batch [300/6000], Loss: 1.5315
Epoch [5/30], Batch [400/6000], Loss: 1.4991
Epoch [5/30], Batch [500/6000], Loss: 1.5150
Epoch [5/30], Batch [600/6000], Loss: 1.6187
Epoch [5/30], Batch [700/6000], Loss: 1.5005
Epoch [5/30], Batch [800/6000], Loss: 1.4911
Epoch [5/30], Batch [900/6000], Loss: 1.5168
Epoch [5/30], Batch [1000/6000], Loss: 1.5093
Epoch [5/30], Batch [1100/6000], Loss: 1.4963
Epoch [5/30], Batch [1200/6000], Loss: 1.4913
Epoch [5/30], Batch [1300/6000], Loss: 1.5019
Epoch [5/30], Batch [1400/6000], Loss: 1.5228
Epoch [5/30], Batch [1500/6000], Loss: 1.4916
Epoch [5/30], Batch [1600/6000], Loss: 1.4905
Epoch [5/30], Batch [1700/6000], Loss: 1.6214
Epoch [5/30], Batch [1800/6000], Loss: 1.5125
Epoch [5/30], Batch [1900/6000], Loss: 1.5076
Epoch [5/30], Batch [2000/6000], Loss: 1.4919
Epoch [5/30], Batch [2100/6000], Loss: 1.5007
Epoch [5/30], Batch [2200/6000], Loss: 1.4953
Epoch [5/30], Batch [2300/6000], Loss: 1.5005
Epoch [5/30], Batch [2400/6000], Loss: 1.4933
Epoch [5/30], Batch [2500/6000], Loss: 1.4960
Epoch [5/30], Batch [2600/6000], Loss: 1.4933
Epoch [5/30], Batch [2700/6000], Loss: 1.4919
Epoch [5/30], Batch [2800/6000], Loss: 1.4933
Epoch [5/30], Batch [2900/6000], Loss: 1.5197
Epoch [5/30], Batch [3000/6000], Loss: 1.5958
Epoch [5/30], Batch [3100/6000], Loss: 1.5373
Epoch [5/30], Batch [3200/6000], Loss: 1.5364
Epoch [5/30], Batch [3300/6000], Loss: 1.4958
Epoch [5/30], Batch [3400/6000], Loss: 1.4908
Epoch [5/30], Batch [3500/6000], Loss: 1.5031
Epoch [5/30], Batch [3600/6000], Loss: 1.5109
Epoch [5/30], Batch [3700/6000], Loss: 1.5220
Epoch [5/30], Batch [3800/6000], Loss: 1.4944
Epoch [5/30], Batch [3900/6000], Loss: 1.5869
Epoch [5/30], Batch [4000/6000], Loss: 1.4969
Epoch [5/30], Batch [4100/6000], Loss: 1.5243
Epoch [5/30], Batch [4200/6000], Loss: 1.4980
Epoch [5/30], Batch [4300/6000], Loss: 1.4905
Epoch [5/30], Batch [4400/6000], Loss: 1.5013
Epoch [5/30], Batch [4500/6000], Loss: 1.5016
Epoch [5/30], Batch [4600/6000], Loss: 1.5077
Epoch [5/30], Batch [4700/6000], Loss: 1.5320
Epoch [5/30], Batch [4800/6000], Loss: 1.5004
Epoch [5/30], Batch [4900/6000], Loss: 1.5080
Epoch [5/30], Batch [5000/6000], Loss: 1.5063
Epoch [5/30], Batch [5100/6000], Loss: 1.5915
Epoch [5/30], Batch [5200/6000], Loss: 1.5444
Epoch [5/30], Batch [5300/6000], Loss: 1.4990
Epoch [5/30], Batch [5400/6000], Loss: 1.5054
Epoch [5/30], Batch [5500/6000], Loss: 1.5946
Epoch [5/30], Batch [5600/6000], Loss: 1.6050
Epoch [5/30], Batch [5700/6000], Loss: 1.4962
Epoch [5/30], Batch [5800/6000], Loss: 1.4923
Epoch [5/30], Batch [5900/6000], Loss: 1.5077
Epoch [5/30], Loss: 1.5210
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 1.5088
Epoch [6/30], Batch [100/6000], Loss: 1.4917
Epoch [6/30], Batch [200/6000], Loss: 1.4926
Epoch [6/30], Batch [300/6000], Loss: 1.5050
Epoch [6/30], Batch [400/6000], Loss: 1.5110
Epoch [6/30], Batch [500/6000], Loss: 1.5041
Epoch [6/30], Batch [600/6000], Loss: 1.4891
Epoch [6/30], Batch [700/6000], Loss: 1.5010
Epoch [6/30], Batch [800/6000], Loss: 1.5021
Epoch [6/30], Batch [900/6000], Loss: 1.4866
Epoch [6/30], Batch [1000/6000], Loss: 1.4933
Epoch [6/30], Batch [1100/6000], Loss: 1.4971
Epoch [6/30], Batch [1200/6000], Loss: 1.5125
Epoch [6/30], Batch [1300/6000], Loss: 1.4909
Epoch [6/30], Batch [1400/6000], Loss: 1.4946
Epoch [6/30], Batch [1500/6000], Loss: 1.4991
Epoch [6/30], Batch [1600/6000], Loss: 1.5210
Epoch [6/30], Batch [1700/6000], Loss: 1.5077
Epoch [6/30], Batch [1800/6000], Loss: 1.5300
Epoch [6/30], Batch [1900/6000], Loss: 1.4952
Epoch [6/30], Batch [2000/6000], Loss: 1.4929
Epoch [6/30], Batch [2100/6000], Loss: 1.4969
Epoch [6/30], Batch [2200/6000], Loss: 1.5112
Epoch [6/30], Batch [2300/6000], Loss: 1.4937
Epoch [6/30], Batch [2400/6000], Loss: 1.5005
Epoch [6/30], Batch [2500/6000], Loss: 1.4981
Epoch [6/30], Batch [2600/6000], Loss: 1.5980
Epoch [6/30], Batch [2700/6000], Loss: 1.5083
Epoch [6/30], Batch [2800/6000], Loss: 1.4969
Epoch [6/30], Batch [2900/6000], Loss: 1.5131
Epoch [6/30], Batch [3000/6000], Loss: 1.5484
Epoch [6/30], Batch [3100/6000], Loss: 1.4950
Epoch [6/30], Batch [3200/6000], Loss: 1.4869
Epoch [6/30], Batch [3300/6000], Loss: 1.5702
Epoch [6/30], Batch [3400/6000], Loss: 1.6077
Epoch [6/30], Batch [3500/6000], Loss: 1.4925
Epoch [6/30], Batch [3600/6000], Loss: 1.5504
Epoch [6/30], Batch [3700/6000], Loss: 1.4950
Epoch [6/30], Batch [3800/6000], Loss: 1.5075
Epoch [6/30], Batch [3900/6000], Loss: 1.4865
Epoch [6/30], Batch [4000/6000], Loss: 1.4943
Epoch [6/30], Batch [4100/6000], Loss: 1.6063
Epoch [6/30], Batch [4200/6000], Loss: 1.6086
Epoch [6/30], Batch [4300/6000], Loss: 1.4925
Epoch [6/30], Batch [4400/6000], Loss: 1.5401
Epoch [6/30], Batch [4500/6000], Loss: 1.4887
Epoch [6/30], Batch [4600/6000], Loss: 1.6018
Epoch [6/30], Batch [4700/6000], Loss: 1.5046
Epoch [6/30], Batch [4800/6000], Loss: 1.5360
Epoch [6/30], Batch [4900/6000], Loss: 1.5057
Epoch [6/30], Batch [5000/6000], Loss: 1.5140
Epoch [6/30], Batch [5100/6000], Loss: 1.6088
Epoch [6/30], Batch [5200/6000], Loss: 1.4921
Epoch [6/30], Batch [5300/6000], Loss: 1.4886
Epoch [6/30], Batch [5400/6000], Loss: 1.4965
Epoch [6/30], Batch [5500/6000], Loss: 1.4854
Epoch [6/30], Batch [5600/6000], Loss: 1.4971
Epoch [6/30], Batch [5700/6000], Loss: 1.5053
Epoch [6/30], Batch [5800/6000], Loss: 1.5048
Epoch [6/30], Batch [5900/6000], Loss: 1.4923
Epoch [6/30], Loss: 1.5147
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 1.5141
Epoch [7/30], Batch [100/6000], Loss: 1.4889
Epoch [7/30], Batch [200/6000], Loss: 1.5027
Epoch [7/30], Batch [300/6000], Loss: 1.5074
Epoch [7/30], Batch [400/6000], Loss: 1.5134
Epoch [7/30], Batch [500/6000], Loss: 1.5126
Epoch [7/30], Batch [600/6000], Loss: 1.4941
Epoch [7/30], Batch [700/6000], Loss: 1.5205
Epoch [7/30], Batch [800/6000], Loss: 1.4944
Epoch [7/30], Batch [900/6000], Loss: 1.5012
Epoch [7/30], Batch [1000/6000], Loss: 1.5072
Epoch [7/30], Batch [1100/6000], Loss: 1.5117
Epoch [7/30], Batch [1200/6000], Loss: 1.4926
Epoch [7/30], Batch [1300/6000], Loss: 1.5248
Epoch [7/30], Batch [1400/6000], Loss: 1.4901
Epoch [7/30], Batch [1500/6000], Loss: 1.5082
Epoch [7/30], Batch [1600/6000], Loss: 1.5126
Epoch [7/30], Batch [1700/6000], Loss: 1.5155
Epoch [7/30], Batch [1800/6000], Loss: 1.5027
Epoch [7/30], Batch [1900/6000], Loss: 1.4914
Epoch [7/30], Batch [2000/6000], Loss: 1.4954
Epoch [7/30], Batch [2100/6000], Loss: 1.5898
Epoch [7/30], Batch [2200/6000], Loss: 1.4916
Epoch [7/30], Batch [2300/6000], Loss: 1.4889
Epoch [7/30], Batch [2400/6000], Loss: 1.4848
Epoch [7/30], Batch [2500/6000], Loss: 1.4899
Epoch [7/30], Batch [2600/6000], Loss: 1.5119
Epoch [7/30], Batch [2700/6000], Loss: 1.4923
Epoch [7/30], Batch [2800/6000], Loss: 1.4914
Epoch [7/30], Batch [2900/6000], Loss: 1.4887
Epoch [7/30], Batch [3000/6000], Loss: 1.4893
Epoch [7/30], Batch [3100/6000], Loss: 1.4843
Epoch [7/30], Batch [3200/6000], Loss: 1.4911
Epoch [7/30], Batch [3300/6000], Loss: 1.4978
Epoch [7/30], Batch [3400/6000], Loss: 1.5016
Epoch [7/30], Batch [3500/6000], Loss: 1.4919
Epoch [7/30], Batch [3600/6000], Loss: 1.4904
Epoch [7/30], Batch [3700/6000], Loss: 1.4936
Epoch [7/30], Batch [3800/6000], Loss: 1.5315
Epoch [7/30], Batch [3900/6000], Loss: 1.5633
Epoch [7/30], Batch [4000/6000], Loss: 1.4905
Epoch [7/30], Batch [4100/6000], Loss: 1.5931
Epoch [7/30], Batch [4200/6000], Loss: 1.5018
Epoch [7/30], Batch [4300/6000], Loss: 1.4877
Epoch [7/30], Batch [4400/6000], Loss: 1.4907
Epoch [7/30], Batch [4500/6000], Loss: 1.4789
Epoch [7/30], Batch [4600/6000], Loss: 1.4911
Epoch [7/30], Batch [4700/6000], Loss: 1.4901
Epoch [7/30], Batch [4800/6000], Loss: 1.5006
Epoch [7/30], Batch [4900/6000], Loss: 1.4854
Epoch [7/30], Batch [5000/6000], Loss: 1.5878
Epoch [7/30], Batch [5100/6000], Loss: 1.4919
Epoch [7/30], Batch [5200/6000], Loss: 1.4935
Epoch [7/30], Batch [5300/6000], Loss: 1.4974
Epoch [7/30], Batch [5400/6000], Loss: 1.5843
Epoch [7/30], Batch [5500/6000], Loss: 1.5881
Epoch [7/30], Batch [5600/6000], Loss: 1.5665
Epoch [7/30], Batch [5700/6000], Loss: 1.5998
Epoch [7/30], Batch [5800/6000], Loss: 1.4935
Epoch [7/30], Batch [5900/6000], Loss: 1.4895
Epoch [7/30], Loss: 1.5104
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 1.4888
Epoch [8/30], Batch [100/6000], Loss: 1.5026
Epoch [8/30], Batch [200/6000], Loss: 1.5039
Epoch [8/30], Batch [300/6000], Loss: 1.4911
Epoch [8/30], Batch [400/6000], Loss: 1.4932
Epoch [8/30], Batch [500/6000], Loss: 1.4945
Epoch [8/30], Batch [600/6000], Loss: 1.4894
Epoch [8/30], Batch [700/6000], Loss: 1.5252
Epoch [8/30], Batch [800/6000], Loss: 1.4905
Epoch [8/30], Batch [900/6000], Loss: 1.4970
Epoch [8/30], Batch [1000/6000], Loss: 1.4854
Epoch [8/30], Batch [1100/6000], Loss: 1.4859
Epoch [8/30], Batch [1200/6000], Loss: 1.4941
Epoch [8/30], Batch [1300/6000], Loss: 1.4925
Epoch [8/30], Batch [1400/6000], Loss: 1.5318
Epoch [8/30], Batch [1500/6000], Loss: 1.5170
Epoch [8/30], Batch [1600/6000], Loss: 1.4925
Epoch [8/30], Batch [1700/6000], Loss: 1.4965
Epoch [8/30], Batch [1800/6000], Loss: 1.4898
Epoch [8/30], Batch [1900/6000], Loss: 1.4920
Epoch [8/30], Batch [2000/6000], Loss: 1.5005
Epoch [8/30], Batch [2100/6000], Loss: 1.4885
Epoch [8/30], Batch [2200/6000], Loss: 1.4878
Epoch [8/30], Batch [2300/6000], Loss: 1.4995
Epoch [8/30], Batch [2400/6000], Loss: 1.4925
Epoch [8/30], Batch [2500/6000], Loss: 1.4895
Epoch [8/30], Batch [2600/6000], Loss: 1.5041
Epoch [8/30], Batch [2700/6000], Loss: 1.5000
Epoch [8/30], Batch [2800/6000], Loss: 1.4920
Epoch [8/30], Batch [2900/6000], Loss: 1.4948
Epoch [8/30], Batch [3000/6000], Loss: 1.5114
Epoch [8/30], Batch [3100/6000], Loss: 1.5162
Epoch [8/30], Batch [3200/6000], Loss: 1.4963
Epoch [8/30], Batch [3300/6000], Loss: 1.4924
Epoch [8/30], Batch [3400/6000], Loss: 1.5025
Epoch [8/30], Batch [3500/6000], Loss: 1.4906
Epoch [8/30], Batch [3600/6000], Loss: 1.4933
Epoch [8/30], Batch [3700/6000], Loss: 1.4889
Epoch [8/30], Batch [3800/6000], Loss: 1.4963
Epoch [8/30], Batch [3900/6000], Loss: 1.4928
Epoch [8/30], Batch [4000/6000], Loss: 1.5004
Epoch [8/30], Batch [4100/6000], Loss: 1.5976
Epoch [8/30], Batch [4200/6000], Loss: 1.4885
Epoch [8/30], Batch [4300/6000], Loss: 1.5016
Epoch [8/30], Batch [4400/6000], Loss: 1.4919
Epoch [8/30], Batch [4500/6000], Loss: 1.4989
Epoch [8/30], Batch [4600/6000], Loss: 1.5028
Epoch [8/30], Batch [4700/6000], Loss: 1.4865
Epoch [8/30], Batch [4800/6000], Loss: 1.5050
Epoch [8/30], Batch [4900/6000], Loss: 1.4900
Epoch [8/30], Batch [5000/6000], Loss: 1.4885
Epoch [8/30], Batch [5100/6000], Loss: 1.4882
Epoch [8/30], Batch [5200/6000], Loss: 1.4981
Epoch [8/30], Batch [5300/6000], Loss: 1.4920
Epoch [8/30], Batch [5400/6000], Loss: 1.5055
Epoch [8/30], Batch [5500/6000], Loss: 1.4877
Epoch [8/30], Batch [5600/6000], Loss: 1.5784
Epoch [8/30], Batch [5700/6000], Loss: 1.4893
Epoch [8/30], Batch [5800/6000], Loss: 1.5081
Epoch [8/30], Batch [5900/6000], Loss: 1.4923
Epoch [8/30], Loss: 1.5068
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 1.4875
Epoch [9/30], Batch [100/6000], Loss: 1.4861
Epoch [9/30], Batch [200/6000], Loss: 1.4963
Epoch [9/30], Batch [300/6000], Loss: 1.4994
Epoch [9/30], Batch [400/6000], Loss: 1.4885
Epoch [9/30], Batch [500/6000], Loss: 1.4969
Epoch [9/30], Batch [600/6000], Loss: 1.5194
Epoch [9/30], Batch [700/6000], Loss: 1.4903
Epoch [9/30], Batch [800/6000], Loss: 1.4881
Epoch [9/30], Batch [900/6000], Loss: 1.4914
Epoch [9/30], Batch [1000/6000], Loss: 1.4996
Epoch [9/30], Batch [1100/6000], Loss: 1.4830
Epoch [9/30], Batch [1200/6000], Loss: 1.4888
Epoch [9/30], Batch [1300/6000], Loss: 1.4873
Epoch [9/30], Batch [1400/6000], Loss: 1.4885
Epoch [9/30], Batch [1500/6000], Loss: 1.4882
Epoch [9/30], Batch [1600/6000], Loss: 1.4883
Epoch [9/30], Batch [1700/6000], Loss: 1.4964
Epoch [9/30], Batch [1800/6000], Loss: 1.4843
Epoch [9/30], Batch [1900/6000], Loss: 1.4819
Epoch [9/30], Batch [2000/6000], Loss: 1.5972
Epoch [9/30], Batch [2100/6000], Loss: 1.4945
Epoch [9/30], Batch [2200/6000], Loss: 1.4842
Epoch [9/30], Batch [2300/6000], Loss: 1.4916
Epoch [9/30], Batch [2400/6000], Loss: 1.4870
Epoch [9/30], Batch [2500/6000], Loss: 1.4868
Epoch [9/30], Batch [2600/6000], Loss: 1.4871
Epoch [9/30], Batch [2700/6000], Loss: 1.4962
Epoch [9/30], Batch [2800/6000], Loss: 1.5936
Epoch [9/30], Batch [2900/6000], Loss: 1.5217
Epoch [9/30], Batch [3000/6000], Loss: 1.4814
Epoch [9/30], Batch [3100/6000], Loss: 1.4889
Epoch [9/30], Batch [3200/6000], Loss: 1.5529
Epoch [9/30], Batch [3300/6000], Loss: 1.4852
Epoch [9/30], Batch [3400/6000], Loss: 1.4926
Epoch [9/30], Batch [3500/6000], Loss: 1.4831
Epoch [9/30], Batch [3600/6000], Loss: 1.4891
Epoch [9/30], Batch [3700/6000], Loss: 1.4798
Epoch [9/30], Batch [3800/6000], Loss: 1.4965
Epoch [9/30], Batch [3900/6000], Loss: 1.4946
Epoch [9/30], Batch [4000/6000], Loss: 1.5051
Epoch [9/30], Batch [4100/6000], Loss: 1.5213
Epoch [9/30], Batch [4200/6000], Loss: 1.4854
Epoch [9/30], Batch [4300/6000], Loss: 1.4874
Epoch [9/30], Batch [4400/6000], Loss: 1.4933
Epoch [9/30], Batch [4500/6000], Loss: 1.5161
Epoch [9/30], Batch [4600/6000], Loss: 1.4833
Epoch [9/30], Batch [4700/6000], Loss: 1.5458
Epoch [9/30], Batch [4800/6000], Loss: 1.4888
Epoch [9/30], Batch [4900/6000], Loss: 1.4919
Epoch [9/30], Batch [5000/6000], Loss: 1.4896
Epoch [9/30], Batch [5100/6000], Loss: 1.4878
Epoch [9/30], Batch [5200/6000], Loss: 1.4867
Epoch [9/30], Batch [5300/6000], Loss: 1.4884
Epoch [9/30], Batch [5400/6000], Loss: 1.4874
Epoch [9/30], Batch [5500/6000], Loss: 1.4912
Epoch [9/30], Batch [5600/6000], Loss: 1.4899
Epoch [9/30], Batch [5700/6000], Loss: 1.4940
Epoch [9/30], Batch [5800/6000], Loss: 1.4874
Epoch [9/30], Batch [5900/6000], Loss: 1.4861
Epoch [9/30], Loss: 1.5039
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 1.5027
Epoch [10/30], Batch [100/6000], Loss: 1.5067
Epoch [10/30], Batch [200/6000], Loss: 1.5085
Epoch [10/30], Batch [300/6000], Loss: 1.4825
Epoch [10/30], Batch [400/6000], Loss: 1.4895
Epoch [10/30], Batch [500/6000], Loss: 1.4969
Epoch [10/30], Batch [600/6000], Loss: 1.4889
Epoch [10/30], Batch [700/6000], Loss: 1.4905
Epoch [10/30], Batch [800/6000], Loss: 1.5052
Epoch [10/30], Batch [900/6000], Loss: 1.4899
Epoch [10/30], Batch [1000/6000], Loss: 1.5034
Epoch [10/30], Batch [1100/6000], Loss: 1.4841
Epoch [10/30], Batch [1200/6000], Loss: 1.4969
Epoch [10/30], Batch [1300/6000], Loss: 1.5105
Epoch [10/30], Batch [1400/6000], Loss: 1.5627
Epoch [10/30], Batch [1500/6000], Loss: 1.5085
Epoch [10/30], Batch [1600/6000], Loss: 1.5071
Epoch [10/30], Batch [1700/6000], Loss: 1.4899
Epoch [10/30], Batch [1800/6000], Loss: 1.4826
Epoch [10/30], Batch [1900/6000], Loss: 1.4865
Epoch [10/30], Batch [2000/6000], Loss: 1.4933
Epoch [10/30], Batch [2100/6000], Loss: 1.4856
Epoch [10/30], Batch [2200/6000], Loss: 1.5833
Epoch [10/30], Batch [2300/6000], Loss: 1.4848
Epoch [10/30], Batch [2400/6000], Loss: 1.4866
Epoch [10/30], Batch [2500/6000], Loss: 1.4814
Epoch [10/30], Batch [2600/6000], Loss: 1.4889
Epoch [10/30], Batch [2700/6000], Loss: 1.5133
Epoch [10/30], Batch [2800/6000], Loss: 1.4974
Epoch [10/30], Batch [2900/6000], Loss: 1.4914
Epoch [10/30], Batch [3000/6000], Loss: 1.5698
Epoch [10/30], Batch [3100/6000], Loss: 1.4916
Epoch [10/30], Batch [3200/6000], Loss: 1.4894
Epoch [10/30], Batch [3300/6000], Loss: 1.5022
Epoch [10/30], Batch [3400/6000], Loss: 1.5084
Epoch [10/30], Batch [3500/6000], Loss: 1.4865
Epoch [10/30], Batch [3600/6000], Loss: 1.4975
Epoch [10/30], Batch [3700/6000], Loss: 1.4852
Epoch [10/30], Batch [3800/6000], Loss: 1.4848
Epoch [10/30], Batch [3900/6000], Loss: 1.4897
Epoch [10/30], Batch [4000/6000], Loss: 1.4938
Epoch [10/30], Batch [4100/6000], Loss: 1.4895
Epoch [10/30], Batch [4200/6000], Loss: 1.5018
Epoch [10/30], Batch [4300/6000], Loss: 1.4924
Epoch [10/30], Batch [4400/6000], Loss: 1.5399
Epoch [10/30], Batch [4500/6000], Loss: 1.4849
Epoch [10/30], Batch [4600/6000], Loss: 1.4813
Epoch [10/30], Batch [4700/6000], Loss: 1.4872
Epoch [10/30], Batch [4800/6000], Loss: 1.4840
Epoch [10/30], Batch [4900/6000], Loss: 1.4862
Epoch [10/30], Batch [5000/6000], Loss: 1.4859
Epoch [10/30], Batch [5100/6000], Loss: 1.4868
Epoch [10/30], Batch [5200/6000], Loss: 1.4993
Epoch [10/30], Batch [5300/6000], Loss: 1.4932
Epoch [10/30], Batch [5400/6000], Loss: 1.4845
Epoch [10/30], Batch [5500/6000], Loss: 1.4826
Epoch [10/30], Batch [5600/6000], Loss: 1.4875
Epoch [10/30], Batch [5700/6000], Loss: 1.4995
Epoch [10/30], Batch [5800/6000], Loss: 1.4846
Epoch [10/30], Batch [5900/6000], Loss: 1.4834
Epoch [10/30], Loss: 1.5008
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 1.4891
Epoch [11/30], Batch [100/6000], Loss: 1.4931
Epoch [11/30], Batch [200/6000], Loss: 1.5003
Epoch [11/30], Batch [300/6000], Loss: 1.4983
Epoch [11/30], Batch [400/6000], Loss: 1.4893
Epoch [11/30], Batch [500/6000], Loss: 1.4848
Epoch [11/30], Batch [600/6000], Loss: 1.4994
Epoch [11/30], Batch [700/6000], Loss: 1.4924
Epoch [11/30], Batch [800/6000], Loss: 1.4966
Epoch [11/30], Batch [900/6000], Loss: 1.4872
Epoch [11/30], Batch [1000/6000], Loss: 1.4890
Epoch [11/30], Batch [1100/6000], Loss: 1.4889
Epoch [11/30], Batch [1200/6000], Loss: 1.4988
Epoch [11/30], Batch [1300/6000], Loss: 1.4879
Epoch [11/30], Batch [1400/6000], Loss: 1.5039
Epoch [11/30], Batch [1500/6000], Loss: 1.5472
Epoch [11/30], Batch [1600/6000], Loss: 1.5820
Epoch [11/30], Batch [1700/6000], Loss: 1.4899
Epoch [11/30], Batch [1800/6000], Loss: 1.5042
Epoch [11/30], Batch [1900/6000], Loss: 1.4852
Epoch [11/30], Batch [2000/6000], Loss: 1.4890
Epoch [11/30], Batch [2100/6000], Loss: 1.5017
Epoch [11/30], Batch [2200/6000], Loss: 1.4812
Epoch [11/30], Batch [2300/6000], Loss: 1.4912
Epoch [11/30], Batch [2400/6000], Loss: 1.4878
Epoch [11/30], Batch [2500/6000], Loss: 1.4873
Epoch [11/30], Batch [2600/6000], Loss: 1.4905
Epoch [11/30], Batch [2700/6000], Loss: 1.5860
Epoch [11/30], Batch [2800/6000], Loss: 1.4820
Epoch [11/30], Batch [2900/6000], Loss: 1.4905
Epoch [11/30], Batch [3000/6000], Loss: 1.4875
Epoch [11/30], Batch [3100/6000], Loss: 1.4907
Epoch [11/30], Batch [3200/6000], Loss: 1.4856
Epoch [11/30], Batch [3300/6000], Loss: 1.4982
Epoch [11/30], Batch [3400/6000], Loss: 1.4900
Epoch [11/30], Batch [3500/6000], Loss: 1.4759
Epoch [11/30], Batch [3600/6000], Loss: 1.4826
Epoch [11/30], Batch [3700/6000], Loss: 1.4811
Epoch [11/30], Batch [3800/6000], Loss: 1.5043
Epoch [11/30], Batch [3900/6000], Loss: 1.4847
Epoch [11/30], Batch [4000/6000], Loss: 1.4835
Epoch [11/30], Batch [4100/6000], Loss: 1.5905
Epoch [11/30], Batch [4200/6000], Loss: 1.4813
Epoch [11/30], Batch [4300/6000], Loss: 1.4900
Epoch [11/30], Batch [4400/6000], Loss: 1.4847
Epoch [11/30], Batch [4500/6000], Loss: 1.4850
Epoch [11/30], Batch [4600/6000], Loss: 1.4891
Epoch [11/30], Batch [4700/6000], Loss: 1.4846
Epoch [11/30], Batch [4800/6000], Loss: 1.4876
Epoch [11/30], Batch [4900/6000], Loss: 1.4848
Epoch [11/30], Batch [5000/6000], Loss: 1.4892
Epoch [11/30], Batch [5100/6000], Loss: 1.4974
Epoch [11/30], Batch [5200/6000], Loss: 1.4887
Epoch [11/30], Batch [5300/6000], Loss: 1.4861
Epoch [11/30], Batch [5400/6000], Loss: 1.4798
Epoch [11/30], Batch [5500/6000], Loss: 1.6019
Epoch [11/30], Batch [5600/6000], Loss: 1.4856
Epoch [11/30], Batch [5700/6000], Loss: 1.4850
Epoch [11/30], Batch [5800/6000], Loss: 1.4882
Epoch [11/30], Batch [5900/6000], Loss: 1.6218
Epoch [11/30], Loss: 1.4980
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 1.4918
Epoch [12/30], Batch [100/6000], Loss: 1.5837
Epoch [12/30], Batch [200/6000], Loss: 1.4816
Epoch [12/30], Batch [300/6000], Loss: 1.4918
Epoch [12/30], Batch [400/6000], Loss: 1.4810
Epoch [12/30], Batch [500/6000], Loss: 1.4878
Epoch [12/30], Batch [600/6000], Loss: 1.4884
Epoch [12/30], Batch [700/6000], Loss: 1.5167
Epoch [12/30], Batch [800/6000], Loss: 1.4851
Epoch [12/30], Batch [900/6000], Loss: 1.5016
Epoch [12/30], Batch [1000/6000], Loss: 1.4979
Epoch [12/30], Batch [1100/6000], Loss: 1.4809
Epoch [12/30], Batch [1200/6000], Loss: 1.4937
Epoch [12/30], Batch [1300/6000], Loss: 1.4974
Epoch [12/30], Batch [1400/6000], Loss: 1.4885
Epoch [12/30], Batch [1500/6000], Loss: 1.4822
Epoch [12/30], Batch [1600/6000], Loss: 1.4863
Epoch [12/30], Batch [1700/6000], Loss: 1.4797
Epoch [12/30], Batch [1800/6000], Loss: 1.4849
Epoch [12/30], Batch [1900/6000], Loss: 1.4886
Epoch [12/30], Batch [2000/6000], Loss: 1.5809
Epoch [12/30], Batch [2100/6000], Loss: 1.4795
Epoch [12/30], Batch [2200/6000], Loss: 1.4852
Epoch [12/30], Batch [2300/6000], Loss: 1.4877
Epoch [12/30], Batch [2400/6000], Loss: 1.4808
Epoch [12/30], Batch [2500/6000], Loss: 1.4839
Epoch [12/30], Batch [2600/6000], Loss: 1.4830
Epoch [12/30], Batch [2700/6000], Loss: 1.4830
Epoch [12/30], Batch [2800/6000], Loss: 1.4826
Epoch [12/30], Batch [2900/6000], Loss: 1.4840
Epoch [12/30], Batch [3000/6000], Loss: 1.4932
Epoch [12/30], Batch [3100/6000], Loss: 1.4812
Epoch [12/30], Batch [3200/6000], Loss: 1.4863
Epoch [12/30], Batch [3300/6000], Loss: 1.4794
Epoch [12/30], Batch [3400/6000], Loss: 1.4842
Epoch [12/30], Batch [3500/6000], Loss: 1.4821
Epoch [12/30], Batch [3600/6000], Loss: 1.4899
Epoch [12/30], Batch [3700/6000], Loss: 1.4830
Epoch [12/30], Batch [3800/6000], Loss: 1.4790
Epoch [12/30], Batch [3900/6000], Loss: 1.4791
Epoch [12/30], Batch [4000/6000], Loss: 1.5113
Epoch [12/30], Batch [4100/6000], Loss: 1.4839
Epoch [12/30], Batch [4200/6000], Loss: 1.4888
Epoch [12/30], Batch [4300/6000], Loss: 1.4847
Epoch [12/30], Batch [4400/6000], Loss: 1.4855
Epoch [12/30], Batch [4500/6000], Loss: 1.4796
Epoch [12/30], Batch [4600/6000], Loss: 1.5005
Epoch [12/30], Batch [4700/6000], Loss: 1.5391
Epoch [12/30], Batch [4800/6000], Loss: 1.4889
Epoch [12/30], Batch [4900/6000], Loss: 1.4917
Epoch [12/30], Batch [5000/6000], Loss: 1.4851
Epoch [12/30], Batch [5100/6000], Loss: 1.4948
Epoch [12/30], Batch [5200/6000], Loss: 1.4859
Epoch [12/30], Batch [5300/6000], Loss: 1.5210
Epoch [12/30], Batch [5400/6000], Loss: 1.4797
Epoch [12/30], Batch [5500/6000], Loss: 1.4886
Epoch [12/30], Batch [5600/6000], Loss: 1.5154
Epoch [12/30], Batch [5700/6000], Loss: 1.4827
Epoch [12/30], Batch [5800/6000], Loss: 1.5014
Epoch [12/30], Batch [5900/6000], Loss: 1.4888
Epoch [12/30], Loss: 1.4954
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 1.4873
Epoch [13/30], Batch [100/6000], Loss: 1.4878
Epoch [13/30], Batch [200/6000], Loss: 1.4884
Epoch [13/30], Batch [300/6000], Loss: 1.4844
Epoch [13/30], Batch [400/6000], Loss: 1.4864
Epoch [13/30], Batch [500/6000], Loss: 1.4871
Epoch [13/30], Batch [600/6000], Loss: 1.4794
Epoch [13/30], Batch [700/6000], Loss: 1.4940
Epoch [13/30], Batch [800/6000], Loss: 1.4859
Epoch [13/30], Batch [900/6000], Loss: 1.4938
Epoch [13/30], Batch [1000/6000], Loss: 1.4759
Epoch [13/30], Batch [1100/6000], Loss: 1.4827
Epoch [13/30], Batch [1200/6000], Loss: 1.4916
Epoch [13/30], Batch [1300/6000], Loss: 1.4839
Epoch [13/30], Batch [1400/6000], Loss: 1.4840
Epoch [13/30], Batch [1500/6000], Loss: 1.4857
Epoch [13/30], Batch [1600/6000], Loss: 1.4826
Epoch [13/30], Batch [1700/6000], Loss: 1.4776
Epoch [13/30], Batch [1800/6000], Loss: 1.4889
Epoch [13/30], Batch [1900/6000], Loss: 1.4808
Epoch [13/30], Batch [2000/6000], Loss: 1.4750
Epoch [13/30], Batch [2100/6000], Loss: 1.4952
Epoch [13/30], Batch [2200/6000], Loss: 1.4872
Epoch [13/30], Batch [2300/6000], Loss: 1.5814
Epoch [13/30], Batch [2400/6000], Loss: 1.4835
Epoch [13/30], Batch [2500/6000], Loss: 1.4912
Epoch [13/30], Batch [2600/6000], Loss: 1.4911
Epoch [13/30], Batch [2700/6000], Loss: 1.4829
Epoch [13/30], Batch [2800/6000], Loss: 1.5677
Epoch [13/30], Batch [2900/6000], Loss: 1.4829
Epoch [13/30], Batch [3000/6000], Loss: 1.4832
Epoch [13/30], Batch [3100/6000], Loss: 1.4836
Epoch [13/30], Batch [3200/6000], Loss: 1.4834
Epoch [13/30], Batch [3300/6000], Loss: 1.4876
Epoch [13/30], Batch [3400/6000], Loss: 1.4859
Epoch [13/30], Batch [3500/6000], Loss: 1.5837
Epoch [13/30], Batch [3600/6000], Loss: 1.5139
Epoch [13/30], Batch [3700/6000], Loss: 1.4904
Epoch [13/30], Batch [3800/6000], Loss: 1.4794
Epoch [13/30], Batch [3900/6000], Loss: 1.4855
Epoch [13/30], Batch [4000/6000], Loss: 1.4794
Epoch [13/30], Batch [4100/6000], Loss: 1.4818
Epoch [13/30], Batch [4200/6000], Loss: 1.4760
Epoch [13/30], Batch [4300/6000], Loss: 1.4828
Epoch [13/30], Batch [4400/6000], Loss: 1.4798
Epoch [13/30], Batch [4500/6000], Loss: 1.4875
Epoch [13/30], Batch [4600/6000], Loss: 1.4805
Epoch [13/30], Batch [4700/6000], Loss: 1.5029
Epoch [13/30], Batch [4800/6000], Loss: 1.4872
Epoch [13/30], Batch [4900/6000], Loss: 1.5026
Epoch [13/30], Batch [5000/6000], Loss: 1.4853
Epoch [13/30], Batch [5100/6000], Loss: 1.4932
Epoch [13/30], Batch [5200/6000], Loss: 1.4861
Epoch [13/30], Batch [5300/6000], Loss: 1.4879
Epoch [13/30], Batch [5400/6000], Loss: 1.4839
Epoch [13/30], Batch [5500/6000], Loss: 1.4903
Epoch [13/30], Batch [5600/6000], Loss: 1.4767
Epoch [13/30], Batch [5700/6000], Loss: 1.4868
Epoch [13/30], Batch [5800/6000], Loss: 1.4990
Epoch [13/30], Batch [5900/6000], Loss: 1.5015
Epoch [13/30], Loss: 1.4940
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 1.4862
Epoch [14/30], Batch [100/6000], Loss: 1.4812
Epoch [14/30], Batch [200/6000], Loss: 1.4908
Epoch [14/30], Batch [300/6000], Loss: 1.5015
Epoch [14/30], Batch [400/6000], Loss: 1.4975
Epoch [14/30], Batch [500/6000], Loss: 1.4849
Epoch [14/30], Batch [600/6000], Loss: 1.4976
Epoch [14/30], Batch [700/6000], Loss: 1.5827
Epoch [14/30], Batch [800/6000], Loss: 1.4840
Epoch [14/30], Batch [900/6000], Loss: 1.4810
Epoch [14/30], Batch [1000/6000], Loss: 1.4871
Epoch [14/30], Batch [1100/6000], Loss: 1.4863
Epoch [14/30], Batch [1200/6000], Loss: 1.4785
Epoch [14/30], Batch [1300/6000], Loss: 1.4823
Epoch [14/30], Batch [1400/6000], Loss: 1.4811
Epoch [14/30], Batch [1500/6000], Loss: 1.4826
Epoch [14/30], Batch [1600/6000], Loss: 1.4761
Epoch [14/30], Batch [1700/6000], Loss: 1.4784
Epoch [14/30], Batch [1800/6000], Loss: 1.5850
Epoch [14/30], Batch [1900/6000], Loss: 1.4786
Epoch [14/30], Batch [2000/6000], Loss: 1.4844
Epoch [14/30], Batch [2100/6000], Loss: 1.4892
Epoch [14/30], Batch [2200/6000], Loss: 1.4820
Epoch [14/30], Batch [2300/6000], Loss: 1.4837
Epoch [14/30], Batch [2400/6000], Loss: 1.4780
Epoch [14/30], Batch [2500/6000], Loss: 1.4883
Epoch [14/30], Batch [2600/6000], Loss: 1.4814
Epoch [14/30], Batch [2700/6000], Loss: 1.4945
Epoch [14/30], Batch [2800/6000], Loss: 1.4825
Epoch [14/30], Batch [2900/6000], Loss: 1.4794
Epoch [14/30], Batch [3000/6000], Loss: 1.4806
Epoch [14/30], Batch [3100/6000], Loss: 1.4897
Epoch [14/30], Batch [3200/6000], Loss: 1.4787
Epoch [14/30], Batch [3300/6000], Loss: 1.4774
Epoch [14/30], Batch [3400/6000], Loss: 1.4749
Epoch [14/30], Batch [3500/6000], Loss: 1.5012
Epoch [14/30], Batch [3600/6000], Loss: 1.4887
Epoch [14/30], Batch [3700/6000], Loss: 1.4923
Epoch [14/30], Batch [3800/6000], Loss: 1.4781
Epoch [14/30], Batch [3900/6000], Loss: 1.4822
Epoch [14/30], Batch [4000/6000], Loss: 1.4821
Epoch [14/30], Batch [4100/6000], Loss: 1.4950
Epoch [14/30], Batch [4200/6000], Loss: 1.4817
Epoch [14/30], Batch [4300/6000], Loss: 1.4909
Epoch [14/30], Batch [4400/6000], Loss: 1.4874
Epoch [14/30], Batch [4500/6000], Loss: 1.5001
Epoch [14/30], Batch [4600/6000], Loss: 1.4897
Epoch [14/30], Batch [4700/6000], Loss: 1.4840
Epoch [14/30], Batch [4800/6000], Loss: 1.5813
Epoch [14/30], Batch [4900/6000], Loss: 1.4782
Epoch [14/30], Batch [5000/6000], Loss: 1.4854
Epoch [14/30], Batch [5100/6000], Loss: 1.4834
Epoch [14/30], Batch [5200/6000], Loss: 1.4846
Epoch [14/30], Batch [5300/6000], Loss: 1.5954
Epoch [14/30], Batch [5400/6000], Loss: 1.5043
Epoch [14/30], Batch [5500/6000], Loss: 1.4781
Epoch [14/30], Batch [5600/6000], Loss: 1.4898
Epoch [14/30], Batch [5700/6000], Loss: 1.4860
Epoch [14/30], Batch [5800/6000], Loss: 1.4873
Epoch [14/30], Batch [5900/6000], Loss: 1.4797
Epoch [14/30], Loss: 1.4918
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 1.4884
Epoch [15/30], Batch [100/6000], Loss: 1.4858
Epoch [15/30], Batch [200/6000], Loss: 1.5793
Epoch [15/30], Batch [300/6000], Loss: 1.4778
Epoch [15/30], Batch [400/6000], Loss: 1.5850
Epoch [15/30], Batch [500/6000], Loss: 1.4842
Epoch [15/30], Batch [600/6000], Loss: 1.4830
Epoch [15/30], Batch [700/6000], Loss: 1.4847
Epoch [15/30], Batch [800/6000], Loss: 1.4838
Epoch [15/30], Batch [900/6000], Loss: 1.4839
Epoch [15/30], Batch [1000/6000], Loss: 1.4934
Epoch [15/30], Batch [1100/6000], Loss: 1.4774
Epoch [15/30], Batch [1200/6000], Loss: 1.4826
Epoch [15/30], Batch [1300/6000], Loss: 1.4933
Epoch [15/30], Batch [1400/6000], Loss: 1.4795
Epoch [15/30], Batch [1500/6000], Loss: 1.4855
Epoch [15/30], Batch [1600/6000], Loss: 1.4846
Epoch [15/30], Batch [1700/6000], Loss: 1.4847
Epoch [15/30], Batch [1800/6000], Loss: 1.4823
Epoch [15/30], Batch [1900/6000], Loss: 1.4836
Epoch [15/30], Batch [2000/6000], Loss: 1.4871
Epoch [15/30], Batch [2100/6000], Loss: 1.4789
Epoch [15/30], Batch [2200/6000], Loss: 1.4791
Epoch [15/30], Batch [2300/6000], Loss: 1.5875
Epoch [15/30], Batch [2400/6000], Loss: 1.4820
Epoch [15/30], Batch [2500/6000], Loss: 1.4803
Epoch [15/30], Batch [2600/6000], Loss: 1.4795
Epoch [15/30], Batch [2700/6000], Loss: 1.4824
Epoch [15/30], Batch [2800/6000], Loss: 1.4931
Epoch [15/30], Batch [2900/6000], Loss: 1.4768
Epoch [15/30], Batch [3000/6000], Loss: 1.4847
Epoch [15/30], Batch [3100/6000], Loss: 1.4810
Epoch [15/30], Batch [3200/6000], Loss: 1.5862
Epoch [15/30], Batch [3300/6000], Loss: 1.4801
Epoch [15/30], Batch [3400/6000], Loss: 1.4852
Epoch [15/30], Batch [3500/6000], Loss: 1.5909
Epoch [15/30], Batch [3600/6000], Loss: 1.4811
Epoch [15/30], Batch [3700/6000], Loss: 1.4846
Epoch [15/30], Batch [3800/6000], Loss: 1.4869
Epoch [15/30], Batch [3900/6000], Loss: 1.4812
Epoch [15/30], Batch [4000/6000], Loss: 1.4857
Epoch [15/30], Batch [4100/6000], Loss: 1.4792
Epoch [15/30], Batch [4200/6000], Loss: 1.4830
Epoch [15/30], Batch [4300/6000], Loss: 1.4759
Epoch [15/30], Batch [4400/6000], Loss: 1.5800
Epoch [15/30], Batch [4500/6000], Loss: 1.4867
Epoch [15/30], Batch [4600/6000], Loss: 1.4785
Epoch [15/30], Batch [4700/6000], Loss: 1.4887
Epoch [15/30], Batch [4800/6000], Loss: 1.4824
Epoch [15/30], Batch [4900/6000], Loss: 1.4885
Epoch [15/30], Batch [5000/6000], Loss: 1.4858
Epoch [15/30], Batch [5100/6000], Loss: 1.4848
Epoch [15/30], Batch [5200/6000], Loss: 1.5039
Epoch [15/30], Batch [5300/6000], Loss: 1.5433
Epoch [15/30], Batch [5400/6000], Loss: 1.4836
Epoch [15/30], Batch [5500/6000], Loss: 1.6348
Epoch [15/30], Batch [5600/6000], Loss: 1.4791
Epoch [15/30], Batch [5700/6000], Loss: 1.4849
Epoch [15/30], Batch [5800/6000], Loss: 1.4796
Epoch [15/30], Batch [5900/6000], Loss: 1.4924
Epoch [15/30], Loss: 1.4907
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 1.4790
Epoch [16/30], Batch [100/6000], Loss: 1.4780
Epoch [16/30], Batch [200/6000], Loss: 1.4770
Epoch [16/30], Batch [300/6000], Loss: 1.4809
Epoch [16/30], Batch [400/6000], Loss: 1.4857
Epoch [16/30], Batch [500/6000], Loss: 1.4781
Epoch [16/30], Batch [600/6000], Loss: 1.5825
Epoch [16/30], Batch [700/6000], Loss: 1.4769
Epoch [16/30], Batch [800/6000], Loss: 1.4778
Epoch [16/30], Batch [900/6000], Loss: 1.4777
Epoch [16/30], Batch [1000/6000], Loss: 1.4861
Epoch [16/30], Batch [1100/6000], Loss: 1.4784
Epoch [16/30], Batch [1200/6000], Loss: 1.4948
Epoch [16/30], Batch [1300/6000], Loss: 1.4947
Epoch [16/30], Batch [1400/6000], Loss: 1.5882
Epoch [16/30], Batch [1500/6000], Loss: 1.4731
Epoch [16/30], Batch [1600/6000], Loss: 1.4848
Epoch [16/30], Batch [1700/6000], Loss: 1.4891
Epoch [16/30], Batch [1800/6000], Loss: 1.4773
Epoch [16/30], Batch [1900/6000], Loss: 1.4824
Epoch [16/30], Batch [2000/6000], Loss: 1.4794
Epoch [16/30], Batch [2100/6000], Loss: 1.5823
Epoch [16/30], Batch [2200/6000], Loss: 1.4882
Epoch [16/30], Batch [2300/6000], Loss: 1.5814
Epoch [16/30], Batch [2400/6000], Loss: 1.4776
Epoch [16/30], Batch [2500/6000], Loss: 1.5708
Epoch [16/30], Batch [2600/6000], Loss: 1.4771
Epoch [16/30], Batch [2700/6000], Loss: 1.4834
Epoch [16/30], Batch [2800/6000], Loss: 1.5090
Epoch [16/30], Batch [2900/6000], Loss: 1.4840
Epoch [16/30], Batch [3000/6000], Loss: 1.4820
Epoch [16/30], Batch [3100/6000], Loss: 1.4801
Epoch [16/30], Batch [3200/6000], Loss: 1.4821
Epoch [16/30], Batch [3300/6000], Loss: 1.4930
Epoch [16/30], Batch [3400/6000], Loss: 1.4853
Epoch [16/30], Batch [3500/6000], Loss: 1.4905
Epoch [16/30], Batch [3600/6000], Loss: 1.4744
Epoch [16/30], Batch [3700/6000], Loss: 1.4821
Epoch [16/30], Batch [3800/6000], Loss: 1.4810
Epoch [16/30], Batch [3900/6000], Loss: 1.4803
Epoch [16/30], Batch [4000/6000], Loss: 1.4850
Epoch [16/30], Batch [4100/6000], Loss: 1.5010
Epoch [16/30], Batch [4200/6000], Loss: 1.4903
Epoch [16/30], Batch [4300/6000], Loss: 1.4772
Epoch [16/30], Batch [4400/6000], Loss: 1.4839
Epoch [16/30], Batch [4500/6000], Loss: 1.4810
Epoch [16/30], Batch [4600/6000], Loss: 1.4941
Epoch [16/30], Batch [4700/6000], Loss: 1.4860
Epoch [16/30], Batch [4800/6000], Loss: 1.4815
Epoch [16/30], Batch [4900/6000], Loss: 1.4835
Epoch [16/30], Batch [5000/6000], Loss: 1.4820
Epoch [16/30], Batch [5100/6000], Loss: 1.4753
Epoch [16/30], Batch [5200/6000], Loss: 1.4955
Epoch [16/30], Batch [5300/6000], Loss: 1.4974
Epoch [16/30], Batch [5400/6000], Loss: 1.4762
Epoch [16/30], Batch [5500/6000], Loss: 1.4931
Epoch [16/30], Batch [5600/6000], Loss: 1.4784
Epoch [16/30], Batch [5700/6000], Loss: 1.4854
Epoch [16/30], Batch [5800/6000], Loss: 1.4799
Epoch [16/30], Batch [5900/6000], Loss: 1.4846
Epoch [16/30], Loss: 1.4892
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 1.4815
Epoch [17/30], Batch [100/6000], Loss: 1.4822
Epoch [17/30], Batch [200/6000], Loss: 1.4857
Epoch [17/30], Batch [300/6000], Loss: 1.4777
Epoch [17/30], Batch [400/6000], Loss: 1.4793
Epoch [17/30], Batch [500/6000], Loss: 1.4823
Epoch [17/30], Batch [600/6000], Loss: 1.5731
Epoch [17/30], Batch [700/6000], Loss: 1.4762
Epoch [17/30], Batch [800/6000], Loss: 1.4783
Epoch [17/30], Batch [900/6000], Loss: 1.4763
Epoch [17/30], Batch [1000/6000], Loss: 1.4798
Epoch [17/30], Batch [1100/6000], Loss: 1.4835
Epoch [17/30], Batch [1200/6000], Loss: 1.4841
Epoch [17/30], Batch [1300/6000], Loss: 1.4813
Epoch [17/30], Batch [1400/6000], Loss: 1.4827
Epoch [17/30], Batch [1500/6000], Loss: 1.4780
Epoch [17/30], Batch [1600/6000], Loss: 1.4784
Epoch [17/30], Batch [1700/6000], Loss: 1.4828
Epoch [17/30], Batch [1800/6000], Loss: 1.4785
Epoch [17/30], Batch [1900/6000], Loss: 1.4767
Epoch [17/30], Batch [2000/6000], Loss: 1.4751
Epoch [17/30], Batch [2100/6000], Loss: 1.4847
Epoch [17/30], Batch [2200/6000], Loss: 1.4775
Epoch [17/30], Batch [2300/6000], Loss: 1.4850
Epoch [17/30], Batch [2400/6000], Loss: 1.4813
Epoch [17/30], Batch [2500/6000], Loss: 1.4786
Epoch [17/30], Batch [2600/6000], Loss: 1.4769
Epoch [17/30], Batch [2700/6000], Loss: 1.4789
Epoch [17/30], Batch [2800/6000], Loss: 1.4815
Epoch [17/30], Batch [2900/6000], Loss: 1.4810
Epoch [17/30], Batch [3000/6000], Loss: 1.4864
Epoch [17/30], Batch [3100/6000], Loss: 1.4815
Epoch [17/30], Batch [3200/6000], Loss: 1.4775
Epoch [17/30], Batch [3300/6000], Loss: 1.4853
Epoch [17/30], Batch [3400/6000], Loss: 1.4809
Epoch [17/30], Batch [3500/6000], Loss: 1.5720
Epoch [17/30], Batch [3600/6000], Loss: 1.4845
Epoch [17/30], Batch [3700/6000], Loss: 1.4794
Epoch [17/30], Batch [3800/6000], Loss: 1.4980
Epoch [17/30], Batch [3900/6000], Loss: 1.4808
Epoch [17/30], Batch [4000/6000], Loss: 1.4815
Epoch [17/30], Batch [4100/6000], Loss: 1.5038
Epoch [17/30], Batch [4200/6000], Loss: 1.4776
Epoch [17/30], Batch [4300/6000], Loss: 1.4839
Epoch [17/30], Batch [4400/6000], Loss: 1.4809
Epoch [17/30], Batch [4500/6000], Loss: 1.4758
Epoch [17/30], Batch [4600/6000], Loss: 1.4859
Epoch [17/30], Batch [4700/6000], Loss: 1.4759
Epoch [17/30], Batch [4800/6000], Loss: 1.4829
Epoch [17/30], Batch [4900/6000], Loss: 1.4807
Epoch [17/30], Batch [5000/6000], Loss: 1.4781
Epoch [17/30], Batch [5100/6000], Loss: 1.4893
Epoch [17/30], Batch [5200/6000], Loss: 1.4820
Epoch [17/30], Batch [5300/6000], Loss: 1.4717
Epoch [17/30], Batch [5400/6000], Loss: 1.4810
Epoch [17/30], Batch [5500/6000], Loss: 1.4857
Epoch [17/30], Batch [5600/6000], Loss: 1.4823
Epoch [17/30], Batch [5700/6000], Loss: 1.4818
Epoch [17/30], Batch [5800/6000], Loss: 1.4807
Epoch [17/30], Batch [5900/6000], Loss: 1.4805
Epoch [17/30], Loss: 1.4880
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 1.4859
Epoch [18/30], Batch [100/6000], Loss: 1.4800
Epoch [18/30], Batch [200/6000], Loss: 1.5782
Epoch [18/30], Batch [300/6000], Loss: 1.4777
Epoch [18/30], Batch [400/6000], Loss: 1.4782
Epoch [18/30], Batch [500/6000], Loss: 1.4778
Epoch [18/30], Batch [600/6000], Loss: 1.4864
Epoch [18/30], Batch [700/6000], Loss: 1.4767
Epoch [18/30], Batch [800/6000], Loss: 1.5823
Epoch [18/30], Batch [900/6000], Loss: 1.4830
Epoch [18/30], Batch [1000/6000], Loss: 1.4774
Epoch [18/30], Batch [1100/6000], Loss: 1.4862
Epoch [18/30], Batch [1200/6000], Loss: 1.4819
Epoch [18/30], Batch [1300/6000], Loss: 1.4796
Epoch [18/30], Batch [1400/6000], Loss: 1.4818
Epoch [18/30], Batch [1500/6000], Loss: 1.4817
Epoch [18/30], Batch [1600/6000], Loss: 1.4765
Epoch [18/30], Batch [1700/6000], Loss: 1.4812
Epoch [18/30], Batch [1800/6000], Loss: 1.4796
Epoch [18/30], Batch [1900/6000], Loss: 1.4763
Epoch [18/30], Batch [2000/6000], Loss: 1.4823
Epoch [18/30], Batch [2100/6000], Loss: 1.4819
Epoch [18/30], Batch [2200/6000], Loss: 1.4850
Epoch [18/30], Batch [2300/6000], Loss: 1.5322
Epoch [18/30], Batch [2400/6000], Loss: 1.4794
Epoch [18/30], Batch [2500/6000], Loss: 1.4817
Epoch [18/30], Batch [2600/6000], Loss: 1.4908
Epoch [18/30], Batch [2700/6000], Loss: 1.4781
Epoch [18/30], Batch [2800/6000], Loss: 1.4780
Epoch [18/30], Batch [2900/6000], Loss: 1.4790
Epoch [18/30], Batch [3000/6000], Loss: 1.4760
Epoch [18/30], Batch [3100/6000], Loss: 1.4816
Epoch [18/30], Batch [3200/6000], Loss: 1.4768
Epoch [18/30], Batch [3300/6000], Loss: 1.4796
Epoch [18/30], Batch [3400/6000], Loss: 1.4794
Epoch [18/30], Batch [3500/6000], Loss: 1.4862
Epoch [18/30], Batch [3600/6000], Loss: 1.4806
Epoch [18/30], Batch [3700/6000], Loss: 1.4762
Epoch [18/30], Batch [3800/6000], Loss: 1.4793
Epoch [18/30], Batch [3900/6000], Loss: 1.4827
Epoch [18/30], Batch [4000/6000], Loss: 1.4762
Epoch [18/30], Batch [4100/6000], Loss: 1.4770
Epoch [18/30], Batch [4200/6000], Loss: 1.4821
Epoch [18/30], Batch [4300/6000], Loss: 1.4822
Epoch [18/30], Batch [4400/6000], Loss: 1.4774
Epoch [18/30], Batch [4500/6000], Loss: 1.4869
Epoch [18/30], Batch [4600/6000], Loss: 1.4762
Epoch [18/30], Batch [4700/6000], Loss: 1.5145
Epoch [18/30], Batch [4800/6000], Loss: 1.4819
Epoch [18/30], Batch [4900/6000], Loss: 1.4759
Epoch [18/30], Batch [5000/6000], Loss: 1.4791
Epoch [18/30], Batch [5100/6000], Loss: 1.5836
Epoch [18/30], Batch [5200/6000], Loss: 1.4801
Epoch [18/30], Batch [5300/6000], Loss: 1.4884
Epoch [18/30], Batch [5400/6000], Loss: 1.4801
Epoch [18/30], Batch [5500/6000], Loss: 1.4813
Epoch [18/30], Batch [5600/6000], Loss: 1.4840
Epoch [18/30], Batch [5700/6000], Loss: 1.4781
Epoch [18/30], Batch [5800/6000], Loss: 1.4780
Epoch [18/30], Batch [5900/6000], Loss: 1.4747
Epoch [18/30], Loss: 1.4871
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 1.4764
Epoch [19/30], Batch [100/6000], Loss: 1.4777
Epoch [19/30], Batch [200/6000], Loss: 1.4751
Epoch [19/30], Batch [300/6000], Loss: 1.4789
Epoch [19/30], Batch [400/6000], Loss: 1.4799
Epoch [19/30], Batch [500/6000], Loss: 1.4824
Epoch [19/30], Batch [600/6000], Loss: 1.4760
Epoch [19/30], Batch [700/6000], Loss: 1.4783
Epoch [19/30], Batch [800/6000], Loss: 1.4754
Epoch [19/30], Batch [900/6000], Loss: 1.4803
Epoch [19/30], Batch [1000/6000], Loss: 1.4822
Epoch [19/30], Batch [1100/6000], Loss: 1.4821
Epoch [19/30], Batch [1200/6000], Loss: 1.4824
Epoch [19/30], Batch [1300/6000], Loss: 1.4765
Epoch [19/30], Batch [1400/6000], Loss: 1.4833
Epoch [19/30], Batch [1500/6000], Loss: 1.4849
Epoch [19/30], Batch [1600/6000], Loss: 1.4815
Epoch [19/30], Batch [1700/6000], Loss: 1.4822
Epoch [19/30], Batch [1800/6000], Loss: 1.4770
Epoch [19/30], Batch [1900/6000], Loss: 1.4771
Epoch [19/30], Batch [2000/6000], Loss: 1.4765
Epoch [19/30], Batch [2100/6000], Loss: 1.4768
Epoch [19/30], Batch [2200/6000], Loss: 1.5803
Epoch [19/30], Batch [2300/6000], Loss: 1.4775
Epoch [19/30], Batch [2400/6000], Loss: 1.4793
Epoch [19/30], Batch [2500/6000], Loss: 1.4733
Epoch [19/30], Batch [2600/6000], Loss: 1.4820
Epoch [19/30], Batch [2700/6000], Loss: 1.4738
Epoch [19/30], Batch [2800/6000], Loss: 1.4745
Epoch [19/30], Batch [2900/6000], Loss: 1.4794
Epoch [19/30], Batch [3000/6000], Loss: 1.4787
Epoch [19/30], Batch [3100/6000], Loss: 1.4783
Epoch [19/30], Batch [3200/6000], Loss: 1.4760
Epoch [19/30], Batch [3300/6000], Loss: 1.4777
Epoch [19/30], Batch [3400/6000], Loss: 1.4786
Epoch [19/30], Batch [3500/6000], Loss: 1.4840
Epoch [19/30], Batch [3600/6000], Loss: 1.4751
Epoch [19/30], Batch [3700/6000], Loss: 1.4800
Epoch [19/30], Batch [3800/6000], Loss: 1.4785
Epoch [19/30], Batch [3900/6000], Loss: 1.4755
Epoch [19/30], Batch [4000/6000], Loss: 1.4749
Epoch [19/30], Batch [4100/6000], Loss: 1.4812
Epoch [19/30], Batch [4200/6000], Loss: 1.4778
Epoch [19/30], Batch [4300/6000], Loss: 1.4752
Epoch [19/30], Batch [4400/6000], Loss: 1.4798
Epoch [19/30], Batch [4500/6000], Loss: 1.4917
Epoch [19/30], Batch [4600/6000], Loss: 1.4771
Epoch [19/30], Batch [4700/6000], Loss: 1.4740
Epoch [19/30], Batch [4800/6000], Loss: 1.4824
Epoch [19/30], Batch [4900/6000], Loss: 1.4801
Epoch [19/30], Batch [5000/6000], Loss: 1.5797
Epoch [19/30], Batch [5100/6000], Loss: 1.4754
Epoch [19/30], Batch [5200/6000], Loss: 1.5801
Epoch [19/30], Batch [5300/6000], Loss: 1.4730
Epoch [19/30], Batch [5400/6000], Loss: 1.4737
Epoch [19/30], Batch [5500/6000], Loss: 1.4906
Epoch [19/30], Batch [5600/6000], Loss: 1.4823
Epoch [19/30], Batch [5700/6000], Loss: 1.4783
Epoch [19/30], Batch [5800/6000], Loss: 1.4790
Epoch [19/30], Batch [5900/6000], Loss: 1.4815
Epoch [19/30], Loss: 1.4857
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 1.4779
Epoch [20/30], Batch [100/6000], Loss: 1.4838
Epoch [20/30], Batch [200/6000], Loss: 1.4775
Epoch [20/30], Batch [300/6000], Loss: 1.4790
Epoch [20/30], Batch [400/6000], Loss: 1.5977
Epoch [20/30], Batch [500/6000], Loss: 1.4789
Epoch [20/30], Batch [600/6000], Loss: 1.4782
Epoch [20/30], Batch [700/6000], Loss: 1.4769
Epoch [20/30], Batch [800/6000], Loss: 1.4798
Epoch [20/30], Batch [900/6000], Loss: 1.4791
Epoch [20/30], Batch [1000/6000], Loss: 1.4780
Epoch [20/30], Batch [1100/6000], Loss: 1.4764
Epoch [20/30], Batch [1200/6000], Loss: 1.4825
Epoch [20/30], Batch [1300/6000], Loss: 1.4745
Epoch [20/30], Batch [1400/6000], Loss: 1.4754
Epoch [20/30], Batch [1500/6000], Loss: 1.4850
Epoch [20/30], Batch [1600/6000], Loss: 1.4820
Epoch [20/30], Batch [1700/6000], Loss: 1.5676
Epoch [20/30], Batch [1800/6000], Loss: 1.4800
Epoch [20/30], Batch [1900/6000], Loss: 1.4750
Epoch [20/30], Batch [2000/6000], Loss: 1.4792
Epoch [20/30], Batch [2100/6000], Loss: 1.4789
Epoch [20/30], Batch [2200/6000], Loss: 1.4810
Epoch [20/30], Batch [2300/6000], Loss: 1.5764
Epoch [20/30], Batch [2400/6000], Loss: 1.4838
Epoch [20/30], Batch [2500/6000], Loss: 1.5781
Epoch [20/30], Batch [2600/6000], Loss: 1.4822
Epoch [20/30], Batch [2700/6000], Loss: 1.4751
Epoch [20/30], Batch [2800/6000], Loss: 1.4741
Epoch [20/30], Batch [2900/6000], Loss: 1.4788
Epoch [20/30], Batch [3000/6000], Loss: 1.4789
Epoch [20/30], Batch [3100/6000], Loss: 1.4815
Epoch [20/30], Batch [3200/6000], Loss: 1.4787
Epoch [20/30], Batch [3300/6000], Loss: 1.4866
Epoch [20/30], Batch [3400/6000], Loss: 1.4769
Epoch [20/30], Batch [3500/6000], Loss: 1.4724
Epoch [20/30], Batch [3600/6000], Loss: 1.4772
Epoch [20/30], Batch [3700/6000], Loss: 1.4819
Epoch [20/30], Batch [3800/6000], Loss: 1.4812
Epoch [20/30], Batch [3900/6000], Loss: 1.4832
Epoch [20/30], Batch [4000/6000], Loss: 1.5822
Epoch [20/30], Batch [4100/6000], Loss: 1.4807
Epoch [20/30], Batch [4200/6000], Loss: 1.4802
Epoch [20/30], Batch [4300/6000], Loss: 1.5301
Epoch [20/30], Batch [4400/6000], Loss: 1.4792
Epoch [20/30], Batch [4500/6000], Loss: 1.4830
Epoch [20/30], Batch [4600/6000], Loss: 1.4808
Epoch [20/30], Batch [4700/6000], Loss: 1.4760
Epoch [20/30], Batch [4800/6000], Loss: 1.4738
Epoch [20/30], Batch [4900/6000], Loss: 1.4773
Epoch [20/30], Batch [5000/6000], Loss: 1.4821
Epoch [20/30], Batch [5100/6000], Loss: 1.5781
Epoch [20/30], Batch [5200/6000], Loss: 1.4827
Epoch [20/30], Batch [5300/6000], Loss: 1.4767
Epoch [20/30], Batch [5400/6000], Loss: 1.4793
Epoch [20/30], Batch [5500/6000], Loss: 1.4816
Epoch [20/30], Batch [5600/6000], Loss: 1.4726
Epoch [20/30], Batch [5700/6000], Loss: 1.4787
Epoch [20/30], Batch [5800/6000], Loss: 1.4771
Epoch [20/30], Batch [5900/6000], Loss: 1.4788
Epoch [20/30], Loss: 1.4855
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 1.4782
Epoch [21/30], Batch [100/6000], Loss: 1.4947
Epoch [21/30], Batch [200/6000], Loss: 1.4768
Epoch [21/30], Batch [300/6000], Loss: 1.4775
Epoch [21/30], Batch [400/6000], Loss: 1.4779
Epoch [21/30], Batch [500/6000], Loss: 1.4788
Epoch [21/30], Batch [600/6000], Loss: 1.4789
Epoch [21/30], Batch [700/6000], Loss: 1.4757
Epoch [21/30], Batch [800/6000], Loss: 1.4832
Epoch [21/30], Batch [900/6000], Loss: 1.4791
Epoch [21/30], Batch [1000/6000], Loss: 1.4802
Epoch [21/30], Batch [1100/6000], Loss: 1.4781
Epoch [21/30], Batch [1200/6000], Loss: 1.6785
Epoch [21/30], Batch [1300/6000], Loss: 1.4829
Epoch [21/30], Batch [1400/6000], Loss: 1.5819
Epoch [21/30], Batch [1500/6000], Loss: 1.4833
Epoch [21/30], Batch [1600/6000], Loss: 1.4922
Epoch [21/30], Batch [1700/6000], Loss: 1.4809
Epoch [21/30], Batch [1800/6000], Loss: 1.4759
Epoch [21/30], Batch [1900/6000], Loss: 1.4789
Epoch [21/30], Batch [2000/6000], Loss: 1.4769
Epoch [21/30], Batch [2100/6000], Loss: 1.4796
Epoch [21/30], Batch [2200/6000], Loss: 1.4802
Epoch [21/30], Batch [2300/6000], Loss: 1.4761
Epoch [21/30], Batch [2400/6000], Loss: 1.4836
Epoch [21/30], Batch [2500/6000], Loss: 1.4765
Epoch [21/30], Batch [2600/6000], Loss: 1.4763
Epoch [21/30], Batch [2700/6000], Loss: 1.4746
Epoch [21/30], Batch [2800/6000], Loss: 1.4776
Epoch [21/30], Batch [2900/6000], Loss: 1.5764
Epoch [21/30], Batch [3000/6000], Loss: 1.4775
Epoch [21/30], Batch [3100/6000], Loss: 1.4776
Epoch [21/30], Batch [3200/6000], Loss: 1.4821
Epoch [21/30], Batch [3300/6000], Loss: 1.4796
Epoch [21/30], Batch [3400/6000], Loss: 1.4941
Epoch [21/30], Batch [3500/6000], Loss: 1.4792
Epoch [21/30], Batch [3600/6000], Loss: 1.4774
Epoch [21/30], Batch [3700/6000], Loss: 1.5843
Epoch [21/30], Batch [3800/6000], Loss: 1.4838
Epoch [21/30], Batch [3900/6000], Loss: 1.4774
Epoch [21/30], Batch [4000/6000], Loss: 1.4839
Epoch [21/30], Batch [4100/6000], Loss: 1.4805
Epoch [21/30], Batch [4200/6000], Loss: 1.4912
Epoch [21/30], Batch [4300/6000], Loss: 1.4779
Epoch [21/30], Batch [4400/6000], Loss: 1.4769
Epoch [21/30], Batch [4500/6000], Loss: 1.4748
Epoch [21/30], Batch [4600/6000], Loss: 1.4785
Epoch [21/30], Batch [4700/6000], Loss: 1.4784
Epoch [21/30], Batch [4800/6000], Loss: 1.4786
Epoch [21/30], Batch [4900/6000], Loss: 1.4820
Epoch [21/30], Batch [5000/6000], Loss: 1.4738
Epoch [21/30], Batch [5100/6000], Loss: 1.4900
Epoch [21/30], Batch [5200/6000], Loss: 1.4736
Epoch [21/30], Batch [5300/6000], Loss: 1.4769
Epoch [21/30], Batch [5400/6000], Loss: 1.4778
Epoch [21/30], Batch [5500/6000], Loss: 1.4750
Epoch [21/30], Batch [5600/6000], Loss: 1.4817
Epoch [21/30], Batch [5700/6000], Loss: 1.4733
Epoch [21/30], Batch [5800/6000], Loss: 1.5825
Epoch [21/30], Batch [5900/6000], Loss: 1.4751
Epoch [21/30], Loss: 1.4846
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 1.4748
Epoch [22/30], Batch [100/6000], Loss: 1.4774
Epoch [22/30], Batch [200/6000], Loss: 1.4814
Epoch [22/30], Batch [300/6000], Loss: 1.4796
Epoch [22/30], Batch [400/6000], Loss: 1.4772
Epoch [22/30], Batch [500/6000], Loss: 1.4735
Epoch [22/30], Batch [600/6000], Loss: 1.5018
Epoch [22/30], Batch [700/6000], Loss: 1.4767
Epoch [22/30], Batch [800/6000], Loss: 1.4807
Epoch [22/30], Batch [900/6000], Loss: 1.4782
Epoch [22/30], Batch [1000/6000], Loss: 1.4774
Epoch [22/30], Batch [1100/6000], Loss: 1.4789
Epoch [22/30], Batch [1200/6000], Loss: 1.4731
Epoch [22/30], Batch [1300/6000], Loss: 1.4747
Epoch [22/30], Batch [1400/6000], Loss: 1.4786
Epoch [22/30], Batch [1500/6000], Loss: 1.4711
Epoch [22/30], Batch [1600/6000], Loss: 1.4736
Epoch [22/30], Batch [1700/6000], Loss: 1.4767
Epoch [22/30], Batch [1800/6000], Loss: 1.4777
Epoch [22/30], Batch [1900/6000], Loss: 1.4779
Epoch [22/30], Batch [2000/6000], Loss: 1.4801
Epoch [22/30], Batch [2100/6000], Loss: 1.4741
Epoch [22/30], Batch [2200/6000], Loss: 1.4913
Epoch [22/30], Batch [2300/6000], Loss: 1.4769
Epoch [22/30], Batch [2400/6000], Loss: 1.4829
Epoch [22/30], Batch [2500/6000], Loss: 1.4804
Epoch [22/30], Batch [2600/6000], Loss: 1.4732
Epoch [22/30], Batch [2700/6000], Loss: 1.4744
Epoch [22/30], Batch [2800/6000], Loss: 1.4766
Epoch [22/30], Batch [2900/6000], Loss: 1.4790
Epoch [22/30], Batch [3000/6000], Loss: 1.4916
Epoch [22/30], Batch [3100/6000], Loss: 1.5032
Epoch [22/30], Batch [3200/6000], Loss: 1.4758
Epoch [22/30], Batch [3300/6000], Loss: 1.4793
Epoch [22/30], Batch [3400/6000], Loss: 1.4951
Epoch [22/30], Batch [3500/6000], Loss: 1.4797
Epoch [22/30], Batch [3600/6000], Loss: 1.4780
Epoch [22/30], Batch [3700/6000], Loss: 1.4756
Epoch [22/30], Batch [3800/6000], Loss: 1.4775
Epoch [22/30], Batch [3900/6000], Loss: 1.4895
Epoch [22/30], Batch [4000/6000], Loss: 1.5838
Epoch [22/30], Batch [4100/6000], Loss: 1.4758
Epoch [22/30], Batch [4200/6000], Loss: 1.4776
Epoch [22/30], Batch [4300/6000], Loss: 1.4802
Epoch [22/30], Batch [4400/6000], Loss: 1.4777
Epoch [22/30], Batch [4500/6000], Loss: 1.4730
Epoch [22/30], Batch [4600/6000], Loss: 1.4770
Epoch [22/30], Batch [4700/6000], Loss: 1.4802
Epoch [22/30], Batch [4800/6000], Loss: 1.4758
Epoch [22/30], Batch [4900/6000], Loss: 1.5743
Epoch [22/30], Batch [5000/6000], Loss: 1.4768
Epoch [22/30], Batch [5100/6000], Loss: 1.4754
Epoch [22/30], Batch [5200/6000], Loss: 1.5785
Epoch [22/30], Batch [5300/6000], Loss: 1.4738
Epoch [22/30], Batch [5400/6000], Loss: 1.4797
Epoch [22/30], Batch [5500/6000], Loss: 1.4787
Epoch [22/30], Batch [5600/6000], Loss: 1.4792
Epoch [22/30], Batch [5700/6000], Loss: 1.4768
Epoch [22/30], Batch [5800/6000], Loss: 1.5843
Epoch [22/30], Batch [5900/6000], Loss: 1.4724
Epoch [22/30], Loss: 1.4839
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 1.4834
Epoch [23/30], Batch [100/6000], Loss: 1.4774
Epoch [23/30], Batch [200/6000], Loss: 1.4753
Epoch [23/30], Batch [300/6000], Loss: 1.4790
Epoch [23/30], Batch [400/6000], Loss: 1.4809
Epoch [23/30], Batch [500/6000], Loss: 1.4786
Epoch [23/30], Batch [600/6000], Loss: 1.4779
Epoch [23/30], Batch [700/6000], Loss: 1.4753
Epoch [23/30], Batch [800/6000], Loss: 1.4818
Epoch [23/30], Batch [900/6000], Loss: 1.5772
Epoch [23/30], Batch [1000/6000], Loss: 1.4731
Epoch [23/30], Batch [1100/6000], Loss: 1.5775
Epoch [23/30], Batch [1200/6000], Loss: 1.4789
Epoch [23/30], Batch [1300/6000], Loss: 1.4751
Epoch [23/30], Batch [1400/6000], Loss: 1.4814
Epoch [23/30], Batch [1500/6000], Loss: 1.4937
Epoch [23/30], Batch [1600/6000], Loss: 1.4743
Epoch [23/30], Batch [1700/6000], Loss: 1.4735
Epoch [23/30], Batch [1800/6000], Loss: 1.4708
Epoch [23/30], Batch [1900/6000], Loss: 1.4787
Epoch [23/30], Batch [2000/6000], Loss: 1.4771
Epoch [23/30], Batch [2100/6000], Loss: 1.4740
Epoch [23/30], Batch [2200/6000], Loss: 1.4811
Epoch [23/30], Batch [2300/6000], Loss: 1.4847
Epoch [23/30], Batch [2400/6000], Loss: 1.4776
Epoch [23/30], Batch [2500/6000], Loss: 1.4789
Epoch [23/30], Batch [2600/6000], Loss: 1.5714
Epoch [23/30], Batch [2700/6000], Loss: 1.4743
Epoch [23/30], Batch [2800/6000], Loss: 1.4796
Epoch [23/30], Batch [2900/6000], Loss: 1.4757
Epoch [23/30], Batch [3000/6000], Loss: 1.4777
Epoch [23/30], Batch [3100/6000], Loss: 1.4982
Epoch [23/30], Batch [3200/6000], Loss: 1.4787
Epoch [23/30], Batch [3300/6000], Loss: 1.4761
Epoch [23/30], Batch [3400/6000], Loss: 1.4828
Epoch [23/30], Batch [3500/6000], Loss: 1.4751
Epoch [23/30], Batch [3600/6000], Loss: 1.4758
Epoch [23/30], Batch [3700/6000], Loss: 1.4816
Epoch [23/30], Batch [3800/6000], Loss: 1.4779
Epoch [23/30], Batch [3900/6000], Loss: 1.4769
Epoch [23/30], Batch [4000/6000], Loss: 1.4734
Epoch [23/30], Batch [4100/6000], Loss: 1.4833
Epoch [23/30], Batch [4200/6000], Loss: 1.4823
Epoch [23/30], Batch [4300/6000], Loss: 1.4902
Epoch [23/30], Batch [4400/6000], Loss: 1.4780
Epoch [23/30], Batch [4500/6000], Loss: 1.4762
Epoch [23/30], Batch [4600/6000], Loss: 1.4802
Epoch [23/30], Batch [4700/6000], Loss: 1.4825
Epoch [23/30], Batch [4800/6000], Loss: 1.4812
Epoch [23/30], Batch [4900/6000], Loss: 1.4755
Epoch [23/30], Batch [5000/6000], Loss: 1.4757
Epoch [23/30], Batch [5100/6000], Loss: 1.4761
Epoch [23/30], Batch [5200/6000], Loss: 1.4795
Epoch [23/30], Batch [5300/6000], Loss: 1.4769
Epoch [23/30], Batch [5400/6000], Loss: 1.5956
Epoch [23/30], Batch [5500/6000], Loss: 1.4733
Epoch [23/30], Batch [5600/6000], Loss: 1.4736
Epoch [23/30], Batch [5700/6000], Loss: 1.5060
Epoch [23/30], Batch [5800/6000], Loss: 1.4815
Epoch [23/30], Batch [5900/6000], Loss: 1.4779
Epoch [23/30], Loss: 1.4830
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 1.4776
Epoch [24/30], Batch [100/6000], Loss: 1.4714
Epoch [24/30], Batch [200/6000], Loss: 1.4753
Epoch [24/30], Batch [300/6000], Loss: 1.4780
Epoch [24/30], Batch [400/6000], Loss: 1.4783
Epoch [24/30], Batch [500/6000], Loss: 1.4847
Epoch [24/30], Batch [600/6000], Loss: 1.4747
Epoch [24/30], Batch [700/6000], Loss: 1.4758
Epoch [24/30], Batch [800/6000], Loss: 1.4780
Epoch [24/30], Batch [900/6000], Loss: 1.4797
Epoch [24/30], Batch [1000/6000], Loss: 1.5155
Epoch [24/30], Batch [1100/6000], Loss: 1.4760
Epoch [24/30], Batch [1200/6000], Loss: 1.4807
Epoch [24/30], Batch [1300/6000], Loss: 1.4758
Epoch [24/30], Batch [1400/6000], Loss: 1.4810
Epoch [24/30], Batch [1500/6000], Loss: 1.5732
Epoch [24/30], Batch [1600/6000], Loss: 1.4763
Epoch [24/30], Batch [1700/6000], Loss: 1.4738
Epoch [24/30], Batch [1800/6000], Loss: 1.4763
Epoch [24/30], Batch [1900/6000], Loss: 1.4737
Epoch [24/30], Batch [2000/6000], Loss: 1.4835
Epoch [24/30], Batch [2100/6000], Loss: 1.4789
Epoch [24/30], Batch [2200/6000], Loss: 1.4785
Epoch [24/30], Batch [2300/6000], Loss: 1.4754
Epoch [24/30], Batch [2400/6000], Loss: 1.4772
Epoch [24/30], Batch [2500/6000], Loss: 1.4739
Epoch [24/30], Batch [2600/6000], Loss: 1.4817
Epoch [24/30], Batch [2700/6000], Loss: 1.4800
Epoch [24/30], Batch [2800/6000], Loss: 1.4735
Epoch [24/30], Batch [2900/6000], Loss: 1.4766
Epoch [24/30], Batch [3000/6000], Loss: 1.4745
Epoch [24/30], Batch [3100/6000], Loss: 1.4740
Epoch [24/30], Batch [3200/6000], Loss: 1.4744
Epoch [24/30], Batch [3300/6000], Loss: 1.4714
Epoch [24/30], Batch [3400/6000], Loss: 1.4760
Epoch [24/30], Batch [3500/6000], Loss: 1.4739
Epoch [24/30], Batch [3600/6000], Loss: 1.4774
Epoch [24/30], Batch [3700/6000], Loss: 1.4745
Epoch [24/30], Batch [3800/6000], Loss: 1.4735
Epoch [24/30], Batch [3900/6000], Loss: 1.4753
Epoch [24/30], Batch [4000/6000], Loss: 1.5796
Epoch [24/30], Batch [4100/6000], Loss: 1.4748
Epoch [24/30], Batch [4200/6000], Loss: 1.4730
Epoch [24/30], Batch [4300/6000], Loss: 1.5781
Epoch [24/30], Batch [4400/6000], Loss: 1.4816
Epoch [24/30], Batch [4500/6000], Loss: 1.4777
Epoch [24/30], Batch [4600/6000], Loss: 1.5789
Epoch [24/30], Batch [4700/6000], Loss: 1.4738
Epoch [24/30], Batch [4800/6000], Loss: 1.4757
Epoch [24/30], Batch [4900/6000], Loss: 1.4787
Epoch [24/30], Batch [5000/6000], Loss: 1.4746
Epoch [24/30], Batch [5100/6000], Loss: 1.4781
Epoch [24/30], Batch [5200/6000], Loss: 1.4757
Epoch [24/30], Batch [5300/6000], Loss: 1.4740
Epoch [24/30], Batch [5400/6000], Loss: 1.4712
Epoch [24/30], Batch [5500/6000], Loss: 1.4870
Epoch [24/30], Batch [5600/6000], Loss: 1.4746
Epoch [24/30], Batch [5700/6000], Loss: 1.4792
Epoch [24/30], Batch [5800/6000], Loss: 1.4801
Epoch [24/30], Batch [5900/6000], Loss: 1.4730
Epoch [24/30], Loss: 1.4821
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 1.5782
Epoch [25/30], Batch [100/6000], Loss: 1.4754
Epoch [25/30], Batch [200/6000], Loss: 1.4744
Epoch [25/30], Batch [300/6000], Loss: 1.4761
Epoch [25/30], Batch [400/6000], Loss: 1.4786
Epoch [25/30], Batch [500/6000], Loss: 1.4788
Epoch [25/30], Batch [600/6000], Loss: 1.4734
Epoch [25/30], Batch [700/6000], Loss: 1.4837
Epoch [25/30], Batch [800/6000], Loss: 1.4743
Epoch [25/30], Batch [900/6000], Loss: 1.4755
Epoch [25/30], Batch [1000/6000], Loss: 1.4760
Epoch [25/30], Batch [1100/6000], Loss: 1.4735
Epoch [25/30], Batch [1200/6000], Loss: 1.4731
Epoch [25/30], Batch [1300/6000], Loss: 1.4792
Epoch [25/30], Batch [1400/6000], Loss: 1.4745
Epoch [25/30], Batch [1500/6000], Loss: 1.4769
Epoch [25/30], Batch [1600/6000], Loss: 1.4763
Epoch [25/30], Batch [1700/6000], Loss: 1.4894
Epoch [25/30], Batch [1800/6000], Loss: 1.4762
Epoch [25/30], Batch [1900/6000], Loss: 1.4757
Epoch [25/30], Batch [2000/6000], Loss: 1.4798
Epoch [25/30], Batch [2100/6000], Loss: 1.4759
Epoch [25/30], Batch [2200/6000], Loss: 1.4829
Epoch [25/30], Batch [2300/6000], Loss: 1.4766
Epoch [25/30], Batch [2400/6000], Loss: 1.4780
Epoch [25/30], Batch [2500/6000], Loss: 1.4770
Epoch [25/30], Batch [2600/6000], Loss: 1.4743
Epoch [25/30], Batch [2700/6000], Loss: 1.4784
Epoch [25/30], Batch [2800/6000], Loss: 1.4757
Epoch [25/30], Batch [2900/6000], Loss: 1.4739
Epoch [25/30], Batch [3000/6000], Loss: 1.4738
Epoch [25/30], Batch [3100/6000], Loss: 1.4784
Epoch [25/30], Batch [3200/6000], Loss: 1.4721
Epoch [25/30], Batch [3300/6000], Loss: 1.4736
Epoch [25/30], Batch [3400/6000], Loss: 1.4787
Epoch [25/30], Batch [3500/6000], Loss: 1.4930
Epoch [25/30], Batch [3600/6000], Loss: 1.4883
Epoch [25/30], Batch [3700/6000], Loss: 1.4731
Epoch [25/30], Batch [3800/6000], Loss: 1.4743
Epoch [25/30], Batch [3900/6000], Loss: 1.4773
Epoch [25/30], Batch [4000/6000], Loss: 1.4723
Epoch [25/30], Batch [4100/6000], Loss: 1.4731
Epoch [25/30], Batch [4200/6000], Loss: 1.4794
Epoch [25/30], Batch [4300/6000], Loss: 1.4731
Epoch [25/30], Batch [4400/6000], Loss: 1.4750
Epoch [25/30], Batch [4500/6000], Loss: 1.4770
Epoch [25/30], Batch [4600/6000], Loss: 1.4788
Epoch [25/30], Batch [4700/6000], Loss: 1.4783
Epoch [25/30], Batch [4800/6000], Loss: 1.4770
Epoch [25/30], Batch [4900/6000], Loss: 1.4761
Epoch [25/30], Batch [5000/6000], Loss: 1.4748
Epoch [25/30], Batch [5100/6000], Loss: 1.4724
Epoch [25/30], Batch [5200/6000], Loss: 1.4767
Epoch [25/30], Batch [5300/6000], Loss: 1.4724
Epoch [25/30], Batch [5400/6000], Loss: 1.4766
Epoch [25/30], Batch [5500/6000], Loss: 1.4731
Epoch [25/30], Batch [5600/6000], Loss: 1.4719
Epoch [25/30], Batch [5700/6000], Loss: 1.4745
Epoch [25/30], Batch [5800/6000], Loss: 1.4756
Epoch [25/30], Batch [5900/6000], Loss: 1.4745
Epoch [25/30], Loss: 1.4818
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 1.4751
Epoch [26/30], Batch [100/6000], Loss: 1.4737
Epoch [26/30], Batch [200/6000], Loss: 1.4756
Epoch [26/30], Batch [300/6000], Loss: 1.4746
Epoch [26/30], Batch [400/6000], Loss: 1.4753
Epoch [26/30], Batch [500/6000], Loss: 1.4768
Epoch [26/30], Batch [600/6000], Loss: 1.4753
Epoch [26/30], Batch [700/6000], Loss: 1.4809
Epoch [26/30], Batch [800/6000], Loss: 1.4722
Epoch [26/30], Batch [900/6000], Loss: 1.4759
Epoch [26/30], Batch [1000/6000], Loss: 1.4717
Epoch [26/30], Batch [1100/6000], Loss: 1.4741
Epoch [26/30], Batch [1200/6000], Loss: 1.4804
Epoch [26/30], Batch [1300/6000], Loss: 1.5826
Epoch [26/30], Batch [1400/6000], Loss: 1.4774
Epoch [26/30], Batch [1500/6000], Loss: 1.4787
Epoch [26/30], Batch [1600/6000], Loss: 1.4745
Epoch [26/30], Batch [1700/6000], Loss: 1.4741
Epoch [26/30], Batch [1800/6000], Loss: 1.4726
Epoch [26/30], Batch [1900/6000], Loss: 1.4716
Epoch [26/30], Batch [2000/6000], Loss: 1.4781
Epoch [26/30], Batch [2100/6000], Loss: 1.4802
Epoch [26/30], Batch [2200/6000], Loss: 1.4781
Epoch [26/30], Batch [2300/6000], Loss: 1.4745
Epoch [26/30], Batch [2400/6000], Loss: 1.4769
Epoch [26/30], Batch [2500/6000], Loss: 1.4757
Epoch [26/30], Batch [2600/6000], Loss: 1.4771
Epoch [26/30], Batch [2700/6000], Loss: 1.4743
Epoch [26/30], Batch [2800/6000], Loss: 1.4764
Epoch [26/30], Batch [2900/6000], Loss: 1.4767
Epoch [26/30], Batch [3000/6000], Loss: 1.4811
Epoch [26/30], Batch [3100/6000], Loss: 1.4755
Epoch [26/30], Batch [3200/6000], Loss: 1.4724
Epoch [26/30], Batch [3300/6000], Loss: 1.4783
Epoch [26/30], Batch [3400/6000], Loss: 1.4744
Epoch [26/30], Batch [3500/6000], Loss: 1.4762
Epoch [26/30], Batch [3600/6000], Loss: 1.4751
Epoch [26/30], Batch [3700/6000], Loss: 1.4780
Epoch [26/30], Batch [3800/6000], Loss: 1.4764
Epoch [26/30], Batch [3900/6000], Loss: 1.4810
Epoch [26/30], Batch [4000/6000], Loss: 1.4779
Epoch [26/30], Batch [4100/6000], Loss: 1.4823
Epoch [26/30], Batch [4200/6000], Loss: 1.4829
Epoch [26/30], Batch [4300/6000], Loss: 1.4746
Epoch [26/30], Batch [4400/6000], Loss: 1.4784
Epoch [26/30], Batch [4500/6000], Loss: 1.4788
Epoch [26/30], Batch [4600/6000], Loss: 1.4749
Epoch [26/30], Batch [4700/6000], Loss: 1.4738
Epoch [26/30], Batch [4800/6000], Loss: 1.4773
Epoch [26/30], Batch [4900/6000], Loss: 1.4725
Epoch [26/30], Batch [5000/6000], Loss: 1.4710
Epoch [26/30], Batch [5100/6000], Loss: 1.4779
Epoch [26/30], Batch [5200/6000], Loss: 1.4756
Epoch [26/30], Batch [5300/6000], Loss: 1.4734
Epoch [26/30], Batch [5400/6000], Loss: 1.4750
Epoch [26/30], Batch [5500/6000], Loss: 1.4773
Epoch [26/30], Batch [5600/6000], Loss: 1.4785
Epoch [26/30], Batch [5700/6000], Loss: 1.4823
Epoch [26/30], Batch [5800/6000], Loss: 1.5144
Epoch [26/30], Batch [5900/6000], Loss: 1.4743
Epoch [26/30], Loss: 1.4809
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 1.4745
Epoch [27/30], Batch [100/6000], Loss: 1.4781
Epoch [27/30], Batch [200/6000], Loss: 1.5741
Epoch [27/30], Batch [300/6000], Loss: 1.4728
Epoch [27/30], Batch [400/6000], Loss: 1.4761
Epoch [27/30], Batch [500/6000], Loss: 1.4696
Epoch [27/30], Batch [600/6000], Loss: 1.4745
Epoch [27/30], Batch [700/6000], Loss: 1.4800
Epoch [27/30], Batch [800/6000], Loss: 1.4787
Epoch [27/30], Batch [900/6000], Loss: 1.4768
Epoch [27/30], Batch [1000/6000], Loss: 1.4730
Epoch [27/30], Batch [1100/6000], Loss: 1.4792
Epoch [27/30], Batch [1200/6000], Loss: 1.4683
Epoch [27/30], Batch [1300/6000], Loss: 1.4744
Epoch [27/30], Batch [1400/6000], Loss: 1.4770
Epoch [27/30], Batch [1500/6000], Loss: 1.4744
Epoch [27/30], Batch [1600/6000], Loss: 1.4788
Epoch [27/30], Batch [1700/6000], Loss: 1.4747
Epoch [27/30], Batch [1800/6000], Loss: 1.4745
Epoch [27/30], Batch [1900/6000], Loss: 1.4768
Epoch [27/30], Batch [2000/6000], Loss: 1.4754
Epoch [27/30], Batch [2100/6000], Loss: 1.4725
Epoch [27/30], Batch [2200/6000], Loss: 1.4731
Epoch [27/30], Batch [2300/6000], Loss: 1.4741
Epoch [27/30], Batch [2400/6000], Loss: 1.4729
Epoch [27/30], Batch [2500/6000], Loss: 1.4768
Epoch [27/30], Batch [2600/6000], Loss: 1.4742
Epoch [27/30], Batch [2700/6000], Loss: 1.4778
Epoch [27/30], Batch [2800/6000], Loss: 1.4772
Epoch [27/30], Batch [2900/6000], Loss: 1.4729
Epoch [27/30], Batch [3000/6000], Loss: 1.4708
Epoch [27/30], Batch [3100/6000], Loss: 1.4776
Epoch [27/30], Batch [3200/6000], Loss: 1.4752
Epoch [27/30], Batch [3300/6000], Loss: 1.4786
Epoch [27/30], Batch [3400/6000], Loss: 1.4757
Epoch [27/30], Batch [3500/6000], Loss: 1.4759
Epoch [27/30], Batch [3600/6000], Loss: 1.4793
Epoch [27/30], Batch [3700/6000], Loss: 1.4741
Epoch [27/30], Batch [3800/6000], Loss: 1.4757
Epoch [27/30], Batch [3900/6000], Loss: 1.4758
Epoch [27/30], Batch [4000/6000], Loss: 1.4736
Epoch [27/30], Batch [4100/6000], Loss: 1.4736
Epoch [27/30], Batch [4200/6000], Loss: 1.4765
Epoch [27/30], Batch [4300/6000], Loss: 1.4742
Epoch [27/30], Batch [4400/6000], Loss: 1.4751
Epoch [27/30], Batch [4500/6000], Loss: 1.4758
Epoch [27/30], Batch [4600/6000], Loss: 1.4746
Epoch [27/30], Batch [4700/6000], Loss: 1.4746
Epoch [27/30], Batch [4800/6000], Loss: 1.4757
Epoch [27/30], Batch [4900/6000], Loss: 1.4756
Epoch [27/30], Batch [5000/6000], Loss: 1.4741
Epoch [27/30], Batch [5100/6000], Loss: 1.4737
Epoch [27/30], Batch [5200/6000], Loss: 1.4750
Epoch [27/30], Batch [5300/6000], Loss: 1.4734
Epoch [27/30], Batch [5400/6000], Loss: 1.4766
Epoch [27/30], Batch [5500/6000], Loss: 1.4735
Epoch [27/30], Batch [5600/6000], Loss: 1.4792
Epoch [27/30], Batch [5700/6000], Loss: 1.4774
Epoch [27/30], Batch [5800/6000], Loss: 1.4746
Epoch [27/30], Batch [5900/6000], Loss: 1.4720
Epoch [27/30], Loss: 1.4803
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 1.4825
Epoch [28/30], Batch [100/6000], Loss: 1.4702
Epoch [28/30], Batch [200/6000], Loss: 1.4782
Epoch [28/30], Batch [300/6000], Loss: 1.4728
Epoch [28/30], Batch [400/6000], Loss: 1.4732
Epoch [28/30], Batch [500/6000], Loss: 1.4751
Epoch [28/30], Batch [600/6000], Loss: 1.4738
Epoch [28/30], Batch [700/6000], Loss: 1.4713
Epoch [28/30], Batch [800/6000], Loss: 1.4775
Epoch [28/30], Batch [900/6000], Loss: 1.4757
Epoch [28/30], Batch [1000/6000], Loss: 1.4693
Epoch [28/30], Batch [1100/6000], Loss: 1.4754
Epoch [28/30], Batch [1200/6000], Loss: 1.4771
Epoch [28/30], Batch [1300/6000], Loss: 1.4750
Epoch [28/30], Batch [1400/6000], Loss: 1.4761
Epoch [28/30], Batch [1500/6000], Loss: 1.4758
Epoch [28/30], Batch [1600/6000], Loss: 1.4755
Epoch [28/30], Batch [1700/6000], Loss: 1.4717
Epoch [28/30], Batch [1800/6000], Loss: 1.4737
Epoch [28/30], Batch [1900/6000], Loss: 1.4770
Epoch [28/30], Batch [2000/6000], Loss: 1.4808
Epoch [28/30], Batch [2100/6000], Loss: 1.4705
Epoch [28/30], Batch [2200/6000], Loss: 1.4750
Epoch [28/30], Batch [2300/6000], Loss: 1.4713
Epoch [28/30], Batch [2400/6000], Loss: 1.4768
Epoch [28/30], Batch [2500/6000], Loss: 1.4744
Epoch [28/30], Batch [2600/6000], Loss: 1.4871
Epoch [28/30], Batch [2700/6000], Loss: 1.4698
Epoch [28/30], Batch [2800/6000], Loss: 1.4766
Epoch [28/30], Batch [2900/6000], Loss: 1.4788
Epoch [28/30], Batch [3000/6000], Loss: 1.4790
Epoch [28/30], Batch [3100/6000], Loss: 1.4720
Epoch [28/30], Batch [3200/6000], Loss: 1.4769
Epoch [28/30], Batch [3300/6000], Loss: 1.4723
Epoch [28/30], Batch [3400/6000], Loss: 1.5780
Epoch [28/30], Batch [3500/6000], Loss: 1.4700
Epoch [28/30], Batch [3600/6000], Loss: 1.4718
Epoch [28/30], Batch [3700/6000], Loss: 1.4748
Epoch [28/30], Batch [3800/6000], Loss: 1.4778
Epoch [28/30], Batch [3900/6000], Loss: 1.4707
Epoch [28/30], Batch [4000/6000], Loss: 1.4761
Epoch [28/30], Batch [4100/6000], Loss: 1.4742
Epoch [28/30], Batch [4200/6000], Loss: 1.4713
Epoch [28/30], Batch [4300/6000], Loss: 1.4728
Epoch [28/30], Batch [4400/6000], Loss: 1.4724
Epoch [28/30], Batch [4500/6000], Loss: 1.4708
Epoch [28/30], Batch [4600/6000], Loss: 1.4763
Epoch [28/30], Batch [4700/6000], Loss: 1.4750
Epoch [28/30], Batch [4800/6000], Loss: 1.4751
Epoch [28/30], Batch [4900/6000], Loss: 1.4725
Epoch [28/30], Batch [5000/6000], Loss: 1.4777
Epoch [28/30], Batch [5100/6000], Loss: 1.4736
Epoch [28/30], Batch [5200/6000], Loss: 1.4777
Epoch [28/30], Batch [5300/6000], Loss: 1.4729
Epoch [28/30], Batch [5400/6000], Loss: 1.4765
Epoch [28/30], Batch [5500/6000], Loss: 1.4742
Epoch [28/30], Batch [5600/6000], Loss: 1.4781
Epoch [28/30], Batch [5700/6000], Loss: 1.4753
Epoch [28/30], Batch [5800/6000], Loss: 1.4777
Epoch [28/30], Batch [5900/6000], Loss: 1.4789
Epoch [28/30], Loss: 1.4799
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 1.4728
Epoch [29/30], Batch [100/6000], Loss: 1.4719
Epoch [29/30], Batch [200/6000], Loss: 1.4925
Epoch [29/30], Batch [300/6000], Loss: 1.4748
Epoch [29/30], Batch [400/6000], Loss: 1.4735
Epoch [29/30], Batch [500/6000], Loss: 1.4743
Epoch [29/30], Batch [600/6000], Loss: 1.4776
Epoch [29/30], Batch [700/6000], Loss: 1.4763
Epoch [29/30], Batch [800/6000], Loss: 1.4775
Epoch [29/30], Batch [900/6000], Loss: 1.4744
Epoch [29/30], Batch [1000/6000], Loss: 1.4739
Epoch [29/30], Batch [1100/6000], Loss: 1.4738
Epoch [29/30], Batch [1200/6000], Loss: 1.4688
Epoch [29/30], Batch [1300/6000], Loss: 1.4776
Epoch [29/30], Batch [1400/6000], Loss: 1.4782
Epoch [29/30], Batch [1500/6000], Loss: 1.4738
Epoch [29/30], Batch [1600/6000], Loss: 1.4739
Epoch [29/30], Batch [1700/6000], Loss: 1.4742
Epoch [29/30], Batch [1800/6000], Loss: 1.4737
Epoch [29/30], Batch [1900/6000], Loss: 1.4819
Epoch [29/30], Batch [2000/6000], Loss: 1.4684
Epoch [29/30], Batch [2100/6000], Loss: 1.4718
Epoch [29/30], Batch [2200/6000], Loss: 1.4701
Epoch [29/30], Batch [2300/6000], Loss: 1.4756
Epoch [29/30], Batch [2400/6000], Loss: 1.4754
Epoch [29/30], Batch [2500/6000], Loss: 1.4816
Epoch [29/30], Batch [2600/6000], Loss: 1.4742
Epoch [29/30], Batch [2700/6000], Loss: 1.4752
Epoch [29/30], Batch [2800/6000], Loss: 1.4783
Epoch [29/30], Batch [2900/6000], Loss: 1.4748
Epoch [29/30], Batch [3000/6000], Loss: 1.4709
Epoch [29/30], Batch [3100/6000], Loss: 1.4768
Epoch [29/30], Batch [3200/6000], Loss: 1.4746
Epoch [29/30], Batch [3300/6000], Loss: 1.4750
Epoch [29/30], Batch [3400/6000], Loss: 1.4736
Epoch [29/30], Batch [3500/6000], Loss: 1.4737
Epoch [29/30], Batch [3600/6000], Loss: 1.4783
Epoch [29/30], Batch [3700/6000], Loss: 1.4795
Epoch [29/30], Batch [3800/6000], Loss: 1.4764
Epoch [29/30], Batch [3900/6000], Loss: 1.4751
Epoch [29/30], Batch [4000/6000], Loss: 1.4726
Epoch [29/30], Batch [4100/6000], Loss: 1.4772
Epoch [29/30], Batch [4200/6000], Loss: 1.4718
Epoch [29/30], Batch [4300/6000], Loss: 1.4749
Epoch [29/30], Batch [4400/6000], Loss: 1.4722
Epoch [29/30], Batch [4500/6000], Loss: 1.5734
Epoch [29/30], Batch [4600/6000], Loss: 1.4721
Epoch [29/30], Batch [4700/6000], Loss: 1.4705
Epoch [29/30], Batch [4800/6000], Loss: 1.4739
Epoch [29/30], Batch [4900/6000], Loss: 1.4751
Epoch [29/30], Batch [5000/6000], Loss: 1.4759
Epoch [29/30], Batch [5100/6000], Loss: 1.4716
Epoch [29/30], Batch [5200/6000], Loss: 1.4762
Epoch [29/30], Batch [5300/6000], Loss: 1.4771
Epoch [29/30], Batch [5400/6000], Loss: 1.4762
Epoch [29/30], Batch [5500/6000], Loss: 1.4783
Epoch [29/30], Batch [5600/6000], Loss: 1.4772
Epoch [29/30], Batch [5700/6000], Loss: 1.4765
Epoch [29/30], Batch [5800/6000], Loss: 1.4898
Epoch [29/30], Batch [5900/6000], Loss: 1.4905
Epoch [29/30], Loss: 1.4796
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 1.4727
Epoch [30/30], Batch [100/6000], Loss: 1.4743
Epoch [30/30], Batch [200/6000], Loss: 1.4761
Epoch [30/30], Batch [300/6000], Loss: 1.4769
Epoch [30/30], Batch [400/6000], Loss: 1.4753
Epoch [30/30], Batch [500/6000], Loss: 1.4784
Epoch [30/30], Batch [600/6000], Loss: 1.4764
Epoch [30/30], Batch [700/6000], Loss: 1.4745
Epoch [30/30], Batch [800/6000], Loss: 1.4767
Epoch [30/30], Batch [900/6000], Loss: 1.4750
Epoch [30/30], Batch [1000/6000], Loss: 1.4702
Epoch [30/30], Batch [1100/6000], Loss: 1.4713
Epoch [30/30], Batch [1200/6000], Loss: 1.4730
Epoch [30/30], Batch [1300/6000], Loss: 1.4723
Epoch [30/30], Batch [1400/6000], Loss: 1.4758
Epoch [30/30], Batch [1500/6000], Loss: 1.4772
Epoch [30/30], Batch [1600/6000], Loss: 1.4791
Epoch [30/30], Batch [1700/6000], Loss: 1.4788
Epoch [30/30], Batch [1800/6000], Loss: 1.4791
Epoch [30/30], Batch [1900/6000], Loss: 1.4747
Epoch [30/30], Batch [2000/6000], Loss: 1.4734
Epoch [30/30], Batch [2100/6000], Loss: 1.4731
Epoch [30/30], Batch [2200/6000], Loss: 1.4735
Epoch [30/30], Batch [2300/6000], Loss: 1.4729
Epoch [30/30], Batch [2400/6000], Loss: 1.4722
Epoch [30/30], Batch [2500/6000], Loss: 1.4756
Epoch [30/30], Batch [2600/6000], Loss: 1.4755
Epoch [30/30], Batch [2700/6000], Loss: 1.4895
Epoch [30/30], Batch [2800/6000], Loss: 1.5745
Epoch [30/30], Batch [2900/6000], Loss: 1.5761
Epoch [30/30], Batch [3000/6000], Loss: 1.4714
Epoch [30/30], Batch [3100/6000], Loss: 1.4925
Epoch [30/30], Batch [3200/6000], Loss: 1.4727
Epoch [30/30], Batch [3300/6000], Loss: 1.4766
Epoch [30/30], Batch [3400/6000], Loss: 1.4782
Epoch [30/30], Batch [3500/6000], Loss: 1.4781
Epoch [30/30], Batch [3600/6000], Loss: 1.4747
Epoch [30/30], Batch [3700/6000], Loss: 1.4784
Epoch [30/30], Batch [3800/6000], Loss: 1.4772
Epoch [30/30], Batch [3900/6000], Loss: 1.4722
Epoch [30/30], Batch [4000/6000], Loss: 1.4749
Epoch [30/30], Batch [4100/6000], Loss: 1.4761
Epoch [30/30], Batch [4200/6000], Loss: 1.4714
Epoch [30/30], Batch [4300/6000], Loss: 1.4747
Epoch [30/30], Batch [4400/6000], Loss: 1.4824
Epoch [30/30], Batch [4500/6000], Loss: 1.4768
Epoch [30/30], Batch [4600/6000], Loss: 1.4723
Epoch [30/30], Batch [4700/6000], Loss: 1.4730
Epoch [30/30], Batch [4800/6000], Loss: 1.5883
Epoch [30/30], Batch [4900/6000], Loss: 1.5731
Epoch [30/30], Batch [5000/6000], Loss: 1.4722
Epoch [30/30], Batch [5100/6000], Loss: 1.4754
Epoch [30/30], Batch [5200/6000], Loss: 1.4703
Epoch [30/30], Batch [5300/6000], Loss: 1.4741
Epoch [30/30], Batch [5400/6000], Loss: 1.4752
Epoch [30/30], Batch [5500/6000], Loss: 1.4740
Epoch [30/30], Batch [5600/6000], Loss: 1.4733
Epoch [30/30], Batch [5700/6000], Loss: 1.4746
Epoch [30/30], Batch [5800/6000], Loss: 1.4694
Epoch [30/30], Batch [5900/6000], Loss: 1.4742
Epoch [30/30], Loss: 1.4789
Visualization saved to figures/visualization_0.png
Test Loss: 1.4999, Accuracy: 97.24%
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 1/300:
  Label Loss: 0.1268
  Image Loss: 0.0071
  Total Loss: 6.3471
  Image grad max: 0.0013847838854417205
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 2/300:
  Label Loss: 0.1268
  Image Loss: 0.0065
  Total Loss: 6.3465
  Image grad max: 0.001359273912385106
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 3/300:
  Label Loss: 0.1268
  Image Loss: 0.0059
  Total Loss: 6.3460
  Image grad max: 0.00133377721067518
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 4/300:
  Label Loss: 0.1268
  Image Loss: 0.0055
  Total Loss: 6.3455
  Image grad max: 0.001308303326368332
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 5/300:
  Label Loss: 0.1268
  Image Loss: 0.0050
  Total Loss: 6.3450
  Image grad max: 0.0012828619219362736
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 6/300:
  Label Loss: 0.1268
  Image Loss: 0.0046
  Total Loss: 6.3446
  Image grad max: 0.001257462427020073
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 7/300:
  Label Loss: 0.1268
  Image Loss: 0.0042
  Total Loss: 6.3442
  Image grad max: 0.0012321148533374071
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 8/300:
  Label Loss: 0.1268
  Image Loss: 0.0038
  Total Loss: 6.3439
  Image grad max: 0.0012068290961906314
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 9/300:
  Label Loss: 0.1268
  Image Loss: 0.0035
  Total Loss: 6.3435
  Image grad max: 0.0011816156329587102
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 10/300:
  Label Loss: 0.1268
  Image Loss: 0.0032
  Total Loss: 6.3432
  Image grad max: 0.0011564845917746425
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 11/300:
  Label Loss: 0.1268
  Image Loss: 0.0029
  Total Loss: 6.3430
  Image grad max: 0.0011314463336020708
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 12/300:
  Label Loss: 0.1268
  Image Loss: 0.0027
  Total Loss: 6.3427
  Image grad max: 0.0011065114522352815
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 13/300:
  Label Loss: 0.1268
  Image Loss: 0.0025
  Total Loss: 6.3425
  Image grad max: 0.0010816904250532389
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 14/300:
  Label Loss: 0.1268
  Image Loss: 0.0023
  Total Loss: 6.3423
  Image grad max: 0.0010569938458502293
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 15/300:
  Label Loss: 0.1268
  Image Loss: 0.0021
  Total Loss: 6.3421
  Image grad max: 0.001032432308420539
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 16/300:
  Label Loss: 0.1268
  Image Loss: 0.0019
  Total Loss: 6.3419
  Image grad max: 0.0010080161737278104
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 17/300:
  Label Loss: 0.1268
  Image Loss: 0.0017
  Total Loss: 6.3417
  Image grad max: 0.00098375603556633
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 18/300:
  Label Loss: 0.1268
  Image Loss: 0.0016
  Total Loss: 6.3416
  Image grad max: 0.000959662429522723
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 19/300:
  Label Loss: 0.1268
  Image Loss: 0.0014
  Total Loss: 6.3414
  Image grad max: 0.0009357456001453102
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 20/300:
  Label Loss: 0.1268
  Image Loss: 0.0013
  Total Loss: 6.3413
  Image grad max: 0.0009120157919824123
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 21/300:
  Label Loss: 0.1268
  Image Loss: 0.0012
  Total Loss: 6.3412
  Image grad max: 0.0008884831913746893
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 22/300:
  Label Loss: 0.1268
  Image Loss: 0.0011
  Total Loss: 6.3411
  Image grad max: 0.0008651576936244965
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 23/300:
  Label Loss: 0.1268
  Image Loss: 0.0010
  Total Loss: 6.3410
  Image grad max: 0.000842049194034189
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 24/300:
  Label Loss: 0.1268
  Image Loss: 0.0009
  Total Loss: 6.3409
  Image grad max: 0.0008191672386601567
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 25/300:
  Label Loss: 0.1268
  Image Loss: 0.0008
  Total Loss: 6.3408
  Image grad max: 0.0007965212571434677
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 26/300:
  Label Loss: 0.1268
  Image Loss: 0.0007
  Total Loss: 6.3407
  Image grad max: 0.000774120504502207
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 27/300:
  Label Loss: 0.1268
  Image Loss: 0.0007
  Total Loss: 6.3407
  Image grad max: 0.0007519739447161555
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 28/300:
  Label Loss: 0.1268
  Image Loss: 0.0006
  Total Loss: 6.3406
  Image grad max: 0.0007300903089344501
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 29/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 6.3406
  Image grad max: 0.0007084779790602624
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 30/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 6.3405
  Image grad max: 0.000687145278789103
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 31/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 6.3405
  Image grad max: 0.0006661000661551952
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 32/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 6.3404
  Image grad max: 0.000645349791739136
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 33/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 6.3404
  Image grad max: 0.0006249019061215222
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 34/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3403
  Image grad max: 0.0006047633360140026
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 35/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3403
  Image grad max: 0.0005849406006745994
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 36/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 6.3403
  Image grad max: 0.0005654399865306914
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 37/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3403
  Image grad max: 0.0005462674889713526
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 38/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0005274286377243698
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 39/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0005089286132715642
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 40/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.0004907722468487918
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 41/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 6.3402
  Image grad max: 0.00047296390403062105
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 42/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3402
  Image grad max: 0.0004555078048724681
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 43/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00043840750004164875
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 44/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00042166622006334364
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 45/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0004052869335282594
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 46/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.000389271997846663
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 47/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0003736236540134996
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 48/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.000358343415427953
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 49/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.0003434326790738851
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 50/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00032889234717004
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 51/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 6.3401
  Image grad max: 0.00031472285627387464
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 52/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3401
  Image grad max: 0.0003009243810083717
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 53/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3401
  Image grad max: 0.0002874964557122439
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 54/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3401
  Image grad max: 0.0002744386438280344
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 55/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00026174969389103353
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 56/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00024942823802120984
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 57/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00023747266095597297
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 58/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00022588061983697116
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 59/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0002146496990462765
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 60/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0002037770173046738
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 61/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0001932595478137955
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 62/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00018309375445824116
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 63/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00017327572277281433
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 64/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00016380146553274244
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 65/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00015466660261154175
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 66/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00014586655015591532
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 67/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0001373963023070246
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 68/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.0001292506931349635
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 69/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00012142438936280087
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 70/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00011391177395125851
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 71/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 0.00010670706251403317
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 72/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.980432514566928e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 73/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.319718083133921e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 74/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.68795468704775e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 75/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.08447293820791e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 76/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.508618728024885e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 77/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.959724123589694e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 78/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.437103729695082e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 79/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.940073970123194e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 80/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.467935261549428e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 81/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.019973104936071e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 82/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.5954875531606376e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 83/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.1937644709832966e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 84/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.814104275079444e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 85/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.4557768231024966e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 86/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.118083259323612e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 87/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.8003081752103753e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 88/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.5017376174218953e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 89/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.2216718207346275e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 90/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9594266632338986e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 91/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7143025615951046e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 92/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.5591327610309236e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 93/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.535138835606631e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 94/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.5052757589728571e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 95/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4702123735332862e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 96/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4306026969279628e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 97/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.387039628752973e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 98/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.340085691481363e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 99/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2902884918730706e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 100/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.238164804817643e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 101/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1841861123684794e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 102/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1287781489954796e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 103/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.0723666491685435e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 104/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.0153316907235421e-05
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 105/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.58022974373307e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 106/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.007751941680908e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 107/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.438771146757063e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 108/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.875872142903972e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 109/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.822501174814533e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 110/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.893661859270651e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 111/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.92163973528659e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 112/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.910083695605863e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 113/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.862643542466685e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 114/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.782967259117868e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 115/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.674249900446739e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 116/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.539683338109171e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 117/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.382156582025345e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 118/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.204407211247599e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 119/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.009171440586215e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 120/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.798882623115787e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 121/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.5758213168010116e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 122/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.342268534353934e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 123/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.100200607761508e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 124/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.958943347650347e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 125/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.809779850096675e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 126/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.647691523336107e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 127/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.4746556088502984e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 128/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.29234466739581e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 129/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.10243080498185e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 130/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.906587037112331e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 131/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.706029358203523e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 132/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.502278898144141e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 133/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.296399765735259e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 134/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.089607955393149e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 135/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.882816145051038e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 136/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.6770888982573524e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 137/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.473186325209099e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 138/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.271868536103284e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 139/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.073743982895394e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 140/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.8795725484087598e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 141/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.6898107989836717e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 142/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.504914846213069e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 143/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.3253405743162148e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 144/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.151392436644528e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 145/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.983374204428401e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 146/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8215902173324139e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 147/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6660403616697295e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 148/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.517028749731253e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 149/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.374707380819018e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 150/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2390764823067002e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 151/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.110135826820624e-06
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 152/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.878854143607896e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 153/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.723254154574533e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 154/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.634557164237776e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 155/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.986822995713737e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 156/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.530665359605337e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 157/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.072986593608221e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 158/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.61682895749982e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 159/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.16523243732081e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 160/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.7212384401973395e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 161/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.286368096018123e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 162/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.8636616750409303e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 163/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.454640022937383e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 164/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.161176778121444e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 165/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.3603657811909216e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 166/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.5215416005485167e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 167/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.646224797648756e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 168/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.737456495400693e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 169/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.798277532496286e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 170/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.8317290318445885e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 171/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.8393318391172215e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 172/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.8241265087890497e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 173/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.787633886531694e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 174/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.732894811037113e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 175/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.661430127976928e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 176/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.574760114588571e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 177/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.474405332326569e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 178/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.363406904099975e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 179/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.2432853913633153e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 180/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.1155610713540227e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 181/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.981754789743718e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 182/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.8433868237698334e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 183/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.750639396253973e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 184/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.6882977977038536e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 185/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.6168331146436685e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 186/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.5377656243108504e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 187/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.4510956109224935e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 188/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.3583436359331245e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 189/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.2610299765801756e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 190/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.1606751943181735e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 191/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.057279289147118e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 192/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9508425452841038e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 193/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8444056593125424e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 194/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7364482118864544e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 195/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6284907644603663e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 196/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.522054020597352e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 197/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4171376960803173e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 198/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.313741790909262e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 199/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2118665893012803e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 200/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1130322263852577e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 201/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.0172390574325618e-07
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 202/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 9.244869403346456e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 203/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.347758750915091e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 204/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.496263521034052e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 205/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 6.675179520243546e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 206/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.8997109420033667e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 207/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 5.154652882310984e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 208/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 4.455210600440296e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 209/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.8013840963913026e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 210/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 3.177968110890106e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 211/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.7217085474262603e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 212/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.5848326146160616e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 213/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.554422096068265e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 214/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.5012035109739372e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 215/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.432827450604691e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 216/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.5088494837177677e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 217/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.6304917355446378e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 218/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.7217232911880274e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 219/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.7825443282836204e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 220/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.8281601061053152e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 221/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.8433653653792135e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 222/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.8433653653792135e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 223/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.8281601061053152e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 224/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.7825443282836204e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 225/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.7217232911880274e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 226/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.6609022540924343e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 227/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.584875957722943e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 228/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.4936442244438695e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 229/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.40241266880048e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 230/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.295975853883192e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 231/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.1895390389659042e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 232/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.067896787139034e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 233/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.946254712947848e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 234/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9157958774940198e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 235/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.870180099672325e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 236/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8245643218506302e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 237/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7637432847550372e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 238/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7029220700237602e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 239/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.776947300413667e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 240/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.622473710938266e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 241/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4900484401891845e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 242/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4140220550018512e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 243/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8382680266881835e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 244/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2619693734450266e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 245/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8532375634094933e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 246/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.5238567740993858e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 247/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6918031420232182e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 248/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.3657665220989657e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 249/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.0107828291315855e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 250/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.5575801981526638e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 251/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7818381436995878e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 252/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7065286073147945e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 253/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.64408895386714e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 254/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4338391807200424e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 255/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.0255985333506032e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 256/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.0810485839840567e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 257/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7477992386716323e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 258/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9093402414682714e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 259/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4195221886836862e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 260/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.3995328451699152e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 261/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 8.261869410830514e-09
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 262/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.2951270278449556e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 263/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.414964678758679e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 264/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.692390405594324e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 265/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.736633770121898e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 266/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1114765108288793e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 267/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.051811135028174e-09
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 268/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.613925881827072e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 269/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6640491651287448e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 270/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 7.406409263666092e-09
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 271/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6750730580383788e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 272/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.3925818720395e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 273/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1559650126002907e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 274/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4466732700668672e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 275/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9602456546863323e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 276/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7364072846248746e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 277/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8301616222515804e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 278/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.878396105325919e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 279/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.357975776272724e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 280/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.4213123122885918e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 281/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.441973473959024e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 282/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7890391390551486e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 283/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.742986910358013e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 284/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.1923558140836121e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 285/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.127492321728596e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 286/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.164675372284819e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 287/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6451568995989874e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 288/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9315216093218623e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 289/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9922717697795633e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 290/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.5064061997804856e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 291/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.8267751755729478e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 292/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.915543990094193e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 293/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.2657310694285115e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 294/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.959467255119307e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 295/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.3758398864638366e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 296/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.9687670160806192e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 297/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.7905264826367784e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 298/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 1.6886607667743192e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 299/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.1322744458984744e-08
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232]]
Adversarial Training Loop 300/300:
  Label Loss: 0.1268
  Image Loss: 0.0000
  Total Loss: 6.3400
  Image grad max: 2.227015194478099e-08
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
