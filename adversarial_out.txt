running dynamical.py
Epoch [1/30], Batch [0/6000], Loss: 7.3077
Epoch [1/30], Batch [100/6000], Loss: 6.1684
Epoch [1/30], Batch [200/6000], Loss: 5.6403
Epoch [1/30], Batch [300/6000], Loss: 5.0874
Epoch [1/30], Batch [400/6000], Loss: 3.8139
Epoch [1/30], Batch [500/6000], Loss: 3.8624
Epoch [1/30], Batch [600/6000], Loss: 4.7230
Epoch [1/30], Batch [700/6000], Loss: 3.7142
Epoch [1/30], Batch [800/6000], Loss: 3.5437
Epoch [1/30], Batch [900/6000], Loss: 3.4297
Epoch [1/30], Batch [1000/6000], Loss: 2.9556
Epoch [1/30], Batch [1100/6000], Loss: 2.8551
Epoch [1/30], Batch [1200/6000], Loss: 3.3487
Epoch [1/30], Batch [1300/6000], Loss: 4.1005
Epoch [1/30], Batch [1400/6000], Loss: 4.8125
Epoch [1/30], Batch [1500/6000], Loss: 2.9860
Epoch [1/30], Batch [1600/6000], Loss: 2.6304
Epoch [1/30], Batch [1700/6000], Loss: 2.4685
Epoch [1/30], Batch [1800/6000], Loss: 2.4169
Epoch [1/30], Batch [1900/6000], Loss: 3.4268
Epoch [1/30], Batch [2000/6000], Loss: 1.4689
Epoch [1/30], Batch [2100/6000], Loss: 3.1110
Epoch [1/30], Batch [2200/6000], Loss: 3.1557
Epoch [1/30], Batch [2300/6000], Loss: 1.7974
Epoch [1/30], Batch [2400/6000], Loss: 1.0122
Epoch [1/30], Batch [2500/6000], Loss: 2.5551
Epoch [1/30], Batch [2600/6000], Loss: 1.1083
Epoch [1/30], Batch [2700/6000], Loss: 2.1759
Epoch [1/30], Batch [2800/6000], Loss: 1.9548
Epoch [1/30], Batch [2900/6000], Loss: 2.1755
Epoch [1/30], Batch [3000/6000], Loss: 2.2573
Epoch [1/30], Batch [3100/6000], Loss: 1.4462
Epoch [1/30], Batch [3200/6000], Loss: 1.9803
Epoch [1/30], Batch [3300/6000], Loss: 3.0477
Epoch [1/30], Batch [3400/6000], Loss: 3.7044
Epoch [1/30], Batch [3500/6000], Loss: 2.5004
Epoch [1/30], Batch [3600/6000], Loss: 2.4338
Epoch [1/30], Batch [3700/6000], Loss: 1.8080
Epoch [1/30], Batch [3800/6000], Loss: 3.3393
Epoch [1/30], Batch [3900/6000], Loss: 1.7664
Epoch [1/30], Batch [4000/6000], Loss: 2.3745
Epoch [1/30], Batch [4100/6000], Loss: 1.7987
Epoch [1/30], Batch [4200/6000], Loss: 3.1706
Epoch [1/30], Batch [4300/6000], Loss: 2.1202
Epoch [1/30], Batch [4400/6000], Loss: 0.7626
Epoch [1/30], Batch [4500/6000], Loss: 2.3085
Epoch [1/30], Batch [4600/6000], Loss: 0.6625
Epoch [1/30], Batch [4700/6000], Loss: 1.9374
Epoch [1/30], Batch [4800/6000], Loss: 1.1721
Epoch [1/30], Batch [4900/6000], Loss: 2.1019
Epoch [1/30], Batch [5000/6000], Loss: 3.1002
Epoch [1/30], Batch [5100/6000], Loss: 1.7386
Epoch [1/30], Batch [5200/6000], Loss: 1.6482
Epoch [1/30], Batch [5300/6000], Loss: 2.7287
Epoch [1/30], Batch [5400/6000], Loss: 2.2326
Epoch [1/30], Batch [5500/6000], Loss: 1.6066
Epoch [1/30], Batch [5600/6000], Loss: 1.3922
Epoch [1/30], Batch [5700/6000], Loss: 3.6357
Epoch [1/30], Batch [5800/6000], Loss: 1.5522
Epoch [1/30], Batch [5900/6000], Loss: 0.6102
Epoch [1/30], Loss: 2.7979
Epoch [2/30], Batch [0/6000], Loss: 1.6520
Epoch [2/30], Batch [100/6000], Loss: 1.4294
Epoch [2/30], Batch [200/6000], Loss: 0.5385
Epoch [2/30], Batch [300/6000], Loss: 0.6753
Epoch [2/30], Batch [400/6000], Loss: 0.9262
Epoch [2/30], Batch [500/6000], Loss: 0.4622
Epoch [2/30], Batch [600/6000], Loss: 0.8337
Epoch [2/30], Batch [700/6000], Loss: 4.7714
Epoch [2/30], Batch [800/6000], Loss: 0.6425
Epoch [2/30], Batch [900/6000], Loss: 2.3187
Epoch [2/30], Batch [1000/6000], Loss: 1.4276
Epoch [2/30], Batch [1100/6000], Loss: 1.9290
Epoch [2/30], Batch [1200/6000], Loss: 1.0000
Epoch [2/30], Batch [1300/6000], Loss: 0.7612
Epoch [2/30], Batch [1400/6000], Loss: 1.1254
Epoch [2/30], Batch [1500/6000], Loss: 1.8281
Epoch [2/30], Batch [1600/6000], Loss: 1.1824
Epoch [2/30], Batch [1700/6000], Loss: 0.3229
Epoch [2/30], Batch [1800/6000], Loss: 1.8369
Epoch [2/30], Batch [1900/6000], Loss: 2.3539
Epoch [2/30], Batch [2000/6000], Loss: 1.7972
Epoch [2/30], Batch [2100/6000], Loss: 3.9962
Epoch [2/30], Batch [2200/6000], Loss: 0.4719
Epoch [2/30], Batch [2300/6000], Loss: 0.4110
Epoch [2/30], Batch [2400/6000], Loss: 1.0576
Epoch [2/30], Batch [2500/6000], Loss: 1.3060
Epoch [2/30], Batch [2600/6000], Loss: 1.2602
Epoch [2/30], Batch [2700/6000], Loss: 0.5057
Epoch [2/30], Batch [2800/6000], Loss: 0.3737
Epoch [2/30], Batch [2900/6000], Loss: 2.7848
Epoch [2/30], Batch [3000/6000], Loss: 1.5894
Epoch [2/30], Batch [3100/6000], Loss: 1.2981
Epoch [2/30], Batch [3200/6000], Loss: 0.5864
Epoch [2/30], Batch [3300/6000], Loss: 0.5805
Epoch [2/30], Batch [3400/6000], Loss: 0.9780
Epoch [2/30], Batch [3500/6000], Loss: 0.3951
Epoch [2/30], Batch [3600/6000], Loss: 1.7675
Epoch [2/30], Batch [3700/6000], Loss: 1.3172
Epoch [2/30], Batch [3800/6000], Loss: 0.9462
Epoch [2/30], Batch [3900/6000], Loss: 0.4477
Epoch [2/30], Batch [4000/6000], Loss: 0.4897
Epoch [2/30], Batch [4100/6000], Loss: 0.9312
Epoch [2/30], Batch [4200/6000], Loss: 2.1540
Epoch [2/30], Batch [4300/6000], Loss: 0.3846
Epoch [2/30], Batch [4400/6000], Loss: 2.8370
Epoch [2/30], Batch [4500/6000], Loss: 2.6111
Epoch [2/30], Batch [4600/6000], Loss: 1.6433
Epoch [2/30], Batch [4700/6000], Loss: 0.9068
Epoch [2/30], Batch [4800/6000], Loss: 1.6947
Epoch [2/30], Batch [4900/6000], Loss: 1.5061
Epoch [2/30], Batch [5000/6000], Loss: 0.5059
Epoch [2/30], Batch [5100/6000], Loss: 1.3450
Epoch [2/30], Batch [5200/6000], Loss: 2.3770
Epoch [2/30], Batch [5300/6000], Loss: 0.6388
Epoch [2/30], Batch [5400/6000], Loss: 0.5057
Epoch [2/30], Batch [5500/6000], Loss: 1.7264
Epoch [2/30], Batch [5600/6000], Loss: 1.0871
Epoch [2/30], Batch [5700/6000], Loss: 0.4067
Epoch [2/30], Batch [5800/6000], Loss: 0.3591
Epoch [2/30], Batch [5900/6000], Loss: 1.7135
Epoch [2/30], Loss: 1.4170
Epoch [3/30], Batch [0/6000], Loss: 0.7647
Epoch [3/30], Batch [100/6000], Loss: 1.6352
Epoch [3/30], Batch [200/6000], Loss: 0.3213
Epoch [3/30], Batch [300/6000], Loss: 0.5215
Epoch [3/30], Batch [400/6000], Loss: 1.1132
Epoch [3/30], Batch [500/6000], Loss: 2.6003
Epoch [3/30], Batch [600/6000], Loss: 0.3835
Epoch [3/30], Batch [700/6000], Loss: 0.4321
Epoch [3/30], Batch [800/6000], Loss: 0.4521
Epoch [3/30], Batch [900/6000], Loss: 1.4653
Epoch [3/30], Batch [1000/6000], Loss: 1.2135
Epoch [3/30], Batch [1100/6000], Loss: 1.1211
Epoch [3/30], Batch [1200/6000], Loss: 0.7052
Epoch [3/30], Batch [1300/6000], Loss: 0.7096
Epoch [3/30], Batch [1400/6000], Loss: 1.8531
Epoch [3/30], Batch [1500/6000], Loss: 0.4292
Epoch [3/30], Batch [1600/6000], Loss: 2.0130
Epoch [3/30], Batch [1700/6000], Loss: 2.1131
Epoch [3/30], Batch [1800/6000], Loss: 0.8555
Epoch [3/30], Batch [1900/6000], Loss: 0.4519
Epoch [3/30], Batch [2000/6000], Loss: 0.9702
Epoch [3/30], Batch [2100/6000], Loss: 1.9121
Epoch [3/30], Batch [2200/6000], Loss: 1.1077
Epoch [3/30], Batch [2300/6000], Loss: 0.9648
Epoch [3/30], Batch [2400/6000], Loss: 0.9287
Epoch [3/30], Batch [2500/6000], Loss: 0.4659
Epoch [3/30], Batch [2600/6000], Loss: 0.8772
Epoch [3/30], Batch [2700/6000], Loss: 0.2631
Epoch [3/30], Batch [2800/6000], Loss: 0.2958
Epoch [3/30], Batch [2900/6000], Loss: 0.3329
Epoch [3/30], Batch [3000/6000], Loss: 0.4189
Epoch [3/30], Batch [3100/6000], Loss: 1.1442
Epoch [3/30], Batch [3200/6000], Loss: 0.2358
Epoch [3/30], Batch [3300/6000], Loss: 1.0355
Epoch [3/30], Batch [3400/6000], Loss: 1.0042
Epoch [3/30], Batch [3500/6000], Loss: 0.8647
Epoch [3/30], Batch [3600/6000], Loss: 0.3957
Epoch [3/30], Batch [3700/6000], Loss: 2.1790
Epoch [3/30], Batch [3800/6000], Loss: 1.7341
Epoch [3/30], Batch [3900/6000], Loss: 0.2331
Epoch [3/30], Batch [4000/6000], Loss: 0.4145
Epoch [3/30], Batch [4100/6000], Loss: 0.2632
Epoch [3/30], Batch [4200/6000], Loss: 0.4262
Epoch [3/30], Batch [4300/6000], Loss: 0.4760
Epoch [3/30], Batch [4400/6000], Loss: 2.1403
Epoch [3/30], Batch [4500/6000], Loss: 1.3147
Epoch [3/30], Batch [4600/6000], Loss: 0.5246
Epoch [3/30], Batch [4700/6000], Loss: 0.9555
Epoch [3/30], Batch [4800/6000], Loss: 0.3171
Epoch [3/30], Batch [4900/6000], Loss: 1.6973
Epoch [3/30], Batch [5000/6000], Loss: 1.4892
Epoch [3/30], Batch [5100/6000], Loss: 1.0498
Epoch [3/30], Batch [5200/6000], Loss: 0.6333
Epoch [3/30], Batch [5300/6000], Loss: 1.4320
Epoch [3/30], Batch [5400/6000], Loss: 1.2043
Epoch [3/30], Batch [5500/6000], Loss: 1.4363
Epoch [3/30], Batch [5600/6000], Loss: 1.0009
Epoch [3/30], Batch [5700/6000], Loss: 0.3107
Epoch [3/30], Batch [5800/6000], Loss: 0.2742
Epoch [3/30], Batch [5900/6000], Loss: 0.2767
Epoch [3/30], Loss: 1.1076
Epoch [4/30], Batch [0/6000], Loss: 1.0751
Epoch [4/30], Batch [100/6000], Loss: 0.3126
Epoch [4/30], Batch [200/6000], Loss: 0.4321
Epoch [4/30], Batch [300/6000], Loss: 0.2561
Epoch [4/30], Batch [400/6000], Loss: 0.3166
Epoch [4/30], Batch [500/6000], Loss: 1.0599
Epoch [4/30], Batch [600/6000], Loss: 0.6344
Epoch [4/30], Batch [700/6000], Loss: 0.8485
Epoch [4/30], Batch [800/6000], Loss: 0.5019
Epoch [4/30], Batch [900/6000], Loss: 1.9033
Epoch [4/30], Batch [1000/6000], Loss: 1.3986
Epoch [4/30], Batch [1100/6000], Loss: 0.8874
Epoch [4/30], Batch [1200/6000], Loss: 0.2976
Epoch [4/30], Batch [1300/6000], Loss: 0.4013
Epoch [4/30], Batch [1400/6000], Loss: 0.3188
Epoch [4/30], Batch [1500/6000], Loss: 0.4990
Epoch [4/30], Batch [1600/6000], Loss: 0.2915
Epoch [4/30], Batch [1700/6000], Loss: 0.2747
Epoch [4/30], Batch [1800/6000], Loss: 0.2960
Epoch [4/30], Batch [1900/6000], Loss: 0.6840
Epoch [4/30], Batch [2000/6000], Loss: 1.0351
Epoch [4/30], Batch [2100/6000], Loss: 1.5137
Epoch [4/30], Batch [2200/6000], Loss: 0.3130
Epoch [4/30], Batch [2300/6000], Loss: 0.4620
Epoch [4/30], Batch [2400/6000], Loss: 0.4069
Epoch [4/30], Batch [2500/6000], Loss: 0.2872
Epoch [4/30], Batch [2600/6000], Loss: 0.3181
Epoch [4/30], Batch [2700/6000], Loss: 1.0894
Epoch [4/30], Batch [2800/6000], Loss: 0.9486
Epoch [4/30], Batch [2900/6000], Loss: 1.3036
Epoch [4/30], Batch [3000/6000], Loss: 0.3059
Epoch [4/30], Batch [3100/6000], Loss: 0.6964
Epoch [4/30], Batch [3200/6000], Loss: 0.3380
Epoch [4/30], Batch [3300/6000], Loss: 0.5792
Epoch [4/30], Batch [3400/6000], Loss: 0.4508
Epoch [4/30], Batch [3500/6000], Loss: 0.4582
Epoch [4/30], Batch [3600/6000], Loss: 0.6786
Epoch [4/30], Batch [3700/6000], Loss: 0.9275
Epoch [4/30], Batch [3800/6000], Loss: 0.3023
Epoch [4/30], Batch [3900/6000], Loss: 0.4728
Epoch [4/30], Batch [4000/6000], Loss: 0.5568
Epoch [4/30], Batch [4100/6000], Loss: 1.8306
Epoch [4/30], Batch [4200/6000], Loss: 0.9823
Epoch [4/30], Batch [4300/6000], Loss: 0.4967
Epoch [4/30], Batch [4400/6000], Loss: 0.9521
Epoch [4/30], Batch [4500/6000], Loss: 0.7695
Epoch [4/30], Batch [4600/6000], Loss: 1.1413
Epoch [4/30], Batch [4700/6000], Loss: 1.6738
Epoch [4/30], Batch [4800/6000], Loss: 1.9056
Epoch [4/30], Batch [4900/6000], Loss: 0.6525
Epoch [4/30], Batch [5000/6000], Loss: 0.5113
Epoch [4/30], Batch [5100/6000], Loss: 0.6458
Epoch [4/30], Batch [5200/6000], Loss: 0.4279
Epoch [4/30], Batch [5300/6000], Loss: 1.2167
Epoch [4/30], Batch [5400/6000], Loss: 1.6656
Epoch [4/30], Batch [5500/6000], Loss: 0.2548
Epoch [4/30], Batch [5600/6000], Loss: 0.2700
Epoch [4/30], Batch [5700/6000], Loss: 0.2491
Epoch [4/30], Batch [5800/6000], Loss: 0.7091
Epoch [4/30], Batch [5900/6000], Loss: 0.2162
Epoch [4/30], Loss: 0.9035
Epoch [5/30], Batch [0/6000], Loss: 0.2156
Epoch [5/30], Batch [100/6000], Loss: 0.5406
Epoch [5/30], Batch [200/6000], Loss: 1.6177
Epoch [5/30], Batch [300/6000], Loss: 0.2742
Epoch [5/30], Batch [400/6000], Loss: 0.4890
Epoch [5/30], Batch [500/6000], Loss: 0.2307
Epoch [5/30], Batch [600/6000], Loss: 0.2206
Epoch [5/30], Batch [700/6000], Loss: 0.2631
Epoch [5/30], Batch [800/6000], Loss: 1.9522
Epoch [5/30], Batch [900/6000], Loss: 1.5267
Epoch [5/30], Batch [1000/6000], Loss: 0.4281
Epoch [5/30], Batch [1100/6000], Loss: 0.2130
Epoch [5/30], Batch [1200/6000], Loss: 2.0827
Epoch [5/30], Batch [1300/6000], Loss: 0.3484
Epoch [5/30], Batch [1400/6000], Loss: 1.0724
Epoch [5/30], Batch [1500/6000], Loss: 0.3017
Epoch [5/30], Batch [1600/6000], Loss: 0.2287
Epoch [5/30], Batch [1700/6000], Loss: 0.2099
Epoch [5/30], Batch [1800/6000], Loss: 0.3946
Epoch [5/30], Batch [1900/6000], Loss: 0.8176
Epoch [5/30], Batch [2000/6000], Loss: 0.2412
Epoch [5/30], Batch [2100/6000], Loss: 0.6962
Epoch [5/30], Batch [2200/6000], Loss: 1.6126
Epoch [5/30], Batch [2300/6000], Loss: 0.9764
Epoch [5/30], Batch [2400/6000], Loss: 0.3446
Epoch [5/30], Batch [2500/6000], Loss: 0.1700
Epoch [5/30], Batch [2600/6000], Loss: 0.4756
Epoch [5/30], Batch [2700/6000], Loss: 0.3377
Epoch [5/30], Batch [2800/6000], Loss: 1.5071
Epoch [5/30], Batch [2900/6000], Loss: 0.7735
Epoch [5/30], Batch [3000/6000], Loss: 0.7228
Epoch [5/30], Batch [3100/6000], Loss: 2.3487
Epoch [5/30], Batch [3200/6000], Loss: 1.3304
Epoch [5/30], Batch [3300/6000], Loss: 0.1750
Epoch [5/30], Batch [3400/6000], Loss: 0.9668
Epoch [5/30], Batch [3500/6000], Loss: 1.3923
Epoch [5/30], Batch [3600/6000], Loss: 0.2060
Epoch [5/30], Batch [3700/6000], Loss: 0.2782
Epoch [5/30], Batch [3800/6000], Loss: 0.2754
Epoch [5/30], Batch [3900/6000], Loss: 0.5746
Epoch [5/30], Batch [4000/6000], Loss: 0.4242
Epoch [5/30], Batch [4100/6000], Loss: 1.2064
Epoch [5/30], Batch [4200/6000], Loss: 0.4611
Epoch [5/30], Batch [4300/6000], Loss: 1.3644
Epoch [5/30], Batch [4400/6000], Loss: 0.2965
Epoch [5/30], Batch [4500/6000], Loss: 0.4145
Epoch [5/30], Batch [4600/6000], Loss: 0.3596
Epoch [5/30], Batch [4700/6000], Loss: 0.4075
Epoch [5/30], Batch [4800/6000], Loss: 0.2871
Epoch [5/30], Batch [4900/6000], Loss: 0.4081
Epoch [5/30], Batch [5000/6000], Loss: 0.2902
Epoch [5/30], Batch [5100/6000], Loss: 0.3756
Epoch [5/30], Batch [5200/6000], Loss: 1.4482
Epoch [5/30], Batch [5300/6000], Loss: 1.1774
Epoch [5/30], Batch [5400/6000], Loss: 0.4049
Epoch [5/30], Batch [5500/6000], Loss: 1.3599
Epoch [5/30], Batch [5600/6000], Loss: 0.4662
Epoch [5/30], Batch [5700/6000], Loss: 0.7511
Epoch [5/30], Batch [5800/6000], Loss: 0.2526
Epoch [5/30], Batch [5900/6000], Loss: 4.5351
Epoch [5/30], Loss: 0.7678
Epoch [6/30], Batch [0/6000], Loss: 0.2347
Epoch [6/30], Batch [100/6000], Loss: 0.2482
Epoch [6/30], Batch [200/6000], Loss: 0.2979
Epoch [6/30], Batch [300/6000], Loss: 0.6750
Epoch [6/30], Batch [400/6000], Loss: 0.2157
Epoch [6/30], Batch [500/6000], Loss: 0.1867
Epoch [6/30], Batch [600/6000], Loss: 0.2079
Epoch [6/30], Batch [700/6000], Loss: 0.3063
Epoch [6/30], Batch [800/6000], Loss: 0.2235
Epoch [6/30], Batch [900/6000], Loss: 1.1415
Epoch [6/30], Batch [1000/6000], Loss: 0.2475
Epoch [6/30], Batch [1100/6000], Loss: 0.1909
Epoch [6/30], Batch [1200/6000], Loss: 0.2283
Epoch [6/30], Batch [1300/6000], Loss: 0.2969
Epoch [6/30], Batch [1400/6000], Loss: 0.1956
Epoch [6/30], Batch [1500/6000], Loss: 0.1898
Epoch [6/30], Batch [1600/6000], Loss: 0.4635
Epoch [6/30], Batch [1700/6000], Loss: 0.2713
Epoch [6/30], Batch [1800/6000], Loss: 0.1981
Epoch [6/30], Batch [1900/6000], Loss: 0.9481
Epoch [6/30], Batch [2000/6000], Loss: 0.3465
Epoch [6/30], Batch [2100/6000], Loss: 2.2950
Epoch [6/30], Batch [2200/6000], Loss: 0.2118
Epoch [6/30], Batch [2300/6000], Loss: 1.0397
Epoch [6/30], Batch [2400/6000], Loss: 0.5966
Epoch [6/30], Batch [2500/6000], Loss: 1.3840
Epoch [6/30], Batch [2600/6000], Loss: 0.3574
Epoch [6/30], Batch [2700/6000], Loss: 0.3040
Epoch [6/30], Batch [2800/6000], Loss: 0.7827
Epoch [6/30], Batch [2900/6000], Loss: 0.2122
Epoch [6/30], Batch [3000/6000], Loss: 1.1770
Epoch [6/30], Batch [3100/6000], Loss: 0.1965
Epoch [6/30], Batch [3200/6000], Loss: 0.2523
Epoch [6/30], Batch [3300/6000], Loss: 1.6422
Epoch [6/30], Batch [3400/6000], Loss: 0.3821
Epoch [6/30], Batch [3500/6000], Loss: 1.0086
Epoch [6/30], Batch [3600/6000], Loss: 0.4693
Epoch [6/30], Batch [3700/6000], Loss: 0.9588
Epoch [6/30], Batch [3800/6000], Loss: 0.1718
Epoch [6/30], Batch [3900/6000], Loss: 0.4042
Epoch [6/30], Batch [4000/6000], Loss: 0.3216
Epoch [6/30], Batch [4100/6000], Loss: 0.6404
Epoch [6/30], Batch [4200/6000], Loss: 1.4321
Epoch [6/30], Batch [4300/6000], Loss: 0.2003
Epoch [6/30], Batch [4400/6000], Loss: 0.2065
Epoch [6/30], Batch [4500/6000], Loss: 0.2764
Epoch [6/30], Batch [4600/6000], Loss: 0.2126
Epoch [6/30], Batch [4700/6000], Loss: 0.2409
Epoch [6/30], Batch [4800/6000], Loss: 0.1874
Epoch [6/30], Batch [4900/6000], Loss: 0.2544
Epoch [6/30], Batch [5000/6000], Loss: 0.2518
Epoch [6/30], Batch [5100/6000], Loss: 0.4891
Epoch [6/30], Batch [5200/6000], Loss: 0.6126
Epoch [6/30], Batch [5300/6000], Loss: 0.4402
Epoch [6/30], Batch [5400/6000], Loss: 0.3689
Epoch [6/30], Batch [5500/6000], Loss: 0.2083
Epoch [6/30], Batch [5600/6000], Loss: 0.2661
Epoch [6/30], Batch [5700/6000], Loss: 0.3025
Epoch [6/30], Batch [5800/6000], Loss: 0.2094
Epoch [6/30], Batch [5900/6000], Loss: 0.3074
Epoch [6/30], Loss: 0.6686
Epoch [7/30], Batch [0/6000], Loss: 0.1882
Epoch [7/30], Batch [100/6000], Loss: 0.2018
Epoch [7/30], Batch [200/6000], Loss: 0.2470
Epoch [7/30], Batch [300/6000], Loss: 0.6041
Epoch [7/30], Batch [400/6000], Loss: 0.2538
Epoch [7/30], Batch [500/6000], Loss: 0.5446
Epoch [7/30], Batch [600/6000], Loss: 0.2198
Epoch [7/30], Batch [700/6000], Loss: 1.4334
Epoch [7/30], Batch [800/6000], Loss: 1.9010
Epoch [7/30], Batch [900/6000], Loss: 0.2417
Epoch [7/30], Batch [1000/6000], Loss: 2.2308
Epoch [7/30], Batch [1100/6000], Loss: 0.1752
Epoch [7/30], Batch [1200/6000], Loss: 0.5336
Epoch [7/30], Batch [1300/6000], Loss: 0.3636
Epoch [7/30], Batch [1400/6000], Loss: 1.4268
Epoch [7/30], Batch [1500/6000], Loss: 0.3576
Epoch [7/30], Batch [1600/6000], Loss: 0.2053
Epoch [7/30], Batch [1700/6000], Loss: 1.4988
Epoch [7/30], Batch [1800/6000], Loss: 0.3197
Epoch [7/30], Batch [1900/6000], Loss: 1.3748
Epoch [7/30], Batch [2000/6000], Loss: 0.2116
Epoch [7/30], Batch [2100/6000], Loss: 0.2045
Epoch [7/30], Batch [2200/6000], Loss: 0.2368
Epoch [7/30], Batch [2300/6000], Loss: 0.9015
Epoch [7/30], Batch [2400/6000], Loss: 0.1993
Epoch [7/30], Batch [2500/6000], Loss: 0.3111
Epoch [7/30], Batch [2600/6000], Loss: 1.6942
Epoch [7/30], Batch [2700/6000], Loss: 0.2229
Epoch [7/30], Batch [2800/6000], Loss: 0.2977
Epoch [7/30], Batch [2900/6000], Loss: 0.4242
Epoch [7/30], Batch [3000/6000], Loss: 0.2015
Epoch [7/30], Batch [3100/6000], Loss: 1.1202
Epoch [7/30], Batch [3200/6000], Loss: 0.3302
Epoch [7/30], Batch [3300/6000], Loss: 0.1747
Epoch [7/30], Batch [3400/6000], Loss: 0.3269
Epoch [7/30], Batch [3500/6000], Loss: 0.3585
Epoch [7/30], Batch [3600/6000], Loss: 1.6624
Epoch [7/30], Batch [3700/6000], Loss: 0.2166
Epoch [7/30], Batch [3800/6000], Loss: 0.1813
Epoch [7/30], Batch [3900/6000], Loss: 0.4251
Epoch [7/30], Batch [4000/6000], Loss: 0.2024
Epoch [7/30], Batch [4100/6000], Loss: 2.9039
Epoch [7/30], Batch [4200/6000], Loss: 0.6316
Epoch [7/30], Batch [4300/6000], Loss: 0.2888
Epoch [7/30], Batch [4400/6000], Loss: 0.1935
Epoch [7/30], Batch [4500/6000], Loss: 0.2320
Epoch [7/30], Batch [4600/6000], Loss: 0.2305
Epoch [7/30], Batch [4700/6000], Loss: 0.4605
Epoch [7/30], Batch [4800/6000], Loss: 0.1933
Epoch [7/30], Batch [4900/6000], Loss: 0.2092
Epoch [7/30], Batch [5000/6000], Loss: 0.3364
Epoch [7/30], Batch [5100/6000], Loss: 2.6179
Epoch [7/30], Batch [5200/6000], Loss: 0.1752
Epoch [7/30], Batch [5300/6000], Loss: 0.2353
Epoch [7/30], Batch [5400/6000], Loss: 0.3481
Epoch [7/30], Batch [5500/6000], Loss: 0.2240
Epoch [7/30], Batch [5600/6000], Loss: 0.6149
Epoch [7/30], Batch [5700/6000], Loss: 0.3172
Epoch [7/30], Batch [5800/6000], Loss: 0.1930
Epoch [7/30], Batch [5900/6000], Loss: 1.5225
Epoch [7/30], Loss: 0.5986
Epoch [8/30], Batch [0/6000], Loss: 0.9677
Epoch [8/30], Batch [100/6000], Loss: 0.4706
Epoch [8/30], Batch [200/6000], Loss: 0.1831
Epoch [8/30], Batch [300/6000], Loss: 0.1569
Epoch [8/30], Batch [400/6000], Loss: 0.2996
Epoch [8/30], Batch [500/6000], Loss: 0.2465
Epoch [8/30], Batch [600/6000], Loss: 0.1967
Epoch [8/30], Batch [700/6000], Loss: 0.2143
Epoch [8/30], Batch [800/6000], Loss: 0.2022
Epoch [8/30], Batch [900/6000], Loss: 0.1949
Epoch [8/30], Batch [1000/6000], Loss: 0.1783
Epoch [8/30], Batch [1100/6000], Loss: 0.6344
Epoch [8/30], Batch [1200/6000], Loss: 0.5325
Epoch [8/30], Batch [1300/6000], Loss: 0.1978
Epoch [8/30], Batch [1400/6000], Loss: 0.3168
Epoch [8/30], Batch [1500/6000], Loss: 0.8423
Epoch [8/30], Batch [1600/6000], Loss: 0.1606
Epoch [8/30], Batch [1700/6000], Loss: 0.1797
Epoch [8/30], Batch [1800/6000], Loss: 0.5124
Epoch [8/30], Batch [1900/6000], Loss: 0.2536
Epoch [8/30], Batch [2000/6000], Loss: 0.2097
Epoch [8/30], Batch [2100/6000], Loss: 1.5404
Epoch [8/30], Batch [2200/6000], Loss: 0.3189
Epoch [8/30], Batch [2300/6000], Loss: 0.1668
Epoch [8/30], Batch [2400/6000], Loss: 2.2604
Epoch [8/30], Batch [2500/6000], Loss: 0.2350
Epoch [8/30], Batch [2600/6000], Loss: 0.3464
Epoch [8/30], Batch [2700/6000], Loss: 0.9341
Epoch [8/30], Batch [2800/6000], Loss: 0.2467
Epoch [8/30], Batch [2900/6000], Loss: 1.7123
Epoch [8/30], Batch [3000/6000], Loss: 1.5681
Epoch [8/30], Batch [3100/6000], Loss: 0.2207
Epoch [8/30], Batch [3200/6000], Loss: 0.1838
Epoch [8/30], Batch [3300/6000], Loss: 1.1550
Epoch [8/30], Batch [3400/6000], Loss: 0.2552
Epoch [8/30], Batch [3500/6000], Loss: 0.3636
Epoch [8/30], Batch [3600/6000], Loss: 0.2427
Epoch [8/30], Batch [3700/6000], Loss: 1.7089
Epoch [8/30], Batch [3800/6000], Loss: 0.2092
Epoch [8/30], Batch [3900/6000], Loss: 0.3530
Epoch [8/30], Batch [4000/6000], Loss: 0.1729
Epoch [8/30], Batch [4100/6000], Loss: 0.3661
Epoch [8/30], Batch [4200/6000], Loss: 0.2143
Epoch [8/30], Batch [4300/6000], Loss: 0.1534
Epoch [8/30], Batch [4400/6000], Loss: 0.3545
Epoch [8/30], Batch [4500/6000], Loss: 0.8253
Epoch [8/30], Batch [4600/6000], Loss: 0.1877
Epoch [8/30], Batch [4700/6000], Loss: 0.5105
Epoch [8/30], Batch [4800/6000], Loss: 2.6163
Epoch [8/30], Batch [4900/6000], Loss: 0.9755
Epoch [8/30], Batch [5000/6000], Loss: 0.3103
Epoch [8/30], Batch [5100/6000], Loss: 1.5842
Epoch [8/30], Batch [5200/6000], Loss: 0.2345
Epoch [8/30], Batch [5300/6000], Loss: 0.2163
Epoch [8/30], Batch [5400/6000], Loss: 1.3230
Epoch [8/30], Batch [5500/6000], Loss: 0.2241
Epoch [8/30], Batch [5600/6000], Loss: 0.3988
Epoch [8/30], Batch [5700/6000], Loss: 0.4810
Epoch [8/30], Batch [5800/6000], Loss: 1.8044
Epoch [8/30], Batch [5900/6000], Loss: 1.9152
Epoch [8/30], Loss: 0.5518
Epoch [9/30], Batch [0/6000], Loss: 0.2217
Epoch [9/30], Batch [100/6000], Loss: 0.3411
Epoch [9/30], Batch [200/6000], Loss: 0.3254
Epoch [9/30], Batch [300/6000], Loss: 1.0027
Epoch [9/30], Batch [400/6000], Loss: 0.2525
Epoch [9/30], Batch [500/6000], Loss: 0.1738
Epoch [9/30], Batch [600/6000], Loss: 0.3066
Epoch [9/30], Batch [700/6000], Loss: 0.1770
Epoch [9/30], Batch [800/6000], Loss: 0.6816
Epoch [9/30], Batch [900/6000], Loss: 0.2297
Epoch [9/30], Batch [1000/6000], Loss: 0.2823
Epoch [9/30], Batch [1100/6000], Loss: 0.1817
Epoch [9/30], Batch [1200/6000], Loss: 0.3725
Epoch [9/30], Batch [1300/6000], Loss: 0.2360
Epoch [9/30], Batch [1400/6000], Loss: 0.2404
Epoch [9/30], Batch [1500/6000], Loss: 0.2071
Epoch [9/30], Batch [1600/6000], Loss: 0.2344
Epoch [9/30], Batch [1700/6000], Loss: 0.1920
Epoch [9/30], Batch [1800/6000], Loss: 0.1904
Epoch [9/30], Batch [1900/6000], Loss: 0.2344
Epoch [9/30], Batch [2000/6000], Loss: 0.1630
Epoch [9/30], Batch [2100/6000], Loss: 0.1948
Epoch [9/30], Batch [2200/6000], Loss: 1.0271
Epoch [9/30], Batch [2300/6000], Loss: 0.6378
Epoch [9/30], Batch [2400/6000], Loss: 0.2125
Epoch [9/30], Batch [2500/6000], Loss: 0.1935
Epoch [9/30], Batch [2600/6000], Loss: 1.3107
Epoch [9/30], Batch [2700/6000], Loss: 0.2922
Epoch [9/30], Batch [2800/6000], Loss: 0.2141
Epoch [9/30], Batch [2900/6000], Loss: 0.1980
Epoch [9/30], Batch [3000/6000], Loss: 1.1006
Epoch [9/30], Batch [3100/6000], Loss: 0.2281
Epoch [9/30], Batch [3200/6000], Loss: 2.8016
Epoch [9/30], Batch [3300/6000], Loss: 0.2090
Epoch [9/30], Batch [3400/6000], Loss: 0.3810
Epoch [9/30], Batch [3500/6000], Loss: 0.2131
Epoch [9/30], Batch [3600/6000], Loss: 1.5490
Epoch [9/30], Batch [3700/6000], Loss: 1.2144
Epoch [9/30], Batch [3800/6000], Loss: 0.1781
Epoch [9/30], Batch [3900/6000], Loss: 0.1189
Epoch [9/30], Batch [4000/6000], Loss: 0.8052
Epoch [9/30], Batch [4100/6000], Loss: 0.1991
Epoch [9/30], Batch [4200/6000], Loss: 0.7640
Epoch [9/30], Batch [4300/6000], Loss: 0.1910
Epoch [9/30], Batch [4400/6000], Loss: 0.1954
Epoch [9/30], Batch [4500/6000], Loss: 0.2268
Epoch [9/30], Batch [4600/6000], Loss: 0.1621
Epoch [9/30], Batch [4700/6000], Loss: 0.6140
Epoch [9/30], Batch [4800/6000], Loss: 0.2220
Epoch [9/30], Batch [4900/6000], Loss: 0.1975
Epoch [9/30], Batch [5000/6000], Loss: 0.2198
Epoch [9/30], Batch [5100/6000], Loss: 0.2536
Epoch [9/30], Batch [5200/6000], Loss: 0.2125
Epoch [9/30], Batch [5300/6000], Loss: 0.2474
Epoch [9/30], Batch [5400/6000], Loss: 0.1636
Epoch [9/30], Batch [5500/6000], Loss: 0.6330
Epoch [9/30], Batch [5600/6000], Loss: 0.2442
Epoch [9/30], Batch [5700/6000], Loss: 0.3194
Epoch [9/30], Batch [5800/6000], Loss: 0.1622
Epoch [9/30], Batch [5900/6000], Loss: 0.2014
Epoch [9/30], Loss: 0.5110
Epoch [10/30], Batch [0/6000], Loss: 0.4912
Epoch [10/30], Batch [100/6000], Loss: 0.3447
Epoch [10/30], Batch [200/6000], Loss: 0.8826
Epoch [10/30], Batch [300/6000], Loss: 0.2798
Epoch [10/30], Batch [400/6000], Loss: 0.2169
Epoch [10/30], Batch [500/6000], Loss: 0.1927
Epoch [10/30], Batch [600/6000], Loss: 0.3700
Epoch [10/30], Batch [700/6000], Loss: 0.2469
Epoch [10/30], Batch [800/6000], Loss: 0.5260
Epoch [10/30], Batch [900/6000], Loss: 0.1804
Epoch [10/30], Batch [1000/6000], Loss: 0.1783
Epoch [10/30], Batch [1100/6000], Loss: 0.1807
Epoch [10/30], Batch [1200/6000], Loss: 0.9704
Epoch [10/30], Batch [1300/6000], Loss: 0.2907
Epoch [10/30], Batch [1400/6000], Loss: 1.0667
Epoch [10/30], Batch [1500/6000], Loss: 0.4065
Epoch [10/30], Batch [1600/6000], Loss: 0.1898
Epoch [10/30], Batch [1700/6000], Loss: 0.2092
Epoch [10/30], Batch [1800/6000], Loss: 0.8981
Epoch [10/30], Batch [1900/6000], Loss: 0.4721
Epoch [10/30], Batch [2000/6000], Loss: 0.2087
Epoch [10/30], Batch [2100/6000], Loss: 0.1970
Epoch [10/30], Batch [2200/6000], Loss: 0.1496
Epoch [10/30], Batch [2300/6000], Loss: 0.6852
Epoch [10/30], Batch [2400/6000], Loss: 0.1919
Epoch [10/30], Batch [2500/6000], Loss: 0.1981
Epoch [10/30], Batch [2600/6000], Loss: 0.2837
Epoch [10/30], Batch [2700/6000], Loss: 0.1899
Epoch [10/30], Batch [2800/6000], Loss: 0.7454
Epoch [10/30], Batch [2900/6000], Loss: 0.1635
Epoch [10/30], Batch [3000/6000], Loss: 0.4416
Epoch [10/30], Batch [3100/6000], Loss: 0.3100
Epoch [10/30], Batch [3200/6000], Loss: 0.7629
Epoch [10/30], Batch [3300/6000], Loss: 0.2909
Epoch [10/30], Batch [3400/6000], Loss: 1.1768
Epoch [10/30], Batch [3500/6000], Loss: 0.1856
Epoch [10/30], Batch [3600/6000], Loss: 0.1712
Epoch [10/30], Batch [3700/6000], Loss: 0.3228
Epoch [10/30], Batch [3800/6000], Loss: 0.8832
Epoch [10/30], Batch [3900/6000], Loss: 0.7882
Epoch [10/30], Batch [4000/6000], Loss: 2.6320
Epoch [10/30], Batch [4100/6000], Loss: 0.4701
Epoch [10/30], Batch [4200/6000], Loss: 0.1569
Epoch [10/30], Batch [4300/6000], Loss: 0.2228
Epoch [10/30], Batch [4400/6000], Loss: 0.1762
Epoch [10/30], Batch [4500/6000], Loss: 0.4210
Epoch [10/30], Batch [4600/6000], Loss: 0.3651
Epoch [10/30], Batch [4700/6000], Loss: 1.8950
Epoch [10/30], Batch [4800/6000], Loss: 0.1669
Epoch [10/30], Batch [4900/6000], Loss: 0.5050
Epoch [10/30], Batch [5000/6000], Loss: 0.1701
Epoch [10/30], Batch [5100/6000], Loss: 0.3250
Epoch [10/30], Batch [5200/6000], Loss: 0.1937
Epoch [10/30], Batch [5300/6000], Loss: 0.1535
Epoch [10/30], Batch [5400/6000], Loss: 0.1780
Epoch [10/30], Batch [5500/6000], Loss: 0.2110
Epoch [10/30], Batch [5600/6000], Loss: 0.3124
Epoch [10/30], Batch [5700/6000], Loss: 0.3460
Epoch [10/30], Batch [5800/6000], Loss: 0.5445
Epoch [10/30], Batch [5900/6000], Loss: 0.2705
Epoch [10/30], Loss: 0.4785
Epoch [11/30], Batch [0/6000], Loss: 0.1895
Epoch [11/30], Batch [100/6000], Loss: 0.1804
Epoch [11/30], Batch [200/6000], Loss: 0.2397
Epoch [11/30], Batch [300/6000], Loss: 0.2282
Epoch [11/30], Batch [400/6000], Loss: 0.3702
Epoch [11/30], Batch [500/6000], Loss: 0.1801
Epoch [11/30], Batch [600/6000], Loss: 1.5537
Epoch [11/30], Batch [700/6000], Loss: 0.1595
Epoch [11/30], Batch [800/6000], Loss: 0.2553
Epoch [11/30], Batch [900/6000], Loss: 0.1795
Epoch [11/30], Batch [1000/6000], Loss: 0.2608
Epoch [11/30], Batch [1100/6000], Loss: 0.2024
Epoch [11/30], Batch [1200/6000], Loss: 0.2485
Epoch [11/30], Batch [1300/6000], Loss: 0.6579
Epoch [11/30], Batch [1400/6000], Loss: 0.1880
Epoch [11/30], Batch [1500/6000], Loss: 0.3528
Epoch [11/30], Batch [1600/6000], Loss: 0.2356
Epoch [11/30], Batch [1700/6000], Loss: 0.1448
Epoch [11/30], Batch [1800/6000], Loss: 0.2586
Epoch [11/30], Batch [1900/6000], Loss: 0.2945
Epoch [11/30], Batch [2000/6000], Loss: 0.3199
Epoch [11/30], Batch [2100/6000], Loss: 0.3350
Epoch [11/30], Batch [2200/6000], Loss: 0.6435
Epoch [11/30], Batch [2300/6000], Loss: 0.3622
Epoch [11/30], Batch [2400/6000], Loss: 1.6449
Epoch [11/30], Batch [2500/6000], Loss: 0.5764
Epoch [11/30], Batch [2600/6000], Loss: 0.1371
Epoch [11/30], Batch [2700/6000], Loss: 2.3004
Epoch [11/30], Batch [2800/6000], Loss: 0.8215
Epoch [11/30], Batch [2900/6000], Loss: 1.5215
Epoch [11/30], Batch [3000/6000], Loss: 0.1895
Epoch [11/30], Batch [3100/6000], Loss: 0.3974
Epoch [11/30], Batch [3200/6000], Loss: 0.2029
Epoch [11/30], Batch [3300/6000], Loss: 0.2128
Epoch [11/30], Batch [3400/6000], Loss: 0.2219
Epoch [11/30], Batch [3500/6000], Loss: 2.6057
Epoch [11/30], Batch [3600/6000], Loss: 0.1821
Epoch [11/30], Batch [3700/6000], Loss: 0.2063
Epoch [11/30], Batch [3800/6000], Loss: 0.1909
Epoch [11/30], Batch [3900/6000], Loss: 0.5246
Epoch [11/30], Batch [4000/6000], Loss: 0.1615
Epoch [11/30], Batch [4100/6000], Loss: 1.0064
Epoch [11/30], Batch [4200/6000], Loss: 2.5035
Epoch [11/30], Batch [4300/6000], Loss: 0.2006
Epoch [11/30], Batch [4400/6000], Loss: 0.2495
Epoch [11/30], Batch [4500/6000], Loss: 0.1702
Epoch [11/30], Batch [4600/6000], Loss: 0.2329
Epoch [11/30], Batch [4700/6000], Loss: 0.1615
Epoch [11/30], Batch [4800/6000], Loss: 0.3702
Epoch [11/30], Batch [4900/6000], Loss: 0.1658
Epoch [11/30], Batch [5000/6000], Loss: 0.1616
Epoch [11/30], Batch [5100/6000], Loss: 0.2745
Epoch [11/30], Batch [5200/6000], Loss: 1.7932
Epoch [11/30], Batch [5300/6000], Loss: 0.2055
Epoch [11/30], Batch [5400/6000], Loss: 0.2386
Epoch [11/30], Batch [5500/6000], Loss: 0.4388
Epoch [11/30], Batch [5600/6000], Loss: 0.7294
Epoch [11/30], Batch [5700/6000], Loss: 1.4028
Epoch [11/30], Batch [5800/6000], Loss: 0.1835
Epoch [11/30], Batch [5900/6000], Loss: 0.3568
Epoch [11/30], Loss: 0.4421
Epoch [12/30], Batch [0/6000], Loss: 0.3667
Epoch [12/30], Batch [100/6000], Loss: 0.1936
Epoch [12/30], Batch [200/6000], Loss: 0.7610
Epoch [12/30], Batch [300/6000], Loss: 0.3343
Epoch [12/30], Batch [400/6000], Loss: 0.2166
Epoch [12/30], Batch [500/6000], Loss: 0.1745
Epoch [12/30], Batch [600/6000], Loss: 0.2327
Epoch [12/30], Batch [700/6000], Loss: 0.5639
Epoch [12/30], Batch [800/6000], Loss: 1.4558
Epoch [12/30], Batch [900/6000], Loss: 0.2411
Epoch [12/30], Batch [1000/6000], Loss: 0.1599
Epoch [12/30], Batch [1100/6000], Loss: 0.1525
Epoch [12/30], Batch [1200/6000], Loss: 0.9089
Epoch [12/30], Batch [1300/6000], Loss: 0.1811
Epoch [12/30], Batch [1400/6000], Loss: 0.3286
Epoch [12/30], Batch [1500/6000], Loss: 0.2194
Epoch [12/30], Batch [1600/6000], Loss: 0.1774
Epoch [12/30], Batch [1700/6000], Loss: 0.3047
Epoch [12/30], Batch [1800/6000], Loss: 0.2793
Epoch [12/30], Batch [1900/6000], Loss: 0.1423
Epoch [12/30], Batch [2000/6000], Loss: 0.3573
Epoch [12/30], Batch [2100/6000], Loss: 0.1353
Epoch [12/30], Batch [2200/6000], Loss: 0.1657
Epoch [12/30], Batch [2300/6000], Loss: 0.1706
Epoch [12/30], Batch [2400/6000], Loss: 0.1472
Epoch [12/30], Batch [2500/6000], Loss: 0.1917
Epoch [12/30], Batch [2600/6000], Loss: 0.2127
Epoch [12/30], Batch [2700/6000], Loss: 0.1767
Epoch [12/30], Batch [2800/6000], Loss: 0.1727
Epoch [12/30], Batch [2900/6000], Loss: 0.5159
Epoch [12/30], Batch [3000/6000], Loss: 0.2983
Epoch [12/30], Batch [3100/6000], Loss: 0.9422
Epoch [12/30], Batch [3200/6000], Loss: 1.0183
Epoch [12/30], Batch [3300/6000], Loss: 0.6942
Epoch [12/30], Batch [3400/6000], Loss: 0.3202
Epoch [12/30], Batch [3500/6000], Loss: 0.1968
Epoch [12/30], Batch [3600/6000], Loss: 0.1408
Epoch [12/30], Batch [3700/6000], Loss: 0.1954
Epoch [12/30], Batch [3800/6000], Loss: 0.1753
Epoch [12/30], Batch [3900/6000], Loss: 2.1956
Epoch [12/30], Batch [4000/6000], Loss: 0.1960
Epoch [12/30], Batch [4100/6000], Loss: 1.6902
Epoch [12/30], Batch [4200/6000], Loss: 0.1561
Epoch [12/30], Batch [4300/6000], Loss: 0.4626
Epoch [12/30], Batch [4400/6000], Loss: 0.1515
Epoch [12/30], Batch [4500/6000], Loss: 0.2745
Epoch [12/30], Batch [4600/6000], Loss: 0.4894
Epoch [12/30], Batch [4700/6000], Loss: 0.1859
Epoch [12/30], Batch [4800/6000], Loss: 0.4122
Epoch [12/30], Batch [4900/6000], Loss: 0.2178
Epoch [12/30], Batch [5000/6000], Loss: 1.2498
Epoch [12/30], Batch [5100/6000], Loss: 0.3701
Epoch [12/30], Batch [5200/6000], Loss: 0.1511
Epoch [12/30], Batch [5300/6000], Loss: 0.2284
Epoch [12/30], Batch [5400/6000], Loss: 0.1996
Epoch [12/30], Batch [5500/6000], Loss: 0.7262
Epoch [12/30], Batch [5600/6000], Loss: 0.3531
Epoch [12/30], Batch [5700/6000], Loss: 0.2002
Epoch [12/30], Batch [5800/6000], Loss: 0.1607
Epoch [12/30], Batch [5900/6000], Loss: 0.3527
Epoch [12/30], Loss: 0.4140
Epoch [13/30], Batch [0/6000], Loss: 0.2510
Epoch [13/30], Batch [100/6000], Loss: 0.9103
Epoch [13/30], Batch [200/6000], Loss: 0.5076
Epoch [13/30], Batch [300/6000], Loss: 0.2490
Epoch [13/30], Batch [400/6000], Loss: 1.7728
Epoch [13/30], Batch [500/6000], Loss: 0.1930
Epoch [13/30], Batch [600/6000], Loss: 0.1892
Epoch [13/30], Batch [700/6000], Loss: 0.1915
Epoch [13/30], Batch [800/6000], Loss: 0.2004
Epoch [13/30], Batch [900/6000], Loss: 0.5703
Epoch [13/30], Batch [1000/6000], Loss: 0.2092
Epoch [13/30], Batch [1100/6000], Loss: 0.2304
Epoch [13/30], Batch [1200/6000], Loss: 1.8586
Epoch [13/30], Batch [1300/6000], Loss: 0.2906
Epoch [13/30], Batch [1400/6000], Loss: 0.1794
Epoch [13/30], Batch [1500/6000], Loss: 1.9780
Epoch [13/30], Batch [1600/6000], Loss: 0.6308
Epoch [13/30], Batch [1700/6000], Loss: 0.4621
Epoch [13/30], Batch [1800/6000], Loss: 0.5871
Epoch [13/30], Batch [1900/6000], Loss: 0.2141
Epoch [13/30], Batch [2000/6000], Loss: 0.1875
Epoch [13/30], Batch [2100/6000], Loss: 0.6675
Epoch [13/30], Batch [2200/6000], Loss: 0.8968
Epoch [13/30], Batch [2300/6000], Loss: 0.1780
Epoch [13/30], Batch [2400/6000], Loss: 0.2477
Epoch [13/30], Batch [2500/6000], Loss: 0.2181
Epoch [13/30], Batch [2600/6000], Loss: 0.4723
Epoch [13/30], Batch [2700/6000], Loss: 0.2240
Epoch [13/30], Batch [2800/6000], Loss: 0.2472
Epoch [13/30], Batch [2900/6000], Loss: 1.0398
Epoch [13/30], Batch [3000/6000], Loss: 0.4497
Epoch [13/30], Batch [3100/6000], Loss: 0.1790
Epoch [13/30], Batch [3200/6000], Loss: 0.1901
Epoch [13/30], Batch [3300/6000], Loss: 0.1925
Epoch [13/30], Batch [3400/6000], Loss: 0.1729
Epoch [13/30], Batch [3500/6000], Loss: 0.5045
Epoch [13/30], Batch [3600/6000], Loss: 0.1817
Epoch [13/30], Batch [3700/6000], Loss: 0.1827
Epoch [13/30], Batch [3800/6000], Loss: 0.2195
Epoch [13/30], Batch [3900/6000], Loss: 0.2305
Epoch [13/30], Batch [4000/6000], Loss: 2.2518
Epoch [13/30], Batch [4100/6000], Loss: 1.8964
Epoch [13/30], Batch [4200/6000], Loss: 0.1578
Epoch [13/30], Batch [4300/6000], Loss: 0.1899
Epoch [13/30], Batch [4400/6000], Loss: 0.1886
Epoch [13/30], Batch [4500/6000], Loss: 0.1477
Epoch [13/30], Batch [4600/6000], Loss: 0.1875
Epoch [13/30], Batch [4700/6000], Loss: 0.1849
Epoch [13/30], Batch [4800/6000], Loss: 0.3184
Epoch [13/30], Batch [4900/6000], Loss: 0.1901
Epoch [13/30], Batch [5000/6000], Loss: 0.1697
Epoch [13/30], Batch [5100/6000], Loss: 0.2104
Epoch [13/30], Batch [5200/6000], Loss: 0.6144
Epoch [13/30], Batch [5300/6000], Loss: 0.9360
Epoch [13/30], Batch [5400/6000], Loss: 0.1748
Epoch [13/30], Batch [5500/6000], Loss: 0.1763
Epoch [13/30], Batch [5600/6000], Loss: 0.1736
Epoch [13/30], Batch [5700/6000], Loss: 0.2158
Epoch [13/30], Batch [5800/6000], Loss: 0.4638
Epoch [13/30], Batch [5900/6000], Loss: 0.2334
Epoch [13/30], Loss: 0.3909
Epoch [14/30], Batch [0/6000], Loss: 0.1776
Epoch [14/30], Batch [100/6000], Loss: 0.1764
Epoch [14/30], Batch [200/6000], Loss: 0.3089
Epoch [14/30], Batch [300/6000], Loss: 0.1998
Epoch [14/30], Batch [400/6000], Loss: 0.1993
Epoch [14/30], Batch [500/6000], Loss: 0.2006
Epoch [14/30], Batch [600/6000], Loss: 0.1802
Epoch [14/30], Batch [700/6000], Loss: 0.9550
Epoch [14/30], Batch [800/6000], Loss: 0.1592
Epoch [14/30], Batch [900/6000], Loss: 0.1920
Epoch [14/30], Batch [1000/6000], Loss: 0.1779
Epoch [14/30], Batch [1100/6000], Loss: 0.1606
Epoch [14/30], Batch [1200/6000], Loss: 0.1964
Epoch [14/30], Batch [1300/6000], Loss: 0.1863
Epoch [14/30], Batch [1400/6000], Loss: 0.1883
Epoch [14/30], Batch [1500/6000], Loss: 0.1589
Epoch [14/30], Batch [1600/6000], Loss: 0.1850
Epoch [14/30], Batch [1700/6000], Loss: 0.2183
Epoch [14/30], Batch [1800/6000], Loss: 0.1816
Epoch [14/30], Batch [1900/6000], Loss: 1.2292
Epoch [14/30], Batch [2000/6000], Loss: 0.1617
Epoch [14/30], Batch [2100/6000], Loss: 0.9112
Epoch [14/30], Batch [2200/6000], Loss: 0.1812
Epoch [14/30], Batch [2300/6000], Loss: 1.0778
Epoch [14/30], Batch [2400/6000], Loss: 0.3008
Epoch [14/30], Batch [2500/6000], Loss: 0.6473
Epoch [14/30], Batch [2600/6000], Loss: 1.0122
Epoch [14/30], Batch [2700/6000], Loss: 0.1679
Epoch [14/30], Batch [2800/6000], Loss: 0.1689
Epoch [14/30], Batch [2900/6000], Loss: 0.2403
Epoch [14/30], Batch [3000/6000], Loss: 0.1581
Epoch [14/30], Batch [3100/6000], Loss: 0.2114
Epoch [14/30], Batch [3200/6000], Loss: 0.1594
Epoch [14/30], Batch [3300/6000], Loss: 0.1697
Epoch [14/30], Batch [3400/6000], Loss: 0.1776
Epoch [14/30], Batch [3500/6000], Loss: 0.1984
Epoch [14/30], Batch [3600/6000], Loss: 1.7551
Epoch [14/30], Batch [3700/6000], Loss: 0.1664
Epoch [14/30], Batch [3800/6000], Loss: 0.2094
Epoch [14/30], Batch [3900/6000], Loss: 0.1926
Epoch [14/30], Batch [4000/6000], Loss: 0.3877
Epoch [14/30], Batch [4100/6000], Loss: 0.1960
Epoch [14/30], Batch [4200/6000], Loss: 1.2891
Epoch [14/30], Batch [4300/6000], Loss: 0.1997
Epoch [14/30], Batch [4400/6000], Loss: 0.9009
Epoch [14/30], Batch [4500/6000], Loss: 0.1844
Epoch [14/30], Batch [4600/6000], Loss: 0.1573
Epoch [14/30], Batch [4700/6000], Loss: 0.1351
Epoch [14/30], Batch [4800/6000], Loss: 0.9911
Epoch [14/30], Batch [4900/6000], Loss: 0.1644
Epoch [14/30], Batch [5000/6000], Loss: 0.1421
Epoch [14/30], Batch [5100/6000], Loss: 0.3866
Epoch [14/30], Batch [5200/6000], Loss: 0.1851
Epoch [14/30], Batch [5300/6000], Loss: 0.2064
Epoch [14/30], Batch [5400/6000], Loss: 0.1890
Epoch [14/30], Batch [5500/6000], Loss: 0.2109
Epoch [14/30], Batch [5600/6000], Loss: 0.1459
Epoch [14/30], Batch [5700/6000], Loss: 1.2075
Epoch [14/30], Batch [5800/6000], Loss: 0.6142
Epoch [14/30], Batch [5900/6000], Loss: 0.1944
Epoch [14/30], Loss: 0.3740
Epoch [15/30], Batch [0/6000], Loss: 0.2057
Epoch [15/30], Batch [100/6000], Loss: 0.1542
Epoch [15/30], Batch [200/6000], Loss: 0.1639
Epoch [15/30], Batch [300/6000], Loss: 0.2313
Epoch [15/30], Batch [400/6000], Loss: 0.2014
Epoch [15/30], Batch [500/6000], Loss: 1.4563
Epoch [15/30], Batch [600/6000], Loss: 0.1604
Epoch [15/30], Batch [700/6000], Loss: 0.2258
Epoch [15/30], Batch [800/6000], Loss: 0.1789
Epoch [15/30], Batch [900/6000], Loss: 0.1733
Epoch [15/30], Batch [1000/6000], Loss: 0.2117
Epoch [15/30], Batch [1100/6000], Loss: 0.2175
Epoch [15/30], Batch [1200/6000], Loss: 0.2245
Epoch [15/30], Batch [1300/6000], Loss: 0.1518
Epoch [15/30], Batch [1400/6000], Loss: 0.1801
Epoch [15/30], Batch [1500/6000], Loss: 0.4058
Epoch [15/30], Batch [1600/6000], Loss: 0.1425
Epoch [15/30], Batch [1700/6000], Loss: 0.2523
Epoch [15/30], Batch [1800/6000], Loss: 0.4389
Epoch [15/30], Batch [1900/6000], Loss: 0.2556
Epoch [15/30], Batch [2000/6000], Loss: 1.0229
Epoch [15/30], Batch [2100/6000], Loss: 0.1845
Epoch [15/30], Batch [2200/6000], Loss: 0.2789
Epoch [15/30], Batch [2300/6000], Loss: 0.1600
Epoch [15/30], Batch [2400/6000], Loss: 0.1437
Epoch [15/30], Batch [2500/6000], Loss: 0.2112
Epoch [15/30], Batch [2600/6000], Loss: 0.2265
Epoch [15/30], Batch [2700/6000], Loss: 0.1746
Epoch [15/30], Batch [2800/6000], Loss: 0.1733
Epoch [15/30], Batch [2900/6000], Loss: 0.3178
Epoch [15/30], Batch [3000/6000], Loss: 0.1593
Epoch [15/30], Batch [3100/6000], Loss: 0.4140
Epoch [15/30], Batch [3200/6000], Loss: 0.1647
Epoch [15/30], Batch [3300/6000], Loss: 0.1990
Epoch [15/30], Batch [3400/6000], Loss: 0.4863
Epoch [15/30], Batch [3500/6000], Loss: 0.6886
Epoch [15/30], Batch [3600/6000], Loss: 1.9217
Epoch [15/30], Batch [3700/6000], Loss: 0.1577
Epoch [15/30], Batch [3800/6000], Loss: 0.1480
Epoch [15/30], Batch [3900/6000], Loss: 0.1656
Epoch [15/30], Batch [4000/6000], Loss: 0.3576
Epoch [15/30], Batch [4100/6000], Loss: 0.1705
Epoch [15/30], Batch [4200/6000], Loss: 0.9187
Epoch [15/30], Batch [4300/6000], Loss: 0.1850
Epoch [15/30], Batch [4400/6000], Loss: 0.6543
Epoch [15/30], Batch [4500/6000], Loss: 0.1887
Epoch [15/30], Batch [4600/6000], Loss: 0.1662
Epoch [15/30], Batch [4700/6000], Loss: 0.2066
Epoch [15/30], Batch [4800/6000], Loss: 0.2005
Epoch [15/30], Batch [4900/6000], Loss: 0.1671
Epoch [15/30], Batch [5000/6000], Loss: 0.1436
Epoch [15/30], Batch [5100/6000], Loss: 0.3249
Epoch [15/30], Batch [5200/6000], Loss: 0.2281
Epoch [15/30], Batch [5300/6000], Loss: 0.1889
Epoch [15/30], Batch [5400/6000], Loss: 0.1767
Epoch [15/30], Batch [5500/6000], Loss: 0.1674
Epoch [15/30], Batch [5600/6000], Loss: 0.2432
Epoch [15/30], Batch [5700/6000], Loss: 0.1434
Epoch [15/30], Batch [5800/6000], Loss: 0.3721
Epoch [15/30], Batch [5900/6000], Loss: 0.2284
Epoch [15/30], Loss: 0.3534
Epoch [16/30], Batch [0/6000], Loss: 0.1781
Epoch [16/30], Batch [100/6000], Loss: 0.1953
Epoch [16/30], Batch [200/6000], Loss: 0.2139
Epoch [16/30], Batch [300/6000], Loss: 0.2369
Epoch [16/30], Batch [400/6000], Loss: 0.1635
Epoch [16/30], Batch [500/6000], Loss: 1.1996
Epoch [16/30], Batch [600/6000], Loss: 0.1622
Epoch [16/30], Batch [700/6000], Loss: 0.1711
Epoch [16/30], Batch [800/6000], Loss: 0.1528
Epoch [16/30], Batch [900/6000], Loss: 0.1508
Epoch [16/30], Batch [1000/6000], Loss: 0.1725
Epoch [16/30], Batch [1100/6000], Loss: 0.1421
Epoch [16/30], Batch [1200/6000], Loss: 0.1940
Epoch [16/30], Batch [1300/6000], Loss: 0.2210
Epoch [16/30], Batch [1400/6000], Loss: 0.2578
Epoch [16/30], Batch [1500/6000], Loss: 0.1627
Epoch [16/30], Batch [1600/6000], Loss: 0.1840
Epoch [16/30], Batch [1700/6000], Loss: 0.1857
Epoch [16/30], Batch [1800/6000], Loss: 0.1500
Epoch [16/30], Batch [1900/6000], Loss: 0.1656
Epoch [16/30], Batch [2000/6000], Loss: 0.9034
Epoch [16/30], Batch [2100/6000], Loss: 0.2226
Epoch [16/30], Batch [2200/6000], Loss: 0.2102
Epoch [16/30], Batch [2300/6000], Loss: 0.2031
Epoch [16/30], Batch [2400/6000], Loss: 0.1689
Epoch [16/30], Batch [2500/6000], Loss: 1.6638
Epoch [16/30], Batch [2600/6000], Loss: 1.4511
Epoch [16/30], Batch [2700/6000], Loss: 0.1976
Epoch [16/30], Batch [2800/6000], Loss: 0.1798
Epoch [16/30], Batch [2900/6000], Loss: 0.1403
Epoch [16/30], Batch [3000/6000], Loss: 0.1784
Epoch [16/30], Batch [3100/6000], Loss: 0.1587
Epoch [16/30], Batch [3200/6000], Loss: 0.2021
Epoch [16/30], Batch [3300/6000], Loss: 0.1943
Epoch [16/30], Batch [3400/6000], Loss: 0.1958
Epoch [16/30], Batch [3500/6000], Loss: 0.1512
Epoch [16/30], Batch [3600/6000], Loss: 2.3239
Epoch [16/30], Batch [3700/6000], Loss: 0.2028
Epoch [16/30], Batch [3800/6000], Loss: 0.1810
Epoch [16/30], Batch [3900/6000], Loss: 0.1612
Epoch [16/30], Batch [4000/6000], Loss: 0.1516
Epoch [16/30], Batch [4100/6000], Loss: 0.1592
Epoch [16/30], Batch [4200/6000], Loss: 0.1538
Epoch [16/30], Batch [4300/6000], Loss: 0.9691
Epoch [16/30], Batch [4400/6000], Loss: 0.1722
Epoch [16/30], Batch [4500/6000], Loss: 0.3247
Epoch [16/30], Batch [4600/6000], Loss: 0.1432
Epoch [16/30], Batch [4700/6000], Loss: 0.1755
Epoch [16/30], Batch [4800/6000], Loss: 0.2324
Epoch [16/30], Batch [4900/6000], Loss: 0.5039
Epoch [16/30], Batch [5000/6000], Loss: 0.2008
Epoch [16/30], Batch [5100/6000], Loss: 1.5026
Epoch [16/30], Batch [5200/6000], Loss: 0.1685
Epoch [16/30], Batch [5300/6000], Loss: 0.1313
Epoch [16/30], Batch [5400/6000], Loss: 0.1434
Epoch [16/30], Batch [5500/6000], Loss: 0.1334
Epoch [16/30], Batch [5600/6000], Loss: 0.1672
Epoch [16/30], Batch [5700/6000], Loss: 0.3884
Epoch [16/30], Batch [5800/6000], Loss: 0.1630
Epoch [16/30], Batch [5900/6000], Loss: 0.1586
Epoch [16/30], Loss: 0.3354
Epoch [17/30], Batch [0/6000], Loss: 0.1412
Epoch [17/30], Batch [100/6000], Loss: 0.1536
Epoch [17/30], Batch [200/6000], Loss: 0.2434
Epoch [17/30], Batch [300/6000], Loss: 0.1670
Epoch [17/30], Batch [400/6000], Loss: 0.2263
Epoch [17/30], Batch [500/6000], Loss: 0.1561
Epoch [17/30], Batch [600/6000], Loss: 0.1835
Epoch [17/30], Batch [700/6000], Loss: 0.1521
Epoch [17/30], Batch [800/6000], Loss: 0.1393
Epoch [17/30], Batch [900/6000], Loss: 0.2478
Epoch [17/30], Batch [1000/6000], Loss: 0.2212
Epoch [17/30], Batch [1100/6000], Loss: 0.1566
Epoch [17/30], Batch [1200/6000], Loss: 0.1605
Epoch [17/30], Batch [1300/6000], Loss: 0.2763
Epoch [17/30], Batch [1400/6000], Loss: 0.2598
Epoch [17/30], Batch [1500/6000], Loss: 0.1697
Epoch [17/30], Batch [1600/6000], Loss: 0.2868
Epoch [17/30], Batch [1700/6000], Loss: 0.2746
Epoch [17/30], Batch [1800/6000], Loss: 0.1617
Epoch [17/30], Batch [1900/6000], Loss: 0.1996
Epoch [17/30], Batch [2000/6000], Loss: 0.2052
Epoch [17/30], Batch [2100/6000], Loss: 0.2118
Epoch [17/30], Batch [2200/6000], Loss: 0.1937
Epoch [17/30], Batch [2300/6000], Loss: 0.1712
Epoch [17/30], Batch [2400/6000], Loss: 0.1753
Epoch [17/30], Batch [2500/6000], Loss: 0.1619
Epoch [17/30], Batch [2600/6000], Loss: 0.3941
Epoch [17/30], Batch [2700/6000], Loss: 0.1592
Epoch [17/30], Batch [2800/6000], Loss: 0.1644
Epoch [17/30], Batch [2900/6000], Loss: 0.1296
Epoch [17/30], Batch [3000/6000], Loss: 0.1423
Epoch [17/30], Batch [3100/6000], Loss: 0.7319
Epoch [17/30], Batch [3200/6000], Loss: 0.2035
Epoch [17/30], Batch [3300/6000], Loss: 0.2322
Epoch [17/30], Batch [3400/6000], Loss: 0.2023
Epoch [17/30], Batch [3500/6000], Loss: 0.1692
Epoch [17/30], Batch [3600/6000], Loss: 0.8295
Epoch [17/30], Batch [3700/6000], Loss: 0.1786
Epoch [17/30], Batch [3800/6000], Loss: 0.3506
Epoch [17/30], Batch [3900/6000], Loss: 0.1717
Epoch [17/30], Batch [4000/6000], Loss: 0.1709
Epoch [17/30], Batch [4100/6000], Loss: 0.5946
Epoch [17/30], Batch [4200/6000], Loss: 0.1500
Epoch [17/30], Batch [4300/6000], Loss: 0.5425
Epoch [17/30], Batch [4400/6000], Loss: 0.1815
Epoch [17/30], Batch [4500/6000], Loss: 0.1671
Epoch [17/30], Batch [4600/6000], Loss: 0.4883
Epoch [17/30], Batch [4700/6000], Loss: 0.1911
Epoch [17/30], Batch [4800/6000], Loss: 0.2188
Epoch [17/30], Batch [4900/6000], Loss: 0.4990
Epoch [17/30], Batch [5000/6000], Loss: 0.1946
Epoch [17/30], Batch [5100/6000], Loss: 0.1880
Epoch [17/30], Batch [5200/6000], Loss: 0.1803
Epoch [17/30], Batch [5300/6000], Loss: 0.1856
Epoch [17/30], Batch [5400/6000], Loss: 0.2161
Epoch [17/30], Batch [5500/6000], Loss: 0.1845
Epoch [17/30], Batch [5600/6000], Loss: 0.8528
Epoch [17/30], Batch [5700/6000], Loss: 0.1761
Epoch [17/30], Batch [5800/6000], Loss: 0.1544
Epoch [17/30], Batch [5900/6000], Loss: 2.6953
Epoch [17/30], Loss: 0.3230
Epoch [18/30], Batch [0/6000], Loss: 0.1462
Epoch [18/30], Batch [100/6000], Loss: 0.1971
Epoch [18/30], Batch [200/6000], Loss: 0.1584
Epoch [18/30], Batch [300/6000], Loss: 1.9505
Epoch [18/30], Batch [400/6000], Loss: 0.2001
Epoch [18/30], Batch [500/6000], Loss: 0.1449
Epoch [18/30], Batch [600/6000], Loss: 0.1618
Epoch [18/30], Batch [700/6000], Loss: 0.1679
Epoch [18/30], Batch [800/6000], Loss: 0.1631
Epoch [18/30], Batch [900/6000], Loss: 0.2300
Epoch [18/30], Batch [1000/6000], Loss: 0.1621
Epoch [18/30], Batch [1100/6000], Loss: 0.2042
Epoch [18/30], Batch [1200/6000], Loss: 0.6483
Epoch [18/30], Batch [1300/6000], Loss: 0.1416
Epoch [18/30], Batch [1400/6000], Loss: 0.1560
Epoch [18/30], Batch [1500/6000], Loss: 0.1358
Epoch [18/30], Batch [1600/6000], Loss: 0.2034
Epoch [18/30], Batch [1700/6000], Loss: 0.1488
Epoch [18/30], Batch [1800/6000], Loss: 0.1552
Epoch [18/30], Batch [1900/6000], Loss: 0.2009
Epoch [18/30], Batch [2000/6000], Loss: 0.3391
Epoch [18/30], Batch [2100/6000], Loss: 0.1560
Epoch [18/30], Batch [2200/6000], Loss: 1.2290
Epoch [18/30], Batch [2300/6000], Loss: 0.1162
Epoch [18/30], Batch [2400/6000], Loss: 0.1437
Epoch [18/30], Batch [2500/6000], Loss: 0.1619
Epoch [18/30], Batch [2600/6000], Loss: 0.1688
Epoch [18/30], Batch [2700/6000], Loss: 1.5574
Epoch [18/30], Batch [2800/6000], Loss: 0.1762
Epoch [18/30], Batch [2900/6000], Loss: 0.1753
Epoch [18/30], Batch [3000/6000], Loss: 0.1590
Epoch [18/30], Batch [3100/6000], Loss: 0.1612
Epoch [18/30], Batch [3200/6000], Loss: 0.1705
Epoch [18/30], Batch [3300/6000], Loss: 0.1736
Epoch [18/30], Batch [3400/6000], Loss: 0.1722
Epoch [18/30], Batch [3500/6000], Loss: 0.1460
Epoch [18/30], Batch [3600/6000], Loss: 0.1618
Epoch [18/30], Batch [3700/6000], Loss: 0.2797
Epoch [18/30], Batch [3800/6000], Loss: 0.1331
Epoch [18/30], Batch [3900/6000], Loss: 0.1245
Epoch [18/30], Batch [4000/6000], Loss: 1.9824
Epoch [18/30], Batch [4100/6000], Loss: 0.1353
Epoch [18/30], Batch [4200/6000], Loss: 0.1401
Epoch [18/30], Batch [4300/6000], Loss: 0.1619
Epoch [18/30], Batch [4400/6000], Loss: 0.4647
Epoch [18/30], Batch [4500/6000], Loss: 0.1533
Epoch [18/30], Batch [4600/6000], Loss: 0.1870
Epoch [18/30], Batch [4700/6000], Loss: 0.1776
Epoch [18/30], Batch [4800/6000], Loss: 0.1670
Epoch [18/30], Batch [4900/6000], Loss: 0.1998
Epoch [18/30], Batch [5000/6000], Loss: 1.2167
Epoch [18/30], Batch [5100/6000], Loss: 0.1702
Epoch [18/30], Batch [5200/6000], Loss: 0.8970
Epoch [18/30], Batch [5300/6000], Loss: 0.2956
Epoch [18/30], Batch [5400/6000], Loss: 0.1769
Epoch [18/30], Batch [5500/6000], Loss: 0.1718
Epoch [18/30], Batch [5600/6000], Loss: 0.1431
Epoch [18/30], Batch [5700/6000], Loss: 0.1781
Epoch [18/30], Batch [5800/6000], Loss: 0.1640
Epoch [18/30], Batch [5900/6000], Loss: 0.1638
Epoch [18/30], Loss: 0.3085
Epoch [19/30], Batch [0/6000], Loss: 0.2017
Epoch [19/30], Batch [100/6000], Loss: 0.1582
Epoch [19/30], Batch [200/6000], Loss: 0.2084
Epoch [19/30], Batch [300/6000], Loss: 0.1540
Epoch [19/30], Batch [400/6000], Loss: 0.1353
Epoch [19/30], Batch [500/6000], Loss: 0.1611
Epoch [19/30], Batch [600/6000], Loss: 0.1512
Epoch [19/30], Batch [700/6000], Loss: 0.9007
Epoch [19/30], Batch [800/6000], Loss: 0.1497
Epoch [19/30], Batch [900/6000], Loss: 0.2082
Epoch [19/30], Batch [1000/6000], Loss: 0.2040
Epoch [19/30], Batch [1100/6000], Loss: 0.1389
Epoch [19/30], Batch [1200/6000], Loss: 0.2069
Epoch [19/30], Batch [1300/6000], Loss: 0.1956
Epoch [19/30], Batch [1400/6000], Loss: 0.1865
Epoch [19/30], Batch [1500/6000], Loss: 0.1575
Epoch [19/30], Batch [1600/6000], Loss: 0.1766
Epoch [19/30], Batch [1700/6000], Loss: 0.1831
Epoch [19/30], Batch [1800/6000], Loss: 1.2030
Epoch [19/30], Batch [1900/6000], Loss: 1.4975
Epoch [19/30], Batch [2000/6000], Loss: 0.1448
Epoch [19/30], Batch [2100/6000], Loss: 0.1854
Epoch [19/30], Batch [2200/6000], Loss: 0.1794
Epoch [19/30], Batch [2300/6000], Loss: 0.1753
Epoch [19/30], Batch [2400/6000], Loss: 0.1586
Epoch [19/30], Batch [2500/6000], Loss: 0.1560
Epoch [19/30], Batch [2600/6000], Loss: 0.1704
Epoch [19/30], Batch [2700/6000], Loss: 0.4283
Epoch [19/30], Batch [2800/6000], Loss: 0.2613
Epoch [19/30], Batch [2900/6000], Loss: 0.1395
Epoch [19/30], Batch [3000/6000], Loss: 0.1814
Epoch [19/30], Batch [3100/6000], Loss: 0.1951
Epoch [19/30], Batch [3200/6000], Loss: 0.1816
Epoch [19/30], Batch [3300/6000], Loss: 1.0389
Epoch [19/30], Batch [3400/6000], Loss: 0.1985
Epoch [19/30], Batch [3500/6000], Loss: 0.1627
Epoch [19/30], Batch [3600/6000], Loss: 1.4451
Epoch [19/30], Batch [3700/6000], Loss: 0.1652
Epoch [19/30], Batch [3800/6000], Loss: 0.1931
Epoch [19/30], Batch [3900/6000], Loss: 0.2790
Epoch [19/30], Batch [4000/6000], Loss: 0.2121
Epoch [19/30], Batch [4100/6000], Loss: 0.2075
Epoch [19/30], Batch [4200/6000], Loss: 0.1579
Epoch [19/30], Batch [4300/6000], Loss: 0.1635
Epoch [19/30], Batch [4400/6000], Loss: 0.1323
Epoch [19/30], Batch [4500/6000], Loss: 0.2856
Epoch [19/30], Batch [4600/6000], Loss: 0.1904
Epoch [19/30], Batch [4700/6000], Loss: 0.2133
Epoch [19/30], Batch [4800/6000], Loss: 0.1592
Epoch [19/30], Batch [4900/6000], Loss: 0.2968
Epoch [19/30], Batch [5000/6000], Loss: 0.1727
Epoch [19/30], Batch [5100/6000], Loss: 0.1665
Epoch [19/30], Batch [5200/6000], Loss: 0.1751
Epoch [19/30], Batch [5300/6000], Loss: 0.1307
Epoch [19/30], Batch [5400/6000], Loss: 0.1203
Epoch [19/30], Batch [5500/6000], Loss: 0.1670
Epoch [19/30], Batch [5600/6000], Loss: 0.1527
Epoch [19/30], Batch [5700/6000], Loss: 0.1891
Epoch [19/30], Batch [5800/6000], Loss: 0.1593
Epoch [19/30], Batch [5900/6000], Loss: 0.2039
Epoch [19/30], Loss: 0.3008
Epoch [20/30], Batch [0/6000], Loss: 0.1431
Epoch [20/30], Batch [100/6000], Loss: 1.0421
Epoch [20/30], Batch [200/6000], Loss: 0.1637
Epoch [20/30], Batch [300/6000], Loss: 0.1287
Epoch [20/30], Batch [400/6000], Loss: 0.1874
Epoch [20/30], Batch [500/6000], Loss: 0.1754
Epoch [20/30], Batch [600/6000], Loss: 0.1685
Epoch [20/30], Batch [700/6000], Loss: 0.1949
Epoch [20/30], Batch [800/6000], Loss: 0.1461
Epoch [20/30], Batch [900/6000], Loss: 0.1826
Epoch [20/30], Batch [1000/6000], Loss: 0.4074
Epoch [20/30], Batch [1100/6000], Loss: 0.1593
Epoch [20/30], Batch [1200/6000], Loss: 0.1502
Epoch [20/30], Batch [1300/6000], Loss: 0.2168
Epoch [20/30], Batch [1400/6000], Loss: 0.1616
Epoch [20/30], Batch [1500/6000], Loss: 0.1592
Epoch [20/30], Batch [1600/6000], Loss: 0.1725
Epoch [20/30], Batch [1700/6000], Loss: 0.1693
Epoch [20/30], Batch [1800/6000], Loss: 0.1408
Epoch [20/30], Batch [1900/6000], Loss: 0.2084
Epoch [20/30], Batch [2000/6000], Loss: 0.1503
Epoch [20/30], Batch [2100/6000], Loss: 0.1402
Epoch [20/30], Batch [2200/6000], Loss: 1.2026
Epoch [20/30], Batch [2300/6000], Loss: 0.1569
Epoch [20/30], Batch [2400/6000], Loss: 1.8627
Epoch [20/30], Batch [2500/6000], Loss: 0.1666
Epoch [20/30], Batch [2600/6000], Loss: 0.1728
Epoch [20/30], Batch [2700/6000], Loss: 0.2209
Epoch [20/30], Batch [2800/6000], Loss: 0.1804
Epoch [20/30], Batch [2900/6000], Loss: 0.1565
Epoch [20/30], Batch [3000/6000], Loss: 0.1529
Epoch [20/30], Batch [3100/6000], Loss: 0.1555
Epoch [20/30], Batch [3200/6000], Loss: 0.1590
Epoch [20/30], Batch [3300/6000], Loss: 0.1760
Epoch [20/30], Batch [3400/6000], Loss: 0.1501
Epoch [20/30], Batch [3500/6000], Loss: 0.6317
Epoch [20/30], Batch [3600/6000], Loss: 0.1962
Epoch [20/30], Batch [3700/6000], Loss: 0.1732
Epoch [20/30], Batch [3800/6000], Loss: 0.1446
Epoch [20/30], Batch [3900/6000], Loss: 0.1536
Epoch [20/30], Batch [4000/6000], Loss: 0.3974
Epoch [20/30], Batch [4100/6000], Loss: 0.4083
Epoch [20/30], Batch [4200/6000], Loss: 0.1553
Epoch [20/30], Batch [4300/6000], Loss: 1.5262
Epoch [20/30], Batch [4400/6000], Loss: 0.4677
Epoch [20/30], Batch [4500/6000], Loss: 3.0799
Epoch [20/30], Batch [4600/6000], Loss: 0.1733
Epoch [20/30], Batch [4700/6000], Loss: 0.1649
Epoch [20/30], Batch [4800/6000], Loss: 0.1717
Epoch [20/30], Batch [4900/6000], Loss: 0.6281
Epoch [20/30], Batch [5000/6000], Loss: 0.7277
Epoch [20/30], Batch [5100/6000], Loss: 2.4707
Epoch [20/30], Batch [5200/6000], Loss: 0.1450
Epoch [20/30], Batch [5300/6000], Loss: 0.7750
Epoch [20/30], Batch [5400/6000], Loss: 0.2714
Epoch [20/30], Batch [5500/6000], Loss: 0.1376
Epoch [20/30], Batch [5600/6000], Loss: 0.1591
Epoch [20/30], Batch [5700/6000], Loss: 0.1588
Epoch [20/30], Batch [5800/6000], Loss: 0.1901
Epoch [20/30], Batch [5900/6000], Loss: 0.1812
Epoch [20/30], Loss: 0.2860
Epoch [21/30], Batch [0/6000], Loss: 0.7517
Epoch [21/30], Batch [100/6000], Loss: 0.1458
Epoch [21/30], Batch [200/6000], Loss: 0.8049
Epoch [21/30], Batch [300/6000], Loss: 0.1743
Epoch [21/30], Batch [400/6000], Loss: 0.1503
Epoch [21/30], Batch [500/6000], Loss: 0.2891
Epoch [21/30], Batch [600/6000], Loss: 0.1774
Epoch [21/30], Batch [700/6000], Loss: 0.1453
Epoch [21/30], Batch [800/6000], Loss: 0.1545
Epoch [21/30], Batch [900/6000], Loss: 0.1608
Epoch [21/30], Batch [1000/6000], Loss: 0.2049
Epoch [21/30], Batch [1100/6000], Loss: 0.8840
Epoch [21/30], Batch [1200/6000], Loss: 0.1562
Epoch [21/30], Batch [1300/6000], Loss: 0.3764
Epoch [21/30], Batch [1400/6000], Loss: 0.1504
Epoch [21/30], Batch [1500/6000], Loss: 0.2375
Epoch [21/30], Batch [1600/6000], Loss: 0.1789
Epoch [21/30], Batch [1700/6000], Loss: 0.1580
Epoch [21/30], Batch [1800/6000], Loss: 0.1645
Epoch [21/30], Batch [1900/6000], Loss: 0.1479
Epoch [21/30], Batch [2000/6000], Loss: 2.6650
Epoch [21/30], Batch [2100/6000], Loss: 0.1677
Epoch [21/30], Batch [2200/6000], Loss: 0.1566
Epoch [21/30], Batch [2300/6000], Loss: 0.5517
Epoch [21/30], Batch [2400/6000], Loss: 0.1582
Epoch [21/30], Batch [2500/6000], Loss: 0.1501
Epoch [21/30], Batch [2600/6000], Loss: 0.1531
Epoch [21/30], Batch [2700/6000], Loss: 0.1525
Epoch [21/30], Batch [2800/6000], Loss: 0.2004
Epoch [21/30], Batch [2900/6000], Loss: 0.2854
Epoch [21/30], Batch [3000/6000], Loss: 0.3610
Epoch [21/30], Batch [3100/6000], Loss: 0.1742
Epoch [21/30], Batch [3200/6000], Loss: 0.1813
Epoch [21/30], Batch [3300/6000], Loss: 0.1307
Epoch [21/30], Batch [3400/6000], Loss: 0.1771
Epoch [21/30], Batch [3500/6000], Loss: 0.2329
Epoch [21/30], Batch [3600/6000], Loss: 0.1984
Epoch [21/30], Batch [3700/6000], Loss: 0.2185
Epoch [21/30], Batch [3800/6000], Loss: 0.3685
Epoch [21/30], Batch [3900/6000], Loss: 0.1716
Epoch [21/30], Batch [4000/6000], Loss: 0.5897
Epoch [21/30], Batch [4100/6000], Loss: 0.1827
Epoch [21/30], Batch [4200/6000], Loss: 0.1722
Epoch [21/30], Batch [4300/6000], Loss: 0.1969
Epoch [21/30], Batch [4400/6000], Loss: 0.1869
Epoch [21/30], Batch [4500/6000], Loss: 0.1749
Epoch [21/30], Batch [4600/6000], Loss: 0.1955
Epoch [21/30], Batch [4700/6000], Loss: 1.3715
Epoch [21/30], Batch [4800/6000], Loss: 0.1337
Epoch [21/30], Batch [4900/6000], Loss: 0.1679
Epoch [21/30], Batch [5000/6000], Loss: 0.4304
Epoch [21/30], Batch [5100/6000], Loss: 0.1463
Epoch [21/30], Batch [5200/6000], Loss: 0.1463
Epoch [21/30], Batch [5300/6000], Loss: 0.1496
Epoch [21/30], Batch [5400/6000], Loss: 0.1336
Epoch [21/30], Batch [5500/6000], Loss: 0.1623
Epoch [21/30], Batch [5600/6000], Loss: 0.1478
Epoch [21/30], Batch [5700/6000], Loss: 0.1615
Epoch [21/30], Batch [5800/6000], Loss: 0.1672
Epoch [21/30], Batch [5900/6000], Loss: 0.5451
Epoch [21/30], Loss: 0.2750
Epoch [22/30], Batch [0/6000], Loss: 0.1513
Epoch [22/30], Batch [100/6000], Loss: 0.2669
Epoch [22/30], Batch [200/6000], Loss: 0.1509
Epoch [22/30], Batch [300/6000], Loss: 0.1534
Epoch [22/30], Batch [400/6000], Loss: 0.2512
Epoch [22/30], Batch [500/6000], Loss: 0.1538
Epoch [22/30], Batch [600/6000], Loss: 0.2474
Epoch [22/30], Batch [700/6000], Loss: 0.1504
Epoch [22/30], Batch [800/6000], Loss: 0.2940
Epoch [22/30], Batch [900/6000], Loss: 0.9048
Epoch [22/30], Batch [1000/6000], Loss: 0.1433
Epoch [22/30], Batch [1100/6000], Loss: 0.1501
Epoch [22/30], Batch [1200/6000], Loss: 0.1423
Epoch [22/30], Batch [1300/6000], Loss: 0.1681
Epoch [22/30], Batch [1400/6000], Loss: 0.1587
Epoch [22/30], Batch [1500/6000], Loss: 0.1686
Epoch [22/30], Batch [1600/6000], Loss: 0.1676
Epoch [22/30], Batch [1700/6000], Loss: 0.1670
Epoch [22/30], Batch [1800/6000], Loss: 0.1771
Epoch [22/30], Batch [1900/6000], Loss: 0.1549
Epoch [22/30], Batch [2000/6000], Loss: 0.1477
Epoch [22/30], Batch [2100/6000], Loss: 0.2343
Epoch [22/30], Batch [2200/6000], Loss: 0.1644
Epoch [22/30], Batch [2300/6000], Loss: 0.1916
Epoch [22/30], Batch [2400/6000], Loss: 0.1581
Epoch [22/30], Batch [2500/6000], Loss: 0.1717
Epoch [22/30], Batch [2600/6000], Loss: 0.8164
Epoch [22/30], Batch [2700/6000], Loss: 0.1467
Epoch [22/30], Batch [2800/6000], Loss: 0.1307
Epoch [22/30], Batch [2900/6000], Loss: 0.7188
Epoch [22/30], Batch [3000/6000], Loss: 0.1852
Epoch [22/30], Batch [3100/6000], Loss: 0.2398
Epoch [22/30], Batch [3200/6000], Loss: 0.1218
Epoch [22/30], Batch [3300/6000], Loss: 0.3489
Epoch [22/30], Batch [3400/6000], Loss: 0.9968
Epoch [22/30], Batch [3500/6000], Loss: 0.3926
Epoch [22/30], Batch [3600/6000], Loss: 0.1504
Epoch [22/30], Batch [3700/6000], Loss: 0.3492
Epoch [22/30], Batch [3800/6000], Loss: 0.3152
Epoch [22/30], Batch [3900/6000], Loss: 0.2286
Epoch [22/30], Batch [4000/6000], Loss: 0.1954
Epoch [22/30], Batch [4100/6000], Loss: 0.5726
Epoch [22/30], Batch [4200/6000], Loss: 0.1304
Epoch [22/30], Batch [4300/6000], Loss: 0.2248
Epoch [22/30], Batch [4400/6000], Loss: 0.1630
Epoch [22/30], Batch [4500/6000], Loss: 0.4403
Epoch [22/30], Batch [4600/6000], Loss: 0.1663
Epoch [22/30], Batch [4700/6000], Loss: 0.1621
Epoch [22/30], Batch [4800/6000], Loss: 0.1460
Epoch [22/30], Batch [4900/6000], Loss: 2.0220
Epoch [22/30], Batch [5000/6000], Loss: 0.1539
Epoch [22/30], Batch [5100/6000], Loss: 0.1498
Epoch [22/30], Batch [5200/6000], Loss: 0.1859
Epoch [22/30], Batch [5300/6000], Loss: 1.9479
Epoch [22/30], Batch [5400/6000], Loss: 0.1518
Epoch [22/30], Batch [5500/6000], Loss: 0.1492
Epoch [22/30], Batch [5600/6000], Loss: 0.1350
Epoch [22/30], Batch [5700/6000], Loss: 0.1741
Epoch [22/30], Batch [5800/6000], Loss: 0.1611
Epoch [22/30], Batch [5900/6000], Loss: 0.1506
Epoch [22/30], Loss: 0.2711
Epoch [23/30], Batch [0/6000], Loss: 1.3564
Epoch [23/30], Batch [100/6000], Loss: 0.1229
Epoch [23/30], Batch [200/6000], Loss: 0.1584
Epoch [23/30], Batch [300/6000], Loss: 0.1980
Epoch [23/30], Batch [400/6000], Loss: 0.1902
Epoch [23/30], Batch [500/6000], Loss: 0.1679
Epoch [23/30], Batch [600/6000], Loss: 0.1634
Epoch [23/30], Batch [700/6000], Loss: 0.1937
Epoch [23/30], Batch [800/6000], Loss: 0.3260
Epoch [23/30], Batch [900/6000], Loss: 0.1254
Epoch [23/30], Batch [1000/6000], Loss: 0.1311
Epoch [23/30], Batch [1100/6000], Loss: 0.1358
Epoch [23/30], Batch [1200/6000], Loss: 0.1645
Epoch [23/30], Batch [1300/6000], Loss: 0.1410
Epoch [23/30], Batch [1400/6000], Loss: 0.1859
Epoch [23/30], Batch [1500/6000], Loss: 0.1710
Epoch [23/30], Batch [1600/6000], Loss: 0.1584
Epoch [23/30], Batch [1700/6000], Loss: 0.5966
Epoch [23/30], Batch [1800/6000], Loss: 0.1762
Epoch [23/30], Batch [1900/6000], Loss: 0.1802
Epoch [23/30], Batch [2000/6000], Loss: 0.1662
Epoch [23/30], Batch [2100/6000], Loss: 0.1642
Epoch [23/30], Batch [2200/6000], Loss: 0.1857
Epoch [23/30], Batch [2300/6000], Loss: 0.1277
Epoch [23/30], Batch [2400/6000], Loss: 1.5405
Epoch [23/30], Batch [2500/6000], Loss: 0.1792
Epoch [23/30], Batch [2600/6000], Loss: 0.1630
Epoch [23/30], Batch [2700/6000], Loss: 0.1482
Epoch [23/30], Batch [2800/6000], Loss: 0.1687
Epoch [23/30], Batch [2900/6000], Loss: 0.1163
Epoch [23/30], Batch [3000/6000], Loss: 0.1607
Epoch [23/30], Batch [3100/6000], Loss: 0.1500
Epoch [23/30], Batch [3200/6000], Loss: 1.2878
Epoch [23/30], Batch [3300/6000], Loss: 0.2385
Epoch [23/30], Batch [3400/6000], Loss: 0.1367
Epoch [23/30], Batch [3500/6000], Loss: 0.3096
Epoch [23/30], Batch [3600/6000], Loss: 0.1979
Epoch [23/30], Batch [3700/6000], Loss: 0.1338
Epoch [23/30], Batch [3800/6000], Loss: 0.1330
Epoch [23/30], Batch [3900/6000], Loss: 0.1405
Epoch [23/30], Batch [4000/6000], Loss: 2.6086
Epoch [23/30], Batch [4100/6000], Loss: 0.3079
Epoch [23/30], Batch [4200/6000], Loss: 0.1894
Epoch [23/30], Batch [4300/6000], Loss: 0.1558
Epoch [23/30], Batch [4400/6000], Loss: 0.1787
Epoch [23/30], Batch [4500/6000], Loss: 0.1503
Epoch [23/30], Batch [4600/6000], Loss: 0.1425
Epoch [23/30], Batch [4700/6000], Loss: 3.3471
Epoch [23/30], Batch [4800/6000], Loss: 0.1466
Epoch [23/30], Batch [4900/6000], Loss: 1.0183
Epoch [23/30], Batch [5000/6000], Loss: 0.1604
Epoch [23/30], Batch [5100/6000], Loss: 0.1844
Epoch [23/30], Batch [5200/6000], Loss: 0.1561
Epoch [23/30], Batch [5300/6000], Loss: 0.2416
Epoch [23/30], Batch [5400/6000], Loss: 0.1617
Epoch [23/30], Batch [5500/6000], Loss: 0.1946
Epoch [23/30], Batch [5600/6000], Loss: 0.1340
Epoch [23/30], Batch [5700/6000], Loss: 0.1653
Epoch [23/30], Batch [5800/6000], Loss: 0.1308
Epoch [23/30], Batch [5900/6000], Loss: 0.1499
Epoch [23/30], Loss: 0.2650
Epoch [24/30], Batch [0/6000], Loss: 0.1677
Epoch [24/30], Batch [100/6000], Loss: 0.8996
Epoch [24/30], Batch [200/6000], Loss: 0.1680
Epoch [24/30], Batch [300/6000], Loss: 0.1567
Epoch [24/30], Batch [400/6000], Loss: 0.7372
Epoch [24/30], Batch [500/6000], Loss: 0.1172
Epoch [24/30], Batch [600/6000], Loss: 0.3161
Epoch [24/30], Batch [700/6000], Loss: 0.2009
Epoch [24/30], Batch [800/6000], Loss: 0.1601
Epoch [24/30], Batch [900/6000], Loss: 0.1681
Epoch [24/30], Batch [1000/6000], Loss: 0.1568
Epoch [24/30], Batch [1100/6000], Loss: 0.1977
Epoch [24/30], Batch [1200/6000], Loss: 0.1756
Epoch [24/30], Batch [1300/6000], Loss: 0.1839
Epoch [24/30], Batch [1400/6000], Loss: 0.1913
Epoch [24/30], Batch [1500/6000], Loss: 0.1598
Epoch [24/30], Batch [1600/6000], Loss: 0.2046
Epoch [24/30], Batch [1700/6000], Loss: 0.1549
Epoch [24/30], Batch [1800/6000], Loss: 0.1639
Epoch [24/30], Batch [1900/6000], Loss: 0.1453
Epoch [24/30], Batch [2000/6000], Loss: 0.1892
Epoch [24/30], Batch [2100/6000], Loss: 0.1543
Epoch [24/30], Batch [2200/6000], Loss: 0.3822
Epoch [24/30], Batch [2300/6000], Loss: 0.1516
Epoch [24/30], Batch [2400/6000], Loss: 0.1556
Epoch [24/30], Batch [2500/6000], Loss: 0.1411
Epoch [24/30], Batch [2600/6000], Loss: 0.1440
Epoch [24/30], Batch [2700/6000], Loss: 0.1536
Epoch [24/30], Batch [2800/6000], Loss: 0.2026
Epoch [24/30], Batch [2900/6000], Loss: 0.1566
Epoch [24/30], Batch [3000/6000], Loss: 0.2004
Epoch [24/30], Batch [3100/6000], Loss: 0.1321
Epoch [24/30], Batch [3200/6000], Loss: 0.1572
Epoch [24/30], Batch [3300/6000], Loss: 0.1371
Epoch [24/30], Batch [3400/6000], Loss: 0.1714
Epoch [24/30], Batch [3500/6000], Loss: 0.1782
Epoch [24/30], Batch [3600/6000], Loss: 0.1361
Epoch [24/30], Batch [3700/6000], Loss: 0.1684
Epoch [24/30], Batch [3800/6000], Loss: 0.1499
Epoch [24/30], Batch [3900/6000], Loss: 0.1484
Epoch [24/30], Batch [4000/6000], Loss: 0.1577
Epoch [24/30], Batch [4100/6000], Loss: 0.1368
Epoch [24/30], Batch [4200/6000], Loss: 0.7620
Epoch [24/30], Batch [4300/6000], Loss: 0.3110
Epoch [24/30], Batch [4400/6000], Loss: 0.2187
Epoch [24/30], Batch [4500/6000], Loss: 0.1944
Epoch [24/30], Batch [4600/6000], Loss: 0.1661
Epoch [24/30], Batch [4700/6000], Loss: 0.3469
Epoch [24/30], Batch [4800/6000], Loss: 0.1550
Epoch [24/30], Batch [4900/6000], Loss: 0.1454
Epoch [24/30], Batch [5000/6000], Loss: 0.1492
Epoch [24/30], Batch [5100/6000], Loss: 0.1412
Epoch [24/30], Batch [5200/6000], Loss: 0.1729
Epoch [24/30], Batch [5300/6000], Loss: 0.1590
Epoch [24/30], Batch [5400/6000], Loss: 0.1582
Epoch [24/30], Batch [5500/6000], Loss: 0.1667
Epoch [24/30], Batch [5600/6000], Loss: 0.1797
Epoch [24/30], Batch [5700/6000], Loss: 0.1400
Epoch [24/30], Batch [5800/6000], Loss: 0.1518
Epoch [24/30], Batch [5900/6000], Loss: 0.1681
Epoch [24/30], Loss: 0.2538
Epoch [25/30], Batch [0/6000], Loss: 0.1704
Epoch [25/30], Batch [100/6000], Loss: 0.1585
Epoch [25/30], Batch [200/6000], Loss: 0.1618
Epoch [25/30], Batch [300/6000], Loss: 0.1500
Epoch [25/30], Batch [400/6000], Loss: 0.6481
Epoch [25/30], Batch [500/6000], Loss: 0.1578
Epoch [25/30], Batch [600/6000], Loss: 0.1389
Epoch [25/30], Batch [700/6000], Loss: 0.1673
Epoch [25/30], Batch [800/6000], Loss: 0.1865
Epoch [25/30], Batch [900/6000], Loss: 0.1550
Epoch [25/30], Batch [1000/6000], Loss: 0.1849
Epoch [25/30], Batch [1100/6000], Loss: 0.2020
Epoch [25/30], Batch [1200/6000], Loss: 0.1764
Epoch [25/30], Batch [1300/6000], Loss: 0.1678
Epoch [25/30], Batch [1400/6000], Loss: 0.1452
Epoch [25/30], Batch [1500/6000], Loss: 0.1576
Epoch [25/30], Batch [1600/6000], Loss: 0.1525
Epoch [25/30], Batch [1700/6000], Loss: 0.2583
Epoch [25/30], Batch [1800/6000], Loss: 0.1510
Epoch [25/30], Batch [1900/6000], Loss: 0.1678
Epoch [25/30], Batch [2000/6000], Loss: 0.1431
Epoch [25/30], Batch [2100/6000], Loss: 0.8892
Epoch [25/30], Batch [2200/6000], Loss: 0.1377
Epoch [25/30], Batch [2300/6000], Loss: 0.1195
Epoch [25/30], Batch [2400/6000], Loss: 2.0934
Epoch [25/30], Batch [2500/6000], Loss: 1.7824
Epoch [25/30], Batch [2600/6000], Loss: 0.1290
Epoch [25/30], Batch [2700/6000], Loss: 0.1440
Epoch [25/30], Batch [2800/6000], Loss: 0.1389
Epoch [25/30], Batch [2900/6000], Loss: 0.1414
Epoch [25/30], Batch [3000/6000], Loss: 0.1505
Epoch [25/30], Batch [3100/6000], Loss: 0.1493
Epoch [25/30], Batch [3200/6000], Loss: 0.1558
Epoch [25/30], Batch [3300/6000], Loss: 0.1068
Epoch [25/30], Batch [3400/6000], Loss: 0.1587
Epoch [25/30], Batch [3500/6000], Loss: 0.2081
Epoch [25/30], Batch [3600/6000], Loss: 2.7938
Epoch [25/30], Batch [3700/6000], Loss: 0.1459
Epoch [25/30], Batch [3800/6000], Loss: 0.1617
Epoch [25/30], Batch [3900/6000], Loss: 0.1640
Epoch [25/30], Batch [4000/6000], Loss: 0.1671
Epoch [25/30], Batch [4100/6000], Loss: 0.3371
Epoch [25/30], Batch [4200/6000], Loss: 0.1574
Epoch [25/30], Batch [4300/6000], Loss: 0.1874
Epoch [25/30], Batch [4400/6000], Loss: 0.1299
Epoch [25/30], Batch [4500/6000], Loss: 0.4262
Epoch [25/30], Batch [4600/6000], Loss: 0.2508
Epoch [25/30], Batch [4700/6000], Loss: 0.1748
Epoch [25/30], Batch [4800/6000], Loss: 0.1760
Epoch [25/30], Batch [4900/6000], Loss: 0.1706
Epoch [25/30], Batch [5000/6000], Loss: 0.1575
Epoch [25/30], Batch [5100/6000], Loss: 0.1982
Epoch [25/30], Batch [5200/6000], Loss: 0.1828
Epoch [25/30], Batch [5300/6000], Loss: 0.2113
Epoch [25/30], Batch [5400/6000], Loss: 0.1703
Epoch [25/30], Batch [5500/6000], Loss: 0.1374
Epoch [25/30], Batch [5600/6000], Loss: 0.1934
Epoch [25/30], Batch [5700/6000], Loss: 0.1454
Epoch [25/30], Batch [5800/6000], Loss: 0.4492
Epoch [25/30], Batch [5900/6000], Loss: 0.1610
Epoch [25/30], Loss: 0.2502
Epoch [26/30], Batch [0/6000], Loss: 0.1455
Epoch [26/30], Batch [100/6000], Loss: 0.1588
Epoch [26/30], Batch [200/6000], Loss: 0.1594
Epoch [26/30], Batch [300/6000], Loss: 0.1604
Epoch [26/30], Batch [400/6000], Loss: 0.1804
Epoch [26/30], Batch [500/6000], Loss: 0.1275
Epoch [26/30], Batch [600/6000], Loss: 0.1233
Epoch [26/30], Batch [700/6000], Loss: 0.1613
Epoch [26/30], Batch [800/6000], Loss: 0.1438
Epoch [26/30], Batch [900/6000], Loss: 0.1568
Epoch [26/30], Batch [1000/6000], Loss: 0.1529
Epoch [26/30], Batch [1100/6000], Loss: 0.1645
Epoch [26/30], Batch [1200/6000], Loss: 0.1548
Epoch [26/30], Batch [1300/6000], Loss: 0.1667
Epoch [26/30], Batch [1400/6000], Loss: 0.1624
Epoch [26/30], Batch [1500/6000], Loss: 0.1528
Epoch [26/30], Batch [1600/6000], Loss: 0.1758
Epoch [26/30], Batch [1700/6000], Loss: 0.1913
Epoch [26/30], Batch [1800/6000], Loss: 0.1497
Epoch [26/30], Batch [1900/6000], Loss: 0.1257
Epoch [26/30], Batch [2000/6000], Loss: 0.1575
Epoch [26/30], Batch [2100/6000], Loss: 0.1794
Epoch [26/30], Batch [2200/6000], Loss: 0.2509
Epoch [26/30], Batch [2300/6000], Loss: 0.1510
Epoch [26/30], Batch [2400/6000], Loss: 0.2441
Epoch [26/30], Batch [2500/6000], Loss: 0.1524
Epoch [26/30], Batch [2600/6000], Loss: 0.1475
Epoch [26/30], Batch [2700/6000], Loss: 0.1978
Epoch [26/30], Batch [2800/6000], Loss: 0.1871
Epoch [26/30], Batch [2900/6000], Loss: 0.1766
Epoch [26/30], Batch [3000/6000], Loss: 0.1534
Epoch [26/30], Batch [3100/6000], Loss: 0.1370
Epoch [26/30], Batch [3200/6000], Loss: 0.1797
Epoch [26/30], Batch [3300/6000], Loss: 0.2339
Epoch [26/30], Batch [3400/6000], Loss: 0.2952
Epoch [26/30], Batch [3500/6000], Loss: 0.1669
Epoch [26/30], Batch [3600/6000], Loss: 0.1515
Epoch [26/30], Batch [3700/6000], Loss: 0.1709
Epoch [26/30], Batch [3800/6000], Loss: 0.1574
Epoch [26/30], Batch [3900/6000], Loss: 0.9919
Epoch [26/30], Batch [4000/6000], Loss: 0.1603
Epoch [26/30], Batch [4100/6000], Loss: 0.1582
Epoch [26/30], Batch [4200/6000], Loss: 0.1359
Epoch [26/30], Batch [4300/6000], Loss: 0.1627
Epoch [26/30], Batch [4400/6000], Loss: 0.1706
Epoch [26/30], Batch [4500/6000], Loss: 0.1441
Epoch [26/30], Batch [4600/6000], Loss: 0.1791
Epoch [26/30], Batch [4700/6000], Loss: 0.1647
Epoch [26/30], Batch [4800/6000], Loss: 0.1083
Epoch [26/30], Batch [4900/6000], Loss: 0.1593
Epoch [26/30], Batch [5000/6000], Loss: 0.1346
Epoch [26/30], Batch [5100/6000], Loss: 0.1318
Epoch [26/30], Batch [5200/6000], Loss: 0.1712
Epoch [26/30], Batch [5300/6000], Loss: 0.3237
Epoch [26/30], Batch [5400/6000], Loss: 0.1642
Epoch [26/30], Batch [5500/6000], Loss: 0.2998
Epoch [26/30], Batch [5600/6000], Loss: 0.1708
Epoch [26/30], Batch [5700/6000], Loss: 0.1751
Epoch [26/30], Batch [5800/6000], Loss: 0.1237
Epoch [26/30], Batch [5900/6000], Loss: 0.1848
Epoch [26/30], Loss: 0.2380
Epoch [27/30], Batch [0/6000], Loss: 0.1472
Epoch [27/30], Batch [100/6000], Loss: 0.1683
Epoch [27/30], Batch [200/6000], Loss: 0.1707
Epoch [27/30], Batch [300/6000], Loss: 0.1170
Epoch [27/30], Batch [400/6000], Loss: 0.1896
Epoch [27/30], Batch [500/6000], Loss: 0.1369
Epoch [27/30], Batch [600/6000], Loss: 0.1591
Epoch [27/30], Batch [700/6000], Loss: 0.1756
Epoch [27/30], Batch [800/6000], Loss: 0.1396
Epoch [27/30], Batch [900/6000], Loss: 0.1695
Epoch [27/30], Batch [1000/6000], Loss: 0.1413
Epoch [27/30], Batch [1100/6000], Loss: 1.1103
Epoch [27/30], Batch [1200/6000], Loss: 0.1392
Epoch [27/30], Batch [1300/6000], Loss: 0.1542
Epoch [27/30], Batch [1400/6000], Loss: 0.4547
Epoch [27/30], Batch [1500/6000], Loss: 0.1531
Epoch [27/30], Batch [1600/6000], Loss: 0.1334
Epoch [27/30], Batch [1700/6000], Loss: 0.1499
Epoch [27/30], Batch [1800/6000], Loss: 0.2086
Epoch [27/30], Batch [1900/6000], Loss: 1.0304
Epoch [27/30], Batch [2000/6000], Loss: 0.1877
Epoch [27/30], Batch [2100/6000], Loss: 0.1759
Epoch [27/30], Batch [2200/6000], Loss: 1.1151
Epoch [27/30], Batch [2300/6000], Loss: 0.1842
Epoch [27/30], Batch [2400/6000], Loss: 0.1788
Epoch [27/30], Batch [2500/6000], Loss: 0.1932
Epoch [27/30], Batch [2600/6000], Loss: 0.1625
Epoch [27/30], Batch [2700/6000], Loss: 0.1267
Epoch [27/30], Batch [2800/6000], Loss: 0.1463
Epoch [27/30], Batch [2900/6000], Loss: 0.1718
Epoch [27/30], Batch [3000/6000], Loss: 0.1757
Epoch [27/30], Batch [3100/6000], Loss: 0.1264
Epoch [27/30], Batch [3200/6000], Loss: 1.1158
Epoch [27/30], Batch [3300/6000], Loss: 0.1251
Epoch [27/30], Batch [3400/6000], Loss: 0.1424
Epoch [27/30], Batch [3500/6000], Loss: 0.1272
Epoch [27/30], Batch [3600/6000], Loss: 0.1754
Epoch [27/30], Batch [3700/6000], Loss: 0.1608
Epoch [27/30], Batch [3800/6000], Loss: 0.1575
Epoch [27/30], Batch [3900/6000], Loss: 0.1336
Epoch [27/30], Batch [4000/6000], Loss: 0.1372
Epoch [27/30], Batch [4100/6000], Loss: 0.1813
Epoch [27/30], Batch [4200/6000], Loss: 0.1689
Epoch [27/30], Batch [4300/6000], Loss: 0.1311
Epoch [27/30], Batch [4400/6000], Loss: 0.1751
Epoch [27/30], Batch [4500/6000], Loss: 0.1498
Epoch [27/30], Batch [4600/6000], Loss: 0.3115
Epoch [27/30], Batch [4700/6000], Loss: 0.1719
Epoch [27/30], Batch [4800/6000], Loss: 0.1481
Epoch [27/30], Batch [4900/6000], Loss: 0.1639
Epoch [27/30], Batch [5000/6000], Loss: 0.1193
Epoch [27/30], Batch [5100/6000], Loss: 0.1656
Epoch [27/30], Batch [5200/6000], Loss: 0.1383
Epoch [27/30], Batch [5300/6000], Loss: 0.1817
Epoch [27/30], Batch [5400/6000], Loss: 0.1622
Epoch [27/30], Batch [5500/6000], Loss: 0.1750
Epoch [27/30], Batch [5600/6000], Loss: 0.1287
Epoch [27/30], Batch [5700/6000], Loss: 0.1257
Epoch [27/30], Batch [5800/6000], Loss: 0.1705
Epoch [27/30], Batch [5900/6000], Loss: 0.1382
Epoch [27/30], Loss: 0.2282
Epoch [28/30], Batch [0/6000], Loss: 0.1303
Epoch [28/30], Batch [100/6000], Loss: 0.1413
Epoch [28/30], Batch [200/6000], Loss: 0.1292
Epoch [28/30], Batch [300/6000], Loss: 0.1167
Epoch [28/30], Batch [400/6000], Loss: 0.1673
Epoch [28/30], Batch [500/6000], Loss: 1.6490
Epoch [28/30], Batch [600/6000], Loss: 0.3558
Epoch [28/30], Batch [700/6000], Loss: 0.1144
Epoch [28/30], Batch [800/6000], Loss: 0.9294
Epoch [28/30], Batch [900/6000], Loss: 0.1523
Epoch [28/30], Batch [1000/6000], Loss: 0.1471
Epoch [28/30], Batch [1100/6000], Loss: 0.1201
Epoch [28/30], Batch [1200/6000], Loss: 0.1552
Epoch [28/30], Batch [1300/6000], Loss: 0.1691
Epoch [28/30], Batch [1400/6000], Loss: 0.1684
Epoch [28/30], Batch [1500/6000], Loss: 0.1301
Epoch [28/30], Batch [1600/6000], Loss: 0.1443
Epoch [28/30], Batch [1700/6000], Loss: 0.1401
Epoch [28/30], Batch [1800/6000], Loss: 0.1319
Epoch [28/30], Batch [1900/6000], Loss: 0.1981
Epoch [28/30], Batch [2000/6000], Loss: 0.1777
Epoch [28/30], Batch [2100/6000], Loss: 0.1383
Epoch [28/30], Batch [2200/6000], Loss: 0.1051
Epoch [28/30], Batch [2300/6000], Loss: 0.6360
Epoch [28/30], Batch [2400/6000], Loss: 0.1198
Epoch [28/30], Batch [2500/6000], Loss: 0.1370
Epoch [28/30], Batch [2600/6000], Loss: 0.6218
Epoch [28/30], Batch [2700/6000], Loss: 0.1539
Epoch [28/30], Batch [2800/6000], Loss: 0.1550
Epoch [28/30], Batch [2900/6000], Loss: 0.1432
Epoch [28/30], Batch [3000/6000], Loss: 0.1817
Epoch [28/30], Batch [3100/6000], Loss: 0.1378
Epoch [28/30], Batch [3200/6000], Loss: 0.1356
Epoch [28/30], Batch [3300/6000], Loss: 0.1639
Epoch [28/30], Batch [3400/6000], Loss: 0.1221
Epoch [28/30], Batch [3500/6000], Loss: 0.1344
Epoch [28/30], Batch [3600/6000], Loss: 0.1788
Epoch [28/30], Batch [3700/6000], Loss: 0.1503
Epoch [28/30], Batch [3800/6000], Loss: 0.1377
Epoch [28/30], Batch [3900/6000], Loss: 0.1208
Epoch [28/30], Batch [4000/6000], Loss: 0.1084
Epoch [28/30], Batch [4100/6000], Loss: 0.1227
Epoch [28/30], Batch [4200/6000], Loss: 0.1973
Epoch [28/30], Batch [4300/6000], Loss: 0.3491
Epoch [28/30], Batch [4400/6000], Loss: 0.1713
Epoch [28/30], Batch [4500/6000], Loss: 0.1283
Epoch [28/30], Batch [4600/6000], Loss: 0.1716
Epoch [28/30], Batch [4700/6000], Loss: 0.2122
Epoch [28/30], Batch [4800/6000], Loss: 0.1440
Epoch [28/30], Batch [4900/6000], Loss: 0.1208
Epoch [28/30], Batch [5000/6000], Loss: 0.1488
Epoch [28/30], Batch [5100/6000], Loss: 0.1458
Epoch [28/30], Batch [5200/6000], Loss: 0.1593
Epoch [28/30], Batch [5300/6000], Loss: 0.1429
Epoch [28/30], Batch [5400/6000], Loss: 0.1454
Epoch [28/30], Batch [5500/6000], Loss: 0.2284
Epoch [28/30], Batch [5600/6000], Loss: 0.1351
Epoch [28/30], Batch [5700/6000], Loss: 0.1521
Epoch [28/30], Batch [5800/6000], Loss: 0.1712
Epoch [28/30], Batch [5900/6000], Loss: 0.1620
Epoch [28/30], Loss: 0.2248
Epoch [29/30], Batch [0/6000], Loss: 0.1216
Epoch [29/30], Batch [100/6000], Loss: 0.1948
Epoch [29/30], Batch [200/6000], Loss: 0.1361
Epoch [29/30], Batch [300/6000], Loss: 0.1391
Epoch [29/30], Batch [400/6000], Loss: 0.1419
Epoch [29/30], Batch [500/6000], Loss: 0.1757
Epoch [29/30], Batch [600/6000], Loss: 0.1365
Epoch [29/30], Batch [700/6000], Loss: 0.1720
Epoch [29/30], Batch [800/6000], Loss: 0.1431
Epoch [29/30], Batch [900/6000], Loss: 0.1197
Epoch [29/30], Batch [1000/6000], Loss: 0.1597
Epoch [29/30], Batch [1100/6000], Loss: 0.1594
Epoch [29/30], Batch [1200/6000], Loss: 0.1433
Epoch [29/30], Batch [1300/6000], Loss: 0.1348
Epoch [29/30], Batch [1400/6000], Loss: 0.1149
Epoch [29/30], Batch [1500/6000], Loss: 0.1665
Epoch [29/30], Batch [1600/6000], Loss: 0.1962
Epoch [29/30], Batch [1700/6000], Loss: 0.1702
Epoch [29/30], Batch [1800/6000], Loss: 0.1574
Epoch [29/30], Batch [1900/6000], Loss: 0.1362
Epoch [29/30], Batch [2000/6000], Loss: 0.1719
Epoch [29/30], Batch [2100/6000], Loss: 0.1460
Epoch [29/30], Batch [2200/6000], Loss: 0.1342
Epoch [29/30], Batch [2300/6000], Loss: 0.4645
Epoch [29/30], Batch [2400/6000], Loss: 0.1268
Epoch [29/30], Batch [2500/6000], Loss: 0.1541
Epoch [29/30], Batch [2600/6000], Loss: 0.1550
Epoch [29/30], Batch [2700/6000], Loss: 0.2782
Epoch [29/30], Batch [2800/6000], Loss: 0.1786
Epoch [29/30], Batch [2900/6000], Loss: 0.1386
Epoch [29/30], Batch [3000/6000], Loss: 0.1198
Epoch [29/30], Batch [3100/6000], Loss: 0.1684
Epoch [29/30], Batch [3200/6000], Loss: 0.1419
Epoch [29/30], Batch [3300/6000], Loss: 0.1674
Epoch [29/30], Batch [3400/6000], Loss: 0.1586
Epoch [29/30], Batch [3500/6000], Loss: 0.1441
Epoch [29/30], Batch [3600/6000], Loss: 0.1406
Epoch [29/30], Batch [3700/6000], Loss: 0.1488
Epoch [29/30], Batch [3800/6000], Loss: 0.2024
Epoch [29/30], Batch [3900/6000], Loss: 0.1433
Epoch [29/30], Batch [4000/6000], Loss: 0.1506
Epoch [29/30], Batch [4100/6000], Loss: 0.1400
Epoch [29/30], Batch [4200/6000], Loss: 0.1158
Epoch [29/30], Batch [4300/6000], Loss: 0.1393
Epoch [29/30], Batch [4400/6000], Loss: 0.1612
Epoch [29/30], Batch [4500/6000], Loss: 0.1915
Epoch [29/30], Batch [4600/6000], Loss: 0.1413
Epoch [29/30], Batch [4700/6000], Loss: 0.1462
Epoch [29/30], Batch [4800/6000], Loss: 0.1729
Epoch [29/30], Batch [4900/6000], Loss: 0.1232
Epoch [29/30], Batch [5000/6000], Loss: 0.1644
Epoch [29/30], Batch [5100/6000], Loss: 0.1818
Epoch [29/30], Batch [5200/6000], Loss: 0.1318
Epoch [29/30], Batch [5300/6000], Loss: 1.1939
Epoch [29/30], Batch [5400/6000], Loss: 0.1712
Epoch [29/30], Batch [5500/6000], Loss: 0.1557
Epoch [29/30], Batch [5600/6000], Loss: 0.1590
Epoch [29/30], Batch [5700/6000], Loss: 0.3261
Epoch [29/30], Batch [5800/6000], Loss: 0.1425
Epoch [29/30], Batch [5900/6000], Loss: 0.1401
Epoch [29/30], Loss: 0.2211
Epoch [30/30], Batch [0/6000], Loss: 0.1331
Epoch [30/30], Batch [100/6000], Loss: 0.1652
Epoch [30/30], Batch [200/6000], Loss: 0.1507
Epoch [30/30], Batch [300/6000], Loss: 0.1197
Epoch [30/30], Batch [400/6000], Loss: 0.1405
Epoch [30/30], Batch [500/6000], Loss: 0.4347
Epoch [30/30], Batch [600/6000], Loss: 0.1226
Epoch [30/30], Batch [700/6000], Loss: 0.1124
Epoch [30/30], Batch [800/6000], Loss: 0.2066
Epoch [30/30], Batch [900/6000], Loss: 0.1357
Epoch [30/30], Batch [1000/6000], Loss: 0.1708
Epoch [30/30], Batch [1100/6000], Loss: 0.1460
Epoch [30/30], Batch [1200/6000], Loss: 0.1466
Epoch [30/30], Batch [1300/6000], Loss: 0.1756
Epoch [30/30], Batch [1400/6000], Loss: 0.1455
Epoch [30/30], Batch [1500/6000], Loss: 0.1106
Epoch [30/30], Batch [1600/6000], Loss: 0.1519
Epoch [30/30], Batch [1700/6000], Loss: 0.1018
Epoch [30/30], Batch [1800/6000], Loss: 0.1775
Epoch [30/30], Batch [1900/6000], Loss: 0.1332
Epoch [30/30], Batch [2000/6000], Loss: 0.1590
Epoch [30/30], Batch [2100/6000], Loss: 0.1323
Epoch [30/30], Batch [2200/6000], Loss: 0.1280
Epoch [30/30], Batch [2300/6000], Loss: 0.1384
Epoch [30/30], Batch [2400/6000], Loss: 0.1467
Epoch [30/30], Batch [2500/6000], Loss: 0.1808
Epoch [30/30], Batch [2600/6000], Loss: 0.1875
Epoch [30/30], Batch [2700/6000], Loss: 0.1396
Epoch [30/30], Batch [2800/6000], Loss: 0.1352
Epoch [30/30], Batch [2900/6000], Loss: 0.1278
Epoch [30/30], Batch [3000/6000], Loss: 0.1604
Epoch [30/30], Batch [3100/6000], Loss: 0.1424
Epoch [30/30], Batch [3200/6000], Loss: 0.1046
Epoch [30/30], Batch [3300/6000], Loss: 0.1443
Epoch [30/30], Batch [3400/6000], Loss: 0.2026
Epoch [30/30], Batch [3500/6000], Loss: 0.1300
Epoch [30/30], Batch [3600/6000], Loss: 0.1614
Epoch [30/30], Batch [3700/6000], Loss: 0.1547
Epoch [30/30], Batch [3800/6000], Loss: 0.3718
Epoch [30/30], Batch [3900/6000], Loss: 0.1517
Epoch [30/30], Batch [4000/6000], Loss: 0.1240
Epoch [30/30], Batch [4100/6000], Loss: 0.1269
Epoch [30/30], Batch [4200/6000], Loss: 0.1400
Epoch [30/30], Batch [4300/6000], Loss: 0.1479
Epoch [30/30], Batch [4400/6000], Loss: 0.1340
Epoch [30/30], Batch [4500/6000], Loss: 0.1335
Epoch [30/30], Batch [4600/6000], Loss: 0.2389
Epoch [30/30], Batch [4700/6000], Loss: 0.1517
Epoch [30/30], Batch [4800/6000], Loss: 0.1159
Epoch [30/30], Batch [4900/6000], Loss: 0.1330
Epoch [30/30], Batch [5000/6000], Loss: 1.7472
Epoch [30/30], Batch [5100/6000], Loss: 0.1460
Epoch [30/30], Batch [5200/6000], Loss: 0.1205
Epoch [30/30], Batch [5300/6000], Loss: 0.1139
Epoch [30/30], Batch [5400/6000], Loss: 0.1555
Epoch [30/30], Batch [5500/6000], Loss: 0.1482
Epoch [30/30], Batch [5600/6000], Loss: 0.1308
Epoch [30/30], Batch [5700/6000], Loss: 0.1360
Epoch [30/30], Batch [5800/6000], Loss: 0.1323
Epoch [30/30], Batch [5900/6000], Loss: 0.1968
Epoch [30/30], Loss: 0.2140
Test Loss: 0.1192, Accuracy: 98.16%
Visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 0.3586
  Image Loss: 0.0603
  Total Loss: 3.6462
  Image grad max: 0.08220534026622772
  Output probs: [[0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 2/300:
  Label Loss: 0.3465
  Image Loss: 0.0599
  Total Loss: 3.5244
  Image grad max: 0.09586285799741745
  Output probs: [[0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 3/300:
  Label Loss: 0.3324
  Image Loss: 0.0591
  Total Loss: 3.3831
  Image grad max: 0.10714615136384964
  Output probs: [[0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 4/300:
  Label Loss: 0.3155
  Image Loss: 0.0585
  Total Loss: 3.2139
  Image grad max: 0.13550767302513123
  Output probs: [[0.    0.    0.001 0.    0.998 0.    0.    0.    0.    0.001]]
Adversarial Training Loop 5/300:
  Label Loss: 0.2963
  Image Loss: 0.0580
  Total Loss: 3.0207
  Image grad max: 0.14631062746047974
  Output probs: [[0.    0.    0.001 0.    0.997 0.    0.001 0.    0.    0.001]]
Adversarial Training Loop 6/300:
  Label Loss: 0.2739
  Image Loss: 0.0575
  Total Loss: 2.7965
  Image grad max: 0.1962990164756775
  Output probs: [[0.    0.    0.002 0.    0.994 0.    0.001 0.    0.001 0.002]]
Adversarial Training Loop 7/300:
  Label Loss: 0.2438
  Image Loss: 0.0572
  Total Loss: 2.4957
  Image grad max: 0.2805291712284088
  Output probs: [[0.    0.001 0.005 0.    0.984 0.    0.001 0.    0.004 0.004]]
Adversarial Training Loop 8/300:
  Label Loss: 0.2045
  Image Loss: 0.0571
  Total Loss: 2.1024
  Image grad max: 0.3528212606906891
  Output probs: [[0.    0.005 0.011 0.    0.947 0.    0.002 0.    0.023 0.011]]
Adversarial Training Loop 9/300:
  Label Loss: 0.1593
  Image Loss: 0.0569
  Total Loss: 1.6495
  Image grad max: 0.3928735852241516
  Output probs: [[0.    0.015 0.026 0.002 0.754 0.001 0.003 0.001 0.171 0.029]]
Adversarial Training Loop 10/300:
  Label Loss: 0.1227
  Image Loss: 0.0568
  Total Loss: 1.2834
  Image grad max: 0.21312564611434937
  Output probs: [[0.    0.021 0.028 0.006 0.313 0.001 0.002 0.001 0.588 0.04 ]]
Adversarial Training Loop 11/300:
  Label Loss: 0.1502
  Image Loss: 0.0569
  Total Loss: 1.5590
  Image grad max: 1.5195127725601196
  Output probs: [[0.    0.019 0.023 0.007 0.322 0.001 0.002 0.001 0.572 0.053]]
Adversarial Training Loop 12/300:
  Label Loss: 0.1341
  Image Loss: 0.0571
  Total Loss: 1.3980
  Image grad max: 1.3447892665863037
  Output probs: [[0.    0.016 0.019 0.006 0.507 0.001 0.001 0.001 0.374 0.075]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0941
  Image Loss: 0.0572
  Total Loss: 0.9981
  Image grad max: 0.705472469329834
  Output probs: [[0.    0.011 0.012 0.004 0.683 0.001 0.001 0.001 0.198 0.089]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0705
  Image Loss: 0.0573
  Total Loss: 0.7621
  Image grad max: 0.33656448125839233
  Output probs: [[0.    0.007 0.007 0.003 0.744 0.001 0.001 0.001 0.122 0.114]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0539
  Image Loss: 0.0575
  Total Loss: 0.5969
  Image grad max: 0.49825918674468994
  Output probs: [[0.    0.005 0.004 0.003 0.68  0.001 0.    0.001 0.105 0.201]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0302
  Image Loss: 0.0577
  Total Loss: 0.3599
  Image grad max: 0.4759504199028015
  Output probs: [[0.    0.003 0.002 0.004 0.479 0.001 0.    0.001 0.098 0.412]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0118
  Image Loss: 0.0579
  Total Loss: 0.1764
  Image grad max: 0.15134310722351074
  Output probs: [[0.    0.001 0.001 0.004 0.251 0.001 0.    0.001 0.068 0.673]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0195
  Image Loss: 0.0582
  Total Loss: 0.2534
  Image grad max: 0.6147587895393372
  Output probs: [[0.    0.001 0.    0.003 0.183 0.001 0.    0.001 0.041 0.771]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0286
  Image Loss: 0.0584
  Total Loss: 0.3444
  Image grad max: 0.8406916260719299
  Output probs: [[0.    0.001 0.    0.003 0.242 0.001 0.    0.001 0.029 0.725]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0178
  Image Loss: 0.0585
  Total Loss: 0.2363
  Image grad max: 0.7208333611488342
  Output probs: [[0.    0.001 0.001 0.002 0.446 0.001 0.    0.001 0.02  0.53 ]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0029
  Image Loss: 0.0585
  Total Loss: 0.0871
  Image grad max: 0.13974952697753906
  Output probs: [[0.    0.001 0.001 0.001 0.675 0.    0.    0.    0.012 0.31 ]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0089
  Image Loss: 0.0584
  Total Loss: 0.1473
  Image grad max: 0.510092556476593
  Output probs: [[0.    0.001 0.    0.001 0.755 0.    0.    0.    0.008 0.234]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0173
  Image Loss: 0.0585
  Total Loss: 0.2316
  Image grad max: 0.7123117446899414
  Output probs: [[0.    0.001 0.    0.001 0.72  0.    0.    0.    0.007 0.27 ]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0126
  Image Loss: 0.0586
  Total Loss: 0.1845
  Image grad max: 0.6378622055053711
  Output probs: [[0.    0.    0.    0.001 0.576 0.    0.    0.    0.008 0.414]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0024
  Image Loss: 0.0588
  Total Loss: 0.0829
  Image grad max: 0.24953517317771912
  Output probs: [[0.    0.    0.    0.001 0.37  0.    0.    0.    0.007 0.62 ]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0043
  Image Loss: 0.0590
  Total Loss: 0.1023
  Image grad max: 0.426356703042984
  Output probs: [[0.    0.    0.    0.001 0.279 0.    0.    0.    0.006 0.712]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0115
  Image Loss: 0.0591
  Total Loss: 0.1742
  Image grad max: 0.7367931604385376
  Output probs: [[0.    0.    0.    0.001 0.329 0.    0.    0.    0.006 0.663]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0069
  Image Loss: 0.0592
  Total Loss: 0.1279
  Image grad max: 0.5740393996238708
  Output probs: [[0.    0.    0.    0.001 0.498 0.    0.    0.    0.005 0.495]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0008
  Image Loss: 0.0592
  Total Loss: 0.0668
  Image grad max: 0.0043422263115644455
  Output probs: [[0.    0.    0.    0.001 0.644 0.    0.    0.    0.004 0.349]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0053
  Image Loss: 0.0591
  Total Loss: 0.1118
  Image grad max: 0.4593844711780548
  Output probs: [[0.    0.    0.    0.    0.678 0.    0.    0.    0.004 0.316]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0077
  Image Loss: 0.0592
  Total Loss: 0.1360
  Image grad max: 0.5553572177886963
  Output probs: [[0.    0.    0.    0.001 0.61  0.    0.    0.    0.004 0.384]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0033
  Image Loss: 0.0593
  Total Loss: 0.0919
  Image grad max: 0.36184245347976685
  Output probs: [[0.    0.    0.    0.001 0.472 0.    0.    0.    0.004 0.522]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0008
  Image Loss: 0.0594
  Total Loss: 0.0671
  Image grad max: 0.0890841856598854
  Output probs: [[0.    0.    0.    0.001 0.364 0.    0.    0.    0.004 0.629]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0043
  Image Loss: 0.0595
  Total Loss: 0.1026
  Image grad max: 0.4644061028957367
  Output probs: [[0.    0.    0.    0.001 0.362 0.    0.    0.    0.004 0.632]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0044
  Image Loss: 0.0595
  Total Loss: 0.1036
  Image grad max: 0.4723089337348938
  Output probs: [[0.    0.    0.    0.001 0.46  0.    0.    0.    0.004 0.534]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0009
  Image Loss: 0.0595
  Total Loss: 0.0681
  Image grad max: 0.12928619980812073
  Output probs: [[0.    0.    0.    0.001 0.577 0.    0.    0.    0.004 0.417]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0019
  Image Loss: 0.0594
  Total Loss: 0.0779
  Image grad max: 0.2628346085548401
  Output probs: [[0.    0.    0.    0.    0.621 0.    0.    0.    0.003 0.374]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0037
  Image Loss: 0.0594
  Total Loss: 0.0963
  Image grad max: 0.39922067523002625
  Output probs: [[0.    0.    0.    0.001 0.577 0.    0.    0.    0.003 0.417]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0018
  Image Loss: 0.0595
  Total Loss: 0.0778
  Image grad max: 0.263618141412735
  Output probs: [[0.    0.    0.    0.001 0.482 0.    0.    0.    0.004 0.513]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0006
  Image Loss: 0.0595
  Total Loss: 0.0655
  Image grad max: 0.05625544488430023
  Output probs: [[0.    0.    0.    0.001 0.409 0.    0.    0.    0.004 0.586]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0022
  Image Loss: 0.0596
  Total Loss: 0.0811
  Image grad max: 0.31346291303634644
  Output probs: [[0.    0.    0.    0.001 0.411 0.    0.    0.    0.003 0.583]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0021
  Image Loss: 0.0596
  Total Loss: 0.0801
  Image grad max: 0.3047807216644287
  Output probs: [[0.    0.    0.    0.001 0.484 0.    0.    0.    0.003 0.511]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0006
  Image Loss: 0.0595
  Total Loss: 0.0651
  Image grad max: 0.04715275019407272
  Output probs: [[0.    0.    0.    0.001 0.56  0.    0.    0.    0.003 0.435]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0013
  Image Loss: 0.0595
  Total Loss: 0.0724
  Image grad max: 0.20684181153774261
  Output probs: [[0.    0.    0.    0.    0.578 0.    0.    0.    0.003 0.417]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0018
  Image Loss: 0.0595
  Total Loss: 0.0775
  Image grad max: 0.26474618911743164
  Output probs: [[0.    0.    0.    0.001 0.533 0.    0.    0.    0.003 0.462]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0008
  Image Loss: 0.0595
  Total Loss: 0.0670
  Image grad max: 0.1170675978064537
  Output probs: [[0.    0.    0.    0.001 0.466 0.    0.    0.    0.003 0.529]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0007
  Image Loss: 0.0595
  Total Loss: 0.0667
  Image grad max: 0.11251457780599594
  Output probs: [[0.    0.    0.    0.001 0.433 0.    0.    0.    0.003 0.562]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0014
  Image Loss: 0.0595
  Total Loss: 0.0730
  Image grad max: 0.22744503617286682
  Output probs: [[0.    0.    0.    0.001 0.458 0.    0.    0.    0.003 0.537]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0008
  Image Loss: 0.0595
  Total Loss: 0.0678
  Image grad max: 0.14084231853485107
  Output probs: [[0.    0.    0.    0.001 0.516 0.    0.    0.    0.003 0.479]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0006
  Image Loss: 0.0594
  Total Loss: 0.0651
  Image grad max: 0.0610940046608448
  Output probs: [[0.    0.    0.    0.001 0.551 0.    0.    0.    0.003 0.444]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0011
  Image Loss: 0.0594
  Total Loss: 0.0701
  Image grad max: 0.17992880940437317
  Output probs: [[0.    0.    0.    0.001 0.538 0.    0.    0.    0.003 0.457]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0008
  Image Loss: 0.0594
  Total Loss: 0.0675
  Image grad max: 0.13427597284317017
  Output probs: [[0.    0.    0.    0.001 0.492 0.    0.    0.    0.003 0.503]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0005
  Image Loss: 0.0594
  Total Loss: 0.0644
  Image grad max: 0.021451829001307487
  Output probs: [[0.    0.    0.    0.001 0.457 0.    0.    0.    0.003 0.538]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0008
  Image Loss: 0.0594
  Total Loss: 0.0676
  Image grad max: 0.1419476866722107
  Output probs: [[0.    0.    0.    0.001 0.462 0.    0.    0.    0.003 0.533]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0007
  Image Loss: 0.0594
  Total Loss: 0.0668
  Image grad max: 0.12544669210910797
  Output probs: [[0.    0.    0.    0.001 0.5   0.    0.    0.    0.003 0.495]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0005
  Image Loss: 0.0593
  Total Loss: 0.0642
  Image grad max: 0.00838245265185833
  Output probs: [[0.    0.    0.    0.001 0.532 0.    0.    0.    0.003 0.463]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0007
  Image Loss: 0.0593
  Total Loss: 0.0664
  Image grad max: 0.11613048613071442
  Output probs: [[0.    0.    0.    0.001 0.529 0.    0.    0.    0.003 0.466]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0007
  Image Loss: 0.0592
  Total Loss: 0.0659
  Image grad max: 0.10456915944814682
  Output probs: [[0.    0.    0.    0.001 0.497 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0005
  Image Loss: 0.0592
  Total Loss: 0.0640
  Image grad max: 0.0033749081194400787
  Output probs: [[0.    0.    0.    0.001 0.471 0.    0.    0.    0.003 0.524]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0006
  Image Loss: 0.0592
  Total Loss: 0.0654
  Image grad max: 0.09399266541004181
  Output probs: [[0.    0.    0.    0.001 0.473 0.    0.    0.    0.003 0.523]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0006
  Image Loss: 0.0591
  Total Loss: 0.0652
  Image grad max: 0.08865448832511902
  Output probs: [[0.    0.    0.    0.001 0.5   0.    0.    0.    0.003 0.496]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0005
  Image Loss: 0.0591
  Total Loss: 0.0639
  Image grad max: 0.005482271313667297
  Output probs: [[0.    0.    0.    0.001 0.522 0.    0.    0.    0.003 0.473]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0006
  Image Loss: 0.0590
  Total Loss: 0.0650
  Image grad max: 0.08239372074604034
  Output probs: [[0.    0.    0.    0.001 0.518 0.    0.    0.    0.003 0.477]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0006
  Image Loss: 0.0590
  Total Loss: 0.0646
  Image grad max: 0.06916259229183197
  Output probs: [[0.    0.    0.    0.001 0.495 0.    0.    0.    0.003 0.5  ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0005
  Image Loss: 0.0590
  Total Loss: 0.0637
  Image grad max: 0.011239134706556797
  Output probs: [[0.    0.    0.    0.001 0.478 0.    0.    0.    0.003 0.517]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0006
  Image Loss: 0.0589
  Total Loss: 0.0645
  Image grad max: 0.07026125490665436
  Output probs: [[0.    0.    0.    0.001 0.483 0.    0.    0.    0.003 0.512]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0005
  Image Loss: 0.0589
  Total Loss: 0.0641
  Image grad max: 0.0533413365483284
  Output probs: [[0.    0.    0.    0.001 0.504 0.    0.    0.    0.003 0.492]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0005
  Image Loss: 0.0588
  Total Loss: 0.0636
  Image grad max: 0.01904773898422718
  Output probs: [[0.    0.    0.    0.001 0.516 0.    0.    0.    0.003 0.479]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0005
  Image Loss: 0.0588
  Total Loss: 0.0641
  Image grad max: 0.06290171295404434
  Output probs: [[0.    0.    0.    0.001 0.508 0.    0.    0.    0.003 0.487]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0005
  Image Loss: 0.0587
  Total Loss: 0.0636
  Image grad max: 0.03543422743678093
  Output probs: [[0.    0.    0.    0.001 0.491 0.    0.    0.    0.003 0.504]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0005
  Image Loss: 0.0587
  Total Loss: 0.0635
  Image grad max: 0.025513770058751106
  Output probs: [[0.    0.    0.    0.001 0.483 0.    0.    0.    0.003 0.512]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0005
  Image Loss: 0.0587
  Total Loss: 0.0638
  Image grad max: 0.05270175263285637
  Output probs: [[0.    0.    0.    0.001 0.492 0.    0.    0.    0.003 0.503]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0005
  Image Loss: 0.0586
  Total Loss: 0.0633
  Image grad max: 0.020352208986878395
  Output probs: [[0.    0.    0.    0.001 0.507 0.    0.    0.    0.003 0.488]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0005
  Image Loss: 0.0585
  Total Loss: 0.0633
  Image grad max: 0.03178099915385246
  Output probs: [[0.    0.    0.    0.001 0.51  0.    0.    0.    0.003 0.485]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0005
  Image Loss: 0.0585
  Total Loss: 0.0634
  Image grad max: 0.04266995191574097
  Output probs: [[0.    0.    0.    0.001 0.5   0.    0.    0.    0.003 0.496]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0005
  Image Loss: 0.0584
  Total Loss: 0.0631
  Image grad max: 0.0053560975939035416
  Output probs: [[0.    0.    0.    0.001 0.489 0.    0.    0.    0.003 0.507]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0005
  Image Loss: 0.0584
  Total Loss: 0.0632
  Image grad max: 0.03350507467985153
  Output probs: [[0.    0.    0.    0.001 0.489 0.    0.    0.    0.003 0.506]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0005
  Image Loss: 0.0583
  Total Loss: 0.0631
  Image grad max: 0.030713805928826332
  Output probs: [[0.    0.    0.    0.001 0.5   0.    0.    0.    0.003 0.495]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0005
  Image Loss: 0.0583
  Total Loss: 0.0629
  Image grad max: 0.008108478039503098
  Output probs: [[0.    0.    0.    0.001 0.508 0.    0.    0.    0.003 0.488]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0005
  Image Loss: 0.0582
  Total Loss: 0.0630
  Image grad max: 0.03318603336811066
  Output probs: [[0.    0.    0.    0.001 0.503 0.    0.    0.    0.003 0.492]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0005
  Image Loss: 0.0582
  Total Loss: 0.0628
  Image grad max: 0.017104486003518105
  Output probs: [[0.    0.    0.    0.001 0.494 0.    0.    0.    0.003 0.502]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0005
  Image Loss: 0.0581
  Total Loss: 0.0627
  Image grad max: 0.01651184819638729
  Output probs: [[0.    0.    0.    0.001 0.491 0.    0.    0.    0.003 0.505]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0005
  Image Loss: 0.0581
  Total Loss: 0.0627
  Image grad max: 0.02727634459733963
  Output probs: [[0.    0.    0.    0.001 0.497 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0005
  Image Loss: 0.0580
  Total Loss: 0.0626
  Image grad max: 0.004560512024909258
  Output probs: [[0.    0.    0.    0.001 0.504 0.    0.    0.    0.003 0.491]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0005
  Image Loss: 0.0580
  Total Loss: 0.0626
  Image grad max: 0.02181801199913025
  Output probs: [[0.    0.    0.    0.001 0.504 0.    0.    0.    0.003 0.492]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0005
  Image Loss: 0.0579
  Total Loss: 0.0625
  Image grad max: 0.01865515112876892
  Output probs: [[0.    0.    0.    0.001 0.496 0.    0.    0.    0.003 0.499]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0005
  Image Loss: 0.0578
  Total Loss: 0.0624
  Image grad max: 0.0065040308982133865
  Output probs: [[0.    0.    0.    0.001 0.492 0.    0.    0.    0.003 0.503]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0005
  Image Loss: 0.0578
  Total Loss: 0.0624
  Image grad max: 0.02096439152956009
  Output probs: [[0.    0.    0.    0.001 0.496 0.    0.    0.    0.003 0.5  ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0005
  Image Loss: 0.0577
  Total Loss: 0.0622
  Image grad max: 0.008033915422856808
  Output probs: [[0.    0.    0.    0.    0.502 0.    0.    0.    0.003 0.493]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0005
  Image Loss: 0.0577
  Total Loss: 0.0622
  Image grad max: 0.014214702881872654
  Output probs: [[0.    0.    0.    0.    0.503 0.    0.    0.    0.003 0.493]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0005
  Image Loss: 0.0576
  Total Loss: 0.0621
  Image grad max: 0.016195673495531082
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0004
  Image Loss: 0.0576
  Total Loss: 0.0620
  Image grad max: 0.0032537514343857765
  Output probs: [[0.    0.    0.    0.    0.494 0.    0.    0.    0.003 0.502]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0004
  Image Loss: 0.0575
  Total Loss: 0.0620
  Image grad max: 0.01566777564585209
  Output probs: [[0.    0.    0.    0.    0.496 0.    0.    0.    0.003 0.499]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0004
  Image Loss: 0.0574
  Total Loss: 0.0619
  Image grad max: 0.0077469185926020145
  Output probs: [[0.    0.    0.    0.    0.501 0.    0.    0.    0.003 0.495]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0004
  Image Loss: 0.0574
  Total Loss: 0.0618
  Image grad max: 0.01005056593567133
  Output probs: [[0.    0.    0.    0.    0.502 0.    0.    0.    0.003 0.494]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0004
  Image Loss: 0.0573
  Total Loss: 0.0618
  Image grad max: 0.01294777262955904
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0004
  Image Loss: 0.0573
  Total Loss: 0.0617
  Image grad max: 0.0027709826827049255
  Output probs: [[0.    0.    0.    0.    0.495 0.    0.    0.    0.003 0.501]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0004
  Image Loss: 0.0572
  Total Loss: 0.0616
  Image grad max: 0.012013674713671207
  Output probs: [[0.    0.    0.    0.    0.497 0.    0.    0.    0.003 0.499]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0004
  Image Loss: 0.0571
  Total Loss: 0.0615
  Image grad max: 0.005762941669672728
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.    0.    0.003 0.495]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0004
  Image Loss: 0.0571
  Total Loss: 0.0615
  Image grad max: 0.008214496076107025
  Output probs: [[0.    0.    0.    0.    0.501 0.    0.    0.    0.003 0.495]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0004
  Image Loss: 0.0570
  Total Loss: 0.0614
  Image grad max: 0.009841631166636944
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0004
  Image Loss: 0.0570
  Total Loss: 0.0613
  Image grad max: 0.0030967898201197386
  Output probs: [[0.    0.    0.    0.    0.496 0.    0.    0.    0.003 0.5  ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0004
  Image Loss: 0.0569
  Total Loss: 0.0613
  Image grad max: 0.0094933295622468
  Output probs: [[0.    0.    0.    0.    0.497 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0004
  Image Loss: 0.0568
  Total Loss: 0.0612
  Image grad max: 0.00372717366553843
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.    0.    0.003 0.495]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0004
  Image Loss: 0.0568
  Total Loss: 0.0611
  Image grad max: 0.007526569068431854
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.    0.    0.003 0.496]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0004
  Image Loss: 0.0567
  Total Loss: 0.0611
  Image grad max: 0.006862590089440346
  Output probs: [[0.    0.    0.    0.    0.497 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0004
  Image Loss: 0.0566
  Total Loss: 0.0610
  Image grad max: 0.0037373085506260395
  Output probs: [[0.    0.    0.    0.    0.496 0.    0.    0.    0.003 0.499]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0004
  Image Loss: 0.0566
  Total Loss: 0.0609
  Image grad max: 0.007294837851077318
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0004
  Image Loss: 0.0565
  Total Loss: 0.0608
  Image grad max: 0.002514780731871724
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.    0.    0.003 0.496]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0004
  Image Loss: 0.0565
  Total Loss: 0.0608
  Image grad max: 0.006960469298064709
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.496]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0004
  Image Loss: 0.0564
  Total Loss: 0.0607
  Image grad max: 0.0037623457610607147
  Output probs: [[0.    0.    0.    0.    0.497 0.    0.    0.    0.003 0.499]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0004
  Image Loss: 0.0563
  Total Loss: 0.0606
  Image grad max: 0.004579175263643265
  Output probs: [[0.    0.    0.    0.    0.497 0.    0.    0.    0.003 0.499]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0004
  Image Loss: 0.0563
  Total Loss: 0.0606
  Image grad max: 0.0048454636707901955
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0004
  Image Loss: 0.0562
  Total Loss: 0.0605
  Image grad max: 0.002971124369651079
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.    0.    0.003 0.496]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0004
  Image Loss: 0.0561
  Total Loss: 0.0604
  Image grad max: 0.005699013359844685
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0004
  Image Loss: 0.0561
  Total Loss: 0.0603
  Image grad max: 0.0025841344613581896
  Output probs: [[0.    0.    0.    0.    0.497 0.    0.    0.    0.003 0.499]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0004
  Image Loss: 0.0560
  Total Loss: 0.0603
  Image grad max: 0.00472223898395896
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0004
  Image Loss: 0.0560
  Total Loss: 0.0602
  Image grad max: 0.003023827215656638
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.496]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0004
  Image Loss: 0.0559
  Total Loss: 0.0601
  Image grad max: 0.00414830818772316
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0004
  Image Loss: 0.0558
  Total Loss: 0.0600
  Image grad max: 0.003457010490819812
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0004
  Image Loss: 0.0558
  Total Loss: 0.0600
  Image grad max: 0.003303289646282792
  Output probs: [[0.    0.    0.    0.    0.497 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0004
  Image Loss: 0.0557
  Total Loss: 0.0599
  Image grad max: 0.0037456732243299484
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0004
  Image Loss: 0.0556
  Total Loss: 0.0598
  Image grad max: 0.002755779307335615
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0004
  Image Loss: 0.0556
  Total Loss: 0.0598
  Image grad max: 0.0038804057985544205
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0004
  Image Loss: 0.0555
  Total Loss: 0.0597
  Image grad max: 0.0025422563776373863
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0004
  Image Loss: 0.0554
  Total Loss: 0.0596
  Image grad max: 0.0036297165788710117
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0004
  Image Loss: 0.0554
  Total Loss: 0.0595
  Image grad max: 0.002554252278059721
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0004
  Image Loss: 0.0553
  Total Loss: 0.0595
  Image grad max: 0.0033054775558412075
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0004
  Image Loss: 0.0552
  Total Loss: 0.0594
  Image grad max: 0.00276327901519835
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0004
  Image Loss: 0.0552
  Total Loss: 0.0593
  Image grad max: 0.0031895795837044716
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0004
  Image Loss: 0.0551
  Total Loss: 0.0592
  Image grad max: 0.002913694130256772
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0004
  Image Loss: 0.0550
  Total Loss: 0.0592
  Image grad max: 0.0028306366875767708
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0004
  Image Loss: 0.0550
  Total Loss: 0.0591
  Image grad max: 0.002838422078639269
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0004
  Image Loss: 0.0549
  Total Loss: 0.0590
  Image grad max: 0.0027538121212273836
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0004
  Image Loss: 0.0548
  Total Loss: 0.0589
  Image grad max: 0.0029984470456838608
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0004
  Image Loss: 0.0548
  Total Loss: 0.0589
  Image grad max: 0.0026958007365465164
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.003 0.497]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0004
  Image Loss: 0.0547
  Total Loss: 0.0588
  Image grad max: 0.0028324974700808525
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.003 0.498]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0004
  Image Loss: 0.0546
  Total Loss: 0.0587
  Image grad max: 0.0025174031034111977
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0004
  Image Loss: 0.0546
  Total Loss: 0.0586
  Image grad max: 0.002929034875705838
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0004
  Image Loss: 0.0545
  Total Loss: 0.0585
  Image grad max: 0.0026153987273573875
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0004
  Image Loss: 0.0544
  Total Loss: 0.0585
  Image grad max: 0.0027938028797507286
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0004
  Image Loss: 0.0544
  Total Loss: 0.0584
  Image grad max: 0.002496031578630209
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0004
  Image Loss: 0.0543
  Total Loss: 0.0583
  Image grad max: 0.002813748549669981
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0004
  Image Loss: 0.0542
  Total Loss: 0.0582
  Image grad max: 0.0025787719059735537
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0004
  Image Loss: 0.0542
  Total Loss: 0.0582
  Image grad max: 0.002749927341938019
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0004
  Image Loss: 0.0541
  Total Loss: 0.0581
  Image grad max: 0.0025062018539756536
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0004
  Image Loss: 0.0540
  Total Loss: 0.0580
  Image grad max: 0.0026994755025953054
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0004
  Image Loss: 0.0540
  Total Loss: 0.0579
  Image grad max: 0.002573746256530285
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0004
  Image Loss: 0.0539
  Total Loss: 0.0579
  Image grad max: 0.002709032502025366
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0004
  Image Loss: 0.0538
  Total Loss: 0.0578
  Image grad max: 0.002492059487849474
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0004
  Image Loss: 0.0538
  Total Loss: 0.0577
  Image grad max: 0.002591019030660391
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0004
  Image Loss: 0.0537
  Total Loss: 0.0576
  Image grad max: 0.002588642295449972
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0004
  Image Loss: 0.0536
  Total Loss: 0.0575
  Image grad max: 0.0026661143638193607
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0004
  Image Loss: 0.0536
  Total Loss: 0.0575
  Image grad max: 0.0024667612742632627
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0004
  Image Loss: 0.0535
  Total Loss: 0.0574
  Image grad max: 0.002512587234377861
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0004
  Image Loss: 0.0534
  Total Loss: 0.0573
  Image grad max: 0.002605627989396453
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0004
  Image Loss: 0.0534
  Total Loss: 0.0572
  Image grad max: 0.0026198243722319603
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0004
  Image Loss: 0.0533
  Total Loss: 0.0572
  Image grad max: 0.002467075129970908
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0004
  Image Loss: 0.0532
  Total Loss: 0.0571
  Image grad max: 0.002461527707055211
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0004
  Image Loss: 0.0531
  Total Loss: 0.0570
  Image grad max: 0.002617623656988144
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0004
  Image Loss: 0.0531
  Total Loss: 0.0569
  Image grad max: 0.0025638011284172535
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0004
  Image Loss: 0.0530
  Total Loss: 0.0568
  Image grad max: 0.002474157838150859
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0004
  Image Loss: 0.0529
  Total Loss: 0.0568
  Image grad max: 0.002512027509510517
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0004
  Image Loss: 0.0529
  Total Loss: 0.0567
  Image grad max: 0.002609922783449292
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0004
  Image Loss: 0.0528
  Total Loss: 0.0566
  Image grad max: 0.0025050861295312643
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0004
  Image Loss: 0.0527
  Total Loss: 0.0565
  Image grad max: 0.0024441955611109734
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0004
  Image Loss: 0.0527
  Total Loss: 0.0565
  Image grad max: 0.002558588981628418
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0004
  Image Loss: 0.0526
  Total Loss: 0.0564
  Image grad max: 0.002572695491835475
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0004
  Image Loss: 0.0525
  Total Loss: 0.0563
  Image grad max: 0.002461100462824106
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0004
  Image Loss: 0.0524
  Total Loss: 0.0562
  Image grad max: 0.002487188670784235
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0004
  Image Loss: 0.0524
  Total Loss: 0.0561
  Image grad max: 0.002576821716502309
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0004
  Image Loss: 0.0523
  Total Loss: 0.0561
  Image grad max: 0.0025184566620737314
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0004
  Image Loss: 0.0522
  Total Loss: 0.0560
  Image grad max: 0.0024541509337723255
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0004
  Image Loss: 0.0522
  Total Loss: 0.0559
  Image grad max: 0.0025340691208839417
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0004
  Image Loss: 0.0521
  Total Loss: 0.0558
  Image grad max: 0.002554376143962145
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0004
  Image Loss: 0.0520
  Total Loss: 0.0557
  Image grad max: 0.0024677899200469255
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0004
  Image Loss: 0.0520
  Total Loss: 0.0557
  Image grad max: 0.0024908692575991154
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0004
  Image Loss: 0.0519
  Total Loss: 0.0556
  Image grad max: 0.002556111430749297
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0004
  Image Loss: 0.0518
  Total Loss: 0.0555
  Image grad max: 0.0024959626607596874
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0004
  Image Loss: 0.0517
  Total Loss: 0.0554
  Image grad max: 0.00246674963273108
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0004
  Image Loss: 0.0517
  Total Loss: 0.0553
  Image grad max: 0.0025355657562613487
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0004
  Image Loss: 0.0516
  Total Loss: 0.0553
  Image grad max: 0.0025195879861712456
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0004
  Image Loss: 0.0515
  Total Loss: 0.0552
  Image grad max: 0.002462774747982621
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0004
  Image Loss: 0.0515
  Total Loss: 0.0551
  Image grad max: 0.0025115911848843098
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0004
  Image Loss: 0.0514
  Total Loss: 0.0550
  Image grad max: 0.002530285157263279
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0004
  Image Loss: 0.0513
  Total Loss: 0.0549
  Image grad max: 0.00246984395198524
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0004
  Image Loss: 0.0513
  Total Loss: 0.0549
  Image grad max: 0.0024913412053138018
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0004
  Image Loss: 0.0512
  Total Loss: 0.0548
  Image grad max: 0.002529157791286707
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0004
  Image Loss: 0.0511
  Total Loss: 0.0547
  Image grad max: 0.0024791385512799025
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0004
  Image Loss: 0.0510
  Total Loss: 0.0546
  Image grad max: 0.002477924572303891
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0004
  Image Loss: 0.0510
  Total Loss: 0.0545
  Image grad max: 0.002522506285458803
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0004
  Image Loss: 0.0509
  Total Loss: 0.0545
  Image grad max: 0.0024866561871021986
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0004
  Image Loss: 0.0508
  Total Loss: 0.0544
  Image grad max: 0.002467972692102194
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0004
  Image Loss: 0.0508
  Total Loss: 0.0543
  Image grad max: 0.0025153120514005423
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0004
  Image Loss: 0.0507
  Total Loss: 0.0542
  Image grad max: 0.0024779948871582747
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0004
  Image Loss: 0.0506
  Total Loss: 0.0541
  Image grad max: 0.0024715790059417486
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0004
  Image Loss: 0.0505
  Total Loss: 0.0541
  Image grad max: 0.00250992551445961
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0004
  Image Loss: 0.0505
  Total Loss: 0.0540
  Image grad max: 0.0024764002300798893
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0004
  Image Loss: 0.0504
  Total Loss: 0.0539
  Image grad max: 0.002467832062393427
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0003
  Image Loss: 0.0503
  Total Loss: 0.0538
  Image grad max: 0.002504587173461914
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0003
  Image Loss: 0.0503
  Total Loss: 0.0537
  Image grad max: 0.002473090775310993
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0003
  Image Loss: 0.0502
  Total Loss: 0.0537
  Image grad max: 0.0024672718718647957
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0003
  Image Loss: 0.0501
  Total Loss: 0.0536
  Image grad max: 0.0024975447449833155
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0003
  Image Loss: 0.0500
  Total Loss: 0.0535
  Image grad max: 0.0024709843564778566
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0003
  Image Loss: 0.0500
  Total Loss: 0.0534
  Image grad max: 0.0024736144114285707
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0003
  Image Loss: 0.0499
  Total Loss: 0.0533
  Image grad max: 0.00248389202170074
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0003
  Image Loss: 0.0498
  Total Loss: 0.0533
  Image grad max: 0.0024667191319167614
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0003
  Image Loss: 0.0498
  Total Loss: 0.0532
  Image grad max: 0.00247129425406456
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0003
  Image Loss: 0.0497
  Total Loss: 0.0531
  Image grad max: 0.002476292196661234
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0003
  Image Loss: 0.0496
  Total Loss: 0.0530
  Image grad max: 0.0024632385466247797
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0003
  Image Loss: 0.0495
  Total Loss: 0.0529
  Image grad max: 0.0024696982000023127
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0003
  Image Loss: 0.0495
  Total Loss: 0.0528
  Image grad max: 0.0024734672624617815
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0003
  Image Loss: 0.0494
  Total Loss: 0.0528
  Image grad max: 0.002460930962115526
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0003
  Image Loss: 0.0493
  Total Loss: 0.0527
  Image grad max: 0.0024699929635971785
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0003
  Image Loss: 0.0492
  Total Loss: 0.0526
  Image grad max: 0.0024650385603308678
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0003
  Image Loss: 0.0492
  Total Loss: 0.0525
  Image grad max: 0.0024620606563985348
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0003
  Image Loss: 0.0491
  Total Loss: 0.0524
  Image grad max: 0.0024659663904458284
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0003
  Image Loss: 0.0490
  Total Loss: 0.0524
  Image grad max: 0.0024594960268586874
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0003
  Image Loss: 0.0490
  Total Loss: 0.0523
  Image grad max: 0.002460962161421776
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0003
  Image Loss: 0.0489
  Total Loss: 0.0522
  Image grad max: 0.00246017356403172
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0003
  Image Loss: 0.0488
  Total Loss: 0.0521
  Image grad max: 0.002456085756421089
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0003
  Image Loss: 0.0487
  Total Loss: 0.0520
  Image grad max: 0.002457838039845228
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0003
  Image Loss: 0.0487
  Total Loss: 0.0520
  Image grad max: 0.0024545511696487665
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0003
  Image Loss: 0.0486
  Total Loss: 0.0519
  Image grad max: 0.002452458254992962
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0003
  Image Loss: 0.0485
  Total Loss: 0.0518
  Image grad max: 0.0024550955276936293
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0003
  Image Loss: 0.0485
  Total Loss: 0.0517
  Image grad max: 0.0024477792903780937
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0003
  Image Loss: 0.0484
  Total Loss: 0.0516
  Image grad max: 0.0024530566297471523
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0003
  Image Loss: 0.0483
  Total Loss: 0.0515
  Image grad max: 0.00244644982740283
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0003
  Image Loss: 0.0482
  Total Loss: 0.0515
  Image grad max: 0.002446485450491309
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0003
  Image Loss: 0.0482
  Total Loss: 0.0514
  Image grad max: 0.0024492614902555943
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0003
  Image Loss: 0.0481
  Total Loss: 0.0513
  Image grad max: 0.0024392339400947094
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0003
  Image Loss: 0.0480
  Total Loss: 0.0512
  Image grad max: 0.002449608873575926
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0003
  Image Loss: 0.0480
  Total Loss: 0.0511
  Image grad max: 0.002436198526993394
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0003
  Image Loss: 0.0479
  Total Loss: 0.0511
  Image grad max: 0.0024442255962640047
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0003
  Image Loss: 0.0478
  Total Loss: 0.0510
  Image grad max: 0.0024385866709053516
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0003
  Image Loss: 0.0477
  Total Loss: 0.0509
  Image grad max: 0.002436296781525016
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0003
  Image Loss: 0.0477
  Total Loss: 0.0508
  Image grad max: 0.002440402517095208
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0003
  Image Loss: 0.0476
  Total Loss: 0.0507
  Image grad max: 0.002431199885904789
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0003
  Image Loss: 0.0475
  Total Loss: 0.0507
  Image grad max: 0.0024382355622947216
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0003
  Image Loss: 0.0474
  Total Loss: 0.0506
  Image grad max: 0.0024309607688337564
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0003
  Image Loss: 0.0474
  Total Loss: 0.0505
  Image grad max: 0.0024312396999448538
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0003
  Image Loss: 0.0473
  Total Loss: 0.0504
  Image grad max: 0.002433568239212036
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0003
  Image Loss: 0.0472
  Total Loss: 0.0503
  Image grad max: 0.002424343256279826
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0003
  Image Loss: 0.0472
  Total Loss: 0.0502
  Image grad max: 0.002433331683278084
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0003
  Image Loss: 0.0471
  Total Loss: 0.0502
  Image grad max: 0.0024215509183704853
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0003
  Image Loss: 0.0470
  Total Loss: 0.0501
  Image grad max: 0.002427356783300638
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0003
  Image Loss: 0.0469
  Total Loss: 0.0500
  Image grad max: 0.002419897587969899
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0003
  Image Loss: 0.0469
  Total Loss: 0.0499
  Image grad max: 0.002420774195343256
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0003
  Image Loss: 0.0468
  Total Loss: 0.0498
  Image grad max: 0.0024154146667569876
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0003
  Image Loss: 0.0467
  Total Loss: 0.0498
  Image grad max: 0.002422120189294219
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0003
  Image Loss: 0.0467
  Total Loss: 0.0497
  Image grad max: 0.0024056248366832733
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0003
  Image Loss: 0.0466
  Total Loss: 0.0496
  Image grad max: 0.0024256811011582613
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0003
  Image Loss: 0.0465
  Total Loss: 0.0495
  Image grad max: 0.002394325565546751
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0003
  Image Loss: 0.0464
  Total Loss: 0.0494
  Image grad max: 0.0024311270099133253
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0003
  Image Loss: 0.0464
  Total Loss: 0.0494
  Image grad max: 0.0023863734677433968
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0003
  Image Loss: 0.0463
  Total Loss: 0.0493
  Image grad max: 0.0024299495853483677
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0003
  Image Loss: 0.0462
  Total Loss: 0.0492
  Image grad max: 0.0023766157682985067
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0003
  Image Loss: 0.0461
  Total Loss: 0.0491
  Image grad max: 0.002437734976410866
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0003
  Image Loss: 0.0461
  Total Loss: 0.0490
  Image grad max: 0.0023550381883978844
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0003
  Image Loss: 0.0460
  Total Loss: 0.0490
  Image grad max: 0.0024650695268064737
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0003
  Image Loss: 0.0459
  Total Loss: 0.0489
  Image grad max: 0.002337472513318062
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0003
  Image Loss: 0.0459
  Total Loss: 0.0488
  Image grad max: 0.002535003935918212
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.499]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0003
  Image Loss: 0.0458
  Total Loss: 0.0487
  Image grad max: 0.0026644112076610327
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.    0.    0.002 0.498]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0003
  Image Loss: 0.0457
  Total Loss: 0.0486
  Image grad max: 0.0027151356916874647
  Output probs: [[0.    0.    0.    0.    0.498 0.    0.    0.    0.002 0.499]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0003
  Image Loss: 0.0456
  Total Loss: 0.0486
  Image grad max: 0.0035310224629938602
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0003
  Image Loss: 0.0456
  Total Loss: 0.0485
  Image grad max: 0.005607134662568569
  Output probs: [[0.    0.    0.    0.    0.497 0.    0.    0.    0.002 0.5  ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0003
  Image Loss: 0.0455
  Total Loss: 0.0484
  Image grad max: 0.008298157714307308
  Output probs: [[0.    0.    0.    0.    0.502 0.    0.    0.    0.002 0.495]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0003
  Image Loss: 0.0454
  Total Loss: 0.0483
  Image grad max: 0.014561804011464119
  Output probs: [[0.    0.    0.    0.    0.494 0.    0.    0.    0.002 0.503]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0003
  Image Loss: 0.0454
  Total Loss: 0.0483
  Image grad max: 0.024069374427199364
  Output probs: [[0.    0.    0.    0.    0.508 0.    0.    0.    0.002 0.489]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0003
  Image Loss: 0.0453
  Total Loss: 0.0483
  Image grad max: 0.04419314116239548
  Output probs: [[0.    0.    0.    0.    0.482 0.    0.    0.    0.002 0.515]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0003
  Image Loss: 0.0452
  Total Loss: 0.0486
  Image grad max: 0.08088163286447525
  Output probs: [[0.    0.    0.    0.    0.531 0.    0.    0.    0.002 0.466]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0005
  Image Loss: 0.0451
  Total Loss: 0.0501
  Image grad max: 0.1581134796142578
  Output probs: [[0.    0.    0.    0.    0.436 0.    0.    0.    0.002 0.561]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0011
  Image Loss: 0.0451
  Total Loss: 0.0559
  Image grad max: 0.3147205412387848
  Output probs: [[0.    0.    0.    0.    0.627 0.    0.    0.    0.002 0.371]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0037
  Image Loss: 0.0449
  Total Loss: 0.0816
  Image grad max: 0.5937703251838684
  Output probs: [[0.    0.    0.    0.    0.297 0.    0.    0.    0.002 0.7  ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0092
  Image Loss: 0.0450
  Total Loss: 0.1372
  Image grad max: 1.0346133708953857
  Output probs: [[0.    0.    0.    0.    0.784 0.    0.    0.    0.001 0.214]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0199
  Image Loss: 0.0447
  Total Loss: 0.2440
  Image grad max: 1.22018563747406
  Output probs: [[0.    0.    0.    0.    0.364 0.    0.    0.    0.002 0.633]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0041
  Image Loss: 0.0449
  Total Loss: 0.0856
  Image grad max: 0.6761120557785034
  Output probs: [[0.    0.    0.    0.    0.446 0.    0.    0.    0.002 0.551]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0008
  Image Loss: 0.0448
  Total Loss: 0.0533
  Image grad max: 0.25909748673439026
  Output probs: [[0.    0.    0.    0.    0.695 0.    0.    0.    0.002 0.303]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0086
  Image Loss: 0.0447
  Total Loss: 0.1310
  Image grad max: 0.8671115636825562
  Output probs: [[0.    0.    0.    0.    0.325 0.    0.    0.    0.002 0.672]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0067
  Image Loss: 0.0448
  Total Loss: 0.1122
  Image grad max: 0.8591250777244568
  Output probs: [[0.    0.    0.    0.    0.573 0.    0.    0.    0.002 0.424]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0014
  Image Loss: 0.0447
  Total Loss: 0.0588
  Image grad max: 0.3416225016117096
  Output probs: [[0.    0.    0.    0.    0.562 0.    0.    0.    0.002 0.435]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0011
  Image Loss: 0.0446
  Total Loss: 0.0558
  Image grad max: 0.2914820909500122
  Output probs: [[0.    0.    0.    0.    0.355 0.    0.    0.    0.002 0.642]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0046
  Image Loss: 0.0447
  Total Loss: 0.0909
  Image grad max: 0.6926397681236267
  Output probs: [[0.    0.    0.    0.    0.628 0.    0.    0.    0.002 0.369]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0038
  Image Loss: 0.0445
  Total Loss: 0.0824
  Image grad max: 0.5765594840049744
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.    0.    0.002 0.497]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0003
  Image Loss: 0.0446
  Total Loss: 0.0477
  Image grad max: 0.006573773920536041
  Output probs: [[0.    0.    0.    0.    0.378 0.    0.    0.    0.002 0.619]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0033
  Image Loss: 0.0446
  Total Loss: 0.0780
  Image grad max: 0.5740258693695068
  Output probs: [[0.    0.    0.    0.    0.634 0.    0.    0.    0.002 0.363]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0042
  Image Loss: 0.0444
  Total Loss: 0.0860
  Image grad max: 0.5976356267929077
  Output probs: [[0.    0.    0.    0.    0.491 0.    0.    0.    0.002 0.506]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0003
  Image Loss: 0.0445
  Total Loss: 0.0479
  Image grad max: 0.036240104585886
  Output probs: [[0.    0.    0.    0.    0.382 0.    0.    0.    0.002 0.615]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0032
  Image Loss: 0.0445
  Total Loss: 0.0760
  Image grad max: 0.5487263202667236
  Output probs: [[0.    0.    0.    0.    0.624 0.    0.    0.    0.002 0.373]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0036
  Image Loss: 0.0443
  Total Loss: 0.0801
  Image grad max: 0.5451036095619202
  Output probs: [[0.    0.    0.    0.    0.508 0.    0.    0.    0.002 0.488]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0004
  Image Loss: 0.0443
  Total Loss: 0.0479
  Image grad max: 0.04432900249958038
  Output probs: [[0.    0.    0.    0.    0.375 0.    0.    0.    0.002 0.622]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0035
  Image Loss: 0.0444
  Total Loss: 0.0794
  Image grad max: 0.5733943581581116
  Output probs: [[0.    0.    0.    0.    0.603 0.    0.    0.    0.002 0.394]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0026
  Image Loss: 0.0442
  Total Loss: 0.0701
  Image grad max: 0.45179834961891174
  Output probs: [[0.    0.    0.    0.    0.537 0.    0.    0.    0.002 0.46 ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0006
  Image Loss: 0.0442
  Total Loss: 0.0507
  Image grad max: 0.17113645374774933
  Output probs: [[0.    0.    0.    0.    0.373 0.    0.    0.    0.002 0.623]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0036
  Image Loss: 0.0443
  Total Loss: 0.0802
  Image grad max: 0.5742349028587341
  Output probs: [[0.    0.    0.    0.    0.568 0.    0.    0.    0.002 0.428]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0013
  Image Loss: 0.0441
  Total Loss: 0.0576
  Image grad max: 0.30236074328422546
  Output probs: [[0.    0.    0.    0.    0.564 0.    0.    0.    0.002 0.433]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0012
  Image Loss: 0.0441
  Total Loss: 0.0563
  Image grad max: 0.28103259205818176
  Output probs: [[0.    0.    0.    0.    0.395 0.    0.    0.    0.002 0.602]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0026
  Image Loss: 0.0441
  Total Loss: 0.0698
  Image grad max: 0.470956414937973
  Output probs: [[0.    0.    0.    0.    0.519 0.    0.    0.    0.002 0.477]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0005
  Image Loss: 0.0440
  Total Loss: 0.0486
  Image grad max: 0.08905196934938431
  Output probs: [[0.    0.    0.    0.    0.576 0.    0.    0.    0.002 0.421]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0016
  Image Loss: 0.0440
  Total Loss: 0.0598
  Image grad max: 0.3254912197589874
Visualization saved to adversarial_figures/adversarial_testing.png
