Epoch [1/30], Batch [0/6000], Loss: 2.5355
Epoch [1/30], Batch [100/6000], Loss: 2.1037
Epoch [1/30], Batch [200/6000], Loss: 1.9544
Epoch [1/30], Batch [300/6000], Loss: 1.8285
Epoch [1/30], Batch [400/6000], Loss: 1.6683
Epoch [1/30], Batch [500/6000], Loss: 1.8457
Epoch [1/30], Batch [600/6000], Loss: 1.6118
Epoch [1/30], Batch [700/6000], Loss: 1.6003
Epoch [1/30], Batch [800/6000], Loss: 1.5779
Epoch [1/30], Batch [900/6000], Loss: 1.5826
Epoch [1/30], Batch [1000/6000], Loss: 1.5808
Epoch [1/30], Batch [1100/6000], Loss: 1.6595
Epoch [1/30], Batch [1200/6000], Loss: 1.6630
Epoch [1/30], Batch [1300/6000], Loss: 1.6574
Epoch [1/30], Batch [1400/6000], Loss: 1.5392
Epoch [1/30], Batch [1500/6000], Loss: 1.6246
Epoch [1/30], Batch [1600/6000], Loss: 1.6318
Epoch [1/30], Batch [1700/6000], Loss: 1.5971
Epoch [1/30], Batch [1800/6000], Loss: 1.5638
Epoch [1/30], Batch [1900/6000], Loss: 1.5849
Epoch [1/30], Batch [2000/6000], Loss: 1.5727
Epoch [1/30], Batch [2100/6000], Loss: 1.5601
Epoch [1/30], Batch [2200/6000], Loss: 1.6923
Epoch [1/30], Batch [2300/6000], Loss: 1.6608
Epoch [1/30], Batch [2400/6000], Loss: 1.5659
Epoch [1/30], Batch [2500/6000], Loss: 1.5834
Epoch [1/30], Batch [2600/6000], Loss: 1.5529
Epoch [1/30], Batch [2700/6000], Loss: 1.5655
Epoch [1/30], Batch [2800/6000], Loss: 1.6637
Epoch [1/30], Batch [2900/6000], Loss: 1.5907
Epoch [1/30], Batch [3000/6000], Loss: 1.5499
Epoch [1/30], Batch [3100/6000], Loss: 1.5580
Epoch [1/30], Batch [3200/6000], Loss: 1.5373
Epoch [1/30], Batch [3300/6000], Loss: 1.5372
Epoch [1/30], Batch [3400/6000], Loss: 1.5985
Epoch [1/30], Batch [3500/6000], Loss: 1.8138
Epoch [1/30], Batch [3600/6000], Loss: 1.5716
Epoch [1/30], Batch [3700/6000], Loss: 1.6886
Epoch [1/30], Batch [3800/6000], Loss: 1.6441
Epoch [1/30], Batch [3900/6000], Loss: 1.5135
Epoch [1/30], Batch [4000/6000], Loss: 1.5326
Epoch [1/30], Batch [4100/6000], Loss: 1.7348
Epoch [1/30], Batch [4200/6000], Loss: 1.5268
Epoch [1/30], Batch [4300/6000], Loss: 1.5684
Epoch [1/30], Batch [4400/6000], Loss: 1.5622
Epoch [1/30], Batch [4500/6000], Loss: 1.5558
Epoch [1/30], Batch [4600/6000], Loss: 1.7512
Epoch [1/30], Batch [4700/6000], Loss: 1.5244
Epoch [1/30], Batch [4800/6000], Loss: 1.6212
Epoch [1/30], Batch [4900/6000], Loss: 1.6494
Epoch [1/30], Batch [5000/6000], Loss: 1.5607
Epoch [1/30], Batch [5100/6000], Loss: 1.6791
Epoch [1/30], Batch [5200/6000], Loss: 1.5927
Epoch [1/30], Batch [5300/6000], Loss: 1.5523
Epoch [1/30], Batch [5400/6000], Loss: 1.5315
Epoch [1/30], Batch [5500/6000], Loss: 1.6401
Epoch [1/30], Batch [5600/6000], Loss: 1.5251
Epoch [1/30], Batch [5700/6000], Loss: 1.5209
Epoch [1/30], Batch [5800/6000], Loss: 1.5754
Epoch [1/30], Batch [5900/6000], Loss: 1.5118
Epoch [1/30], Loss: 1.6386
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 1.5377
Epoch [2/30], Batch [100/6000], Loss: 1.6386
Epoch [2/30], Batch [200/6000], Loss: 1.5157
Epoch [2/30], Batch [300/6000], Loss: 1.6249
Epoch [2/30], Batch [400/6000], Loss: 1.5761
Epoch [2/30], Batch [500/6000], Loss: 1.6104
Epoch [2/30], Batch [600/6000], Loss: 1.5263
Epoch [2/30], Batch [700/6000], Loss: 1.5326
Epoch [2/30], Batch [800/6000], Loss: 1.6856
Epoch [2/30], Batch [900/6000], Loss: 1.6120
Epoch [2/30], Batch [1000/6000], Loss: 1.5636
Epoch [2/30], Batch [1100/6000], Loss: 1.5217
Epoch [2/30], Batch [1200/6000], Loss: 1.5169
Epoch [2/30], Batch [1300/6000], Loss: 1.5960
Epoch [2/30], Batch [1400/6000], Loss: 1.5264
Epoch [2/30], Batch [1500/6000], Loss: 1.5228
Epoch [2/30], Batch [1600/6000], Loss: 1.6668
Epoch [2/30], Batch [1700/6000], Loss: 1.5117
Epoch [2/30], Batch [1800/6000], Loss: 1.6105
Epoch [2/30], Batch [1900/6000], Loss: 1.5658
Epoch [2/30], Batch [2000/6000], Loss: 1.7079
Epoch [2/30], Batch [2100/6000], Loss: 1.5446
Epoch [2/30], Batch [2200/6000], Loss: 1.7232
Epoch [2/30], Batch [2300/6000], Loss: 1.6327
Epoch [2/30], Batch [2400/6000], Loss: 1.5190
Epoch [2/30], Batch [2500/6000], Loss: 1.5333
Epoch [2/30], Batch [2600/6000], Loss: 1.5002
Epoch [2/30], Batch [2700/6000], Loss: 1.5093
Epoch [2/30], Batch [2800/6000], Loss: 1.5316
Epoch [2/30], Batch [2900/6000], Loss: 1.6159
Epoch [2/30], Batch [3000/6000], Loss: 1.5052
Epoch [2/30], Batch [3100/6000], Loss: 1.5167
Epoch [2/30], Batch [3200/6000], Loss: 1.5082
Epoch [2/30], Batch [3300/6000], Loss: 1.5521
Epoch [2/30], Batch [3400/6000], Loss: 1.5526
Epoch [2/30], Batch [3500/6000], Loss: 1.4950
Epoch [2/30], Batch [3600/6000], Loss: 1.6254
Epoch [2/30], Batch [3700/6000], Loss: 1.4976
Epoch [2/30], Batch [3800/6000], Loss: 1.5007
Epoch [2/30], Batch [3900/6000], Loss: 1.5080
Epoch [2/30], Batch [4000/6000], Loss: 1.5017
Epoch [2/30], Batch [4100/6000], Loss: 1.5380
Epoch [2/30], Batch [4200/6000], Loss: 1.5519
Epoch [2/30], Batch [4300/6000], Loss: 1.5066
Epoch [2/30], Batch [4400/6000], Loss: 1.5020
Epoch [2/30], Batch [4500/6000], Loss: 1.5542
Epoch [2/30], Batch [4600/6000], Loss: 1.5416
Epoch [2/30], Batch [4700/6000], Loss: 1.5348
Epoch [2/30], Batch [4800/6000], Loss: 1.5139
Epoch [2/30], Batch [4900/6000], Loss: 1.5630
Epoch [2/30], Batch [5000/6000], Loss: 1.7304
Epoch [2/30], Batch [5100/6000], Loss: 1.5120
Epoch [2/30], Batch [5200/6000], Loss: 1.5031
Epoch [2/30], Batch [5300/6000], Loss: 1.5108
Epoch [2/30], Batch [5400/6000], Loss: 1.5369
Epoch [2/30], Batch [5500/6000], Loss: 1.6149
Epoch [2/30], Batch [5600/6000], Loss: 1.5698
Epoch [2/30], Batch [5700/6000], Loss: 1.5116
Epoch [2/30], Batch [5800/6000], Loss: 1.5172
Epoch [2/30], Batch [5900/6000], Loss: 1.5064
Epoch [2/30], Loss: 1.5670
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 1.5757
Epoch [3/30], Batch [100/6000], Loss: 1.6003
Epoch [3/30], Batch [200/6000], Loss: 1.6234
Epoch [3/30], Batch [300/6000], Loss: 1.5112
Epoch [3/30], Batch [400/6000], Loss: 1.6288
Epoch [3/30], Batch [500/6000], Loss: 1.5061
Epoch [3/30], Batch [600/6000], Loss: 1.7057
Epoch [3/30], Batch [700/6000], Loss: 1.6124
Epoch [3/30], Batch [800/6000], Loss: 1.5196
Epoch [3/30], Batch [900/6000], Loss: 1.5010
Epoch [3/30], Batch [1000/6000], Loss: 1.4912
Epoch [3/30], Batch [1100/6000], Loss: 1.5023
Epoch [3/30], Batch [1200/6000], Loss: 1.5225
Epoch [3/30], Batch [1300/6000], Loss: 1.6311
Epoch [3/30], Batch [1400/6000], Loss: 1.6219
Epoch [3/30], Batch [1500/6000], Loss: 1.5451
Epoch [3/30], Batch [1600/6000], Loss: 1.5832
Epoch [3/30], Batch [1700/6000], Loss: 1.6045
Epoch [3/30], Batch [1800/6000], Loss: 1.4991
Epoch [3/30], Batch [1900/6000], Loss: 1.6331
Epoch [3/30], Batch [2000/6000], Loss: 1.5072
Epoch [3/30], Batch [2100/6000], Loss: 1.5056
Epoch [3/30], Batch [2200/6000], Loss: 1.5465
Epoch [3/30], Batch [2300/6000], Loss: 1.5076
Epoch [3/30], Batch [2400/6000], Loss: 1.5767
Epoch [3/30], Batch [2500/6000], Loss: 1.5079
Epoch [3/30], Batch [2600/6000], Loss: 1.6132
Epoch [3/30], Batch [2700/6000], Loss: 1.4987
Epoch [3/30], Batch [2800/6000], Loss: 1.5350
Epoch [3/30], Batch [2900/6000], Loss: 1.5017
Epoch [3/30], Batch [3000/6000], Loss: 1.8162
Epoch [3/30], Batch [3100/6000], Loss: 1.5271
Epoch [3/30], Batch [3200/6000], Loss: 1.5004
Epoch [3/30], Batch [3300/6000], Loss: 1.8467
Epoch [3/30], Batch [3400/6000], Loss: 1.5049
Epoch [3/30], Batch [3500/6000], Loss: 1.5082
Epoch [3/30], Batch [3600/6000], Loss: 1.5347
Epoch [3/30], Batch [3700/6000], Loss: 1.6094
Epoch [3/30], Batch [3800/6000], Loss: 1.4957
Epoch [3/30], Batch [3900/6000], Loss: 1.5051
Epoch [3/30], Batch [4000/6000], Loss: 1.5296
Epoch [3/30], Batch [4100/6000], Loss: 1.5278
Epoch [3/30], Batch [4200/6000], Loss: 1.5126
Epoch [3/30], Batch [4300/6000], Loss: 1.6245
Epoch [3/30], Batch [4400/6000], Loss: 1.5250
Epoch [3/30], Batch [4500/6000], Loss: 1.4978
Epoch [3/30], Batch [4600/6000], Loss: 1.5866
Epoch [3/30], Batch [4700/6000], Loss: 1.5076
Epoch [3/30], Batch [4800/6000], Loss: 1.4900
Epoch [3/30], Batch [4900/6000], Loss: 1.5187
Epoch [3/30], Batch [5000/6000], Loss: 1.5049
Epoch [3/30], Batch [5100/6000], Loss: 1.5044
Epoch [3/30], Batch [5200/6000], Loss: 1.4928
Epoch [3/30], Batch [5300/6000], Loss: 1.5712
Epoch [3/30], Batch [5400/6000], Loss: 1.6289
Epoch [3/30], Batch [5500/6000], Loss: 1.7298
Epoch [3/30], Batch [5600/6000], Loss: 1.6231
Epoch [3/30], Batch [5700/6000], Loss: 1.6166
Epoch [3/30], Batch [5800/6000], Loss: 1.5876
Epoch [3/30], Batch [5900/6000], Loss: 1.6823
Epoch [3/30], Loss: 1.5480
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 1.5017
Epoch [4/30], Batch [100/6000], Loss: 1.5763
Epoch [4/30], Batch [200/6000], Loss: 1.5192
Epoch [4/30], Batch [300/6000], Loss: 1.5070
Epoch [4/30], Batch [400/6000], Loss: 1.5101
Epoch [4/30], Batch [500/6000], Loss: 1.5176
Epoch [4/30], Batch [600/6000], Loss: 1.4897
Epoch [4/30], Batch [700/6000], Loss: 1.5026
Epoch [4/30], Batch [800/6000], Loss: 1.4991
Epoch [4/30], Batch [900/6000], Loss: 1.4823
Epoch [4/30], Batch [1000/6000], Loss: 1.5254
Epoch [4/30], Batch [1100/6000], Loss: 1.5097
Epoch [4/30], Batch [1200/6000], Loss: 1.5168
Epoch [4/30], Batch [1300/6000], Loss: 1.6182
Epoch [4/30], Batch [1400/6000], Loss: 1.4873
Epoch [4/30], Batch [1500/6000], Loss: 1.4995
Epoch [4/30], Batch [1600/6000], Loss: 1.5204
Epoch [4/30], Batch [1700/6000], Loss: 1.4922
Epoch [4/30], Batch [1800/6000], Loss: 1.4902
Epoch [4/30], Batch [1900/6000], Loss: 1.5128
Epoch [4/30], Batch [2000/6000], Loss: 1.4943
Epoch [4/30], Batch [2100/6000], Loss: 1.5075
Epoch [4/30], Batch [2200/6000], Loss: 1.6190
Epoch [4/30], Batch [2300/6000], Loss: 1.5020
Epoch [4/30], Batch [2400/6000], Loss: 1.5011
Epoch [4/30], Batch [2500/6000], Loss: 1.5036
Epoch [4/30], Batch [2600/6000], Loss: 1.5441
Epoch [4/30], Batch [2700/6000], Loss: 1.4915
Epoch [4/30], Batch [2800/6000], Loss: 1.6019
Epoch [4/30], Batch [2900/6000], Loss: 1.5049
Epoch [4/30], Batch [3000/6000], Loss: 1.6215
Epoch [4/30], Batch [3100/6000], Loss: 1.4929
Epoch [4/30], Batch [3200/6000], Loss: 1.5001
Epoch [4/30], Batch [3300/6000], Loss: 1.5047
Epoch [4/30], Batch [3400/6000], Loss: 1.5013
Epoch [4/30], Batch [3500/6000], Loss: 1.6265
Epoch [4/30], Batch [3600/6000], Loss: 1.4880
Epoch [4/30], Batch [3700/6000], Loss: 1.5917
Epoch [4/30], Batch [3800/6000], Loss: 1.5276
Epoch [4/30], Batch [3900/6000], Loss: 1.4969
Epoch [4/30], Batch [4000/6000], Loss: 1.5195
Epoch [4/30], Batch [4100/6000], Loss: 1.5359
Epoch [4/30], Batch [4200/6000], Loss: 1.5825
Epoch [4/30], Batch [4300/6000], Loss: 1.4959
Epoch [4/30], Batch [4400/6000], Loss: 1.5153
Epoch [4/30], Batch [4500/6000], Loss: 1.5249
Epoch [4/30], Batch [4600/6000], Loss: 1.4995
Epoch [4/30], Batch [4700/6000], Loss: 1.6042
Epoch [4/30], Batch [4800/6000], Loss: 1.5041
Epoch [4/30], Batch [4900/6000], Loss: 1.4994
Epoch [4/30], Batch [5000/6000], Loss: 1.5186
Epoch [4/30], Batch [5100/6000], Loss: 1.5770
Epoch [4/30], Batch [5200/6000], Loss: 1.5010
Epoch [4/30], Batch [5300/6000], Loss: 1.5331
Epoch [4/30], Batch [5400/6000], Loss: 1.5081
Epoch [4/30], Batch [5500/6000], Loss: 1.5017
Epoch [4/30], Batch [5600/6000], Loss: 1.5952
Epoch [4/30], Batch [5700/6000], Loss: 1.4957
Epoch [4/30], Batch [5800/6000], Loss: 1.5309
Epoch [4/30], Batch [5900/6000], Loss: 1.5534
Epoch [4/30], Loss: 1.5357
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 1.4964
Epoch [5/30], Batch [100/6000], Loss: 1.6077
Epoch [5/30], Batch [200/6000], Loss: 1.5848
Epoch [5/30], Batch [300/6000], Loss: 1.4991
Epoch [5/30], Batch [400/6000], Loss: 1.5217
Epoch [5/30], Batch [500/6000], Loss: 1.4985
Epoch [5/30], Batch [600/6000], Loss: 1.5097
Epoch [5/30], Batch [700/6000], Loss: 1.5011
Epoch [5/30], Batch [800/6000], Loss: 1.5133
Epoch [5/30], Batch [900/6000], Loss: 1.4852
Epoch [5/30], Batch [1000/6000], Loss: 1.4923
Epoch [5/30], Batch [1100/6000], Loss: 1.5221
Epoch [5/30], Batch [1200/6000], Loss: 1.5014
Epoch [5/30], Batch [1300/6000], Loss: 1.5168
Epoch [5/30], Batch [1400/6000], Loss: 1.5867
Epoch [5/30], Batch [1500/6000], Loss: 1.5094
Epoch [5/30], Batch [1600/6000], Loss: 1.5017
Epoch [5/30], Batch [1700/6000], Loss: 1.5997
Epoch [5/30], Batch [1800/6000], Loss: 1.4951
Epoch [5/30], Batch [1900/6000], Loss: 1.6058
Epoch [5/30], Batch [2000/6000], Loss: 1.5282
Epoch [5/30], Batch [2100/6000], Loss: 1.4902
Epoch [5/30], Batch [2200/6000], Loss: 1.4993
Epoch [5/30], Batch [2300/6000], Loss: 1.5233
Epoch [5/30], Batch [2400/6000], Loss: 1.5045
Epoch [5/30], Batch [2500/6000], Loss: 1.5959
Epoch [5/30], Batch [2600/6000], Loss: 1.5233
Epoch [5/30], Batch [2700/6000], Loss: 1.5129
Epoch [5/30], Batch [2800/6000], Loss: 1.6061
Epoch [5/30], Batch [2900/6000], Loss: 1.4911
Epoch [5/30], Batch [3000/6000], Loss: 1.6041
Epoch [5/30], Batch [3100/6000], Loss: 1.4828
Epoch [5/30], Batch [3200/6000], Loss: 1.4887
Epoch [5/30], Batch [3300/6000], Loss: 1.4954
Epoch [5/30], Batch [3400/6000], Loss: 1.4898
Epoch [5/30], Batch [3500/6000], Loss: 1.5017
Epoch [5/30], Batch [3600/6000], Loss: 1.5033
Epoch [5/30], Batch [3700/6000], Loss: 1.4968
Epoch [5/30], Batch [3800/6000], Loss: 1.5203
Epoch [5/30], Batch [3900/6000], Loss: 1.5038
Epoch [5/30], Batch [4000/6000], Loss: 1.5037
Epoch [5/30], Batch [4100/6000], Loss: 1.5962
Epoch [5/30], Batch [4200/6000], Loss: 1.5024
Epoch [5/30], Batch [4300/6000], Loss: 1.5186
Epoch [5/30], Batch [4400/6000], Loss: 1.5202
Epoch [5/30], Batch [4500/6000], Loss: 1.4828
Epoch [5/30], Batch [4600/6000], Loss: 1.5391
Epoch [5/30], Batch [4700/6000], Loss: 1.4893
Epoch [5/30], Batch [4800/6000], Loss: 1.5084
Epoch [5/30], Batch [4900/6000], Loss: 1.4830
Epoch [5/30], Batch [5000/6000], Loss: 1.5153
Epoch [5/30], Batch [5100/6000], Loss: 1.4806
Epoch [5/30], Batch [5200/6000], Loss: 1.4912
Epoch [5/30], Batch [5300/6000], Loss: 1.4893
Epoch [5/30], Batch [5400/6000], Loss: 1.5093
Epoch [5/30], Batch [5500/6000], Loss: 1.5086
Epoch [5/30], Batch [5600/6000], Loss: 1.4998
Epoch [5/30], Batch [5700/6000], Loss: 1.5044
Epoch [5/30], Batch [5800/6000], Loss: 1.6408
Epoch [5/30], Batch [5900/6000], Loss: 1.5030
Epoch [5/30], Loss: 1.5270
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 1.5954
Epoch [6/30], Batch [100/6000], Loss: 1.5034
Epoch [6/30], Batch [200/6000], Loss: 1.4935
Epoch [6/30], Batch [300/6000], Loss: 1.5172
Epoch [6/30], Batch [400/6000], Loss: 1.6269
Epoch [6/30], Batch [500/6000], Loss: 1.4927
Epoch [6/30], Batch [600/6000], Loss: 1.5063
Epoch [6/30], Batch [700/6000], Loss: 1.5140
Epoch [6/30], Batch [800/6000], Loss: 1.4873
Epoch [6/30], Batch [900/6000], Loss: 1.5964
Epoch [6/30], Batch [1000/6000], Loss: 1.4941
Epoch [6/30], Batch [1100/6000], Loss: 1.4919
Epoch [6/30], Batch [1200/6000], Loss: 1.5945
Epoch [6/30], Batch [1300/6000], Loss: 1.5638
Epoch [6/30], Batch [1400/6000], Loss: 1.5183
Epoch [6/30], Batch [1500/6000], Loss: 1.4851
Epoch [6/30], Batch [1600/6000], Loss: 1.4955
Epoch [6/30], Batch [1700/6000], Loss: 1.4868
Epoch [6/30], Batch [1800/6000], Loss: 1.5052
Epoch [6/30], Batch [1900/6000], Loss: 1.5018
Epoch [6/30], Batch [2000/6000], Loss: 1.4895
Epoch [6/30], Batch [2100/6000], Loss: 1.4921
Epoch [6/30], Batch [2200/6000], Loss: 1.5111
Epoch [6/30], Batch [2300/6000], Loss: 1.5102
Epoch [6/30], Batch [2400/6000], Loss: 1.6325
Epoch [6/30], Batch [2500/6000], Loss: 1.5043
Epoch [6/30], Batch [2600/6000], Loss: 1.5828
Epoch [6/30], Batch [2700/6000], Loss: 1.5968
Epoch [6/30], Batch [2800/6000], Loss: 1.4962
Epoch [6/30], Batch [2900/6000], Loss: 1.4978
Epoch [6/30], Batch [3000/6000], Loss: 1.4924
Epoch [6/30], Batch [3100/6000], Loss: 1.5105
Epoch [6/30], Batch [3200/6000], Loss: 1.4858
Epoch [6/30], Batch [3300/6000], Loss: 1.5023
Epoch [6/30], Batch [3400/6000], Loss: 1.4884
Epoch [6/30], Batch [3500/6000], Loss: 1.5955
Epoch [6/30], Batch [3600/6000], Loss: 1.4873
Epoch [6/30], Batch [3700/6000], Loss: 1.4817
Epoch [6/30], Batch [3800/6000], Loss: 1.4971
Epoch [6/30], Batch [3900/6000], Loss: 1.5450
Epoch [6/30], Batch [4000/6000], Loss: 1.5265
Epoch [6/30], Batch [4100/6000], Loss: 1.4846
Epoch [6/30], Batch [4200/6000], Loss: 1.5094
Epoch [6/30], Batch [4300/6000], Loss: 1.5007
Epoch [6/30], Batch [4400/6000], Loss: 1.4931
Epoch [6/30], Batch [4500/6000], Loss: 1.4938
Epoch [6/30], Batch [4600/6000], Loss: 1.4931
Epoch [6/30], Batch [4700/6000], Loss: 1.4913
Epoch [6/30], Batch [4800/6000], Loss: 1.4993
Epoch [6/30], Batch [4900/6000], Loss: 1.5144
Epoch [6/30], Batch [5000/6000], Loss: 1.5090
Epoch [6/30], Batch [5100/6000], Loss: 1.5159
Epoch [6/30], Batch [5200/6000], Loss: 1.5868
Epoch [6/30], Batch [5300/6000], Loss: 1.6273
Epoch [6/30], Batch [5400/6000], Loss: 1.4880
Epoch [6/30], Batch [5500/6000], Loss: 1.5965
Epoch [6/30], Batch [5600/6000], Loss: 1.4912
Epoch [6/30], Batch [5700/6000], Loss: 1.4942
Epoch [6/30], Batch [5800/6000], Loss: 1.4875
Epoch [6/30], Batch [5900/6000], Loss: 1.4898
Epoch [6/30], Loss: 1.5216
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 1.5900
Epoch [7/30], Batch [100/6000], Loss: 1.4916
Epoch [7/30], Batch [200/6000], Loss: 1.4832
Epoch [7/30], Batch [300/6000], Loss: 1.4813
Epoch [7/30], Batch [400/6000], Loss: 1.4974
Epoch [7/30], Batch [500/6000], Loss: 1.4863
Epoch [7/30], Batch [600/6000], Loss: 1.5914
Epoch [7/30], Batch [700/6000], Loss: 1.5068
Epoch [7/30], Batch [800/6000], Loss: 1.5896
Epoch [7/30], Batch [900/6000], Loss: 1.5005
Epoch [7/30], Batch [1000/6000], Loss: 1.4975
Epoch [7/30], Batch [1100/6000], Loss: 1.5029
Epoch [7/30], Batch [1200/6000], Loss: 1.5861
Epoch [7/30], Batch [1300/6000], Loss: 1.4812
Epoch [7/30], Batch [1400/6000], Loss: 1.5858
Epoch [7/30], Batch [1500/6000], Loss: 1.4953
Epoch [7/30], Batch [1600/6000], Loss: 1.4863
Epoch [7/30], Batch [1700/6000], Loss: 1.5006
Epoch [7/30], Batch [1800/6000], Loss: 1.5000
Epoch [7/30], Batch [1900/6000], Loss: 1.4868
Epoch [7/30], Batch [2000/6000], Loss: 1.4893
Epoch [7/30], Batch [2100/6000], Loss: 1.5051
Epoch [7/30], Batch [2200/6000], Loss: 1.6879
Epoch [7/30], Batch [2300/6000], Loss: 1.4958
Epoch [7/30], Batch [2400/6000], Loss: 1.5024
Epoch [7/30], Batch [2500/6000], Loss: 1.5983
Epoch [7/30], Batch [2600/6000], Loss: 1.4927
Epoch [7/30], Batch [2700/6000], Loss: 1.4892
Epoch [7/30], Batch [2800/6000], Loss: 1.5038
Epoch [7/30], Batch [2900/6000], Loss: 1.4878
Epoch [7/30], Batch [3000/6000], Loss: 1.5141
Epoch [7/30], Batch [3100/6000], Loss: 1.5129
Epoch [7/30], Batch [3200/6000], Loss: 1.4873
Epoch [7/30], Batch [3300/6000], Loss: 1.4837
Epoch [7/30], Batch [3400/6000], Loss: 1.4962
Epoch [7/30], Batch [3500/6000], Loss: 1.4987
Epoch [7/30], Batch [3600/6000], Loss: 1.5171
Epoch [7/30], Batch [3700/6000], Loss: 1.5696
Epoch [7/30], Batch [3800/6000], Loss: 1.6855
Epoch [7/30], Batch [3900/6000], Loss: 1.5181
Epoch [7/30], Batch [4000/6000], Loss: 1.5063
Epoch [7/30], Batch [4100/6000], Loss: 1.5097
Epoch [7/30], Batch [4200/6000], Loss: 1.5161
Epoch [7/30], Batch [4300/6000], Loss: 1.5102
Epoch [7/30], Batch [4400/6000], Loss: 1.4824
Epoch [7/30], Batch [4500/6000], Loss: 1.4935
Epoch [7/30], Batch [4600/6000], Loss: 1.6029
Epoch [7/30], Batch [4700/6000], Loss: 1.4788
Epoch [7/30], Batch [4800/6000], Loss: 1.4895
Epoch [7/30], Batch [4900/6000], Loss: 1.4969
Epoch [7/30], Batch [5000/6000], Loss: 1.4888
Epoch [7/30], Batch [5100/6000], Loss: 1.4817
Epoch [7/30], Batch [5200/6000], Loss: 1.4910
Epoch [7/30], Batch [5300/6000], Loss: 1.5967
Epoch [7/30], Batch [5400/6000], Loss: 1.5000
Epoch [7/30], Batch [5500/6000], Loss: 1.5002
Epoch [7/30], Batch [5600/6000], Loss: 1.4884
Epoch [7/30], Batch [5700/6000], Loss: 1.5154
Epoch [7/30], Batch [5800/6000], Loss: 1.5733
Epoch [7/30], Batch [5900/6000], Loss: 1.5082
Epoch [7/30], Loss: 1.5164
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 1.5079
Epoch [8/30], Batch [100/6000], Loss: 1.5808
Epoch [8/30], Batch [200/6000], Loss: 1.6660
Epoch [8/30], Batch [300/6000], Loss: 1.6025
Epoch [8/30], Batch [400/6000], Loss: 1.4846
Epoch [8/30], Batch [500/6000], Loss: 1.4843
Epoch [8/30], Batch [600/6000], Loss: 1.4871
Epoch [8/30], Batch [700/6000], Loss: 1.5974
Epoch [8/30], Batch [800/6000], Loss: 1.4940
Epoch [8/30], Batch [900/6000], Loss: 1.4863
Epoch [8/30], Batch [1000/6000], Loss: 1.5303
Epoch [8/30], Batch [1100/6000], Loss: 1.5034
Epoch [8/30], Batch [1200/6000], Loss: 1.4904
Epoch [8/30], Batch [1300/6000], Loss: 1.5241
Epoch [8/30], Batch [1400/6000], Loss: 1.4816
Epoch [8/30], Batch [1500/6000], Loss: 1.5162
Epoch [8/30], Batch [1600/6000], Loss: 1.6039
Epoch [8/30], Batch [1700/6000], Loss: 1.4855
Epoch [8/30], Batch [1800/6000], Loss: 1.4992
Epoch [8/30], Batch [1900/6000], Loss: 1.5169
Epoch [8/30], Batch [2000/6000], Loss: 1.4973
Epoch [8/30], Batch [2100/6000], Loss: 1.4898
Epoch [8/30], Batch [2200/6000], Loss: 1.4811
Epoch [8/30], Batch [2300/6000], Loss: 1.4886
Epoch [8/30], Batch [2400/6000], Loss: 1.4851
Epoch [8/30], Batch [2500/6000], Loss: 1.5618
Epoch [8/30], Batch [2600/6000], Loss: 1.4823
Epoch [8/30], Batch [2700/6000], Loss: 1.4875
Epoch [8/30], Batch [2800/6000], Loss: 1.4885
Epoch [8/30], Batch [2900/6000], Loss: 1.4982
Epoch [8/30], Batch [3000/6000], Loss: 1.5006
Epoch [8/30], Batch [3100/6000], Loss: 1.4859
Epoch [8/30], Batch [3200/6000], Loss: 1.5979
Epoch [8/30], Batch [3300/6000], Loss: 1.4856
Epoch [8/30], Batch [3400/6000], Loss: 1.5999
Epoch [8/30], Batch [3500/6000], Loss: 1.4837
Epoch [8/30], Batch [3600/6000], Loss: 1.4874
Epoch [8/30], Batch [3700/6000], Loss: 1.5338
Epoch [8/30], Batch [3800/6000], Loss: 1.4846
Epoch [8/30], Batch [3900/6000], Loss: 1.4839
Epoch [8/30], Batch [4000/6000], Loss: 1.5063
Epoch [8/30], Batch [4100/6000], Loss: 1.4893
Epoch [8/30], Batch [4200/6000], Loss: 1.5975
Epoch [8/30], Batch [4300/6000], Loss: 1.4869
Epoch [8/30], Batch [4400/6000], Loss: 1.5011
Epoch [8/30], Batch [4500/6000], Loss: 1.5043
Epoch [8/30], Batch [4600/6000], Loss: 1.4946
Epoch [8/30], Batch [4700/6000], Loss: 1.5017
Epoch [8/30], Batch [4800/6000], Loss: 1.4870
Epoch [8/30], Batch [4900/6000], Loss: 1.6027
Epoch [8/30], Batch [5000/6000], Loss: 1.6172
Epoch [8/30], Batch [5100/6000], Loss: 1.4840
Epoch [8/30], Batch [5200/6000], Loss: 1.5315
Epoch [8/30], Batch [5300/6000], Loss: 1.5250
Epoch [8/30], Batch [5400/6000], Loss: 1.5408
Epoch [8/30], Batch [5500/6000], Loss: 1.4848
Epoch [8/30], Batch [5600/6000], Loss: 1.4803
Epoch [8/30], Batch [5700/6000], Loss: 1.6007
Epoch [8/30], Batch [5800/6000], Loss: 1.4981
Epoch [8/30], Batch [5900/6000], Loss: 1.5226
Epoch [8/30], Loss: 1.5124
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 1.7973
Epoch [9/30], Batch [100/6000], Loss: 1.4793
Epoch [9/30], Batch [200/6000], Loss: 1.4873
Epoch [9/30], Batch [300/6000], Loss: 1.4892
Epoch [9/30], Batch [400/6000], Loss: 1.4877
Epoch [9/30], Batch [500/6000], Loss: 1.4843
Epoch [9/30], Batch [600/6000], Loss: 1.4851
Epoch [9/30], Batch [700/6000], Loss: 1.4850
Epoch [9/30], Batch [800/6000], Loss: 1.4823
Epoch [9/30], Batch [900/6000], Loss: 1.4830
Epoch [9/30], Batch [1000/6000], Loss: 1.4885
Epoch [9/30], Batch [1100/6000], Loss: 1.4878
Epoch [9/30], Batch [1200/6000], Loss: 1.5682
Epoch [9/30], Batch [1300/6000], Loss: 1.5006
Epoch [9/30], Batch [1400/6000], Loss: 1.5035
Epoch [9/30], Batch [1500/6000], Loss: 1.4867
Epoch [9/30], Batch [1600/6000], Loss: 1.5087
Epoch [9/30], Batch [1700/6000], Loss: 1.4798
Epoch [9/30], Batch [1800/6000], Loss: 1.4844
Epoch [9/30], Batch [1900/6000], Loss: 1.4937
Epoch [9/30], Batch [2000/6000], Loss: 1.4943
Epoch [9/30], Batch [2100/6000], Loss: 1.4793
Epoch [9/30], Batch [2200/6000], Loss: 1.4774
Epoch [9/30], Batch [2300/6000], Loss: 1.5943
Epoch [9/30], Batch [2400/6000], Loss: 1.5119
Epoch [9/30], Batch [2500/6000], Loss: 1.4866
Epoch [9/30], Batch [2600/6000], Loss: 1.4891
Epoch [9/30], Batch [2700/6000], Loss: 1.4848
Epoch [9/30], Batch [2800/6000], Loss: 1.4848
Epoch [9/30], Batch [2900/6000], Loss: 1.5118
Epoch [9/30], Batch [3000/6000], Loss: 1.4954
Epoch [9/30], Batch [3100/6000], Loss: 1.4903
Epoch [9/30], Batch [3200/6000], Loss: 1.4773
Epoch [9/30], Batch [3300/6000], Loss: 1.4861
Epoch [9/30], Batch [3400/6000], Loss: 1.4858
Epoch [9/30], Batch [3500/6000], Loss: 1.4827
Epoch [9/30], Batch [3600/6000], Loss: 1.5519
Epoch [9/30], Batch [3700/6000], Loss: 1.4969
Epoch [9/30], Batch [3800/6000], Loss: 1.5498
Epoch [9/30], Batch [3900/6000], Loss: 1.4767
Epoch [9/30], Batch [4000/6000], Loss: 1.4828
Epoch [9/30], Batch [4100/6000], Loss: 1.4914
Epoch [9/30], Batch [4200/6000], Loss: 1.4863
Epoch [9/30], Batch [4300/6000], Loss: 1.4814
Epoch [9/30], Batch [4400/6000], Loss: 1.4850
Epoch [9/30], Batch [4500/6000], Loss: 1.5824
Epoch [9/30], Batch [4600/6000], Loss: 1.4868
Epoch [9/30], Batch [4700/6000], Loss: 1.4830
Epoch [9/30], Batch [4800/6000], Loss: 1.5819
Epoch [9/30], Batch [4900/6000], Loss: 1.4798
Epoch [9/30], Batch [5000/6000], Loss: 1.4910
Epoch [9/30], Batch [5100/6000], Loss: 1.4954
Epoch [9/30], Batch [5200/6000], Loss: 1.5031
Epoch [9/30], Batch [5300/6000], Loss: 1.6021
Epoch [9/30], Batch [5400/6000], Loss: 1.5867
Epoch [9/30], Batch [5500/6000], Loss: 1.4821
Epoch [9/30], Batch [5600/6000], Loss: 1.5084
Epoch [9/30], Batch [5700/6000], Loss: 1.4819
Epoch [9/30], Batch [5800/6000], Loss: 1.4803
Epoch [9/30], Batch [5900/6000], Loss: 1.5382
Epoch [9/30], Loss: 1.5089
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 1.4959
Epoch [10/30], Batch [100/6000], Loss: 1.4797
Epoch [10/30], Batch [200/6000], Loss: 1.4903
Epoch [10/30], Batch [300/6000], Loss: 1.4835
Epoch [10/30], Batch [400/6000], Loss: 1.4783
Epoch [10/30], Batch [500/6000], Loss: 1.4845
Epoch [10/30], Batch [600/6000], Loss: 1.4923
Epoch [10/30], Batch [700/6000], Loss: 1.5139
Epoch [10/30], Batch [800/6000], Loss: 1.5850
Epoch [10/30], Batch [900/6000], Loss: 1.4795
Epoch [10/30], Batch [1000/6000], Loss: 1.4837
Epoch [10/30], Batch [1100/6000], Loss: 1.4843
Epoch [10/30], Batch [1200/6000], Loss: 1.5961
Epoch [10/30], Batch [1300/6000], Loss: 1.5105
Epoch [10/30], Batch [1400/6000], Loss: 1.5031
Epoch [10/30], Batch [1500/6000], Loss: 1.4799
Epoch [10/30], Batch [1600/6000], Loss: 1.4894
Epoch [10/30], Batch [1700/6000], Loss: 1.4764
Epoch [10/30], Batch [1800/6000], Loss: 1.4844
Epoch [10/30], Batch [1900/6000], Loss: 1.4832
Epoch [10/30], Batch [2000/6000], Loss: 1.4903
Epoch [10/30], Batch [2100/6000], Loss: 1.4876
Epoch [10/30], Batch [2200/6000], Loss: 1.5057
Epoch [10/30], Batch [2300/6000], Loss: 1.4813
Epoch [10/30], Batch [2400/6000], Loss: 1.4821
Epoch [10/30], Batch [2500/6000], Loss: 1.4861
Epoch [10/30], Batch [2600/6000], Loss: 1.4949
Epoch [10/30], Batch [2700/6000], Loss: 1.5842
Epoch [10/30], Batch [2800/6000], Loss: 1.5002
Epoch [10/30], Batch [2900/6000], Loss: 1.4799
Epoch [10/30], Batch [3000/6000], Loss: 1.5027
Epoch [10/30], Batch [3100/6000], Loss: 1.4812
Epoch [10/30], Batch [3200/6000], Loss: 1.5761
Epoch [10/30], Batch [3300/6000], Loss: 1.4951
Epoch [10/30], Batch [3400/6000], Loss: 1.6204
Epoch [10/30], Batch [3500/6000], Loss: 1.5150
Epoch [10/30], Batch [3600/6000], Loss: 1.4836
Epoch [10/30], Batch [3700/6000], Loss: 1.4860
Epoch [10/30], Batch [3800/6000], Loss: 1.4845
Epoch [10/30], Batch [3900/6000], Loss: 1.4831
Epoch [10/30], Batch [4000/6000], Loss: 1.5085
Epoch [10/30], Batch [4100/6000], Loss: 1.5075
Epoch [10/30], Batch [4200/6000], Loss: 1.4825
Epoch [10/30], Batch [4300/6000], Loss: 1.4863
Epoch [10/30], Batch [4400/6000], Loss: 1.4778
Epoch [10/30], Batch [4500/6000], Loss: 1.4920
Epoch [10/30], Batch [4600/6000], Loss: 1.4822
Epoch [10/30], Batch [4700/6000], Loss: 1.7141
Epoch [10/30], Batch [4800/6000], Loss: 1.4956
Epoch [10/30], Batch [4900/6000], Loss: 1.5008
Epoch [10/30], Batch [5000/6000], Loss: 1.4867
Epoch [10/30], Batch [5100/6000], Loss: 1.5010
Epoch [10/30], Batch [5200/6000], Loss: 1.5393
Epoch [10/30], Batch [5300/6000], Loss: 1.5180
Epoch [10/30], Batch [5400/6000], Loss: 1.4936
Epoch [10/30], Batch [5500/6000], Loss: 1.4919
Epoch [10/30], Batch [5600/6000], Loss: 1.4971
Epoch [10/30], Batch [5700/6000], Loss: 1.4731
Epoch [10/30], Batch [5800/6000], Loss: 1.4841
Epoch [10/30], Batch [5900/6000], Loss: 1.4825
Epoch [10/30], Loss: 1.5054
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 1.4798
Epoch [11/30], Batch [100/6000], Loss: 1.4949
Epoch [11/30], Batch [200/6000], Loss: 1.4829
Epoch [11/30], Batch [300/6000], Loss: 1.4853
Epoch [11/30], Batch [400/6000], Loss: 1.4876
Epoch [11/30], Batch [500/6000], Loss: 1.4852
Epoch [11/30], Batch [600/6000], Loss: 1.4994
Epoch [11/30], Batch [700/6000], Loss: 1.4949
Epoch [11/30], Batch [800/6000], Loss: 1.5840
Epoch [11/30], Batch [900/6000], Loss: 1.4794
Epoch [11/30], Batch [1000/6000], Loss: 1.4888
Epoch [11/30], Batch [1100/6000], Loss: 1.6889
Epoch [11/30], Batch [1200/6000], Loss: 1.6101
Epoch [11/30], Batch [1300/6000], Loss: 1.4941
Epoch [11/30], Batch [1400/6000], Loss: 1.5926
Epoch [11/30], Batch [1500/6000], Loss: 1.4869
Epoch [11/30], Batch [1600/6000], Loss: 1.4803
Epoch [11/30], Batch [1700/6000], Loss: 1.4861
Epoch [11/30], Batch [1800/6000], Loss: 1.4941
Epoch [11/30], Batch [1900/6000], Loss: 1.4822
Epoch [11/30], Batch [2000/6000], Loss: 1.4922
Epoch [11/30], Batch [2100/6000], Loss: 1.5033
Epoch [11/30], Batch [2200/6000], Loss: 1.4805
Epoch [11/30], Batch [2300/6000], Loss: 1.5060
Epoch [11/30], Batch [2400/6000], Loss: 1.4889
Epoch [11/30], Batch [2500/6000], Loss: 1.4852
Epoch [11/30], Batch [2600/6000], Loss: 1.4925
Epoch [11/30], Batch [2700/6000], Loss: 1.4965
Epoch [11/30], Batch [2800/6000], Loss: 1.4793
Epoch [11/30], Batch [2900/6000], Loss: 1.4789
Epoch [11/30], Batch [3000/6000], Loss: 1.4878
Epoch [11/30], Batch [3100/6000], Loss: 1.4802
Epoch [11/30], Batch [3200/6000], Loss: 1.5800
Epoch [11/30], Batch [3300/6000], Loss: 1.4843
Epoch [11/30], Batch [3400/6000], Loss: 1.4775
Epoch [11/30], Batch [3500/6000], Loss: 1.5010
Epoch [11/30], Batch [3600/6000], Loss: 1.4854
Epoch [11/30], Batch [3700/6000], Loss: 1.4828
Epoch [11/30], Batch [3800/6000], Loss: 1.4793
Epoch [11/30], Batch [3900/6000], Loss: 1.6708
Epoch [11/30], Batch [4000/6000], Loss: 1.5032
Epoch [11/30], Batch [4100/6000], Loss: 1.4826
Epoch [11/30], Batch [4200/6000], Loss: 1.6385
Epoch [11/30], Batch [4300/6000], Loss: 1.4844
Epoch [11/30], Batch [4400/6000], Loss: 1.4945
Epoch [11/30], Batch [4500/6000], Loss: 1.4966
Epoch [11/30], Batch [4600/6000], Loss: 1.4826
Epoch [11/30], Batch [4700/6000], Loss: 1.5028
Epoch [11/30], Batch [4800/6000], Loss: 1.5005
Epoch [11/30], Batch [4900/6000], Loss: 1.4856
Epoch [11/30], Batch [5000/6000], Loss: 1.4915
Epoch [11/30], Batch [5100/6000], Loss: 1.4793
Epoch [11/30], Batch [5200/6000], Loss: 1.4759
Epoch [11/30], Batch [5300/6000], Loss: 1.4955
Epoch [11/30], Batch [5400/6000], Loss: 1.5098
Epoch [11/30], Batch [5500/6000], Loss: 1.4777
Epoch [11/30], Batch [5600/6000], Loss: 1.6038
Epoch [11/30], Batch [5700/6000], Loss: 1.4951
Epoch [11/30], Batch [5800/6000], Loss: 1.4804
Epoch [11/30], Batch [5900/6000], Loss: 1.4791
Epoch [11/30], Loss: 1.5027
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 1.5002
Epoch [12/30], Batch [100/6000], Loss: 1.5067
Epoch [12/30], Batch [200/6000], Loss: 1.4901
Epoch [12/30], Batch [300/6000], Loss: 1.4919
Epoch [12/30], Batch [400/6000], Loss: 1.5033
Epoch [12/30], Batch [500/6000], Loss: 1.4972
Epoch [12/30], Batch [600/6000], Loss: 1.4844
Epoch [12/30], Batch [700/6000], Loss: 1.4802
Epoch [12/30], Batch [800/6000], Loss: 1.4805
Epoch [12/30], Batch [900/6000], Loss: 1.5049
Epoch [12/30], Batch [1000/6000], Loss: 1.4807
Epoch [12/30], Batch [1100/6000], Loss: 1.5854
Epoch [12/30], Batch [1200/6000], Loss: 1.4975
Epoch [12/30], Batch [1300/6000], Loss: 1.5008
Epoch [12/30], Batch [1400/6000], Loss: 1.5062
Epoch [12/30], Batch [1500/6000], Loss: 1.4967
Epoch [12/30], Batch [1600/6000], Loss: 1.4788
Epoch [12/30], Batch [1700/6000], Loss: 1.6104
Epoch [12/30], Batch [1800/6000], Loss: 1.5157
Epoch [12/30], Batch [1900/6000], Loss: 1.4766
Epoch [12/30], Batch [2000/6000], Loss: 1.5051
Epoch [12/30], Batch [2100/6000], Loss: 1.4885
Epoch [12/30], Batch [2200/6000], Loss: 1.4796
Epoch [12/30], Batch [2300/6000], Loss: 1.5011
Epoch [12/30], Batch [2400/6000], Loss: 1.4830
Epoch [12/30], Batch [2500/6000], Loss: 1.4858
Epoch [12/30], Batch [2600/6000], Loss: 1.5884
Epoch [12/30], Batch [2700/6000], Loss: 1.4776
Epoch [12/30], Batch [2800/6000], Loss: 1.4762
Epoch [12/30], Batch [2900/6000], Loss: 1.5080
Epoch [12/30], Batch [3000/6000], Loss: 1.4805
Epoch [12/30], Batch [3100/6000], Loss: 1.5446
Epoch [12/30], Batch [3200/6000], Loss: 1.4871
Epoch [12/30], Batch [3300/6000], Loss: 1.4767
Epoch [12/30], Batch [3400/6000], Loss: 1.5069
Epoch [12/30], Batch [3500/6000], Loss: 1.4912
Epoch [12/30], Batch [3600/6000], Loss: 1.4788
Epoch [12/30], Batch [3700/6000], Loss: 1.4925
Epoch [12/30], Batch [3800/6000], Loss: 1.4837
Epoch [12/30], Batch [3900/6000], Loss: 1.4776
Epoch [12/30], Batch [4000/6000], Loss: 1.4796
Epoch [12/30], Batch [4100/6000], Loss: 1.4846
Epoch [12/30], Batch [4200/6000], Loss: 1.5825
Epoch [12/30], Batch [4300/6000], Loss: 1.4906
Epoch [12/30], Batch [4400/6000], Loss: 1.5770
Epoch [12/30], Batch [4500/6000], Loss: 1.5185
Epoch [12/30], Batch [4600/6000], Loss: 1.4789
Epoch [12/30], Batch [4700/6000], Loss: 1.4798
Epoch [12/30], Batch [4800/6000], Loss: 1.4807
Epoch [12/30], Batch [4900/6000], Loss: 1.4824
Epoch [12/30], Batch [5000/6000], Loss: 1.4751
Epoch [12/30], Batch [5100/6000], Loss: 1.5213
Epoch [12/30], Batch [5200/6000], Loss: 1.4811
Epoch [12/30], Batch [5300/6000], Loss: 1.4954
Epoch [12/30], Batch [5400/6000], Loss: 1.5009
Epoch [12/30], Batch [5500/6000], Loss: 1.4913
Epoch [12/30], Batch [5600/6000], Loss: 1.4840
Epoch [12/30], Batch [5700/6000], Loss: 1.4941
Epoch [12/30], Batch [5800/6000], Loss: 1.5884
Epoch [12/30], Batch [5900/6000], Loss: 1.5029
Epoch [12/30], Loss: 1.5004
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 1.4842
Epoch [13/30], Batch [100/6000], Loss: 1.5780
Epoch [13/30], Batch [200/6000], Loss: 1.4804
Epoch [13/30], Batch [300/6000], Loss: 1.4874
Epoch [13/30], Batch [400/6000], Loss: 1.5150
Epoch [13/30], Batch [500/6000], Loss: 1.4750
Epoch [13/30], Batch [600/6000], Loss: 1.4995
Epoch [13/30], Batch [700/6000], Loss: 1.4935
Epoch [13/30], Batch [800/6000], Loss: 1.4977
Epoch [13/30], Batch [900/6000], Loss: 1.4794
Epoch [13/30], Batch [1000/6000], Loss: 1.4812
Epoch [13/30], Batch [1100/6000], Loss: 1.4971
Epoch [13/30], Batch [1200/6000], Loss: 1.4912
Epoch [13/30], Batch [1300/6000], Loss: 1.4900
Epoch [13/30], Batch [1400/6000], Loss: 1.4923
Epoch [13/30], Batch [1500/6000], Loss: 1.4850
Epoch [13/30], Batch [1600/6000], Loss: 1.4776
Epoch [13/30], Batch [1700/6000], Loss: 1.4770
Epoch [13/30], Batch [1800/6000], Loss: 1.5961
Epoch [13/30], Batch [1900/6000], Loss: 1.5028
Epoch [13/30], Batch [2000/6000], Loss: 1.4803
Epoch [13/30], Batch [2100/6000], Loss: 1.4951
Epoch [13/30], Batch [2200/6000], Loss: 1.5900
Epoch [13/30], Batch [2300/6000], Loss: 1.4786
Epoch [13/30], Batch [2400/6000], Loss: 1.4779
Epoch [13/30], Batch [2500/6000], Loss: 1.4812
Epoch [13/30], Batch [2600/6000], Loss: 1.4775
Epoch [13/30], Batch [2700/6000], Loss: 1.5924
Epoch [13/30], Batch [2800/6000], Loss: 1.4794
Epoch [13/30], Batch [2900/6000], Loss: 1.4827
Epoch [13/30], Batch [3000/6000], Loss: 1.5928
Epoch [13/30], Batch [3100/6000], Loss: 1.4798
Epoch [13/30], Batch [3200/6000], Loss: 1.4906
Epoch [13/30], Batch [3300/6000], Loss: 1.4808
Epoch [13/30], Batch [3400/6000], Loss: 1.5551
Epoch [13/30], Batch [3500/6000], Loss: 1.4824
Epoch [13/30], Batch [3600/6000], Loss: 1.5908
Epoch [13/30], Batch [3700/6000], Loss: 1.4853
Epoch [13/30], Batch [3800/6000], Loss: 1.4831
Epoch [13/30], Batch [3900/6000], Loss: 1.5097
Epoch [13/30], Batch [4000/6000], Loss: 1.4985
Epoch [13/30], Batch [4100/6000], Loss: 1.5075
Epoch [13/30], Batch [4200/6000], Loss: 1.5001
Epoch [13/30], Batch [4300/6000], Loss: 1.4865
Epoch [13/30], Batch [4400/6000], Loss: 1.5007
Epoch [13/30], Batch [4500/6000], Loss: 1.4754
Epoch [13/30], Batch [4600/6000], Loss: 1.4885
Epoch [13/30], Batch [4700/6000], Loss: 1.4755
Epoch [13/30], Batch [4800/6000], Loss: 1.4860
Epoch [13/30], Batch [4900/6000], Loss: 1.4946
Epoch [13/30], Batch [5000/6000], Loss: 1.4947
Epoch [13/30], Batch [5100/6000], Loss: 1.4910
Epoch [13/30], Batch [5200/6000], Loss: 1.4856
Epoch [13/30], Batch [5300/6000], Loss: 1.4846
Epoch [13/30], Batch [5400/6000], Loss: 1.5817
Epoch [13/30], Batch [5500/6000], Loss: 1.4874
Epoch [13/30], Batch [5600/6000], Loss: 1.4842
Epoch [13/30], Batch [5700/6000], Loss: 1.4812
Epoch [13/30], Batch [5800/6000], Loss: 1.4756
Epoch [13/30], Batch [5900/6000], Loss: 1.4894
Epoch [13/30], Loss: 1.4987
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 1.4951
Epoch [14/30], Batch [100/6000], Loss: 1.4783
Epoch [14/30], Batch [200/6000], Loss: 1.4886
Epoch [14/30], Batch [300/6000], Loss: 1.5056
Epoch [14/30], Batch [400/6000], Loss: 1.4785
Epoch [14/30], Batch [500/6000], Loss: 1.4867
Epoch [14/30], Batch [600/6000], Loss: 1.4862
Epoch [14/30], Batch [700/6000], Loss: 1.4860
Epoch [14/30], Batch [800/6000], Loss: 1.4799
Epoch [14/30], Batch [900/6000], Loss: 1.4835
Epoch [14/30], Batch [1000/6000], Loss: 1.4836
Epoch [14/30], Batch [1100/6000], Loss: 1.4823
Epoch [14/30], Batch [1200/6000], Loss: 1.5117
Epoch [14/30], Batch [1300/6000], Loss: 1.4845
Epoch [14/30], Batch [1400/6000], Loss: 1.4773
Epoch [14/30], Batch [1500/6000], Loss: 1.4830
Epoch [14/30], Batch [1600/6000], Loss: 1.4929
Epoch [14/30], Batch [1700/6000], Loss: 1.4863
Epoch [14/30], Batch [1800/6000], Loss: 1.4770
Epoch [14/30], Batch [1900/6000], Loss: 1.4763
Epoch [14/30], Batch [2000/6000], Loss: 1.4826
Epoch [14/30], Batch [2100/6000], Loss: 1.4799
Epoch [14/30], Batch [2200/6000], Loss: 1.4808
Epoch [14/30], Batch [2300/6000], Loss: 1.4942
Epoch [14/30], Batch [2400/6000], Loss: 1.4804
Epoch [14/30], Batch [2500/6000], Loss: 1.5057
Epoch [14/30], Batch [2600/6000], Loss: 1.4818
Epoch [14/30], Batch [2700/6000], Loss: 1.4928
Epoch [14/30], Batch [2800/6000], Loss: 1.4864
Epoch [14/30], Batch [2900/6000], Loss: 1.4820
Epoch [14/30], Batch [3000/6000], Loss: 1.5088
Epoch [14/30], Batch [3100/6000], Loss: 1.4794
Epoch [14/30], Batch [3200/6000], Loss: 1.4866
Epoch [14/30], Batch [3300/6000], Loss: 1.4849
Epoch [14/30], Batch [3400/6000], Loss: 1.4840
Epoch [14/30], Batch [3500/6000], Loss: 1.4803
Epoch [14/30], Batch [3600/6000], Loss: 1.5648
Epoch [14/30], Batch [3700/6000], Loss: 1.5603
Epoch [14/30], Batch [3800/6000], Loss: 1.4997
Epoch [14/30], Batch [3900/6000], Loss: 1.4996
Epoch [14/30], Batch [4000/6000], Loss: 1.4823
Epoch [14/30], Batch [4100/6000], Loss: 1.5049
Epoch [14/30], Batch [4200/6000], Loss: 1.4775
Epoch [14/30], Batch [4300/6000], Loss: 1.4801
Epoch [14/30], Batch [4400/6000], Loss: 1.4928
Epoch [14/30], Batch [4500/6000], Loss: 1.5215
Epoch [14/30], Batch [4600/6000], Loss: 1.4833
Epoch [14/30], Batch [4700/6000], Loss: 1.4811
Epoch [14/30], Batch [4800/6000], Loss: 1.4789
Epoch [14/30], Batch [4900/6000], Loss: 1.4809
Epoch [14/30], Batch [5000/6000], Loss: 1.5039
Epoch [14/30], Batch [5100/6000], Loss: 1.4774
Epoch [14/30], Batch [5200/6000], Loss: 1.4887
Epoch [14/30], Batch [5300/6000], Loss: 1.4754
Epoch [14/30], Batch [5400/6000], Loss: 1.5790
Epoch [14/30], Batch [5500/6000], Loss: 1.4882
Epoch [14/30], Batch [5600/6000], Loss: 1.4782
Epoch [14/30], Batch [5700/6000], Loss: 1.4848
Epoch [14/30], Batch [5800/6000], Loss: 1.4747
Epoch [14/30], Batch [5900/6000], Loss: 1.4936
Epoch [14/30], Loss: 1.4965
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 1.5490
Epoch [15/30], Batch [100/6000], Loss: 1.5776
Epoch [15/30], Batch [200/6000], Loss: 1.4806
Epoch [15/30], Batch [300/6000], Loss: 1.4775
Epoch [15/30], Batch [400/6000], Loss: 1.4806
Epoch [15/30], Batch [500/6000], Loss: 1.4809
Epoch [15/30], Batch [600/6000], Loss: 1.4829
Epoch [15/30], Batch [700/6000], Loss: 1.4782
Epoch [15/30], Batch [800/6000], Loss: 1.4802
Epoch [15/30], Batch [900/6000], Loss: 1.4778
Epoch [15/30], Batch [1000/6000], Loss: 1.4779
Epoch [15/30], Batch [1100/6000], Loss: 1.4724
Epoch [15/30], Batch [1200/6000], Loss: 1.4962
Epoch [15/30], Batch [1300/6000], Loss: 1.4899
Epoch [15/30], Batch [1400/6000], Loss: 1.4794
Epoch [15/30], Batch [1500/6000], Loss: 1.5042
Epoch [15/30], Batch [1600/6000], Loss: 1.4794
Epoch [15/30], Batch [1700/6000], Loss: 1.4796
Epoch [15/30], Batch [1800/6000], Loss: 1.4786
Epoch [15/30], Batch [1900/6000], Loss: 1.4814
Epoch [15/30], Batch [2000/6000], Loss: 1.4816
Epoch [15/30], Batch [2100/6000], Loss: 1.5025
Epoch [15/30], Batch [2200/6000], Loss: 1.4902
Epoch [15/30], Batch [2300/6000], Loss: 1.4803
Epoch [15/30], Batch [2400/6000], Loss: 1.4815
Epoch [15/30], Batch [2500/6000], Loss: 1.4774
Epoch [15/30], Batch [2600/6000], Loss: 1.4795
Epoch [15/30], Batch [2700/6000], Loss: 1.4796
Epoch [15/30], Batch [2800/6000], Loss: 1.4756
Epoch [15/30], Batch [2900/6000], Loss: 1.4894
Epoch [15/30], Batch [3000/6000], Loss: 1.4790
Epoch [15/30], Batch [3100/6000], Loss: 1.4941
Epoch [15/30], Batch [3200/6000], Loss: 1.4810
Epoch [15/30], Batch [3300/6000], Loss: 1.4805
Epoch [15/30], Batch [3400/6000], Loss: 1.4722
Epoch [15/30], Batch [3500/6000], Loss: 1.4938
Epoch [15/30], Batch [3600/6000], Loss: 1.5096
Epoch [15/30], Batch [3700/6000], Loss: 1.4772
Epoch [15/30], Batch [3800/6000], Loss: 1.5056
Epoch [15/30], Batch [3900/6000], Loss: 1.4777
Epoch [15/30], Batch [4000/6000], Loss: 1.4790
Epoch [15/30], Batch [4100/6000], Loss: 1.4766
Epoch [15/30], Batch [4200/6000], Loss: 1.4758
Epoch [15/30], Batch [4300/6000], Loss: 1.4926
Epoch [15/30], Batch [4400/6000], Loss: 1.4822
Epoch [15/30], Batch [4500/6000], Loss: 1.4737
Epoch [15/30], Batch [4600/6000], Loss: 1.5029
Epoch [15/30], Batch [4700/6000], Loss: 1.4864
Epoch [15/30], Batch [4800/6000], Loss: 1.4807
Epoch [15/30], Batch [4900/6000], Loss: 1.4847
Epoch [15/30], Batch [5000/6000], Loss: 1.4783
Epoch [15/30], Batch [5100/6000], Loss: 1.4980
Epoch [15/30], Batch [5200/6000], Loss: 1.4786
Epoch [15/30], Batch [5300/6000], Loss: 1.4750
Epoch [15/30], Batch [5400/6000], Loss: 1.4807
Epoch [15/30], Batch [5500/6000], Loss: 1.4779
Epoch [15/30], Batch [5600/6000], Loss: 1.4813
Epoch [15/30], Batch [5700/6000], Loss: 1.5777
Epoch [15/30], Batch [5800/6000], Loss: 1.5137
Epoch [15/30], Batch [5900/6000], Loss: 1.5827
Epoch [15/30], Loss: 1.4955
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 1.5095
Epoch [16/30], Batch [100/6000], Loss: 1.5047
Epoch [16/30], Batch [200/6000], Loss: 1.5176
Epoch [16/30], Batch [300/6000], Loss: 1.4823
Epoch [16/30], Batch [400/6000], Loss: 1.4773
Epoch [16/30], Batch [500/6000], Loss: 1.5025
Epoch [16/30], Batch [600/6000], Loss: 1.4781
Epoch [16/30], Batch [700/6000], Loss: 1.4893
Epoch [16/30], Batch [800/6000], Loss: 1.4765
Epoch [16/30], Batch [900/6000], Loss: 1.4941
Epoch [16/30], Batch [1000/6000], Loss: 1.4790
Epoch [16/30], Batch [1100/6000], Loss: 1.4806
Epoch [16/30], Batch [1200/6000], Loss: 1.4807
Epoch [16/30], Batch [1300/6000], Loss: 1.4805
Epoch [16/30], Batch [1400/6000], Loss: 1.4737
Epoch [16/30], Batch [1500/6000], Loss: 1.4953
Epoch [16/30], Batch [1600/6000], Loss: 1.4725
Epoch [16/30], Batch [1700/6000], Loss: 1.4912
Epoch [16/30], Batch [1800/6000], Loss: 1.4777
Epoch [16/30], Batch [1900/6000], Loss: 1.4826
Epoch [16/30], Batch [2000/6000], Loss: 1.4937
Epoch [16/30], Batch [2100/6000], Loss: 1.6038
Epoch [16/30], Batch [2200/6000], Loss: 1.4782
Epoch [16/30], Batch [2300/6000], Loss: 1.4871
Epoch [16/30], Batch [2400/6000], Loss: 1.4775
Epoch [16/30], Batch [2500/6000], Loss: 1.4948
Epoch [16/30], Batch [2600/6000], Loss: 1.5718
Epoch [16/30], Batch [2700/6000], Loss: 1.4789
Epoch [16/30], Batch [2800/6000], Loss: 1.4748
Epoch [16/30], Batch [2900/6000], Loss: 1.4766
Epoch [16/30], Batch [3000/6000], Loss: 1.4784
Epoch [16/30], Batch [3100/6000], Loss: 1.4788
Epoch [16/30], Batch [3200/6000], Loss: 1.4880
Epoch [16/30], Batch [3300/6000], Loss: 1.4848
Epoch [16/30], Batch [3400/6000], Loss: 1.4960
Epoch [16/30], Batch [3500/6000], Loss: 1.4761
Epoch [16/30], Batch [3600/6000], Loss: 1.4761
Epoch [16/30], Batch [3700/6000], Loss: 1.5098
Epoch [16/30], Batch [3800/6000], Loss: 1.4839
Epoch [16/30], Batch [3900/6000], Loss: 1.4802
Epoch [16/30], Batch [4000/6000], Loss: 1.5795
Epoch [16/30], Batch [4100/6000], Loss: 1.4796
Epoch [16/30], Batch [4200/6000], Loss: 1.4908
Epoch [16/30], Batch [4300/6000], Loss: 1.4739
Epoch [16/30], Batch [4400/6000], Loss: 1.4783
Epoch [16/30], Batch [4500/6000], Loss: 1.4710
Epoch [16/30], Batch [4600/6000], Loss: 1.4805
Epoch [16/30], Batch [4700/6000], Loss: 1.4823
Epoch [16/30], Batch [4800/6000], Loss: 1.4789
Epoch [16/30], Batch [4900/6000], Loss: 1.4917
Epoch [16/30], Batch [5000/6000], Loss: 1.4821
Epoch [16/30], Batch [5100/6000], Loss: 1.4827
Epoch [16/30], Batch [5200/6000], Loss: 1.4854
Epoch [16/30], Batch [5300/6000], Loss: 1.5945
Epoch [16/30], Batch [5400/6000], Loss: 1.4768
Epoch [16/30], Batch [5500/6000], Loss: 1.5948
Epoch [16/30], Batch [5600/6000], Loss: 1.4772
Epoch [16/30], Batch [5700/6000], Loss: 1.4763
Epoch [16/30], Batch [5800/6000], Loss: 1.4852
Epoch [16/30], Batch [5900/6000], Loss: 1.4784
Epoch [16/30], Loss: 1.4938
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 1.5798
Epoch [17/30], Batch [100/6000], Loss: 1.4703
Epoch [17/30], Batch [200/6000], Loss: 1.4958
Epoch [17/30], Batch [300/6000], Loss: 1.4822
Epoch [17/30], Batch [400/6000], Loss: 1.5937
Epoch [17/30], Batch [500/6000], Loss: 1.4774
Epoch [17/30], Batch [600/6000], Loss: 1.4760
Epoch [17/30], Batch [700/6000], Loss: 1.4765
Epoch [17/30], Batch [800/6000], Loss: 1.5015
Epoch [17/30], Batch [900/6000], Loss: 1.4777
Epoch [17/30], Batch [1000/6000], Loss: 1.4758
Epoch [17/30], Batch [1100/6000], Loss: 1.4800
Epoch [17/30], Batch [1200/6000], Loss: 1.4806
Epoch [17/30], Batch [1300/6000], Loss: 1.4770
Epoch [17/30], Batch [1400/6000], Loss: 1.4790
Epoch [17/30], Batch [1500/6000], Loss: 1.4748
Epoch [17/30], Batch [1600/6000], Loss: 1.5076
Epoch [17/30], Batch [1700/6000], Loss: 1.4792
Epoch [17/30], Batch [1800/6000], Loss: 1.5347
Epoch [17/30], Batch [1900/6000], Loss: 1.4759
Epoch [17/30], Batch [2000/6000], Loss: 1.4765
Epoch [17/30], Batch [2100/6000], Loss: 1.4782
Epoch [17/30], Batch [2200/6000], Loss: 1.4811
Epoch [17/30], Batch [2300/6000], Loss: 1.4790
Epoch [17/30], Batch [2400/6000], Loss: 1.5781
Epoch [17/30], Batch [2500/6000], Loss: 1.5423
Epoch [17/30], Batch [2600/6000], Loss: 1.4760
Epoch [17/30], Batch [2700/6000], Loss: 1.5856
Epoch [17/30], Batch [2800/6000], Loss: 1.4785
Epoch [17/30], Batch [2900/6000], Loss: 1.5812
Epoch [17/30], Batch [3000/6000], Loss: 1.4803
Epoch [17/30], Batch [3100/6000], Loss: 1.4782
Epoch [17/30], Batch [3200/6000], Loss: 1.4812
Epoch [17/30], Batch [3300/6000], Loss: 1.4757
Epoch [17/30], Batch [3400/6000], Loss: 1.4744
Epoch [17/30], Batch [3500/6000], Loss: 1.4887
Epoch [17/30], Batch [3600/6000], Loss: 1.4789
Epoch [17/30], Batch [3700/6000], Loss: 1.4797
Epoch [17/30], Batch [3800/6000], Loss: 1.4941
Epoch [17/30], Batch [3900/6000], Loss: 1.4846
Epoch [17/30], Batch [4000/6000], Loss: 1.5875
Epoch [17/30], Batch [4100/6000], Loss: 1.4909
Epoch [17/30], Batch [4200/6000], Loss: 1.4794
Epoch [17/30], Batch [4300/6000], Loss: 1.4776
Epoch [17/30], Batch [4400/6000], Loss: 1.4828
Epoch [17/30], Batch [4500/6000], Loss: 1.4774
Epoch [17/30], Batch [4600/6000], Loss: 1.4774
Epoch [17/30], Batch [4700/6000], Loss: 1.4771
Epoch [17/30], Batch [4800/6000], Loss: 1.4770
Epoch [17/30], Batch [4900/6000], Loss: 1.5766
Epoch [17/30], Batch [5000/6000], Loss: 1.4884
Epoch [17/30], Batch [5100/6000], Loss: 1.4735
Epoch [17/30], Batch [5200/6000], Loss: 1.4749
Epoch [17/30], Batch [5300/6000], Loss: 1.4763
Epoch [17/30], Batch [5400/6000], Loss: 1.4913
Epoch [17/30], Batch [5500/6000], Loss: 1.4825
Epoch [17/30], Batch [5600/6000], Loss: 1.4899
Epoch [17/30], Batch [5700/6000], Loss: 1.4959
Epoch [17/30], Batch [5800/6000], Loss: 1.4854
Epoch [17/30], Batch [5900/6000], Loss: 1.4785
Epoch [17/30], Loss: 1.4922
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 1.5009
Epoch [18/30], Batch [100/6000], Loss: 1.4753
Epoch [18/30], Batch [200/6000], Loss: 1.6093
Epoch [18/30], Batch [300/6000], Loss: 1.4731
Epoch [18/30], Batch [400/6000], Loss: 1.4796
Epoch [18/30], Batch [500/6000], Loss: 1.4793
Epoch [18/30], Batch [600/6000], Loss: 1.4768
Epoch [18/30], Batch [700/6000], Loss: 1.4769
Epoch [18/30], Batch [800/6000], Loss: 1.4885
Epoch [18/30], Batch [900/6000], Loss: 1.4798
Epoch [18/30], Batch [1000/6000], Loss: 1.4757
Epoch [18/30], Batch [1100/6000], Loss: 1.4817
Epoch [18/30], Batch [1200/6000], Loss: 1.4847
Epoch [18/30], Batch [1300/6000], Loss: 1.4812
Epoch [18/30], Batch [1400/6000], Loss: 1.4807
Epoch [18/30], Batch [1500/6000], Loss: 1.4745
Epoch [18/30], Batch [1600/6000], Loss: 1.4920
Epoch [18/30], Batch [1700/6000], Loss: 1.4763
Epoch [18/30], Batch [1800/6000], Loss: 1.4750
Epoch [18/30], Batch [1900/6000], Loss: 1.4730
Epoch [18/30], Batch [2000/6000], Loss: 1.4843
Epoch [18/30], Batch [2100/6000], Loss: 1.4785
Epoch [18/30], Batch [2200/6000], Loss: 1.5813
Epoch [18/30], Batch [2300/6000], Loss: 1.4785
Epoch [18/30], Batch [2400/6000], Loss: 1.4783
Epoch [18/30], Batch [2500/6000], Loss: 1.4798
Epoch [18/30], Batch [2600/6000], Loss: 1.4772
Epoch [18/30], Batch [2700/6000], Loss: 1.4782
Epoch [18/30], Batch [2800/6000], Loss: 1.4782
Epoch [18/30], Batch [2900/6000], Loss: 1.5024
Epoch [18/30], Batch [3000/6000], Loss: 1.4814
Epoch [18/30], Batch [3100/6000], Loss: 1.4751
Epoch [18/30], Batch [3200/6000], Loss: 1.4788
Epoch [18/30], Batch [3300/6000], Loss: 1.4757
Epoch [18/30], Batch [3400/6000], Loss: 1.4831
Epoch [18/30], Batch [3500/6000], Loss: 1.4755
Epoch [18/30], Batch [3600/6000], Loss: 1.4772
Epoch [18/30], Batch [3700/6000], Loss: 1.4775
Epoch [18/30], Batch [3800/6000], Loss: 1.4815
Epoch [18/30], Batch [3900/6000], Loss: 1.4757
Epoch [18/30], Batch [4000/6000], Loss: 1.4785
Epoch [18/30], Batch [4100/6000], Loss: 1.4863
Epoch [18/30], Batch [4200/6000], Loss: 1.4995
Epoch [18/30], Batch [4300/6000], Loss: 1.4795
Epoch [18/30], Batch [4400/6000], Loss: 1.4936
Epoch [18/30], Batch [4500/6000], Loss: 1.5806
Epoch [18/30], Batch [4600/6000], Loss: 1.4748
Epoch [18/30], Batch [4700/6000], Loss: 1.4850
Epoch [18/30], Batch [4800/6000], Loss: 1.4956
Epoch [18/30], Batch [4900/6000], Loss: 1.4887
Epoch [18/30], Batch [5000/6000], Loss: 1.4762
Epoch [18/30], Batch [5100/6000], Loss: 1.4797
Epoch [18/30], Batch [5200/6000], Loss: 1.4902
Epoch [18/30], Batch [5300/6000], Loss: 1.4738
Epoch [18/30], Batch [5400/6000], Loss: 1.4823
Epoch [18/30], Batch [5500/6000], Loss: 1.5007
Epoch [18/30], Batch [5600/6000], Loss: 1.4779
Epoch [18/30], Batch [5700/6000], Loss: 1.4897
Epoch [18/30], Batch [5800/6000], Loss: 1.4811
Epoch [18/30], Batch [5900/6000], Loss: 1.4703
Epoch [18/30], Loss: 1.4917
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 1.4907
Epoch [19/30], Batch [100/6000], Loss: 1.4778
Epoch [19/30], Batch [200/6000], Loss: 1.4791
Epoch [19/30], Batch [300/6000], Loss: 1.4727
Epoch [19/30], Batch [400/6000], Loss: 1.4798
Epoch [19/30], Batch [500/6000], Loss: 1.4805
Epoch [19/30], Batch [600/6000], Loss: 1.4746
Epoch [19/30], Batch [700/6000], Loss: 1.4756
Epoch [19/30], Batch [800/6000], Loss: 1.4805
Epoch [19/30], Batch [900/6000], Loss: 1.4737
Epoch [19/30], Batch [1000/6000], Loss: 1.4806
Epoch [19/30], Batch [1100/6000], Loss: 1.4802
Epoch [19/30], Batch [1200/6000], Loss: 1.4796
Epoch [19/30], Batch [1300/6000], Loss: 1.4800
Epoch [19/30], Batch [1400/6000], Loss: 1.4771
Epoch [19/30], Batch [1500/6000], Loss: 1.4851
Epoch [19/30], Batch [1600/6000], Loss: 1.4931
Epoch [19/30], Batch [1700/6000], Loss: 1.4958
Epoch [19/30], Batch [1800/6000], Loss: 1.4743
Epoch [19/30], Batch [1900/6000], Loss: 1.4789
Epoch [19/30], Batch [2000/6000], Loss: 1.4786
Epoch [19/30], Batch [2100/6000], Loss: 1.5137
Epoch [19/30], Batch [2200/6000], Loss: 1.4750
Epoch [19/30], Batch [2300/6000], Loss: 1.4768
Epoch [19/30], Batch [2400/6000], Loss: 1.4777
Epoch [19/30], Batch [2500/6000], Loss: 1.4780
Epoch [19/30], Batch [2600/6000], Loss: 1.4839
Epoch [19/30], Batch [2700/6000], Loss: 1.4793
Epoch [19/30], Batch [2800/6000], Loss: 1.4863
Epoch [19/30], Batch [2900/6000], Loss: 1.4841
Epoch [19/30], Batch [3000/6000], Loss: 1.4739
Epoch [19/30], Batch [3100/6000], Loss: 1.4813
Epoch [19/30], Batch [3200/6000], Loss: 1.5061
Epoch [19/30], Batch [3300/6000], Loss: 1.4751
Epoch [19/30], Batch [3400/6000], Loss: 1.5046
Epoch [19/30], Batch [3500/6000], Loss: 1.4906
Epoch [19/30], Batch [3600/6000], Loss: 1.4988
Epoch [19/30], Batch [3700/6000], Loss: 1.4757
Epoch [19/30], Batch [3800/6000], Loss: 1.4869
Epoch [19/30], Batch [3900/6000], Loss: 1.4768
Epoch [19/30], Batch [4000/6000], Loss: 1.4801
Epoch [19/30], Batch [4100/6000], Loss: 1.4835
Epoch [19/30], Batch [4200/6000], Loss: 1.4813
Epoch [19/30], Batch [4300/6000], Loss: 1.5041
Epoch [19/30], Batch [4400/6000], Loss: 1.4953
Epoch [19/30], Batch [4500/6000], Loss: 1.5791
Epoch [19/30], Batch [4600/6000], Loss: 1.4793
Epoch [19/30], Batch [4700/6000], Loss: 1.4817
Epoch [19/30], Batch [4800/6000], Loss: 1.4805
Epoch [19/30], Batch [4900/6000], Loss: 1.5151
Epoch [19/30], Batch [5000/6000], Loss: 1.4967
Epoch [19/30], Batch [5100/6000], Loss: 1.4778
Epoch [19/30], Batch [5200/6000], Loss: 1.4756
Epoch [19/30], Batch [5300/6000], Loss: 1.4797
Epoch [19/30], Batch [5400/6000], Loss: 1.4746
Epoch [19/30], Batch [5500/6000], Loss: 1.4765
Epoch [19/30], Batch [5600/6000], Loss: 1.5810
Epoch [19/30], Batch [5700/6000], Loss: 1.4834
Epoch [19/30], Batch [5800/6000], Loss: 1.4909
Epoch [19/30], Batch [5900/6000], Loss: 1.5060
Epoch [19/30], Loss: 1.4909
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 1.4767
Epoch [20/30], Batch [100/6000], Loss: 1.4801
Epoch [20/30], Batch [200/6000], Loss: 1.4758
Epoch [20/30], Batch [300/6000], Loss: 1.4762
Epoch [20/30], Batch [400/6000], Loss: 1.4797
Epoch [20/30], Batch [500/6000], Loss: 1.4760
Epoch [20/30], Batch [600/6000], Loss: 1.4933
Epoch [20/30], Batch [700/6000], Loss: 1.4791
Epoch [20/30], Batch [800/6000], Loss: 1.4865
Epoch [20/30], Batch [900/6000], Loss: 1.4738
Epoch [20/30], Batch [1000/6000], Loss: 1.4944
Epoch [20/30], Batch [1100/6000], Loss: 1.4810
Epoch [20/30], Batch [1200/6000], Loss: 1.4786
Epoch [20/30], Batch [1300/6000], Loss: 1.4934
Epoch [20/30], Batch [1400/6000], Loss: 1.4737
Epoch [20/30], Batch [1500/6000], Loss: 1.4778
Epoch [20/30], Batch [1600/6000], Loss: 1.4768
Epoch [20/30], Batch [1700/6000], Loss: 1.4980
Epoch [20/30], Batch [1800/6000], Loss: 1.4776
Epoch [20/30], Batch [1900/6000], Loss: 1.4788
Epoch [20/30], Batch [2000/6000], Loss: 1.4984
Epoch [20/30], Batch [2100/6000], Loss: 1.4774
Epoch [20/30], Batch [2200/6000], Loss: 1.4792
Epoch [20/30], Batch [2300/6000], Loss: 1.4724
Epoch [20/30], Batch [2400/6000], Loss: 1.4730
Epoch [20/30], Batch [2500/6000], Loss: 1.5786
Epoch [20/30], Batch [2600/6000], Loss: 1.4931
Epoch [20/30], Batch [2700/6000], Loss: 1.4759
Epoch [20/30], Batch [2800/6000], Loss: 1.4788
Epoch [20/30], Batch [2900/6000], Loss: 1.4748
Epoch [20/30], Batch [3000/6000], Loss: 1.4744
Epoch [20/30], Batch [3100/6000], Loss: 1.4781
Epoch [20/30], Batch [3200/6000], Loss: 1.5739
Epoch [20/30], Batch [3300/6000], Loss: 1.4772
Epoch [20/30], Batch [3400/6000], Loss: 1.4764
Epoch [20/30], Batch [3500/6000], Loss: 1.4776
Epoch [20/30], Batch [3600/6000], Loss: 1.4743
Epoch [20/30], Batch [3700/6000], Loss: 1.4762
Epoch [20/30], Batch [3800/6000], Loss: 1.4770
Epoch [20/30], Batch [3900/6000], Loss: 1.4748
Epoch [20/30], Batch [4000/6000], Loss: 1.4859
Epoch [20/30], Batch [4100/6000], Loss: 1.4896
Epoch [20/30], Batch [4200/6000], Loss: 1.4767
Epoch [20/30], Batch [4300/6000], Loss: 1.4742
Epoch [20/30], Batch [4400/6000], Loss: 1.4728
Epoch [20/30], Batch [4500/6000], Loss: 1.4758
Epoch [20/30], Batch [4600/6000], Loss: 1.4798
Epoch [20/30], Batch [4700/6000], Loss: 1.4751
Epoch [20/30], Batch [4800/6000], Loss: 1.4793
Epoch [20/30], Batch [4900/6000], Loss: 1.4792
Epoch [20/30], Batch [5000/6000], Loss: 1.4775
Epoch [20/30], Batch [5100/6000], Loss: 1.4770
Epoch [20/30], Batch [5200/6000], Loss: 1.5740
Epoch [20/30], Batch [5300/6000], Loss: 1.4732
Epoch [20/30], Batch [5400/6000], Loss: 1.4780
Epoch [20/30], Batch [5500/6000], Loss: 1.4937
Epoch [20/30], Batch [5600/6000], Loss: 1.4730
Epoch [20/30], Batch [5700/6000], Loss: 1.5913
Epoch [20/30], Batch [5800/6000], Loss: 1.4886
Epoch [20/30], Batch [5900/6000], Loss: 1.4768
Epoch [20/30], Loss: 1.4892
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 1.5156
Epoch [21/30], Batch [100/6000], Loss: 1.4902
Epoch [21/30], Batch [200/6000], Loss: 1.4782
Epoch [21/30], Batch [300/6000], Loss: 1.4753
Epoch [21/30], Batch [400/6000], Loss: 1.4748
Epoch [21/30], Batch [500/6000], Loss: 1.5061
Epoch [21/30], Batch [600/6000], Loss: 1.4759
Epoch [21/30], Batch [700/6000], Loss: 1.4728
Epoch [21/30], Batch [800/6000], Loss: 1.4926
Epoch [21/30], Batch [900/6000], Loss: 1.4809
Epoch [21/30], Batch [1000/6000], Loss: 1.4762
Epoch [21/30], Batch [1100/6000], Loss: 1.4775
Epoch [21/30], Batch [1200/6000], Loss: 1.4759
Epoch [21/30], Batch [1300/6000], Loss: 1.4759
Epoch [21/30], Batch [1400/6000], Loss: 1.4903
Epoch [21/30], Batch [1500/6000], Loss: 1.4803
Epoch [21/30], Batch [1600/6000], Loss: 1.4802
Epoch [21/30], Batch [1700/6000], Loss: 1.4755
Epoch [21/30], Batch [1800/6000], Loss: 1.4809
Epoch [21/30], Batch [1900/6000], Loss: 1.4974
Epoch [21/30], Batch [2000/6000], Loss: 1.4833
Epoch [21/30], Batch [2100/6000], Loss: 1.4735
Epoch [21/30], Batch [2200/6000], Loss: 1.4798
Epoch [21/30], Batch [2300/6000], Loss: 1.4762
Epoch [21/30], Batch [2400/6000], Loss: 1.4898
Epoch [21/30], Batch [2500/6000], Loss: 1.4773
Epoch [21/30], Batch [2600/6000], Loss: 1.4770
Epoch [21/30], Batch [2700/6000], Loss: 1.6102
Epoch [21/30], Batch [2800/6000], Loss: 1.4762
Epoch [21/30], Batch [2900/6000], Loss: 1.4929
Epoch [21/30], Batch [3000/6000], Loss: 1.4835
Epoch [21/30], Batch [3100/6000], Loss: 1.4872
Epoch [21/30], Batch [3200/6000], Loss: 1.4768
Epoch [21/30], Batch [3300/6000], Loss: 1.4928
Epoch [21/30], Batch [3400/6000], Loss: 1.4883
Epoch [21/30], Batch [3500/6000], Loss: 1.4850
Epoch [21/30], Batch [3600/6000], Loss: 1.4717
Epoch [21/30], Batch [3700/6000], Loss: 1.4752
Epoch [21/30], Batch [3800/6000], Loss: 1.4742
Epoch [21/30], Batch [3900/6000], Loss: 1.4759
Epoch [21/30], Batch [4000/6000], Loss: 1.4741
Epoch [21/30], Batch [4100/6000], Loss: 1.4779
Epoch [21/30], Batch [4200/6000], Loss: 1.4754
Epoch [21/30], Batch [4300/6000], Loss: 1.4937
Epoch [21/30], Batch [4400/6000], Loss: 1.4945
Epoch [21/30], Batch [4500/6000], Loss: 1.4729
Epoch [21/30], Batch [4600/6000], Loss: 1.4753
Epoch [21/30], Batch [4700/6000], Loss: 1.4735
Epoch [21/30], Batch [4800/6000], Loss: 1.4740
Epoch [21/30], Batch [4900/6000], Loss: 1.4755
Epoch [21/30], Batch [5000/6000], Loss: 1.4770
Epoch [21/30], Batch [5100/6000], Loss: 1.4755
Epoch [21/30], Batch [5200/6000], Loss: 1.4759
Epoch [21/30], Batch [5300/6000], Loss: 1.4747
Epoch [21/30], Batch [5400/6000], Loss: 1.4756
Epoch [21/30], Batch [5500/6000], Loss: 1.4750
Epoch [21/30], Batch [5600/6000], Loss: 1.4731
Epoch [21/30], Batch [5700/6000], Loss: 1.4775
Epoch [21/30], Batch [5800/6000], Loss: 1.4779
Epoch [21/30], Batch [5900/6000], Loss: 1.4778
Epoch [21/30], Loss: 1.4883
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 1.4757
Epoch [22/30], Batch [100/6000], Loss: 1.4831
Epoch [22/30], Batch [200/6000], Loss: 1.4752
Epoch [22/30], Batch [300/6000], Loss: 1.4791
Epoch [22/30], Batch [400/6000], Loss: 1.5021
Epoch [22/30], Batch [500/6000], Loss: 1.4747
Epoch [22/30], Batch [600/6000], Loss: 1.4764
Epoch [22/30], Batch [700/6000], Loss: 1.4703
Epoch [22/30], Batch [800/6000], Loss: 1.4907
Epoch [22/30], Batch [900/6000], Loss: 1.4771
Epoch [22/30], Batch [1000/6000], Loss: 1.4760
Epoch [22/30], Batch [1100/6000], Loss: 1.4955
Epoch [22/30], Batch [1200/6000], Loss: 1.4799
Epoch [22/30], Batch [1300/6000], Loss: 1.4886
Epoch [22/30], Batch [1400/6000], Loss: 1.4931
Epoch [22/30], Batch [1500/6000], Loss: 1.4739
Epoch [22/30], Batch [1600/6000], Loss: 1.5902
Epoch [22/30], Batch [1700/6000], Loss: 1.4849
Epoch [22/30], Batch [1800/6000], Loss: 1.5399
Epoch [22/30], Batch [1900/6000], Loss: 1.4782
Epoch [22/30], Batch [2000/6000], Loss: 1.4906
Epoch [22/30], Batch [2100/6000], Loss: 1.4808
Epoch [22/30], Batch [2200/6000], Loss: 1.4762
Epoch [22/30], Batch [2300/6000], Loss: 1.4769
Epoch [22/30], Batch [2400/6000], Loss: 1.4761
Epoch [22/30], Batch [2500/6000], Loss: 1.4779
Epoch [22/30], Batch [2600/6000], Loss: 1.4784
Epoch [22/30], Batch [2700/6000], Loss: 1.4797
Epoch [22/30], Batch [2800/6000], Loss: 1.4799
Epoch [22/30], Batch [2900/6000], Loss: 1.4740
Epoch [22/30], Batch [3000/6000], Loss: 1.4782
Epoch [22/30], Batch [3100/6000], Loss: 1.4765
Epoch [22/30], Batch [3200/6000], Loss: 1.4779
Epoch [22/30], Batch [3300/6000], Loss: 1.4741
Epoch [22/30], Batch [3400/6000], Loss: 1.4858
Epoch [22/30], Batch [3500/6000], Loss: 1.4766
Epoch [22/30], Batch [3600/6000], Loss: 1.4756
Epoch [22/30], Batch [3700/6000], Loss: 1.4859
Epoch [22/30], Batch [3800/6000], Loss: 1.4736
Epoch [22/30], Batch [3900/6000], Loss: 1.4750
Epoch [22/30], Batch [4000/6000], Loss: 1.4787
Epoch [22/30], Batch [4100/6000], Loss: 1.4785
Epoch [22/30], Batch [4200/6000], Loss: 1.4749
Epoch [22/30], Batch [4300/6000], Loss: 1.4729
Epoch [22/30], Batch [4400/6000], Loss: 1.4735
Epoch [22/30], Batch [4500/6000], Loss: 1.4762
Epoch [22/30], Batch [4600/6000], Loss: 1.4761
Epoch [22/30], Batch [4700/6000], Loss: 1.4757
Epoch [22/30], Batch [4800/6000], Loss: 1.4778
Epoch [22/30], Batch [4900/6000], Loss: 1.4785
Epoch [22/30], Batch [5000/6000], Loss: 1.4937
Epoch [22/30], Batch [5100/6000], Loss: 1.4782
Epoch [22/30], Batch [5200/6000], Loss: 1.4754
Epoch [22/30], Batch [5300/6000], Loss: 1.4762
Epoch [22/30], Batch [5400/6000], Loss: 1.4774
Epoch [22/30], Batch [5500/6000], Loss: 1.4756
Epoch [22/30], Batch [5600/6000], Loss: 1.4738
Epoch [22/30], Batch [5700/6000], Loss: 1.4775
Epoch [22/30], Batch [5800/6000], Loss: 1.4984
Epoch [22/30], Batch [5900/6000], Loss: 1.5755
Epoch [22/30], Loss: 1.4880
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 1.4818
Epoch [23/30], Batch [100/6000], Loss: 1.4751
Epoch [23/30], Batch [200/6000], Loss: 1.4724
Epoch [23/30], Batch [300/6000], Loss: 1.4870
Epoch [23/30], Batch [400/6000], Loss: 1.4744
Epoch [23/30], Batch [500/6000], Loss: 1.4746
Epoch [23/30], Batch [600/6000], Loss: 1.4816
Epoch [23/30], Batch [700/6000], Loss: 1.4947
Epoch [23/30], Batch [800/6000], Loss: 1.4758
Epoch [23/30], Batch [900/6000], Loss: 1.5161
Epoch [23/30], Batch [1000/6000], Loss: 1.4756
Epoch [23/30], Batch [1100/6000], Loss: 1.4732
Epoch [23/30], Batch [1200/6000], Loss: 1.4765
Epoch [23/30], Batch [1300/6000], Loss: 1.4777
Epoch [23/30], Batch [1400/6000], Loss: 1.4887
Epoch [23/30], Batch [1500/6000], Loss: 1.4746
Epoch [23/30], Batch [1600/6000], Loss: 1.4760
Epoch [23/30], Batch [1700/6000], Loss: 1.4727
Epoch [23/30], Batch [1800/6000], Loss: 1.5054
Epoch [23/30], Batch [1900/6000], Loss: 1.4789
Epoch [23/30], Batch [2000/6000], Loss: 1.5023
Epoch [23/30], Batch [2100/6000], Loss: 1.4760
Epoch [23/30], Batch [2200/6000], Loss: 1.4771
Epoch [23/30], Batch [2300/6000], Loss: 1.4797
Epoch [23/30], Batch [2400/6000], Loss: 1.4786
Epoch [23/30], Batch [2500/6000], Loss: 1.4776
Epoch [23/30], Batch [2600/6000], Loss: 1.4872
Epoch [23/30], Batch [2700/6000], Loss: 1.4748
Epoch [23/30], Batch [2800/6000], Loss: 1.4916
Epoch [23/30], Batch [2900/6000], Loss: 1.4775
Epoch [23/30], Batch [3000/6000], Loss: 1.4773
Epoch [23/30], Batch [3100/6000], Loss: 1.4758
Epoch [23/30], Batch [3200/6000], Loss: 1.4755
Epoch [23/30], Batch [3300/6000], Loss: 1.4764
Epoch [23/30], Batch [3400/6000], Loss: 1.4790
Epoch [23/30], Batch [3500/6000], Loss: 1.4867
Epoch [23/30], Batch [3600/6000], Loss: 1.4771
Epoch [23/30], Batch [3700/6000], Loss: 1.4781
Epoch [23/30], Batch [3800/6000], Loss: 1.4820
Epoch [23/30], Batch [3900/6000], Loss: 1.4739
Epoch [23/30], Batch [4000/6000], Loss: 1.4762
Epoch [23/30], Batch [4100/6000], Loss: 1.4753
Epoch [23/30], Batch [4200/6000], Loss: 1.4697
Epoch [23/30], Batch [4300/6000], Loss: 1.4814
Epoch [23/30], Batch [4400/6000], Loss: 1.4771
Epoch [23/30], Batch [4500/6000], Loss: 1.4798
Epoch [23/30], Batch [4600/6000], Loss: 1.5214
Epoch [23/30], Batch [4700/6000], Loss: 1.4968
Epoch [23/30], Batch [4800/6000], Loss: 1.4768
Epoch [23/30], Batch [4900/6000], Loss: 1.4767
Epoch [23/30], Batch [5000/6000], Loss: 1.5437
Epoch [23/30], Batch [5100/6000], Loss: 1.4722
Epoch [23/30], Batch [5200/6000], Loss: 1.4749
Epoch [23/30], Batch [5300/6000], Loss: 1.4773
Epoch [23/30], Batch [5400/6000], Loss: 1.4725
Epoch [23/30], Batch [5500/6000], Loss: 1.4810
Epoch [23/30], Batch [5600/6000], Loss: 1.4761
Epoch [23/30], Batch [5700/6000], Loss: 1.4858
Epoch [23/30], Batch [5800/6000], Loss: 1.4876
Epoch [23/30], Batch [5900/6000], Loss: 1.4740
Epoch [23/30], Loss: 1.4873
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 1.4744
Epoch [24/30], Batch [100/6000], Loss: 1.4773
Epoch [24/30], Batch [200/6000], Loss: 1.4770
Epoch [24/30], Batch [300/6000], Loss: 1.4731
Epoch [24/30], Batch [400/6000], Loss: 1.4777
Epoch [24/30], Batch [500/6000], Loss: 1.4752
Epoch [24/30], Batch [600/6000], Loss: 1.4732
Epoch [24/30], Batch [700/6000], Loss: 1.4769
Epoch [24/30], Batch [800/6000], Loss: 1.4709
Epoch [24/30], Batch [900/6000], Loss: 1.4783
Epoch [24/30], Batch [1000/6000], Loss: 1.4761
Epoch [24/30], Batch [1100/6000], Loss: 1.4784
Epoch [24/30], Batch [1200/6000], Loss: 1.4954
Epoch [24/30], Batch [1300/6000], Loss: 1.4756
Epoch [24/30], Batch [1400/6000], Loss: 1.4773
Epoch [24/30], Batch [1500/6000], Loss: 1.4914
Epoch [24/30], Batch [1600/6000], Loss: 1.4739
Epoch [24/30], Batch [1700/6000], Loss: 1.4784
Epoch [24/30], Batch [1800/6000], Loss: 1.4768
Epoch [24/30], Batch [1900/6000], Loss: 1.4870
Epoch [24/30], Batch [2000/6000], Loss: 1.4939
Epoch [24/30], Batch [2100/6000], Loss: 1.4724
Epoch [24/30], Batch [2200/6000], Loss: 1.4793
Epoch [24/30], Batch [2300/6000], Loss: 1.4766
Epoch [24/30], Batch [2400/6000], Loss: 1.4769
Epoch [24/30], Batch [2500/6000], Loss: 1.4747
Epoch [24/30], Batch [2600/6000], Loss: 1.4759
Epoch [24/30], Batch [2700/6000], Loss: 1.4820
Epoch [24/30], Batch [2800/6000], Loss: 1.4724
Epoch [24/30], Batch [2900/6000], Loss: 1.4742
Epoch [24/30], Batch [3000/6000], Loss: 1.4737
Epoch [24/30], Batch [3100/6000], Loss: 1.4774
Epoch [24/30], Batch [3200/6000], Loss: 1.5176
Epoch [24/30], Batch [3300/6000], Loss: 1.4741
Epoch [24/30], Batch [3400/6000], Loss: 1.4741
Epoch [24/30], Batch [3500/6000], Loss: 1.4766
Epoch [24/30], Batch [3600/6000], Loss: 1.4718
Epoch [24/30], Batch [3700/6000], Loss: 1.4730
Epoch [24/30], Batch [3800/6000], Loss: 1.5767
Epoch [24/30], Batch [3900/6000], Loss: 1.4705
Epoch [24/30], Batch [4000/6000], Loss: 1.4780
Epoch [24/30], Batch [4100/6000], Loss: 1.4768
Epoch [24/30], Batch [4200/6000], Loss: 1.4757
Epoch [24/30], Batch [4300/6000], Loss: 1.4773
Epoch [24/30], Batch [4400/6000], Loss: 1.4751
Epoch [24/30], Batch [4500/6000], Loss: 1.4758
Epoch [24/30], Batch [4600/6000], Loss: 1.4794
Epoch [24/30], Batch [4700/6000], Loss: 1.5028
Epoch [24/30], Batch [4800/6000], Loss: 1.4703
Epoch [24/30], Batch [4900/6000], Loss: 1.4743
Epoch [24/30], Batch [5000/6000], Loss: 1.4835
Epoch [24/30], Batch [5100/6000], Loss: 1.4793
Epoch [24/30], Batch [5200/6000], Loss: 1.4734
Epoch [24/30], Batch [5300/6000], Loss: 1.4750
Epoch [24/30], Batch [5400/6000], Loss: 1.5063
Epoch [24/30], Batch [5500/6000], Loss: 1.4777
Epoch [24/30], Batch [5600/6000], Loss: 1.4738
Epoch [24/30], Batch [5700/6000], Loss: 1.4790
Epoch [24/30], Batch [5800/6000], Loss: 1.4724
Epoch [24/30], Batch [5900/6000], Loss: 1.4920
Epoch [24/30], Loss: 1.4863
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 1.4714
Epoch [25/30], Batch [100/6000], Loss: 1.4867
Epoch [25/30], Batch [200/6000], Loss: 1.4800
Epoch [25/30], Batch [300/6000], Loss: 1.4724
Epoch [25/30], Batch [400/6000], Loss: 1.4739
Epoch [25/30], Batch [500/6000], Loss: 1.4813
Epoch [25/30], Batch [600/6000], Loss: 1.4784
Epoch [25/30], Batch [700/6000], Loss: 1.5781
Epoch [25/30], Batch [800/6000], Loss: 1.4844
Epoch [25/30], Batch [900/6000], Loss: 1.4719
Epoch [25/30], Batch [1000/6000], Loss: 1.5026
Epoch [25/30], Batch [1100/6000], Loss: 1.4768
Epoch [25/30], Batch [1200/6000], Loss: 1.4886
Epoch [25/30], Batch [1300/6000], Loss: 1.4781
Epoch [25/30], Batch [1400/6000], Loss: 1.4765
Epoch [25/30], Batch [1500/6000], Loss: 1.4775
Epoch [25/30], Batch [1600/6000], Loss: 1.4768
Epoch [25/30], Batch [1700/6000], Loss: 1.4712
Epoch [25/30], Batch [1800/6000], Loss: 1.4746
Epoch [25/30], Batch [1900/6000], Loss: 1.4927
Epoch [25/30], Batch [2000/6000], Loss: 1.4709
Epoch [25/30], Batch [2100/6000], Loss: 1.5707
Epoch [25/30], Batch [2200/6000], Loss: 1.4751
Epoch [25/30], Batch [2300/6000], Loss: 1.4735
Epoch [25/30], Batch [2400/6000], Loss: 1.4699
Epoch [25/30], Batch [2500/6000], Loss: 1.5732
Epoch [25/30], Batch [2600/6000], Loss: 1.4760
Epoch [25/30], Batch [2700/6000], Loss: 1.4829
Epoch [25/30], Batch [2800/6000], Loss: 1.4723
Epoch [25/30], Batch [2900/6000], Loss: 1.4937
Epoch [25/30], Batch [3000/6000], Loss: 1.4735
Epoch [25/30], Batch [3100/6000], Loss: 1.4797
Epoch [25/30], Batch [3200/6000], Loss: 1.4741
Epoch [25/30], Batch [3300/6000], Loss: 1.4745
Epoch [25/30], Batch [3400/6000], Loss: 1.4913
Epoch [25/30], Batch [3500/6000], Loss: 1.4803
Epoch [25/30], Batch [3600/6000], Loss: 1.4725
Epoch [25/30], Batch [3700/6000], Loss: 1.5636
Epoch [25/30], Batch [3800/6000], Loss: 1.4699
Epoch [25/30], Batch [3900/6000], Loss: 1.5028
Epoch [25/30], Batch [4000/6000], Loss: 1.4871
Epoch [25/30], Batch [4100/6000], Loss: 1.4742
Epoch [25/30], Batch [4200/6000], Loss: 1.5346
Epoch [25/30], Batch [4300/6000], Loss: 1.6934
Epoch [25/30], Batch [4400/6000], Loss: 1.4772
Epoch [25/30], Batch [4500/6000], Loss: 1.4847
Epoch [25/30], Batch [4600/6000], Loss: 1.4722
Epoch [25/30], Batch [4700/6000], Loss: 1.4885
Epoch [25/30], Batch [4800/6000], Loss: 1.4767
Epoch [25/30], Batch [4900/6000], Loss: 1.4796
Epoch [25/30], Batch [5000/6000], Loss: 1.4723
Epoch [25/30], Batch [5100/6000], Loss: 1.4759
Epoch [25/30], Batch [5200/6000], Loss: 1.4745
Epoch [25/30], Batch [5300/6000], Loss: 1.4767
Epoch [25/30], Batch [5400/6000], Loss: 1.4754
Epoch [25/30], Batch [5500/6000], Loss: 1.4759
Epoch [25/30], Batch [5600/6000], Loss: 1.4949
Epoch [25/30], Batch [5700/6000], Loss: 1.4761
Epoch [25/30], Batch [5800/6000], Loss: 1.4751
Epoch [25/30], Batch [5900/6000], Loss: 1.4797
Epoch [25/30], Loss: 1.4857
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 1.4732
Epoch [26/30], Batch [100/6000], Loss: 1.4755
Epoch [26/30], Batch [200/6000], Loss: 1.4713
Epoch [26/30], Batch [300/6000], Loss: 1.4741
Epoch [26/30], Batch [400/6000], Loss: 1.4756
Epoch [26/30], Batch [500/6000], Loss: 1.4801
Epoch [26/30], Batch [600/6000], Loss: 1.4723
Epoch [26/30], Batch [700/6000], Loss: 1.5015
Epoch [26/30], Batch [800/6000], Loss: 1.4725
Epoch [26/30], Batch [900/6000], Loss: 1.4730
Epoch [26/30], Batch [1000/6000], Loss: 1.4851
Epoch [26/30], Batch [1100/6000], Loss: 1.4756
Epoch [26/30], Batch [1200/6000], Loss: 1.4728
Epoch [26/30], Batch [1300/6000], Loss: 1.4788
Epoch [26/30], Batch [1400/6000], Loss: 1.4745
Epoch [26/30], Batch [1500/6000], Loss: 1.4759
Epoch [26/30], Batch [1600/6000], Loss: 1.4733
Epoch [26/30], Batch [1700/6000], Loss: 1.4740
Epoch [26/30], Batch [1800/6000], Loss: 1.4735
Epoch [26/30], Batch [1900/6000], Loss: 1.4734
Epoch [26/30], Batch [2000/6000], Loss: 1.4997
Epoch [26/30], Batch [2100/6000], Loss: 1.4731
Epoch [26/30], Batch [2200/6000], Loss: 1.4758
Epoch [26/30], Batch [2300/6000], Loss: 1.4773
Epoch [26/30], Batch [2400/6000], Loss: 1.4872
Epoch [26/30], Batch [2500/6000], Loss: 1.4785
Epoch [26/30], Batch [2600/6000], Loss: 1.4781
Epoch [26/30], Batch [2700/6000], Loss: 1.4743
Epoch [26/30], Batch [2800/6000], Loss: 1.4921
Epoch [26/30], Batch [2900/6000], Loss: 1.4802
Epoch [26/30], Batch [3000/6000], Loss: 1.4864
Epoch [26/30], Batch [3100/6000], Loss: 1.4771
Epoch [26/30], Batch [3200/6000], Loss: 1.4774
Epoch [26/30], Batch [3300/6000], Loss: 1.4810
Epoch [26/30], Batch [3400/6000], Loss: 1.4764
Epoch [26/30], Batch [3500/6000], Loss: 1.4704
Epoch [26/30], Batch [3600/6000], Loss: 1.4735
Epoch [26/30], Batch [3700/6000], Loss: 1.4900
Epoch [26/30], Batch [3800/6000], Loss: 1.4752
Epoch [26/30], Batch [3900/6000], Loss: 1.4914
Epoch [26/30], Batch [4000/6000], Loss: 1.4754
Epoch [26/30], Batch [4100/6000], Loss: 1.4756
Epoch [26/30], Batch [4200/6000], Loss: 1.5736
Epoch [26/30], Batch [4300/6000], Loss: 1.4719
Epoch [26/30], Batch [4400/6000], Loss: 1.4741
Epoch [26/30], Batch [4500/6000], Loss: 1.5863
Epoch [26/30], Batch [4600/6000], Loss: 1.4748
Epoch [26/30], Batch [4700/6000], Loss: 1.4876
Epoch [26/30], Batch [4800/6000], Loss: 1.5767
Epoch [26/30], Batch [4900/6000], Loss: 1.4788
Epoch [26/30], Batch [5000/6000], Loss: 1.5663
Epoch [26/30], Batch [5100/6000], Loss: 1.4763
Epoch [26/30], Batch [5200/6000], Loss: 1.4738
Epoch [26/30], Batch [5300/6000], Loss: 1.4747
Epoch [26/30], Batch [5400/6000], Loss: 1.4780
Epoch [26/30], Batch [5500/6000], Loss: 1.4933
Epoch [26/30], Batch [5600/6000], Loss: 1.5789
Epoch [26/30], Batch [5700/6000], Loss: 1.4726
Epoch [26/30], Batch [5800/6000], Loss: 1.4729
Epoch [26/30], Batch [5900/6000], Loss: 1.4859
Epoch [26/30], Loss: 1.4854
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 1.4884
Epoch [27/30], Batch [100/6000], Loss: 1.4722
Epoch [27/30], Batch [200/6000], Loss: 1.4773
Epoch [27/30], Batch [300/6000], Loss: 1.4735
Epoch [27/30], Batch [400/6000], Loss: 1.4920
Epoch [27/30], Batch [500/6000], Loss: 1.4734
Epoch [27/30], Batch [600/6000], Loss: 1.4743
Epoch [27/30], Batch [700/6000], Loss: 1.4789
Epoch [27/30], Batch [800/6000], Loss: 1.4770
Epoch [27/30], Batch [900/6000], Loss: 1.4760
Epoch [27/30], Batch [1000/6000], Loss: 1.4722
Epoch [27/30], Batch [1100/6000], Loss: 1.4749
Epoch [27/30], Batch [1200/6000], Loss: 1.4766
Epoch [27/30], Batch [1300/6000], Loss: 1.4760
Epoch [27/30], Batch [1400/6000], Loss: 1.4748
Epoch [27/30], Batch [1500/6000], Loss: 1.4780
Epoch [27/30], Batch [1600/6000], Loss: 1.4924
Epoch [27/30], Batch [1700/6000], Loss: 1.4754
Epoch [27/30], Batch [1800/6000], Loss: 1.4736
Epoch [27/30], Batch [1900/6000], Loss: 1.5472
Epoch [27/30], Batch [2000/6000], Loss: 1.5019
Epoch [27/30], Batch [2100/6000], Loss: 1.5734
Epoch [27/30], Batch [2200/6000], Loss: 1.4781
Epoch [27/30], Batch [2300/6000], Loss: 1.4760
Epoch [27/30], Batch [2400/6000], Loss: 1.5871
Epoch [27/30], Batch [2500/6000], Loss: 1.6879
Epoch [27/30], Batch [2600/6000], Loss: 1.4760
Epoch [27/30], Batch [2700/6000], Loss: 1.4750
Epoch [27/30], Batch [2800/6000], Loss: 1.4756
Epoch [27/30], Batch [2900/6000], Loss: 1.4743
Epoch [27/30], Batch [3000/6000], Loss: 1.4776
Epoch [27/30], Batch [3100/6000], Loss: 1.4768
Epoch [27/30], Batch [3200/6000], Loss: 1.4888
Epoch [27/30], Batch [3300/6000], Loss: 1.4772
Epoch [27/30], Batch [3400/6000], Loss: 1.4750
Epoch [27/30], Batch [3500/6000], Loss: 1.4736
Epoch [27/30], Batch [3600/6000], Loss: 1.4945
Epoch [27/30], Batch [3700/6000], Loss: 1.4747
Epoch [27/30], Batch [3800/6000], Loss: 1.4760
Epoch [27/30], Batch [3900/6000], Loss: 1.4728
Epoch [27/30], Batch [4000/6000], Loss: 1.4730
Epoch [27/30], Batch [4100/6000], Loss: 1.4867
Epoch [27/30], Batch [4200/6000], Loss: 1.4799
Epoch [27/30], Batch [4300/6000], Loss: 1.5708
Epoch [27/30], Batch [4400/6000], Loss: 1.5754
Epoch [27/30], Batch [4500/6000], Loss: 1.4766
Epoch [27/30], Batch [4600/6000], Loss: 1.4768
Epoch [27/30], Batch [4700/6000], Loss: 1.4716
Epoch [27/30], Batch [4800/6000], Loss: 1.4786
Epoch [27/30], Batch [4900/6000], Loss: 1.4758
Epoch [27/30], Batch [5000/6000], Loss: 1.4794
Epoch [27/30], Batch [5100/6000], Loss: 1.4877
Epoch [27/30], Batch [5200/6000], Loss: 1.4886
Epoch [27/30], Batch [5300/6000], Loss: 1.4796
Epoch [27/30], Batch [5400/6000], Loss: 1.4734
Epoch [27/30], Batch [5500/6000], Loss: 1.4727
Epoch [27/30], Batch [5600/6000], Loss: 1.4769
Epoch [27/30], Batch [5700/6000], Loss: 1.4736
Epoch [27/30], Batch [5800/6000], Loss: 1.4769
Epoch [27/30], Batch [5900/6000], Loss: 1.4939
Epoch [27/30], Loss: 1.4849
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 1.4788
Epoch [28/30], Batch [100/6000], Loss: 1.4738
Epoch [28/30], Batch [200/6000], Loss: 1.4895
Epoch [28/30], Batch [300/6000], Loss: 1.4775
Epoch [28/30], Batch [400/6000], Loss: 1.4827
Epoch [28/30], Batch [500/6000], Loss: 1.4865
Epoch [28/30], Batch [600/6000], Loss: 1.4715
Epoch [28/30], Batch [700/6000], Loss: 1.4734
Epoch [28/30], Batch [800/6000], Loss: 1.4698
Epoch [28/30], Batch [900/6000], Loss: 1.4769
Epoch [28/30], Batch [1000/6000], Loss: 1.4743
Epoch [28/30], Batch [1100/6000], Loss: 1.4960
Epoch [28/30], Batch [1200/6000], Loss: 1.4773
Epoch [28/30], Batch [1300/6000], Loss: 1.4759
Epoch [28/30], Batch [1400/6000], Loss: 1.4749
Epoch [28/30], Batch [1500/6000], Loss: 1.4717
Epoch [28/30], Batch [1600/6000], Loss: 1.4716
Epoch [28/30], Batch [1700/6000], Loss: 1.4725
Epoch [28/30], Batch [1800/6000], Loss: 1.4771
Epoch [28/30], Batch [1900/6000], Loss: 1.4909
Epoch [28/30], Batch [2000/6000], Loss: 1.4748
Epoch [28/30], Batch [2100/6000], Loss: 1.4722
Epoch [28/30], Batch [2200/6000], Loss: 1.4728
Epoch [28/30], Batch [2300/6000], Loss: 1.4772
Epoch [28/30], Batch [2400/6000], Loss: 1.4739
Epoch [28/30], Batch [2500/6000], Loss: 1.4865
Epoch [28/30], Batch [2600/6000], Loss: 1.4859
Epoch [28/30], Batch [2700/6000], Loss: 1.4743
Epoch [28/30], Batch [2800/6000], Loss: 1.4838
Epoch [28/30], Batch [2900/6000], Loss: 1.4781
Epoch [28/30], Batch [3000/6000], Loss: 1.5056
Epoch [28/30], Batch [3100/6000], Loss: 1.4741
Epoch [28/30], Batch [3200/6000], Loss: 1.4727
Epoch [28/30], Batch [3300/6000], Loss: 1.4734
Epoch [28/30], Batch [3400/6000], Loss: 1.5007
Epoch [28/30], Batch [3500/6000], Loss: 1.4771
Epoch [28/30], Batch [3600/6000], Loss: 1.4776
Epoch [28/30], Batch [3700/6000], Loss: 1.4901
Epoch [28/30], Batch [3800/6000], Loss: 1.5784
Epoch [28/30], Batch [3900/6000], Loss: 1.4752
Epoch [28/30], Batch [4000/6000], Loss: 1.4888
Epoch [28/30], Batch [4100/6000], Loss: 1.4736
Epoch [28/30], Batch [4200/6000], Loss: 1.4860
Epoch [28/30], Batch [4300/6000], Loss: 1.4727
Epoch [28/30], Batch [4400/6000], Loss: 1.4717
Epoch [28/30], Batch [4500/6000], Loss: 1.4760
Epoch [28/30], Batch [4600/6000], Loss: 1.4748
Epoch [28/30], Batch [4700/6000], Loss: 1.4824
Epoch [28/30], Batch [4800/6000], Loss: 1.4758
Epoch [28/30], Batch [4900/6000], Loss: 1.4712
Epoch [28/30], Batch [5000/6000], Loss: 1.4771
Epoch [28/30], Batch [5100/6000], Loss: 1.4755
Epoch [28/30], Batch [5200/6000], Loss: 1.4749
Epoch [28/30], Batch [5300/6000], Loss: 1.4715
Epoch [28/30], Batch [5400/6000], Loss: 1.4830
Epoch [28/30], Batch [5500/6000], Loss: 1.5866
Epoch [28/30], Batch [5600/6000], Loss: 1.4870
Epoch [28/30], Batch [5700/6000], Loss: 1.4739
Epoch [28/30], Batch [5800/6000], Loss: 1.4867
Epoch [28/30], Batch [5900/6000], Loss: 1.4863
Epoch [28/30], Loss: 1.4842
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 1.4718
Epoch [29/30], Batch [100/6000], Loss: 1.4770
Epoch [29/30], Batch [200/6000], Loss: 1.4744
Epoch [29/30], Batch [300/6000], Loss: 1.4781
Epoch [29/30], Batch [400/6000], Loss: 1.4759
Epoch [29/30], Batch [500/6000], Loss: 1.4790
Epoch [29/30], Batch [600/6000], Loss: 1.4824
Epoch [29/30], Batch [700/6000], Loss: 1.4749
Epoch [29/30], Batch [800/6000], Loss: 1.4891
Epoch [29/30], Batch [900/6000], Loss: 1.4742
Epoch [29/30], Batch [1000/6000], Loss: 1.4734
Epoch [29/30], Batch [1100/6000], Loss: 1.4721
Epoch [29/30], Batch [1200/6000], Loss: 1.4710
Epoch [29/30], Batch [1300/6000], Loss: 1.5797
Epoch [29/30], Batch [1400/6000], Loss: 1.4770
Epoch [29/30], Batch [1500/6000], Loss: 1.4775
Epoch [29/30], Batch [1600/6000], Loss: 1.4752
Epoch [29/30], Batch [1700/6000], Loss: 1.4758
Epoch [29/30], Batch [1800/6000], Loss: 1.4746
Epoch [29/30], Batch [1900/6000], Loss: 1.4750
Epoch [29/30], Batch [2000/6000], Loss: 1.4743
Epoch [29/30], Batch [2100/6000], Loss: 1.4840
Epoch [29/30], Batch [2200/6000], Loss: 1.4748
Epoch [29/30], Batch [2300/6000], Loss: 1.4741
Epoch [29/30], Batch [2400/6000], Loss: 1.4758
Epoch [29/30], Batch [2500/6000], Loss: 1.4769
Epoch [29/30], Batch [2600/6000], Loss: 1.4762
Epoch [29/30], Batch [2700/6000], Loss: 1.4766
Epoch [29/30], Batch [2800/6000], Loss: 1.4737
Epoch [29/30], Batch [2900/6000], Loss: 1.4754
Epoch [29/30], Batch [3000/6000], Loss: 1.4798
Epoch [29/30], Batch [3100/6000], Loss: 1.4786
Epoch [29/30], Batch [3200/6000], Loss: 1.4730
Epoch [29/30], Batch [3300/6000], Loss: 1.4753
Epoch [29/30], Batch [3400/6000], Loss: 1.4690
Epoch [29/30], Batch [3500/6000], Loss: 1.4734
Epoch [29/30], Batch [3600/6000], Loss: 1.4741
Epoch [29/30], Batch [3700/6000], Loss: 1.4784
Epoch [29/30], Batch [3800/6000], Loss: 1.4750
Epoch [29/30], Batch [3900/6000], Loss: 1.4727
Epoch [29/30], Batch [4000/6000], Loss: 1.4770
Epoch [29/30], Batch [4100/6000], Loss: 1.4745
Epoch [29/30], Batch [4200/6000], Loss: 1.4715
Epoch [29/30], Batch [4300/6000], Loss: 1.4713
Epoch [29/30], Batch [4400/6000], Loss: 1.4769
Epoch [29/30], Batch [4500/6000], Loss: 1.4751
Epoch [29/30], Batch [4600/6000], Loss: 1.4743
Epoch [29/30], Batch [4700/6000], Loss: 1.4719
Epoch [29/30], Batch [4800/6000], Loss: 1.4806
Epoch [29/30], Batch [4900/6000], Loss: 1.4747
Epoch [29/30], Batch [5000/6000], Loss: 1.4786
Epoch [29/30], Batch [5100/6000], Loss: 1.4760
Epoch [29/30], Batch [5200/6000], Loss: 1.4777
Epoch [29/30], Batch [5300/6000], Loss: 1.4813
Epoch [29/30], Batch [5400/6000], Loss: 1.4752
Epoch [29/30], Batch [5500/6000], Loss: 1.4746
Epoch [29/30], Batch [5600/6000], Loss: 1.4713
Epoch [29/30], Batch [5700/6000], Loss: 1.4750
Epoch [29/30], Batch [5800/6000], Loss: 1.4739
Epoch [29/30], Batch [5900/6000], Loss: 1.4765
Epoch [29/30], Loss: 1.4834
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 1.4735
Epoch [30/30], Batch [100/6000], Loss: 1.4753
Epoch [30/30], Batch [200/6000], Loss: 1.4738
Epoch [30/30], Batch [300/6000], Loss: 1.4907
Epoch [30/30], Batch [400/6000], Loss: 1.4774
Epoch [30/30], Batch [500/6000], Loss: 1.4771
Epoch [30/30], Batch [600/6000], Loss: 1.5760
Epoch [30/30], Batch [700/6000], Loss: 1.4743
Epoch [30/30], Batch [800/6000], Loss: 1.4727
Epoch [30/30], Batch [900/6000], Loss: 1.4867
Epoch [30/30], Batch [1000/6000], Loss: 1.4723
Epoch [30/30], Batch [1100/6000], Loss: 1.4782
Epoch [30/30], Batch [1200/6000], Loss: 1.4731
Epoch [30/30], Batch [1300/6000], Loss: 1.4786
Epoch [30/30], Batch [1400/6000], Loss: 1.4769
Epoch [30/30], Batch [1500/6000], Loss: 1.4725
Epoch [30/30], Batch [1600/6000], Loss: 1.4784
Epoch [30/30], Batch [1700/6000], Loss: 1.4721
Epoch [30/30], Batch [1800/6000], Loss: 1.4786
Epoch [30/30], Batch [1900/6000], Loss: 1.5748
Epoch [30/30], Batch [2000/6000], Loss: 1.4773
Epoch [30/30], Batch [2100/6000], Loss: 1.4775
Epoch [30/30], Batch [2200/6000], Loss: 1.4758
Epoch [30/30], Batch [2300/6000], Loss: 1.4733
Epoch [30/30], Batch [2400/6000], Loss: 1.4771
Epoch [30/30], Batch [2500/6000], Loss: 1.4748
Epoch [30/30], Batch [2600/6000], Loss: 1.4721
Epoch [30/30], Batch [2700/6000], Loss: 1.4720
Epoch [30/30], Batch [2800/6000], Loss: 1.4829
Epoch [30/30], Batch [2900/6000], Loss: 1.4880
Epoch [30/30], Batch [3000/6000], Loss: 1.4713
Epoch [30/30], Batch [3100/6000], Loss: 1.4743
Epoch [30/30], Batch [3200/6000], Loss: 1.4719
Epoch [30/30], Batch [3300/6000], Loss: 1.4716
Epoch [30/30], Batch [3400/6000], Loss: 1.4859
Epoch [30/30], Batch [3500/6000], Loss: 1.4727
Epoch [30/30], Batch [3600/6000], Loss: 1.4732
Epoch [30/30], Batch [3700/6000], Loss: 1.4736
Epoch [30/30], Batch [3800/6000], Loss: 1.4748
Epoch [30/30], Batch [3900/6000], Loss: 1.4894
Epoch [30/30], Batch [4000/6000], Loss: 1.4736
Epoch [30/30], Batch [4100/6000], Loss: 1.4779
Epoch [30/30], Batch [4200/6000], Loss: 1.4758
Epoch [30/30], Batch [4300/6000], Loss: 1.4753
Epoch [30/30], Batch [4400/6000], Loss: 1.5748
Epoch [30/30], Batch [4500/6000], Loss: 1.4739
Epoch [30/30], Batch [4600/6000], Loss: 1.5497
Epoch [30/30], Batch [4700/6000], Loss: 1.4703
Epoch [30/30], Batch [4800/6000], Loss: 1.4883
Epoch [30/30], Batch [4900/6000], Loss: 1.4767
Epoch [30/30], Batch [5000/6000], Loss: 1.4784
Epoch [30/30], Batch [5100/6000], Loss: 1.5941
Epoch [30/30], Batch [5200/6000], Loss: 1.4759
Epoch [30/30], Batch [5300/6000], Loss: 1.4721
Epoch [30/30], Batch [5400/6000], Loss: 1.5719
Epoch [30/30], Batch [5500/6000], Loss: 1.4692
Epoch [30/30], Batch [5600/6000], Loss: 1.5713
Epoch [30/30], Batch [5700/6000], Loss: 1.4861
Epoch [30/30], Batch [5800/6000], Loss: 1.4703
Epoch [30/30], Batch [5900/6000], Loss: 1.4751
Epoch [30/30], Loss: 1.4831
Visualization saved to figures/visualization_0.png
Test Loss: 1.4939, Accuracy: 97.66%
  Output probs: [[0.085 0.085 0.085 0.089 0.085 0.085 0.085 0.085 0.231 0.085]]
Adversarial Training Loop 1/300:
  Label Loss: 0.1272
  Image Loss: 0.0233
  Total Loss: 127.2460
  Image grad max: 3.066157341003418
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 2/300:
  Label Loss: 0.1768
  Image Loss: 0.5116
  Total Loss: 177.3119
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 3/300:
  Label Loss: 0.1768
  Image Loss: 0.5105
  Total Loss: 177.3107
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 4/300:
  Label Loss: 0.1768
  Image Loss: 0.5098
  Total Loss: 177.3101
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 5/300:
  Label Loss: 0.1768
  Image Loss: 0.5088
  Total Loss: 177.3091
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 6/300:
  Label Loss: 0.1768
  Image Loss: 0.5073
  Total Loss: 177.3076
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 7/300:
  Label Loss: 0.1768
  Image Loss: 0.5054
  Total Loss: 177.3057
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 8/300:
  Label Loss: 0.1768
  Image Loss: 0.5039
  Total Loss: 177.3042
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 9/300:
  Label Loss: 0.1768
  Image Loss: 0.5027
  Total Loss: 177.3030
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 10/300:
  Label Loss: 0.1768
  Image Loss: 0.5011
  Total Loss: 177.3014
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 11/300:
  Label Loss: 0.1768
  Image Loss: 0.4991
  Total Loss: 177.2994
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 12/300:
  Label Loss: 0.1768
  Image Loss: 0.4967
  Total Loss: 177.2970
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 13/300:
  Label Loss: 0.1768
  Image Loss: 0.4940
  Total Loss: 177.2943
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 14/300:
  Label Loss: 0.1768
  Image Loss: 0.4912
  Total Loss: 177.2915
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 15/300:
  Label Loss: 0.1768
  Image Loss: 0.4883
  Total Loss: 177.2886
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 16/300:
  Label Loss: 0.1768
  Image Loss: 0.4852
  Total Loss: 177.2855
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 17/300:
  Label Loss: 0.1768
  Image Loss: 0.4821
  Total Loss: 177.2824
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 18/300:
  Label Loss: 0.1768
  Image Loss: 0.4788
  Total Loss: 177.2791
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 19/300:
  Label Loss: 0.1768
  Image Loss: 0.4751
  Total Loss: 177.2754
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 20/300:
  Label Loss: 0.1768
  Image Loss: 0.4710
  Total Loss: 177.2713
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 21/300:
  Label Loss: 0.1768
  Image Loss: 0.4667
  Total Loss: 177.2670
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 22/300:
  Label Loss: 0.1768
  Image Loss: 0.4620
  Total Loss: 177.2623
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 23/300:
  Label Loss: 0.1768
  Image Loss: 0.4572
  Total Loss: 177.2575
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 24/300:
  Label Loss: 0.1768
  Image Loss: 0.4523
  Total Loss: 177.2526
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 25/300:
  Label Loss: 0.1768
  Image Loss: 0.4473
  Total Loss: 177.2476
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 26/300:
  Label Loss: 0.1768
  Image Loss: 0.4422
  Total Loss: 177.2425
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 27/300:
  Label Loss: 0.1768
  Image Loss: 0.4371
  Total Loss: 177.2374
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 28/300:
  Label Loss: 0.1768
  Image Loss: 0.4319
  Total Loss: 177.2322
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 29/300:
  Label Loss: 0.1768
  Image Loss: 0.4266
  Total Loss: 177.2269
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 30/300:
  Label Loss: 0.1768
  Image Loss: 0.4212
  Total Loss: 177.2215
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 31/300:
  Label Loss: 0.1768
  Image Loss: 0.4155
  Total Loss: 177.2158
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 32/300:
  Label Loss: 0.1768
  Image Loss: 0.4097
  Total Loss: 177.2100
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 33/300:
  Label Loss: 0.1768
  Image Loss: 0.4037
  Total Loss: 177.2039
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 34/300:
  Label Loss: 0.1768
  Image Loss: 0.3974
  Total Loss: 177.1977
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 35/300:
  Label Loss: 0.1768
  Image Loss: 0.3909
  Total Loss: 177.1911
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 36/300:
  Label Loss: 0.1768
  Image Loss: 0.3840
  Total Loss: 177.1843
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 37/300:
  Label Loss: 0.1768
  Image Loss: 0.3770
  Total Loss: 177.1772
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 38/300:
  Label Loss: 0.1768
  Image Loss: 0.3696
  Total Loss: 177.1699
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 39/300:
  Label Loss: 0.1768
  Image Loss: 0.3620
  Total Loss: 177.1623
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 40/300:
  Label Loss: 0.1768
  Image Loss: 0.3541
  Total Loss: 177.1544
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 41/300:
  Label Loss: 0.1768
  Image Loss: 0.3460
  Total Loss: 177.1463
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 42/300:
  Label Loss: 0.1768
  Image Loss: 0.3377
  Total Loss: 177.1380
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 43/300:
  Label Loss: 0.1768
  Image Loss: 0.3292
  Total Loss: 177.1294
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 44/300:
  Label Loss: 0.1768
  Image Loss: 0.3204
  Total Loss: 177.1207
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 45/300:
  Label Loss: 0.1768
  Image Loss: 0.3116
  Total Loss: 177.1119
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 46/300:
  Label Loss: 0.1768
  Image Loss: 0.3026
  Total Loss: 177.1029
  Image grad max: 0.0025510203558951616
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 47/300:
  Label Loss: 0.1768
  Image Loss: 0.2936
  Total Loss: 177.0939
  Image grad max: 0.0025509530678391457
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 48/300:
  Label Loss: 0.1768
  Image Loss: 0.2846
  Total Loss: 177.0849
  Image grad max: 0.0025493165012449026
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 49/300:
  Label Loss: 0.1768
  Image Loss: 0.2756
  Total Loss: 177.0759
  Image grad max: 0.0025462377816438675
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 50/300:
  Label Loss: 0.1768
  Image Loss: 0.2666
  Total Loss: 177.0669
  Image grad max: 0.002541834954172373
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 51/300:
  Label Loss: 0.1768
  Image Loss: 0.2578
  Total Loss: 177.0581
  Image grad max: 0.0025362162850797176
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 52/300:
  Label Loss: 0.1768
  Image Loss: 0.2490
  Total Loss: 177.0493
  Image grad max: 0.002529480727389455
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 53/300:
  Label Loss: 0.1768
  Image Loss: 0.2405
  Total Loss: 177.0408
  Image grad max: 0.0025217204820364714
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 54/300:
  Label Loss: 0.1768
  Image Loss: 0.2320
  Total Loss: 177.0323
  Image grad max: 0.0025130193680524826
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 55/300:
  Label Loss: 0.1768
  Image Loss: 0.2238
  Total Loss: 177.0241
  Image grad max: 0.0025034553837031126
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 56/300:
  Label Loss: 0.1768
  Image Loss: 0.2157
  Total Loss: 177.0160
  Image grad max: 0.0024930997751653194
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 57/300:
  Label Loss: 0.1768
  Image Loss: 0.2078
  Total Loss: 177.0081
  Image grad max: 0.002482018666341901
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 58/300:
  Label Loss: 0.1768
  Image Loss: 0.2002
  Total Loss: 177.0005
  Image grad max: 0.0024702725932002068
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 59/300:
  Label Loss: 0.1768
  Image Loss: 0.1927
  Total Loss: 176.9930
  Image grad max: 0.0024579172022640705
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 60/300:
  Label Loss: 0.1768
  Image Loss: 0.1855
  Total Loss: 176.9857
  Image grad max: 0.0024450041819363832
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 61/300:
  Label Loss: 0.1768
  Image Loss: 0.1784
  Total Loss: 176.9787
  Image grad max: 0.0024315807968378067
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 62/300:
  Label Loss: 0.1768
  Image Loss: 0.1716
  Total Loss: 176.9719
  Image grad max: 0.002417690819129348
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 63/300:
  Label Loss: 0.1768
  Image Loss: 0.1650
  Total Loss: 176.9653
  Image grad max: 0.002403374994173646
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 64/300:
  Label Loss: 0.1768
  Image Loss: 0.1586
  Total Loss: 176.9589
  Image grad max: 0.0023886701092123985
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 65/300:
  Label Loss: 0.1768
  Image Loss: 0.1524
  Total Loss: 176.9527
  Image grad max: 0.0023736106231808662
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 66/300:
  Label Loss: 0.1768
  Image Loss: 0.1464
  Total Loss: 176.9467
  Image grad max: 0.002358228201046586
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 67/300:
  Label Loss: 0.1768
  Image Loss: 0.1407
  Total Loss: 176.9410
  Image grad max: 0.0023425521794706583
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 68/300:
  Label Loss: 0.1768
  Image Loss: 0.1351
  Total Loss: 176.9354
  Image grad max: 0.002326609566807747
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 69/300:
  Label Loss: 0.1768
  Image Loss: 0.1298
  Total Loss: 176.9300
  Image grad max: 0.0023104252759367228
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 70/300:
  Label Loss: 0.1768
  Image Loss: 0.1246
  Total Loss: 176.9249
  Image grad max: 0.0022940225899219513
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 71/300:
  Label Loss: 0.1768
  Image Loss: 0.1196
  Total Loss: 176.9199
  Image grad max: 0.002277422696352005
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 72/300:
  Label Loss: 0.1768
  Image Loss: 0.1148
  Total Loss: 176.9151
  Image grad max: 0.002260645618662238
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 73/300:
  Label Loss: 0.1768
  Image Loss: 0.1102
  Total Loss: 176.9105
  Image grad max: 0.0022437095176428556
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 74/300:
  Label Loss: 0.1768
  Image Loss: 0.1058
  Total Loss: 176.9061
  Image grad max: 0.0022266313899308443
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 75/300:
  Label Loss: 0.1768
  Image Loss: 0.1015
  Total Loss: 176.9018
  Image grad max: 0.0022094270680099726
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 76/300:
  Label Loss: 0.1768
  Image Loss: 0.0974
  Total Loss: 176.8977
  Image grad max: 0.0021921112202107906
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 77/300:
  Label Loss: 0.1768
  Image Loss: 0.0935
  Total Loss: 176.8938
  Image grad max: 0.0021746971178799868
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 78/300:
  Label Loss: 0.1768
  Image Loss: 0.0897
  Total Loss: 176.8900
  Image grad max: 0.0021571977995336056
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 79/300:
  Label Loss: 0.1768
  Image Loss: 0.0861
  Total Loss: 176.8864
  Image grad max: 0.002139168092980981
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 80/300:
  Label Loss: 0.1768
  Image Loss: 0.0826
  Total Loss: 176.8829
  Image grad max: 0.0021215176675468683
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 81/300:
  Label Loss: 0.1768
  Image Loss: 0.0792
  Total Loss: 176.8795
  Image grad max: 0.002103822771459818
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 82/300:
  Label Loss: 0.1768
  Image Loss: 0.0760
  Total Loss: 176.8763
  Image grad max: 0.002085589338093996
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 83/300:
  Label Loss: 0.1768
  Image Loss: 0.0730
  Total Loss: 176.8732
  Image grad max: 0.0020672716200351715
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 84/300:
  Label Loss: 0.1768
  Image Loss: 0.0700
  Total Loss: 176.8703
  Image grad max: 0.0020486621651798487
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 85/300:
  Label Loss: 0.1768
  Image Loss: 0.0672
  Total Loss: 176.8674
  Image grad max: 0.0020292247645556927
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 86/300:
  Label Loss: 0.1768
  Image Loss: 0.0644
  Total Loss: 176.8647
  Image grad max: 0.002008656272664666
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 87/300:
  Label Loss: 0.1768
  Image Loss: 0.0618
  Total Loss: 176.8621
  Image grad max: 0.0019864682108163834
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 88/300:
  Label Loss: 0.1768
  Image Loss: 0.0593
  Total Loss: 176.8596
  Image grad max: 0.0019778492860496044
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 89/300:
  Label Loss: 0.1768
  Image Loss: 0.0569
  Total Loss: 176.8571
  Image grad max: 0.002019039820879698
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 90/300:
  Label Loss: 0.1768
  Image Loss: 0.0546
  Total Loss: 176.8548
  Image grad max: 0.002125430153682828
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 91/300:
  Label Loss: 0.1768
  Image Loss: 0.0524
  Total Loss: 176.8525
  Image grad max: 0.002443247940391302
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 92/300:
  Label Loss: 0.1768
  Image Loss: 0.0503
  Total Loss: 176.8497
  Image grad max: 0.003954012878239155
  Output probs: [[0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 93/300:
  Label Loss: 0.1768
  Image Loss: 0.0486
  Total Loss: 176.8360
  Image grad max: 0.04752235487103462
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.231 0.085]]
Adversarial Training Loop 94/300:
  Label Loss: 0.1269
  Image Loss: 0.0539
  Total Loss: 127.0032
  Image grad max: 0.8039421439170837
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 95/300:
  Label Loss: 0.1268
  Image Loss: 0.1286
  Total Loss: 126.9289
  Image grad max: 0.002551384037360549
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 96/300:
  Label Loss: 0.1268
  Image Loss: 0.2174
  Total Loss: 127.0191
  Image grad max: 0.0058393338695168495
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 97/300:
  Label Loss: 0.1268
  Image Loss: 0.2700
  Total Loss: 127.0710
  Image grad max: 0.003773990087211132
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 98/300:
  Label Loss: 0.1268
  Image Loss: 0.3002
  Total Loss: 127.1011
  Image grad max: 0.0034477051813155413
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 99/300:
  Label Loss: 0.1268
  Image Loss: 0.3193
  Total Loss: 127.1203
  Image grad max: 0.0040659671649336815
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 100/300:
  Label Loss: 0.1268
  Image Loss: 0.3315
  Total Loss: 127.1331
  Image grad max: 0.005758065264672041
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 101/300:
  Label Loss: 0.1268
  Image Loss: 0.3401
  Total Loss: 127.1415
  Image grad max: 0.0053643835708498955
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 102/300:
  Label Loss: 0.1268
  Image Loss: 0.3454
  Total Loss: 127.1469
  Image grad max: 0.005923864431679249
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 103/300:
  Label Loss: 0.1268
  Image Loss: 0.3483
  Total Loss: 127.1501
  Image grad max: 0.0064001744613051414
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 104/300:
  Label Loss: 0.1268
  Image Loss: 0.3483
  Total Loss: 127.1496
  Image grad max: 0.005089356563985348
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 105/300:
  Label Loss: 0.1268
  Image Loss: 0.3464
  Total Loss: 127.1471
  Image grad max: 0.0036758161149919033
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 106/300:
  Label Loss: 0.1268
  Image Loss: 0.3438
  Total Loss: 127.1442
  Image grad max: 0.002974374918267131
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 107/300:
  Label Loss: 0.1268
  Image Loss: 0.3409
  Total Loss: 127.1413
  Image grad max: 0.0027246144600212574
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 108/300:
  Label Loss: 0.1268
  Image Loss: 0.3378
  Total Loss: 127.1381
  Image grad max: 0.002632704796269536
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 109/300:
  Label Loss: 0.1268
  Image Loss: 0.3343
  Total Loss: 127.1346
  Image grad max: 0.0026148364413529634
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 110/300:
  Label Loss: 0.1268
  Image Loss: 0.3305
  Total Loss: 127.1309
  Image grad max: 0.002626472618430853
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 111/300:
  Label Loss: 0.1268
  Image Loss: 0.3266
  Total Loss: 127.1270
  Image grad max: 0.0026515803765505552
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 112/300:
  Label Loss: 0.1268
  Image Loss: 0.3225
  Total Loss: 127.1229
  Image grad max: 0.002692402806133032
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 113/300:
  Label Loss: 0.1268
  Image Loss: 0.3182
  Total Loss: 127.1186
  Image grad max: 0.002797435037791729
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 114/300:
  Label Loss: 0.1268
  Image Loss: 0.3135
  Total Loss: 127.1139
  Image grad max: 0.0029104626737535
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 115/300:
  Label Loss: 0.1268
  Image Loss: 0.3083
  Total Loss: 127.1088
  Image grad max: 0.003123495727777481
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 116/300:
  Label Loss: 0.1268
  Image Loss: 0.3028
  Total Loss: 127.1034
  Image grad max: 0.0034965616650879383
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 117/300:
  Label Loss: 0.1268
  Image Loss: 0.2968
  Total Loss: 127.0976
  Image grad max: 0.0038804523646831512
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 118/300:
  Label Loss: 0.1268
  Image Loss: 0.2906
  Total Loss: 127.0914
  Image grad max: 0.00407295860350132
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 119/300:
  Label Loss: 0.1268
  Image Loss: 0.2838
  Total Loss: 127.0846
  Image grad max: 0.003816746873781085
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 120/300:
  Label Loss: 0.1268
  Image Loss: 0.2767
  Total Loss: 127.0773
  Image grad max: 0.0033353609032928944
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 121/300:
  Label Loss: 0.1268
  Image Loss: 0.2690
  Total Loss: 127.0695
  Image grad max: 0.0029776045121252537
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 122/300:
  Label Loss: 0.1268
  Image Loss: 0.2609
  Total Loss: 127.0613
  Image grad max: 0.0027678643818944693
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 123/300:
  Label Loss: 0.1268
  Image Loss: 0.2523
  Total Loss: 127.0527
  Image grad max: 0.0030511324293911457
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 124/300:
  Label Loss: 0.1268
  Image Loss: 0.2433
  Total Loss: 127.0443
  Image grad max: 0.005582341458648443
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 125/300:
  Label Loss: 0.1268
  Image Loss: 0.2342
  Total Loss: 127.0362
  Image grad max: 0.010649969801306725
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 126/300:
  Label Loss: 0.1268
  Image Loss: 0.2253
  Total Loss: 127.0258
  Image grad max: 0.0035376413725316525
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 127/300:
  Label Loss: 0.1268
  Image Loss: 0.2162
  Total Loss: 127.0166
  Image grad max: 0.002591002732515335
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 128/300:
  Label Loss: 0.1268
  Image Loss: 0.2070
  Total Loss: 127.0073
  Image grad max: 0.0025465087965130806
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 129/300:
  Label Loss: 0.1268
  Image Loss: 0.1977
  Total Loss: 126.9980
  Image grad max: 0.002549707656726241
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 130/300:
  Label Loss: 0.1268
  Image Loss: 0.1883
  Total Loss: 126.9886
  Image grad max: 0.0025485511869192123
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 131/300:
  Label Loss: 0.1268
  Image Loss: 0.1789
  Total Loss: 126.9792
  Image grad max: 0.002543805632740259
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 132/300:
  Label Loss: 0.1268
  Image Loss: 0.1697
  Total Loss: 126.9700
  Image grad max: 0.002536165527999401
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 133/300:
  Label Loss: 0.1268
  Image Loss: 0.1607
  Total Loss: 126.9610
  Image grad max: 0.0025259258691221476
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 134/300:
  Label Loss: 0.1268
  Image Loss: 0.1519
  Total Loss: 126.9522
  Image grad max: 0.002513330662623048
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 135/300:
  Label Loss: 0.1268
  Image Loss: 0.1435
  Total Loss: 126.9438
  Image grad max: 0.0024985321797430515
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 136/300:
  Label Loss: 0.1268
  Image Loss: 0.1354
  Total Loss: 126.9357
  Image grad max: 0.0024815122596919537
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 137/300:
  Label Loss: 0.1268
  Image Loss: 0.1276
  Total Loss: 126.9279
  Image grad max: 0.0024620324838906527
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 138/300:
  Label Loss: 0.1268
  Image Loss: 0.1202
  Total Loss: 126.9206
  Image grad max: 0.0024388267192989588
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 139/300:
  Label Loss: 0.1268
  Image Loss: 0.1132
  Total Loss: 126.9135
  Image grad max: 0.002408357569947839
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 140/300:
  Label Loss: 0.1268
  Image Loss: 0.1065
  Total Loss: 126.9068
  Image grad max: 0.0024672760628163815
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 141/300:
  Label Loss: 0.1268
  Image Loss: 0.1001
  Total Loss: 126.9005
  Image grad max: 0.0026821857318282127
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 142/300:
  Label Loss: 0.1268
  Image Loss: 0.0940
  Total Loss: 126.8945
  Image grad max: 0.0028991184663027525
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 143/300:
  Label Loss: 0.1268
  Image Loss: 0.0884
  Total Loss: 126.8888
  Image grad max: 0.002713915891945362
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 144/300:
  Label Loss: 0.1268
  Image Loss: 0.0830
  Total Loss: 126.8834
  Image grad max: 0.002417063806205988
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 145/300:
  Label Loss: 0.1268
  Image Loss: 0.0780
  Total Loss: 126.8784
  Image grad max: 0.0022843708284199238
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 146/300:
  Label Loss: 0.1268
  Image Loss: 0.0733
  Total Loss: 126.8737
  Image grad max: 0.0022457833401858807
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 147/300:
  Label Loss: 0.1268
  Image Loss: 0.0689
  Total Loss: 126.8693
  Image grad max: 0.002219889312982559
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 148/300:
  Label Loss: 0.1268
  Image Loss: 0.0648
  Total Loss: 126.8651
  Image grad max: 0.0022242749109864235
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 149/300:
  Label Loss: 0.1268
  Image Loss: 0.0609
  Total Loss: 126.8612
  Image grad max: 0.0023436585906893015
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 150/300:
  Label Loss: 0.1268
  Image Loss: 0.0572
  Total Loss: 126.8576
  Image grad max: 0.0027531443629413843
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 151/300:
  Label Loss: 0.1268
  Image Loss: 0.0537
  Total Loss: 126.8542
  Image grad max: 0.003005256410688162
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 152/300:
  Label Loss: 0.1268
  Image Loss: 0.0506
  Total Loss: 126.8510
  Image grad max: 0.0024380190297961235
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 153/300:
  Label Loss: 0.1268
  Image Loss: 0.0476
  Total Loss: 126.8479
  Image grad max: 0.0021007026080042124
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 154/300:
  Label Loss: 0.1268
  Image Loss: 0.0448
  Total Loss: 126.8451
  Image grad max: 0.0020315770525485277
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 155/300:
  Label Loss: 0.1268
  Image Loss: 0.0422
  Total Loss: 126.8425
  Image grad max: 0.0020043980330228806
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 156/300:
  Label Loss: 0.1268
  Image Loss: 0.0397
  Total Loss: 126.8400
  Image grad max: 0.0019763209857046604
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 157/300:
  Label Loss: 0.1268
  Image Loss: 0.0374
  Total Loss: 126.8377
  Image grad max: 0.0019481910858303308
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 158/300:
  Label Loss: 0.1268
  Image Loss: 0.0352
  Total Loss: 126.8355
  Image grad max: 0.0019203119445592165
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 159/300:
  Label Loss: 0.1268
  Image Loss: 0.0331
  Total Loss: 126.8334
  Image grad max: 0.0018928692443296313
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 160/300:
  Label Loss: 0.1268
  Image Loss: 0.0312
  Total Loss: 126.8315
  Image grad max: 0.0018660003552213311
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 161/300:
  Label Loss: 0.1268
  Image Loss: 0.0293
  Total Loss: 126.8297
  Image grad max: 0.0018413560464978218
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 162/300:
  Label Loss: 0.1268
  Image Loss: 0.0276
  Total Loss: 126.8279
  Image grad max: 0.0018227559048682451
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 163/300:
  Label Loss: 0.1268
  Image Loss: 0.0260
  Total Loss: 126.8263
  Image grad max: 0.0018588807433843613
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 164/300:
  Label Loss: 0.1268
  Image Loss: 0.0245
  Total Loss: 126.8249
  Image grad max: 0.0019900454208254814
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 165/300:
  Label Loss: 0.1268
  Image Loss: 0.0231
  Total Loss: 126.8235
  Image grad max: 0.002044509630650282
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 166/300:
  Label Loss: 0.1268
  Image Loss: 0.0217
  Total Loss: 126.8221
  Image grad max: 0.001910847146064043
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 167/300:
  Label Loss: 0.1268
  Image Loss: 0.0205
  Total Loss: 126.8209
  Image grad max: 0.0017380353529006243
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 168/300:
  Label Loss: 0.1268
  Image Loss: 0.0194
  Total Loss: 126.8197
  Image grad max: 0.0016746498877182603
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 169/300:
  Label Loss: 0.1268
  Image Loss: 0.0183
  Total Loss: 126.8186
  Image grad max: 0.0016353810206055641
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 170/300:
  Label Loss: 0.1268
  Image Loss: 0.0173
  Total Loss: 126.8176
  Image grad max: 0.0016019362956285477
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 171/300:
  Label Loss: 0.1268
  Image Loss: 0.0163
  Total Loss: 126.8166
  Image grad max: 0.001571836182847619
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 172/300:
  Label Loss: 0.1268
  Image Loss: 0.0154
  Total Loss: 126.8157
  Image grad max: 0.0015440526185557246
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 173/300:
  Label Loss: 0.1268
  Image Loss: 0.0145
  Total Loss: 126.8149
  Image grad max: 0.0015181214548647404
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 174/300:
  Label Loss: 0.1268
  Image Loss: 0.0137
  Total Loss: 126.8140
  Image grad max: 0.0014937258092686534
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 175/300:
  Label Loss: 0.1268
  Image Loss: 0.0129
  Total Loss: 126.8133
  Image grad max: 0.0014710226096212864
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 176/300:
  Label Loss: 0.1268
  Image Loss: 0.0122
  Total Loss: 126.8126
  Image grad max: 0.0014502685517072678
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 177/300:
  Label Loss: 0.1268
  Image Loss: 0.0115
  Total Loss: 126.8119
  Image grad max: 0.0014317500172182918
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 178/300:
  Label Loss: 0.1268
  Image Loss: 0.0109
  Total Loss: 126.8112
  Image grad max: 0.0014158653793856502
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 179/300:
  Label Loss: 0.1268
  Image Loss: 0.0103
  Total Loss: 126.8106
  Image grad max: 0.0014020842500030994
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 180/300:
  Label Loss: 0.1268
  Image Loss: 0.0097
  Total Loss: 126.8101
  Image grad max: 0.0013887427048757672
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 181/300:
  Label Loss: 0.1268
  Image Loss: 0.0092
  Total Loss: 126.8095
  Image grad max: 0.0013762377202510834
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 182/300:
  Label Loss: 0.1268
  Image Loss: 0.0087
  Total Loss: 126.8090
  Image grad max: 0.0013622855767607689
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 183/300:
  Label Loss: 0.1268
  Image Loss: 0.0082
  Total Loss: 126.8086
  Image grad max: 0.0013441168703138828
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 184/300:
  Label Loss: 0.1268
  Image Loss: 0.0078
  Total Loss: 126.8081
  Image grad max: 0.0013205896830186248
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 185/300:
  Label Loss: 0.1268
  Image Loss: 0.0073
  Total Loss: 126.8077
  Image grad max: 0.001292187487706542
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 186/300:
  Label Loss: 0.1268
  Image Loss: 0.0069
  Total Loss: 126.8073
  Image grad max: 0.0012614601291716099
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 187/300:
  Label Loss: 0.1268
  Image Loss: 0.0066
  Total Loss: 126.8069
  Image grad max: 0.0012309966841712594
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 188/300:
  Label Loss: 0.1268
  Image Loss: 0.0062
  Total Loss: 126.8066
  Image grad max: 0.0012012553634122014
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 189/300:
  Label Loss: 0.1268
  Image Loss: 0.0059
  Total Loss: 126.8062
  Image grad max: 0.0011736376909539104
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 190/300:
  Label Loss: 0.1268
  Image Loss: 0.0056
  Total Loss: 126.8059
  Image grad max: 0.001147958217188716
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 191/300:
  Label Loss: 0.1268
  Image Loss: 0.0053
  Total Loss: 126.8056
  Image grad max: 0.0011240338208153844
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 192/300:
  Label Loss: 0.1268
  Image Loss: 0.0050
  Total Loss: 126.8053
  Image grad max: 0.0011015984928235412
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 193/300:
  Label Loss: 0.1268
  Image Loss: 0.0047
  Total Loss: 126.8051
  Image grad max: 0.0010803829645738006
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 194/300:
  Label Loss: 0.1268
  Image Loss: 0.0045
  Total Loss: 126.8048
  Image grad max: 0.0010604748968034983
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 195/300:
  Label Loss: 0.1268
  Image Loss: 0.0043
  Total Loss: 126.8046
  Image grad max: 0.0010417161975055933
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 196/300:
  Label Loss: 0.1268
  Image Loss: 0.0040
  Total Loss: 126.8044
  Image grad max: 0.0010240819538012147
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 197/300:
  Label Loss: 0.1268
  Image Loss: 0.0038
  Total Loss: 126.8042
  Image grad max: 0.0010075898608192801
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 198/300:
  Label Loss: 0.1268
  Image Loss: 0.0036
  Total Loss: 126.8040
  Image grad max: 0.0009918666910380125
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 199/300:
  Label Loss: 0.1268
  Image Loss: 0.0034
  Total Loss: 126.8038
  Image grad max: 0.0009765257127583027
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 200/300:
  Label Loss: 0.1268
  Image Loss: 0.0033
  Total Loss: 126.8036
  Image grad max: 0.0009614050504751503
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 201/300:
  Label Loss: 0.1268
  Image Loss: 0.0031
  Total Loss: 126.8034
  Image grad max: 0.000946233922149986
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 202/300:
  Label Loss: 0.1268
  Image Loss: 0.0029
  Total Loss: 126.8033
  Image grad max: 0.0009303230326622725
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 203/300:
  Label Loss: 0.1268
  Image Loss: 0.0028
  Total Loss: 126.8031
  Image grad max: 0.0009137439192272723
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 204/300:
  Label Loss: 0.1268
  Image Loss: 0.0027
  Total Loss: 126.8030
  Image grad max: 0.0008963621221482754
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 205/300:
  Label Loss: 0.1268
  Image Loss: 0.0025
  Total Loss: 126.8028
  Image grad max: 0.0008780937059782445
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 206/300:
  Label Loss: 0.1268
  Image Loss: 0.0024
  Total Loss: 126.8027
  Image grad max: 0.0008594620157964528
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 207/300:
  Label Loss: 0.1268
  Image Loss: 0.0023
  Total Loss: 126.8026
  Image grad max: 0.0008405220578424633
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 208/300:
  Label Loss: 0.1268
  Image Loss: 0.0022
  Total Loss: 126.8025
  Image grad max: 0.0008219426963478327
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 209/300:
  Label Loss: 0.1268
  Image Loss: 0.0021
  Total Loss: 126.8024
  Image grad max: 0.0008037128136493266
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 210/300:
  Label Loss: 0.1268
  Image Loss: 0.0020
  Total Loss: 126.8023
  Image grad max: 0.0007859542383812368
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 211/300:
  Label Loss: 0.1268
  Image Loss: 0.0019
  Total Loss: 126.8022
  Image grad max: 0.00076879165135324
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 212/300:
  Label Loss: 0.1268
  Image Loss: 0.0018
  Total Loss: 126.8021
  Image grad max: 0.0007525468245148659
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 213/300:
  Label Loss: 0.1268
  Image Loss: 0.0017
  Total Loss: 126.8020
  Image grad max: 0.0007370918756350875
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 214/300:
  Label Loss: 0.1268
  Image Loss: 0.0016
  Total Loss: 126.8019
  Image grad max: 0.0007222812855616212
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 215/300:
  Label Loss: 0.1268
  Image Loss: 0.0015
  Total Loss: 126.8019
  Image grad max: 0.0007080520736053586
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 216/300:
  Label Loss: 0.1268
  Image Loss: 0.0015
  Total Loss: 126.8018
  Image grad max: 0.000694500282406807
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 217/300:
  Label Loss: 0.1268
  Image Loss: 0.0014
  Total Loss: 126.8017
  Image grad max: 0.0006817295798100531
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 218/300:
  Label Loss: 0.1268
  Image Loss: 0.0013
  Total Loss: 126.8017
  Image grad max: 0.0006693784380331635
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 219/300:
  Label Loss: 0.1268
  Image Loss: 0.0013
  Total Loss: 126.8016
  Image grad max: 0.0006572635611519217
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 220/300:
  Label Loss: 0.1268
  Image Loss: 0.0012
  Total Loss: 126.8015
  Image grad max: 0.0006452900124713778
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 221/300:
  Label Loss: 0.1268
  Image Loss: 0.0011
  Total Loss: 126.8015
  Image grad max: 0.0006333335768431425
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 222/300:
  Label Loss: 0.1268
  Image Loss: 0.0011
  Total Loss: 126.8014
  Image grad max: 0.0006215512403286994
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 223/300:
  Label Loss: 0.1268
  Image Loss: 0.0010
  Total Loss: 126.8014
  Image grad max: 0.0006096092402003706
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 224/300:
  Label Loss: 0.1268
  Image Loss: 0.0010
  Total Loss: 126.8013
  Image grad max: 0.0005976252141408622
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 225/300:
  Label Loss: 0.1268
  Image Loss: 0.0009
  Total Loss: 126.8013
  Image grad max: 0.000585364643484354
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 226/300:
  Label Loss: 0.1268
  Image Loss: 0.0009
  Total Loss: 126.8012
  Image grad max: 0.0005728845717385411
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 227/300:
  Label Loss: 0.1268
  Image Loss: 0.0009
  Total Loss: 126.8012
  Image grad max: 0.0005604143370874226
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 228/300:
  Label Loss: 0.1268
  Image Loss: 0.0008
  Total Loss: 126.8012
  Image grad max: 0.0005481213447637856
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 229/300:
  Label Loss: 0.1268
  Image Loss: 0.0008
  Total Loss: 126.8011
  Image grad max: 0.0005359636270441115
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 230/300:
  Label Loss: 0.1268
  Image Loss: 0.0008
  Total Loss: 126.8011
  Image grad max: 0.0005241489270702004
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 231/300:
  Label Loss: 0.1268
  Image Loss: 0.0007
  Total Loss: 126.8011
  Image grad max: 0.0005126810283400118
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 232/300:
  Label Loss: 0.1268
  Image Loss: 0.0007
  Total Loss: 126.8010
  Image grad max: 0.0005014738417230546
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 233/300:
  Label Loss: 0.1268
  Image Loss: 0.0007
  Total Loss: 126.8010
  Image grad max: 0.0004905529785901308
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 234/300:
  Label Loss: 0.1268
  Image Loss: 0.0006
  Total Loss: 126.8010
  Image grad max: 0.00047996651846915483
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 235/300:
  Label Loss: 0.1268
  Image Loss: 0.0006
  Total Loss: 126.8009
  Image grad max: 0.0004703819577116519
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 236/300:
  Label Loss: 0.1268
  Image Loss: 0.0006
  Total Loss: 126.8009
  Image grad max: 0.00046131107956171036
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 237/300:
  Label Loss: 0.1268
  Image Loss: 0.0006
  Total Loss: 126.8009
  Image grad max: 0.00045268802205100656
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 238/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 126.8009
  Image grad max: 0.0004442453500814736
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 239/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 126.8008
  Image grad max: 0.00043593806913122535
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 240/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 126.8008
  Image grad max: 0.00042772770393639803
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 241/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 126.8008
  Image grad max: 0.00041960441740229726
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 242/300:
  Label Loss: 0.1268
  Image Loss: 0.0005
  Total Loss: 126.8008
  Image grad max: 0.00041152461199089885
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 243/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 126.8008
  Image grad max: 0.00040349934715777636
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 244/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 126.8007
  Image grad max: 0.0003955925640184432
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 245/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 126.8007
  Image grad max: 0.0003878544957842678
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 246/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 126.8007
  Image grad max: 0.0003802370629273355
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 247/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 126.8007
  Image grad max: 0.00037295103538781404
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 248/300:
  Label Loss: 0.1268
  Image Loss: 0.0004
  Total Loss: 126.8007
  Image grad max: 0.00036565071786753833
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 249/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8007
  Image grad max: 0.00035836175084114075
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 250/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8007
  Image grad max: 0.0003510962997097522
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 251/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8007
  Image grad max: 0.0003439500869717449
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 252/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8006
  Image grad max: 0.00033692485885694623
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 253/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8006
  Image grad max: 0.0003299618256278336
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 254/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8006
  Image grad max: 0.0003230688162147999
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 255/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8006
  Image grad max: 0.00031625613337382674
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 256/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8006
  Image grad max: 0.0003096333530265838
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 257/300:
  Label Loss: 0.1268
  Image Loss: 0.0003
  Total Loss: 126.8006
  Image grad max: 0.00030322172096930444
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 258/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8006
  Image grad max: 0.00029708404326811433
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 259/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8006
  Image grad max: 0.00029107776936143637
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 260/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8006
  Image grad max: 0.00028517324244603515
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 261/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8006
  Image grad max: 0.00027967305504716933
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 262/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8006
  Image grad max: 0.00027436623349785805
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 263/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.0002691432600840926
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 264/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00026396114844828844
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 265/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.0002588182978797704
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 266/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00025362049927935004
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 267/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00024837907403707504
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 268/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.0002431058674119413
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 269/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00023782829521223903
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 270/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00023257930297404528
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 271/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.0002273958525620401
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 272/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00022228650050237775
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 273/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.0002173308894271031
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 274/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00021255332103464752
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 275/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00020786623645108193
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 276/300:
  Label Loss: 0.1268
  Image Loss: 0.0002
  Total Loss: 126.8005
  Image grad max: 0.00020325806690379977
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 277/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.00019886469817720354
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 278/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.00019470436382107437
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 279/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.0001906252873595804
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 280/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.0001866196107584983
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 281/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.0001826893276302144
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 282/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.00017882240354083478
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 283/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.00017501064576208591
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 284/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.00017124967416748405
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 285/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.00016753998352214694
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 286/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.00016388471703976393
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 287/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.00016028375830501318
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 288/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.0001568945008330047
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 289/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.0001535605260869488
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 290/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8005
  Image grad max: 0.000150265172123909
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 291/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00014700269093737006
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 292/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00014122179709374905
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 293/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00013773946557193995
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 294/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00013401059550233185
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 295/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00013021565973758698
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 296/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.0001262629812117666
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 297/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00012223412340972573
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 298/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00011820586223620921
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 299/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00011425110278651118
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085]]
Adversarial Training Loop 300/300:
  Label Loss: 0.1268
  Image Loss: 0.0001
  Total Loss: 126.8004
  Image grad max: 0.00011042473488487303
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
