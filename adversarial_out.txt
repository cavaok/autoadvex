running dynamical.py
Epoch [1/30], Batch [0/6000], Loss: 7.2916
Epoch [1/30], Batch [100/6000], Loss: 4.8663
Epoch [1/30], Batch [200/6000], Loss: 3.2617
Epoch [1/30], Batch [300/6000], Loss: 3.3895
Epoch [1/30], Batch [400/6000], Loss: 2.3925
Epoch [1/30], Batch [500/6000], Loss: 2.2817
Epoch [1/30], Batch [600/6000], Loss: 1.5987
Epoch [1/30], Batch [700/6000], Loss: 1.7913
Epoch [1/30], Batch [800/6000], Loss: 2.2878
Epoch [1/30], Batch [900/6000], Loss: 1.7900
Epoch [1/30], Batch [1000/6000], Loss: 4.0222
Epoch [1/30], Batch [1100/6000], Loss: 0.8782
Epoch [1/30], Batch [1200/6000], Loss: 1.5168
Epoch [1/30], Batch [1300/6000], Loss: 2.7733
Epoch [1/30], Batch [1400/6000], Loss: 1.2305
Epoch [1/30], Batch [1500/6000], Loss: 6.0630
Epoch [1/30], Batch [1600/6000], Loss: 1.2269
Epoch [1/30], Batch [1700/6000], Loss: 3.1095
Epoch [1/30], Batch [1800/6000], Loss: 1.6240
Epoch [1/30], Batch [1900/6000], Loss: 1.3280
Epoch [1/30], Batch [2000/6000], Loss: 0.8002
Epoch [1/30], Batch [2100/6000], Loss: 2.4058
Epoch [1/30], Batch [2200/6000], Loss: 1.6757
Epoch [1/30], Batch [2300/6000], Loss: 1.1293
Epoch [1/30], Batch [2400/6000], Loss: 0.9223
Epoch [1/30], Batch [2500/6000], Loss: 2.8007
Epoch [1/30], Batch [2600/6000], Loss: 1.0059
Epoch [1/30], Batch [2700/6000], Loss: 3.3440
Epoch [1/30], Batch [2800/6000], Loss: 0.7389
Epoch [1/30], Batch [2900/6000], Loss: 2.8059
Epoch [1/30], Batch [3000/6000], Loss: 1.8387
Epoch [1/30], Batch [3100/6000], Loss: 0.7676
Epoch [1/30], Batch [3200/6000], Loss: 1.4021
Epoch [1/30], Batch [3300/6000], Loss: 3.6848
Epoch [1/30], Batch [3400/6000], Loss: 0.9871
Epoch [1/30], Batch [3500/6000], Loss: 0.5079
Epoch [1/30], Batch [3600/6000], Loss: 1.1048
Epoch [1/30], Batch [3700/6000], Loss: 1.2442
Epoch [1/30], Batch [3800/6000], Loss: 0.4846
Epoch [1/30], Batch [3900/6000], Loss: 2.1637
Epoch [1/30], Batch [4000/6000], Loss: 1.7329
Epoch [1/30], Batch [4100/6000], Loss: 1.9554
Epoch [1/30], Batch [4200/6000], Loss: 1.4386
Epoch [1/30], Batch [4300/6000], Loss: 0.9289
Epoch [1/30], Batch [4400/6000], Loss: 1.5559
Epoch [1/30], Batch [4500/6000], Loss: 0.9156
Epoch [1/30], Batch [4600/6000], Loss: 0.7818
Epoch [1/30], Batch [4700/6000], Loss: 1.7874
Epoch [1/30], Batch [4800/6000], Loss: 1.1322
Epoch [1/30], Batch [4900/6000], Loss: 0.3657
Epoch [1/30], Batch [5000/6000], Loss: 1.1659
Epoch [1/30], Batch [5100/6000], Loss: 3.9204
Epoch [1/30], Batch [5200/6000], Loss: 0.4083
Epoch [1/30], Batch [5300/6000], Loss: 1.7554
Epoch [1/30], Batch [5400/6000], Loss: 2.1055
Epoch [1/30], Batch [5500/6000], Loss: 1.1262
Epoch [1/30], Batch [5600/6000], Loss: 0.6410
Epoch [1/30], Batch [5700/6000], Loss: 0.4665
Epoch [1/30], Batch [5800/6000], Loss: 0.3857
Epoch [1/30], Batch [5900/6000], Loss: 0.3879
Epoch [1/30], Loss: 1.8164
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 2.9328
Epoch [2/30], Batch [100/6000], Loss: 0.6187
Epoch [2/30], Batch [200/6000], Loss: 0.8484
Epoch [2/30], Batch [300/6000], Loss: 2.5897
Epoch [2/30], Batch [400/6000], Loss: 0.5146
Epoch [2/30], Batch [500/6000], Loss: 0.5524
Epoch [2/30], Batch [600/6000], Loss: 0.4639
Epoch [2/30], Batch [700/6000], Loss: 3.2321
Epoch [2/30], Batch [800/6000], Loss: 1.4228
Epoch [2/30], Batch [900/6000], Loss: 0.6703
Epoch [2/30], Batch [1000/6000], Loss: 0.2680
Epoch [2/30], Batch [1100/6000], Loss: 2.1013
Epoch [2/30], Batch [1200/6000], Loss: 1.4915
Epoch [2/30], Batch [1300/6000], Loss: 0.3517
Epoch [2/30], Batch [1400/6000], Loss: 1.2626
Epoch [2/30], Batch [1500/6000], Loss: 1.3560
Epoch [2/30], Batch [1600/6000], Loss: 2.4213
Epoch [2/30], Batch [1700/6000], Loss: 0.4368
Epoch [2/30], Batch [1800/6000], Loss: 1.2514
Epoch [2/30], Batch [1900/6000], Loss: 0.6437
Epoch [2/30], Batch [2000/6000], Loss: 0.5843
Epoch [2/30], Batch [2100/6000], Loss: 1.2367
Epoch [2/30], Batch [2200/6000], Loss: 2.4656
Epoch [2/30], Batch [2300/6000], Loss: 0.4861
Epoch [2/30], Batch [2400/6000], Loss: 0.3612
Epoch [2/30], Batch [2500/6000], Loss: 1.6398
Epoch [2/30], Batch [2600/6000], Loss: 0.7118
Epoch [2/30], Batch [2700/6000], Loss: 0.9470
Epoch [2/30], Batch [2800/6000], Loss: 0.5581
Epoch [2/30], Batch [2900/6000], Loss: 0.2052
Epoch [2/30], Batch [3000/6000], Loss: 1.0313
Epoch [2/30], Batch [3100/6000], Loss: 1.7513
Epoch [2/30], Batch [3200/6000], Loss: 1.2198
Epoch [2/30], Batch [3300/6000], Loss: 0.6946
Epoch [2/30], Batch [3400/6000], Loss: 1.6595
Epoch [2/30], Batch [3500/6000], Loss: 0.4976
Epoch [2/30], Batch [3600/6000], Loss: 0.3938
Epoch [2/30], Batch [3700/6000], Loss: 1.9461
Epoch [2/30], Batch [3800/6000], Loss: 0.4189
Epoch [2/30], Batch [3900/6000], Loss: 1.7105
Epoch [2/30], Batch [4000/6000], Loss: 2.2370
Epoch [2/30], Batch [4100/6000], Loss: 0.6128
Epoch [2/30], Batch [4200/6000], Loss: 0.4431
Epoch [2/30], Batch [4300/6000], Loss: 0.7200
Epoch [2/30], Batch [4400/6000], Loss: 0.9496
Epoch [2/30], Batch [4500/6000], Loss: 0.3474
Epoch [2/30], Batch [4600/6000], Loss: 0.2683
Epoch [2/30], Batch [4700/6000], Loss: 0.7036
Epoch [2/30], Batch [4800/6000], Loss: 0.5017
Epoch [2/30], Batch [4900/6000], Loss: 0.8546
Epoch [2/30], Batch [5000/6000], Loss: 0.8253
Epoch [2/30], Batch [5100/6000], Loss: 0.8809
Epoch [2/30], Batch [5200/6000], Loss: 1.7879
Epoch [2/30], Batch [5300/6000], Loss: 3.3168
Epoch [2/30], Batch [5400/6000], Loss: 2.6314
Epoch [2/30], Batch [5500/6000], Loss: 0.7999
Epoch [2/30], Batch [5600/6000], Loss: 3.1622
Epoch [2/30], Batch [5700/6000], Loss: 3.0778
Epoch [2/30], Batch [5800/6000], Loss: 0.5816
Epoch [2/30], Batch [5900/6000], Loss: 0.4117
Epoch [2/30], Loss: 1.0951
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 1.4934
Epoch [3/30], Batch [100/6000], Loss: 1.3863
Epoch [3/30], Batch [200/6000], Loss: 1.2357
Epoch [3/30], Batch [300/6000], Loss: 0.3504
Epoch [3/30], Batch [400/6000], Loss: 1.0525
Epoch [3/30], Batch [500/6000], Loss: 0.2515
Epoch [3/30], Batch [600/6000], Loss: 0.5066
Epoch [3/30], Batch [700/6000], Loss: 0.3825
Epoch [3/30], Batch [800/6000], Loss: 0.2928
Epoch [3/30], Batch [900/6000], Loss: 0.2158
Epoch [3/30], Batch [1000/6000], Loss: 0.2331
Epoch [3/30], Batch [1100/6000], Loss: 0.4992
Epoch [3/30], Batch [1200/6000], Loss: 0.6737
Epoch [3/30], Batch [1300/6000], Loss: 0.5436
Epoch [3/30], Batch [1400/6000], Loss: 0.6137
Epoch [3/30], Batch [1500/6000], Loss: 1.5027
Epoch [3/30], Batch [1600/6000], Loss: 1.0543
Epoch [3/30], Batch [1700/6000], Loss: 0.3109
Epoch [3/30], Batch [1800/6000], Loss: 0.6381
Epoch [3/30], Batch [1900/6000], Loss: 0.2099
Epoch [3/30], Batch [2000/6000], Loss: 0.5009
Epoch [3/30], Batch [2100/6000], Loss: 0.8888
Epoch [3/30], Batch [2200/6000], Loss: 0.4647
Epoch [3/30], Batch [2300/6000], Loss: 0.2820
Epoch [3/30], Batch [2400/6000], Loss: 0.2601
Epoch [3/30], Batch [2500/6000], Loss: 1.9588
Epoch [3/30], Batch [2600/6000], Loss: 0.9452
Epoch [3/30], Batch [2700/6000], Loss: 0.8576
Epoch [3/30], Batch [2800/6000], Loss: 0.4349
Epoch [3/30], Batch [2900/6000], Loss: 0.2615
Epoch [3/30], Batch [3000/6000], Loss: 0.3390
Epoch [3/30], Batch [3100/6000], Loss: 0.3058
Epoch [3/30], Batch [3200/6000], Loss: 0.6523
Epoch [3/30], Batch [3300/6000], Loss: 0.7016
Epoch [3/30], Batch [3400/6000], Loss: 0.2925
Epoch [3/30], Batch [3500/6000], Loss: 0.2633
Epoch [3/30], Batch [3600/6000], Loss: 0.3972
Epoch [3/30], Batch [3700/6000], Loss: 1.0789
Epoch [3/30], Batch [3800/6000], Loss: 0.2804
Epoch [3/30], Batch [3900/6000], Loss: 0.9062
Epoch [3/30], Batch [4000/6000], Loss: 0.8011
Epoch [3/30], Batch [4100/6000], Loss: 0.7463
Epoch [3/30], Batch [4200/6000], Loss: 0.2032
Epoch [3/30], Batch [4300/6000], Loss: 0.5223
Epoch [3/30], Batch [4400/6000], Loss: 0.3422
Epoch [3/30], Batch [4500/6000], Loss: 0.3228
Epoch [3/30], Batch [4600/6000], Loss: 0.4167
Epoch [3/30], Batch [4700/6000], Loss: 0.7632
Epoch [3/30], Batch [4800/6000], Loss: 2.2989
Epoch [3/30], Batch [4900/6000], Loss: 0.6393
Epoch [3/30], Batch [5000/6000], Loss: 0.3184
Epoch [3/30], Batch [5100/6000], Loss: 0.4406
Epoch [3/30], Batch [5200/6000], Loss: 0.2287
Epoch [3/30], Batch [5300/6000], Loss: 0.8846
Epoch [3/30], Batch [5400/6000], Loss: 0.2676
Epoch [3/30], Batch [5500/6000], Loss: 0.3397
Epoch [3/30], Batch [5600/6000], Loss: 0.5417
Epoch [3/30], Batch [5700/6000], Loss: 0.8922
Epoch [3/30], Batch [5800/6000], Loss: 0.2278
Epoch [3/30], Batch [5900/6000], Loss: 0.2507
Epoch [3/30], Loss: 0.8793
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.5750
Epoch [4/30], Batch [100/6000], Loss: 0.3606
Epoch [4/30], Batch [200/6000], Loss: 0.4167
Epoch [4/30], Batch [300/6000], Loss: 1.3471
Epoch [4/30], Batch [400/6000], Loss: 2.0157
Epoch [4/30], Batch [500/6000], Loss: 0.2701
Epoch [4/30], Batch [600/6000], Loss: 0.6872
Epoch [4/30], Batch [700/6000], Loss: 1.1496
Epoch [4/30], Batch [800/6000], Loss: 0.7727
Epoch [4/30], Batch [900/6000], Loss: 0.2194
Epoch [4/30], Batch [1000/6000], Loss: 0.1838
Epoch [4/30], Batch [1100/6000], Loss: 0.6596
Epoch [4/30], Batch [1200/6000], Loss: 0.2330
Epoch [4/30], Batch [1300/6000], Loss: 0.3841
Epoch [4/30], Batch [1400/6000], Loss: 0.1978
Epoch [4/30], Batch [1500/6000], Loss: 0.7511
Epoch [4/30], Batch [1600/6000], Loss: 2.9279
Epoch [4/30], Batch [1700/6000], Loss: 0.3088
Epoch [4/30], Batch [1800/6000], Loss: 0.2368
Epoch [4/30], Batch [1900/6000], Loss: 1.3073
Epoch [4/30], Batch [2000/6000], Loss: 1.1900
Epoch [4/30], Batch [2100/6000], Loss: 1.1839
Epoch [4/30], Batch [2200/6000], Loss: 0.1936
Epoch [4/30], Batch [2300/6000], Loss: 0.2350
Epoch [4/30], Batch [2400/6000], Loss: 0.3582
Epoch [4/30], Batch [2500/6000], Loss: 0.2118
Epoch [4/30], Batch [2600/6000], Loss: 1.6800
Epoch [4/30], Batch [2700/6000], Loss: 0.9389
Epoch [4/30], Batch [2800/6000], Loss: 0.2311
Epoch [4/30], Batch [2900/6000], Loss: 1.2188
Epoch [4/30], Batch [3000/6000], Loss: 0.5002
Epoch [4/30], Batch [3100/6000], Loss: 3.2000
Epoch [4/30], Batch [3200/6000], Loss: 0.5208
Epoch [4/30], Batch [3300/6000], Loss: 0.5165
Epoch [4/30], Batch [3400/6000], Loss: 0.2430
Epoch [4/30], Batch [3500/6000], Loss: 0.5858
Epoch [4/30], Batch [3600/6000], Loss: 0.9158
Epoch [4/30], Batch [3700/6000], Loss: 1.1358
Epoch [4/30], Batch [3800/6000], Loss: 0.3449
Epoch [4/30], Batch [3900/6000], Loss: 0.6436
Epoch [4/30], Batch [4000/6000], Loss: 1.1076
Epoch [4/30], Batch [4100/6000], Loss: 0.4476
Epoch [4/30], Batch [4200/6000], Loss: 0.5521
Epoch [4/30], Batch [4300/6000], Loss: 0.3275
Epoch [4/30], Batch [4400/6000], Loss: 0.2019
Epoch [4/30], Batch [4500/6000], Loss: 4.8378
Epoch [4/30], Batch [4600/6000], Loss: 0.2527
Epoch [4/30], Batch [4700/6000], Loss: 1.6864
Epoch [4/30], Batch [4800/6000], Loss: 1.9326
Epoch [4/30], Batch [4900/6000], Loss: 0.6678
Epoch [4/30], Batch [5000/6000], Loss: 1.4930
Epoch [4/30], Batch [5100/6000], Loss: 0.3981
Epoch [4/30], Batch [5200/6000], Loss: 0.2539
Epoch [4/30], Batch [5300/6000], Loss: 1.5878
Epoch [4/30], Batch [5400/6000], Loss: 0.4694
Epoch [4/30], Batch [5500/6000], Loss: 0.2820
Epoch [4/30], Batch [5600/6000], Loss: 0.2702
Epoch [4/30], Batch [5700/6000], Loss: 0.1898
Epoch [4/30], Batch [5800/6000], Loss: 0.2336
Epoch [4/30], Batch [5900/6000], Loss: 0.6123
Epoch [4/30], Loss: 0.7335
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.3567
Epoch [5/30], Batch [100/6000], Loss: 2.5726
Epoch [5/30], Batch [200/6000], Loss: 0.1964
Epoch [5/30], Batch [300/6000], Loss: 0.2198
Epoch [5/30], Batch [400/6000], Loss: 0.2439
Epoch [5/30], Batch [500/6000], Loss: 0.7889
Epoch [5/30], Batch [600/6000], Loss: 0.5503
Epoch [5/30], Batch [700/6000], Loss: 0.7492
Epoch [5/30], Batch [800/6000], Loss: 0.4765
Epoch [5/30], Batch [900/6000], Loss: 0.2197
Epoch [5/30], Batch [1000/6000], Loss: 1.1089
Epoch [5/30], Batch [1100/6000], Loss: 0.2107
Epoch [5/30], Batch [1200/6000], Loss: 0.6811
Epoch [5/30], Batch [1300/6000], Loss: 0.2404
Epoch [5/30], Batch [1400/6000], Loss: 0.2537
Epoch [5/30], Batch [1500/6000], Loss: 1.8663
Epoch [5/30], Batch [1600/6000], Loss: 0.1875
Epoch [5/30], Batch [1700/6000], Loss: 1.6282
Epoch [5/30], Batch [1800/6000], Loss: 0.2398
Epoch [5/30], Batch [1900/6000], Loss: 0.4926
Epoch [5/30], Batch [2000/6000], Loss: 0.2377
Epoch [5/30], Batch [2100/6000], Loss: 0.6280
Epoch [5/30], Batch [2200/6000], Loss: 0.2794
Epoch [5/30], Batch [2300/6000], Loss: 0.3187
Epoch [5/30], Batch [2400/6000], Loss: 0.4136
Epoch [5/30], Batch [2500/6000], Loss: 0.2602
Epoch [5/30], Batch [2600/6000], Loss: 0.3273
Epoch [5/30], Batch [2700/6000], Loss: 0.2780
Epoch [5/30], Batch [2800/6000], Loss: 0.1718
Epoch [5/30], Batch [2900/6000], Loss: 0.2050
Epoch [5/30], Batch [3000/6000], Loss: 0.7012
Epoch [5/30], Batch [3100/6000], Loss: 0.2312
Epoch [5/30], Batch [3200/6000], Loss: 5.1046
Epoch [5/30], Batch [3300/6000], Loss: 1.9561
Epoch [5/30], Batch [3400/6000], Loss: 2.4000
Epoch [5/30], Batch [3500/6000], Loss: 0.7032
Epoch [5/30], Batch [3600/6000], Loss: 1.0738
Epoch [5/30], Batch [3700/6000], Loss: 1.2849
Epoch [5/30], Batch [3800/6000], Loss: 0.5068
Epoch [5/30], Batch [3900/6000], Loss: 0.4216
Epoch [5/30], Batch [4000/6000], Loss: 0.3325
Epoch [5/30], Batch [4100/6000], Loss: 0.1770
Epoch [5/30], Batch [4200/6000], Loss: 1.7625
Epoch [5/30], Batch [4300/6000], Loss: 0.4125
Epoch [5/30], Batch [4400/6000], Loss: 0.6155
Epoch [5/30], Batch [4500/6000], Loss: 1.2499
Epoch [5/30], Batch [4600/6000], Loss: 0.5555
Epoch [5/30], Batch [4700/6000], Loss: 0.5532
Epoch [5/30], Batch [4800/6000], Loss: 0.7753
Epoch [5/30], Batch [4900/6000], Loss: 3.4789
Epoch [5/30], Batch [5000/6000], Loss: 0.6586
Epoch [5/30], Batch [5100/6000], Loss: 0.8813
Epoch [5/30], Batch [5200/6000], Loss: 0.1684
Epoch [5/30], Batch [5300/6000], Loss: 0.2428
Epoch [5/30], Batch [5400/6000], Loss: 0.2705
Epoch [5/30], Batch [5500/6000], Loss: 2.1944
Epoch [5/30], Batch [5600/6000], Loss: 0.4579
Epoch [5/30], Batch [5700/6000], Loss: 1.0383
Epoch [5/30], Batch [5800/6000], Loss: 1.2110
Epoch [5/30], Batch [5900/6000], Loss: 1.3930
Epoch [5/30], Loss: 0.6388
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.7467
Epoch [6/30], Batch [100/6000], Loss: 0.3025
Epoch [6/30], Batch [200/6000], Loss: 0.2582
Epoch [6/30], Batch [300/6000], Loss: 0.1783
Epoch [6/30], Batch [400/6000], Loss: 1.2826
Epoch [6/30], Batch [500/6000], Loss: 0.1797
Epoch [6/30], Batch [600/6000], Loss: 0.8834
Epoch [6/30], Batch [700/6000], Loss: 0.1882
Epoch [6/30], Batch [800/6000], Loss: 2.1088
Epoch [6/30], Batch [900/6000], Loss: 0.2753
Epoch [6/30], Batch [1000/6000], Loss: 0.6919
Epoch [6/30], Batch [1100/6000], Loss: 0.2097
Epoch [6/30], Batch [1200/6000], Loss: 0.2783
Epoch [6/30], Batch [1300/6000], Loss: 0.3127
Epoch [6/30], Batch [1400/6000], Loss: 0.2001
Epoch [6/30], Batch [1500/6000], Loss: 0.1935
Epoch [6/30], Batch [1600/6000], Loss: 0.1587
Epoch [6/30], Batch [1700/6000], Loss: 0.1912
Epoch [6/30], Batch [1800/6000], Loss: 0.9653
Epoch [6/30], Batch [1900/6000], Loss: 0.5648
Epoch [6/30], Batch [2000/6000], Loss: 0.2434
Epoch [6/30], Batch [2100/6000], Loss: 1.0933
Epoch [6/30], Batch [2200/6000], Loss: 0.1654
Epoch [6/30], Batch [2300/6000], Loss: 0.1970
Epoch [6/30], Batch [2400/6000], Loss: 0.2816
Epoch [6/30], Batch [2500/6000], Loss: 0.1728
Epoch [6/30], Batch [2600/6000], Loss: 0.3208
Epoch [6/30], Batch [2700/6000], Loss: 0.5894
Epoch [6/30], Batch [2800/6000], Loss: 0.2059
Epoch [6/30], Batch [2900/6000], Loss: 0.1838
Epoch [6/30], Batch [3000/6000], Loss: 0.2366
Epoch [6/30], Batch [3100/6000], Loss: 0.1931
Epoch [6/30], Batch [3200/6000], Loss: 0.1874
Epoch [6/30], Batch [3300/6000], Loss: 0.2743
Epoch [6/30], Batch [3400/6000], Loss: 0.2368
Epoch [6/30], Batch [3500/6000], Loss: 0.5714
Epoch [6/30], Batch [3600/6000], Loss: 0.5365
Epoch [6/30], Batch [3700/6000], Loss: 0.1896
Epoch [6/30], Batch [3800/6000], Loss: 0.2029
Epoch [6/30], Batch [3900/6000], Loss: 0.7890
Epoch [6/30], Batch [4000/6000], Loss: 0.2293
Epoch [6/30], Batch [4100/6000], Loss: 0.3702
Epoch [6/30], Batch [4200/6000], Loss: 0.3148
Epoch [6/30], Batch [4300/6000], Loss: 0.1842
Epoch [6/30], Batch [4400/6000], Loss: 0.1927
Epoch [6/30], Batch [4500/6000], Loss: 0.2414
Epoch [6/30], Batch [4600/6000], Loss: 0.7184
Epoch [6/30], Batch [4700/6000], Loss: 0.2051
Epoch [6/30], Batch [4800/6000], Loss: 0.7805
Epoch [6/30], Batch [4900/6000], Loss: 0.5677
Epoch [6/30], Batch [5000/6000], Loss: 0.5087
Epoch [6/30], Batch [5100/6000], Loss: 0.3402
Epoch [6/30], Batch [5200/6000], Loss: 0.9362
Epoch [6/30], Batch [5300/6000], Loss: 0.1952
Epoch [6/30], Batch [5400/6000], Loss: 0.6994
Epoch [6/30], Batch [5500/6000], Loss: 0.6333
Epoch [6/30], Batch [5600/6000], Loss: 0.2930
Epoch [6/30], Batch [5700/6000], Loss: 0.4790
Epoch [6/30], Batch [5800/6000], Loss: 0.2477
Epoch [6/30], Batch [5900/6000], Loss: 0.2154
Epoch [6/30], Loss: 0.5669
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 1.8628
Epoch [7/30], Batch [100/6000], Loss: 0.1904
Epoch [7/30], Batch [200/6000], Loss: 1.2596
Epoch [7/30], Batch [300/6000], Loss: 0.1669
Epoch [7/30], Batch [400/6000], Loss: 1.0892
Epoch [7/30], Batch [500/6000], Loss: 0.2601
Epoch [7/30], Batch [600/6000], Loss: 0.2202
Epoch [7/30], Batch [700/6000], Loss: 0.4788
Epoch [7/30], Batch [800/6000], Loss: 0.1759
Epoch [7/30], Batch [900/6000], Loss: 0.4526
Epoch [7/30], Batch [1000/6000], Loss: 0.2717
Epoch [7/30], Batch [1100/6000], Loss: 0.1822
Epoch [7/30], Batch [1200/6000], Loss: 0.2800
Epoch [7/30], Batch [1300/6000], Loss: 0.2456
Epoch [7/30], Batch [1400/6000], Loss: 0.3337
Epoch [7/30], Batch [1500/6000], Loss: 2.0004
Epoch [7/30], Batch [1600/6000], Loss: 0.1603
Epoch [7/30], Batch [1700/6000], Loss: 0.3040
Epoch [7/30], Batch [1800/6000], Loss: 0.6118
Epoch [7/30], Batch [1900/6000], Loss: 0.3710
Epoch [7/30], Batch [2000/6000], Loss: 0.3375
Epoch [7/30], Batch [2100/6000], Loss: 0.9240
Epoch [7/30], Batch [2200/6000], Loss: 0.1826
Epoch [7/30], Batch [2300/6000], Loss: 0.3038
Epoch [7/30], Batch [2400/6000], Loss: 0.1864
Epoch [7/30], Batch [2500/6000], Loss: 0.5806
Epoch [7/30], Batch [2600/6000], Loss: 0.2946
Epoch [7/30], Batch [2700/6000], Loss: 0.2013
Epoch [7/30], Batch [2800/6000], Loss: 0.2114
Epoch [7/30], Batch [2900/6000], Loss: 0.1996
Epoch [7/30], Batch [3000/6000], Loss: 0.1547
Epoch [7/30], Batch [3100/6000], Loss: 0.2176
Epoch [7/30], Batch [3200/6000], Loss: 0.2349
Epoch [7/30], Batch [3300/6000], Loss: 0.4290
Epoch [7/30], Batch [3400/6000], Loss: 0.4390
Epoch [7/30], Batch [3500/6000], Loss: 1.4614
Epoch [7/30], Batch [3600/6000], Loss: 0.2594
Epoch [7/30], Batch [3700/6000], Loss: 0.2544
Epoch [7/30], Batch [3800/6000], Loss: 0.3544
Epoch [7/30], Batch [3900/6000], Loss: 1.4570
Epoch [7/30], Batch [4000/6000], Loss: 0.1588
Epoch [7/30], Batch [4100/6000], Loss: 0.2086
Epoch [7/30], Batch [4200/6000], Loss: 0.3264
Epoch [7/30], Batch [4300/6000], Loss: 0.1560
Epoch [7/30], Batch [4400/6000], Loss: 0.1699
Epoch [7/30], Batch [4500/6000], Loss: 0.2772
Epoch [7/30], Batch [4600/6000], Loss: 0.2607
Epoch [7/30], Batch [4700/6000], Loss: 0.1845
Epoch [7/30], Batch [4800/6000], Loss: 0.2221
Epoch [7/30], Batch [4900/6000], Loss: 0.2095
Epoch [7/30], Batch [5000/6000], Loss: 0.2386
Epoch [7/30], Batch [5100/6000], Loss: 0.1956
Epoch [7/30], Batch [5200/6000], Loss: 0.5736
Epoch [7/30], Batch [5300/6000], Loss: 0.1902
Epoch [7/30], Batch [5400/6000], Loss: 0.3189
Epoch [7/30], Batch [5500/6000], Loss: 1.1699
Epoch [7/30], Batch [5600/6000], Loss: 0.3101
Epoch [7/30], Batch [5700/6000], Loss: 0.4097
Epoch [7/30], Batch [5800/6000], Loss: 0.1600
Epoch [7/30], Batch [5900/6000], Loss: 0.3429
Epoch [7/30], Loss: 0.5088
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.1609
Epoch [8/30], Batch [100/6000], Loss: 1.2835
Epoch [8/30], Batch [200/6000], Loss: 0.1582
Epoch [8/30], Batch [300/6000], Loss: 0.1744
Epoch [8/30], Batch [400/6000], Loss: 0.1932
Epoch [8/30], Batch [500/6000], Loss: 1.1872
Epoch [8/30], Batch [600/6000], Loss: 0.1946
Epoch [8/30], Batch [700/6000], Loss: 0.1722
Epoch [8/30], Batch [800/6000], Loss: 0.2284
Epoch [8/30], Batch [900/6000], Loss: 0.3863
Epoch [8/30], Batch [1000/6000], Loss: 1.1482
Epoch [8/30], Batch [1100/6000], Loss: 0.6464
Epoch [8/30], Batch [1200/6000], Loss: 0.2171
Epoch [8/30], Batch [1300/6000], Loss: 0.1739
Epoch [8/30], Batch [1400/6000], Loss: 0.1688
Epoch [8/30], Batch [1500/6000], Loss: 0.2903
Epoch [8/30], Batch [1600/6000], Loss: 1.5101
Epoch [8/30], Batch [1700/6000], Loss: 0.8554
Epoch [8/30], Batch [1800/6000], Loss: 1.1766
Epoch [8/30], Batch [1900/6000], Loss: 0.4713
Epoch [8/30], Batch [2000/6000], Loss: 0.3766
Epoch [8/30], Batch [2100/6000], Loss: 0.1679
Epoch [8/30], Batch [2200/6000], Loss: 0.1721
Epoch [8/30], Batch [2300/6000], Loss: 0.2165
Epoch [8/30], Batch [2400/6000], Loss: 0.1646
Epoch [8/30], Batch [2500/6000], Loss: 1.1045
Epoch [8/30], Batch [2600/6000], Loss: 0.3671
Epoch [8/30], Batch [2700/6000], Loss: 0.8400
Epoch [8/30], Batch [2800/6000], Loss: 0.1717
Epoch [8/30], Batch [2900/6000], Loss: 0.1778
Epoch [8/30], Batch [3000/6000], Loss: 1.3245
Epoch [8/30], Batch [3100/6000], Loss: 0.4650
Epoch [8/30], Batch [3200/6000], Loss: 0.2236
Epoch [8/30], Batch [3300/6000], Loss: 0.7598
Epoch [8/30], Batch [3400/6000], Loss: 0.2363
Epoch [8/30], Batch [3500/6000], Loss: 0.1948
Epoch [8/30], Batch [3600/6000], Loss: 0.6698
Epoch [8/30], Batch [3700/6000], Loss: 0.1728
Epoch [8/30], Batch [3800/6000], Loss: 0.1935
Epoch [8/30], Batch [3900/6000], Loss: 1.4433
Epoch [8/30], Batch [4000/6000], Loss: 0.2362
Epoch [8/30], Batch [4100/6000], Loss: 0.2214
Epoch [8/30], Batch [4200/6000], Loss: 0.2031
Epoch [8/30], Batch [4300/6000], Loss: 0.2638
Epoch [8/30], Batch [4400/6000], Loss: 0.2831
Epoch [8/30], Batch [4500/6000], Loss: 0.1423
Epoch [8/30], Batch [4600/6000], Loss: 0.2064
Epoch [8/30], Batch [4700/6000], Loss: 0.1755
Epoch [8/30], Batch [4800/6000], Loss: 0.2563
Epoch [8/30], Batch [4900/6000], Loss: 0.1919
Epoch [8/30], Batch [5000/6000], Loss: 0.1470
Epoch [8/30], Batch [5100/6000], Loss: 0.1681
Epoch [8/30], Batch [5200/6000], Loss: 0.1676
Epoch [8/30], Batch [5300/6000], Loss: 0.1827
Epoch [8/30], Batch [5400/6000], Loss: 0.1366
Epoch [8/30], Batch [5500/6000], Loss: 1.2067
Epoch [8/30], Batch [5600/6000], Loss: 0.2133
Epoch [8/30], Batch [5700/6000], Loss: 0.3220
Epoch [8/30], Batch [5800/6000], Loss: 0.1543
Epoch [8/30], Batch [5900/6000], Loss: 0.1830
Epoch [8/30], Loss: 0.4721
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 1.3337
Epoch [9/30], Batch [100/6000], Loss: 1.0873
Epoch [9/30], Batch [200/6000], Loss: 0.8378
Epoch [9/30], Batch [300/6000], Loss: 0.3471
Epoch [9/30], Batch [400/6000], Loss: 0.2116
Epoch [9/30], Batch [500/6000], Loss: 0.2397
Epoch [9/30], Batch [600/6000], Loss: 0.1546
Epoch [9/30], Batch [700/6000], Loss: 0.2219
Epoch [9/30], Batch [800/6000], Loss: 0.1745
Epoch [9/30], Batch [900/6000], Loss: 0.3799
Epoch [9/30], Batch [1000/6000], Loss: 0.3822
Epoch [9/30], Batch [1100/6000], Loss: 1.2325
Epoch [9/30], Batch [1200/6000], Loss: 0.1572
Epoch [9/30], Batch [1300/6000], Loss: 0.3253
Epoch [9/30], Batch [1400/6000], Loss: 1.6545
Epoch [9/30], Batch [1500/6000], Loss: 0.2553
Epoch [9/30], Batch [1600/6000], Loss: 0.4235
Epoch [9/30], Batch [1700/6000], Loss: 0.3282
Epoch [9/30], Batch [1800/6000], Loss: 0.1849
Epoch [9/30], Batch [1900/6000], Loss: 0.1921
Epoch [9/30], Batch [2000/6000], Loss: 0.6850
Epoch [9/30], Batch [2100/6000], Loss: 0.1820
Epoch [9/30], Batch [2200/6000], Loss: 0.6103
Epoch [9/30], Batch [2300/6000], Loss: 0.7611
Epoch [9/30], Batch [2400/6000], Loss: 0.2824
Epoch [9/30], Batch [2500/6000], Loss: 0.8829
Epoch [9/30], Batch [2600/6000], Loss: 1.0923
Epoch [9/30], Batch [2700/6000], Loss: 0.1653
Epoch [9/30], Batch [2800/6000], Loss: 0.1399
Epoch [9/30], Batch [2900/6000], Loss: 0.1693
Epoch [9/30], Batch [3000/6000], Loss: 0.1434
Epoch [9/30], Batch [3100/6000], Loss: 0.1692
Epoch [9/30], Batch [3200/6000], Loss: 0.1819
Epoch [9/30], Batch [3300/6000], Loss: 0.1406
Epoch [9/30], Batch [3400/6000], Loss: 0.1447
Epoch [9/30], Batch [3500/6000], Loss: 1.0168
Epoch [9/30], Batch [3600/6000], Loss: 0.1450
Epoch [9/30], Batch [3700/6000], Loss: 0.1449
Epoch [9/30], Batch [3800/6000], Loss: 0.1525
Epoch [9/30], Batch [3900/6000], Loss: 0.1930
Epoch [9/30], Batch [4000/6000], Loss: 0.1678
Epoch [9/30], Batch [4100/6000], Loss: 2.4914
Epoch [9/30], Batch [4200/6000], Loss: 0.3531
Epoch [9/30], Batch [4300/6000], Loss: 0.1745
Epoch [9/30], Batch [4400/6000], Loss: 0.4915
Epoch [9/30], Batch [4500/6000], Loss: 0.1412
Epoch [9/30], Batch [4600/6000], Loss: 0.1356
Epoch [9/30], Batch [4700/6000], Loss: 0.5175
Epoch [9/30], Batch [4800/6000], Loss: 0.2298
Epoch [9/30], Batch [4900/6000], Loss: 0.1483
Epoch [9/30], Batch [5000/6000], Loss: 0.3143
Epoch [9/30], Batch [5100/6000], Loss: 0.1702
Epoch [9/30], Batch [5200/6000], Loss: 0.1623
Epoch [9/30], Batch [5300/6000], Loss: 0.1717
Epoch [9/30], Batch [5400/6000], Loss: 0.1306
Epoch [9/30], Batch [5500/6000], Loss: 1.1473
Epoch [9/30], Batch [5600/6000], Loss: 0.1832
Epoch [9/30], Batch [5700/6000], Loss: 0.1319
Epoch [9/30], Batch [5800/6000], Loss: 0.1347
Epoch [9/30], Batch [5900/6000], Loss: 0.7765
Epoch [9/30], Loss: 0.4300
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.7247
Epoch [10/30], Batch [100/6000], Loss: 0.3638
Epoch [10/30], Batch [200/6000], Loss: 0.2286
Epoch [10/30], Batch [300/6000], Loss: 1.0395
Epoch [10/30], Batch [400/6000], Loss: 0.1495
Epoch [10/30], Batch [500/6000], Loss: 0.2557
Epoch [10/30], Batch [600/6000], Loss: 0.1658
Epoch [10/30], Batch [700/6000], Loss: 1.3309
Epoch [10/30], Batch [800/6000], Loss: 0.1888
Epoch [10/30], Batch [900/6000], Loss: 0.1456
Epoch [10/30], Batch [1000/6000], Loss: 0.1117
Epoch [10/30], Batch [1100/6000], Loss: 0.1786
Epoch [10/30], Batch [1200/6000], Loss: 0.1279
Epoch [10/30], Batch [1300/6000], Loss: 0.1570
Epoch [10/30], Batch [1400/6000], Loss: 0.1502
Epoch [10/30], Batch [1500/6000], Loss: 0.2138
Epoch [10/30], Batch [1600/6000], Loss: 0.4463
Epoch [10/30], Batch [1700/6000], Loss: 1.3166
Epoch [10/30], Batch [1800/6000], Loss: 0.1551
Epoch [10/30], Batch [1900/6000], Loss: 0.1566
Epoch [10/30], Batch [2000/6000], Loss: 0.1359
Epoch [10/30], Batch [2100/6000], Loss: 0.1588
Epoch [10/30], Batch [2200/6000], Loss: 2.8825
Epoch [10/30], Batch [2300/6000], Loss: 0.7705
Epoch [10/30], Batch [2400/6000], Loss: 0.1948
Epoch [10/30], Batch [2500/6000], Loss: 0.1698
Epoch [10/30], Batch [2600/6000], Loss: 0.7045
Epoch [10/30], Batch [2700/6000], Loss: 0.1819
Epoch [10/30], Batch [2800/6000], Loss: 0.4048
Epoch [10/30], Batch [2900/6000], Loss: 0.1536
Epoch [10/30], Batch [3000/6000], Loss: 0.3225
Epoch [10/30], Batch [3100/6000], Loss: 0.2023
Epoch [10/30], Batch [3200/6000], Loss: 0.1642
Epoch [10/30], Batch [3300/6000], Loss: 0.1264
Epoch [10/30], Batch [3400/6000], Loss: 0.2574
Epoch [10/30], Batch [3500/6000], Loss: 2.0746
Epoch [10/30], Batch [3600/6000], Loss: 1.0580
Epoch [10/30], Batch [3700/6000], Loss: 0.2209
Epoch [10/30], Batch [3800/6000], Loss: 0.1712
Epoch [10/30], Batch [3900/6000], Loss: 0.1210
Epoch [10/30], Batch [4000/6000], Loss: 0.1717
Epoch [10/30], Batch [4100/6000], Loss: 0.7414
Epoch [10/30], Batch [4200/6000], Loss: 1.2224
Epoch [10/30], Batch [4300/6000], Loss: 0.1358
Epoch [10/30], Batch [4400/6000], Loss: 0.8391
Epoch [10/30], Batch [4500/6000], Loss: 0.2172
Epoch [10/30], Batch [4600/6000], Loss: 0.1432
Epoch [10/30], Batch [4700/6000], Loss: 0.3662
Epoch [10/30], Batch [4800/6000], Loss: 0.2410
Epoch [10/30], Batch [4900/6000], Loss: 0.2407
Epoch [10/30], Batch [5000/6000], Loss: 0.1639
Epoch [10/30], Batch [5100/6000], Loss: 0.8258
Epoch [10/30], Batch [5200/6000], Loss: 0.1501
Epoch [10/30], Batch [5300/6000], Loss: 0.4160
Epoch [10/30], Batch [5400/6000], Loss: 0.7145
Epoch [10/30], Batch [5500/6000], Loss: 0.1509
Epoch [10/30], Batch [5600/6000], Loss: 0.8246
Epoch [10/30], Batch [5700/6000], Loss: 0.3920
Epoch [10/30], Batch [5800/6000], Loss: 0.5492
Epoch [10/30], Batch [5900/6000], Loss: 0.2014
Epoch [10/30], Loss: 0.4007
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.1391
Epoch [11/30], Batch [100/6000], Loss: 0.1524
Epoch [11/30], Batch [200/6000], Loss: 0.2726
Epoch [11/30], Batch [300/6000], Loss: 0.1522
Epoch [11/30], Batch [400/6000], Loss: 0.1654
Epoch [11/30], Batch [500/6000], Loss: 0.1095
Epoch [11/30], Batch [600/6000], Loss: 0.1655
Epoch [11/30], Batch [700/6000], Loss: 0.2121
Epoch [11/30], Batch [800/6000], Loss: 0.1323
Epoch [11/30], Batch [900/6000], Loss: 0.7790
Epoch [11/30], Batch [1000/6000], Loss: 0.6866
Epoch [11/30], Batch [1100/6000], Loss: 0.1527
Epoch [11/30], Batch [1200/6000], Loss: 0.2167
Epoch [11/30], Batch [1300/6000], Loss: 0.1403
Epoch [11/30], Batch [1400/6000], Loss: 0.3574
Epoch [11/30], Batch [1500/6000], Loss: 0.5255
Epoch [11/30], Batch [1600/6000], Loss: 0.1931
Epoch [11/30], Batch [1700/6000], Loss: 1.5996
Epoch [11/30], Batch [1800/6000], Loss: 0.3342
Epoch [11/30], Batch [1900/6000], Loss: 1.0899
Epoch [11/30], Batch [2000/6000], Loss: 0.1500
Epoch [11/30], Batch [2100/6000], Loss: 0.2408
Epoch [11/30], Batch [2200/6000], Loss: 0.6010
Epoch [11/30], Batch [2300/6000], Loss: 0.1498
Epoch [11/30], Batch [2400/6000], Loss: 0.1807
Epoch [11/30], Batch [2500/6000], Loss: 0.9039
Epoch [11/30], Batch [2600/6000], Loss: 0.1622
Epoch [11/30], Batch [2700/6000], Loss: 0.1459
Epoch [11/30], Batch [2800/6000], Loss: 0.2268
Epoch [11/30], Batch [2900/6000], Loss: 0.1547
Epoch [11/30], Batch [3000/6000], Loss: 0.2554
Epoch [11/30], Batch [3100/6000], Loss: 0.1534
Epoch [11/30], Batch [3200/6000], Loss: 0.2745
Epoch [11/30], Batch [3300/6000], Loss: 0.1807
Epoch [11/30], Batch [3400/6000], Loss: 0.1727
Epoch [11/30], Batch [3500/6000], Loss: 0.1502
Epoch [11/30], Batch [3600/6000], Loss: 0.6895
Epoch [11/30], Batch [3700/6000], Loss: 0.1504
Epoch [11/30], Batch [3800/6000], Loss: 0.1393
Epoch [11/30], Batch [3900/6000], Loss: 0.1177
Epoch [11/30], Batch [4000/6000], Loss: 0.1648
Epoch [11/30], Batch [4100/6000], Loss: 0.2480
Epoch [11/30], Batch [4200/6000], Loss: 0.2233
Epoch [11/30], Batch [4300/6000], Loss: 0.1505
Epoch [11/30], Batch [4400/6000], Loss: 0.3423
Epoch [11/30], Batch [4500/6000], Loss: 0.2255
Epoch [11/30], Batch [4600/6000], Loss: 0.2574
Epoch [11/30], Batch [4700/6000], Loss: 0.1883
Epoch [11/30], Batch [4800/6000], Loss: 0.2887
Epoch [11/30], Batch [4900/6000], Loss: 0.1562
Epoch [11/30], Batch [5000/6000], Loss: 0.1665
Epoch [11/30], Batch [5100/6000], Loss: 0.1405
Epoch [11/30], Batch [5200/6000], Loss: 0.1259
Epoch [11/30], Batch [5300/6000], Loss: 0.6520
Epoch [11/30], Batch [5400/6000], Loss: 0.2369
Epoch [11/30], Batch [5500/6000], Loss: 0.1648
Epoch [11/30], Batch [5600/6000], Loss: 0.7290
Epoch [11/30], Batch [5700/6000], Loss: 0.1307
Epoch [11/30], Batch [5800/6000], Loss: 0.2078
Epoch [11/30], Batch [5900/6000], Loss: 2.7257
Epoch [11/30], Loss: 0.3716
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.2008
Epoch [12/30], Batch [100/6000], Loss: 0.1446
Epoch [12/30], Batch [200/6000], Loss: 0.1737
Epoch [12/30], Batch [300/6000], Loss: 0.1576
Epoch [12/30], Batch [400/6000], Loss: 0.9028
Epoch [12/30], Batch [500/6000], Loss: 0.1492
Epoch [12/30], Batch [600/6000], Loss: 0.1464
Epoch [12/30], Batch [700/6000], Loss: 0.1465
Epoch [12/30], Batch [800/6000], Loss: 1.2238
Epoch [12/30], Batch [900/6000], Loss: 0.1289
Epoch [12/30], Batch [1000/6000], Loss: 0.2437
Epoch [12/30], Batch [1100/6000], Loss: 0.2681
Epoch [12/30], Batch [1200/6000], Loss: 0.1729
Epoch [12/30], Batch [1300/6000], Loss: 0.8599
Epoch [12/30], Batch [1400/6000], Loss: 0.5332
Epoch [12/30], Batch [1500/6000], Loss: 0.3160
Epoch [12/30], Batch [1600/6000], Loss: 0.1671
Epoch [12/30], Batch [1700/6000], Loss: 0.2071
Epoch [12/30], Batch [1800/6000], Loss: 0.1941
Epoch [12/30], Batch [1900/6000], Loss: 0.1737
Epoch [12/30], Batch [2000/6000], Loss: 0.2167
Epoch [12/30], Batch [2100/6000], Loss: 0.1354
Epoch [12/30], Batch [2200/6000], Loss: 0.1387
Epoch [12/30], Batch [2300/6000], Loss: 0.1310
Epoch [12/30], Batch [2400/6000], Loss: 0.2847
Epoch [12/30], Batch [2500/6000], Loss: 0.1375
Epoch [12/30], Batch [2600/6000], Loss: 1.4818
Epoch [12/30], Batch [2700/6000], Loss: 0.1314
Epoch [12/30], Batch [2800/6000], Loss: 0.5645
Epoch [12/30], Batch [2900/6000], Loss: 0.1530
Epoch [12/30], Batch [3000/6000], Loss: 0.7255
Epoch [12/30], Batch [3100/6000], Loss: 0.2902
Epoch [12/30], Batch [3200/6000], Loss: 0.2524
Epoch [12/30], Batch [3300/6000], Loss: 0.4670
Epoch [12/30], Batch [3400/6000], Loss: 0.2344
Epoch [12/30], Batch [3500/6000], Loss: 0.1891
Epoch [12/30], Batch [3600/6000], Loss: 1.1516
Epoch [12/30], Batch [3700/6000], Loss: 0.1662
Epoch [12/30], Batch [3800/6000], Loss: 0.3873
Epoch [12/30], Batch [3900/6000], Loss: 0.1596
Epoch [12/30], Batch [4000/6000], Loss: 0.1579
Epoch [12/30], Batch [4100/6000], Loss: 0.7762
Epoch [12/30], Batch [4200/6000], Loss: 0.2988
Epoch [12/30], Batch [4300/6000], Loss: 0.1493
Epoch [12/30], Batch [4400/6000], Loss: 0.2198
Epoch [12/30], Batch [4500/6000], Loss: 0.1657
Epoch [12/30], Batch [4600/6000], Loss: 0.3460
Epoch [12/30], Batch [4700/6000], Loss: 0.1440
Epoch [12/30], Batch [4800/6000], Loss: 0.1666
Epoch [12/30], Batch [4900/6000], Loss: 1.4248
Epoch [12/30], Batch [5000/6000], Loss: 0.1308
Epoch [12/30], Batch [5100/6000], Loss: 0.6155
Epoch [12/30], Batch [5200/6000], Loss: 0.3665
Epoch [12/30], Batch [5300/6000], Loss: 0.3949
Epoch [12/30], Batch [5400/6000], Loss: 0.7001
Epoch [12/30], Batch [5500/6000], Loss: 0.4559
Epoch [12/30], Batch [5600/6000], Loss: 1.3875
Epoch [12/30], Batch [5700/6000], Loss: 0.1983
Epoch [12/30], Batch [5800/6000], Loss: 0.1463
Epoch [12/30], Batch [5900/6000], Loss: 0.2907
Epoch [12/30], Loss: 0.3429
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.1364
Epoch [13/30], Batch [100/6000], Loss: 0.1495
Epoch [13/30], Batch [200/6000], Loss: 0.9797
Epoch [13/30], Batch [300/6000], Loss: 0.1738
Epoch [13/30], Batch [400/6000], Loss: 0.1498
Epoch [13/30], Batch [500/6000], Loss: 0.1635
Epoch [13/30], Batch [600/6000], Loss: 0.1378
Epoch [13/30], Batch [700/6000], Loss: 0.1986
Epoch [13/30], Batch [800/6000], Loss: 0.1616
Epoch [13/30], Batch [900/6000], Loss: 0.1388
Epoch [13/30], Batch [1000/6000], Loss: 0.1045
Epoch [13/30], Batch [1100/6000], Loss: 0.2073
Epoch [13/30], Batch [1200/6000], Loss: 0.1331
Epoch [13/30], Batch [1300/6000], Loss: 0.1449
Epoch [13/30], Batch [1400/6000], Loss: 0.1551
Epoch [13/30], Batch [1500/6000], Loss: 0.1275
Epoch [13/30], Batch [1600/6000], Loss: 0.1954
Epoch [13/30], Batch [1700/6000], Loss: 0.1443
Epoch [13/30], Batch [1800/6000], Loss: 0.1708
Epoch [13/30], Batch [1900/6000], Loss: 0.1464
Epoch [13/30], Batch [2000/6000], Loss: 1.9662
Epoch [13/30], Batch [2100/6000], Loss: 0.1951
Epoch [13/30], Batch [2200/6000], Loss: 0.1406
Epoch [13/30], Batch [2300/6000], Loss: 0.1820
Epoch [13/30], Batch [2400/6000], Loss: 0.1303
Epoch [13/30], Batch [2500/6000], Loss: 0.1495
Epoch [13/30], Batch [2600/6000], Loss: 0.1338
Epoch [13/30], Batch [2700/6000], Loss: 0.1850
Epoch [13/30], Batch [2800/6000], Loss: 0.2041
Epoch [13/30], Batch [2900/6000], Loss: 0.1648
Epoch [13/30], Batch [3000/6000], Loss: 0.1495
Epoch [13/30], Batch [3100/6000], Loss: 0.1504
Epoch [13/30], Batch [3200/6000], Loss: 0.1276
Epoch [13/30], Batch [3300/6000], Loss: 0.2064
Epoch [13/30], Batch [3400/6000], Loss: 0.1446
Epoch [13/30], Batch [3500/6000], Loss: 0.1345
Epoch [13/30], Batch [3600/6000], Loss: 0.2130
Epoch [13/30], Batch [3700/6000], Loss: 0.1825
Epoch [13/30], Batch [3800/6000], Loss: 0.8043
Epoch [13/30], Batch [3900/6000], Loss: 0.3864
Epoch [13/30], Batch [4000/6000], Loss: 0.1155
Epoch [13/30], Batch [4100/6000], Loss: 0.3836
Epoch [13/30], Batch [4200/6000], Loss: 0.1485
Epoch [13/30], Batch [4300/6000], Loss: 0.2104
Epoch [13/30], Batch [4400/6000], Loss: 0.1404
Epoch [13/30], Batch [4500/6000], Loss: 0.1728
Epoch [13/30], Batch [4600/6000], Loss: 0.1441
Epoch [13/30], Batch [4700/6000], Loss: 0.1930
Epoch [13/30], Batch [4800/6000], Loss: 1.1759
Epoch [13/30], Batch [4900/6000], Loss: 0.8051
Epoch [13/30], Batch [5000/6000], Loss: 0.8917
Epoch [13/30], Batch [5100/6000], Loss: 0.2926
Epoch [13/30], Batch [5200/6000], Loss: 0.1439
Epoch [13/30], Batch [5300/6000], Loss: 0.1935
Epoch [13/30], Batch [5400/6000], Loss: 1.5234
Epoch [13/30], Batch [5500/6000], Loss: 0.1614
Epoch [13/30], Batch [5600/6000], Loss: 0.1567
Epoch [13/30], Batch [5700/6000], Loss: 0.1517
Epoch [13/30], Batch [5800/6000], Loss: 0.2397
Epoch [13/30], Batch [5900/6000], Loss: 0.1415
Epoch [13/30], Loss: 0.3242
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.6545
Epoch [14/30], Batch [100/6000], Loss: 0.1385
Epoch [14/30], Batch [200/6000], Loss: 0.1789
Epoch [14/30], Batch [300/6000], Loss: 0.4407
Epoch [14/30], Batch [400/6000], Loss: 0.1375
Epoch [14/30], Batch [500/6000], Loss: 0.1185
Epoch [14/30], Batch [600/6000], Loss: 0.2075
Epoch [14/30], Batch [700/6000], Loss: 0.1535
Epoch [14/30], Batch [800/6000], Loss: 0.1640
Epoch [14/30], Batch [900/6000], Loss: 0.5717
Epoch [14/30], Batch [1000/6000], Loss: 0.1631
Epoch [14/30], Batch [1100/6000], Loss: 0.2837
Epoch [14/30], Batch [1200/6000], Loss: 0.1517
Epoch [14/30], Batch [1300/6000], Loss: 0.1283
Epoch [14/30], Batch [1400/6000], Loss: 0.1357
Epoch [14/30], Batch [1500/6000], Loss: 0.2439
Epoch [14/30], Batch [1600/6000], Loss: 0.1340
Epoch [14/30], Batch [1700/6000], Loss: 0.1476
Epoch [14/30], Batch [1800/6000], Loss: 0.1236
Epoch [14/30], Batch [1900/6000], Loss: 0.2367
Epoch [14/30], Batch [2000/6000], Loss: 0.1582
Epoch [14/30], Batch [2100/6000], Loss: 0.1263
Epoch [14/30], Batch [2200/6000], Loss: 0.1560
Epoch [14/30], Batch [2300/6000], Loss: 0.1904
Epoch [14/30], Batch [2400/6000], Loss: 2.5987
Epoch [14/30], Batch [2500/6000], Loss: 1.3401
Epoch [14/30], Batch [2600/6000], Loss: 0.1733
Epoch [14/30], Batch [2700/6000], Loss: 0.2154
Epoch [14/30], Batch [2800/6000], Loss: 0.1890
Epoch [14/30], Batch [2900/6000], Loss: 0.2487
Epoch [14/30], Batch [3000/6000], Loss: 0.1564
Epoch [14/30], Batch [3100/6000], Loss: 0.1429
Epoch [14/30], Batch [3200/6000], Loss: 0.1165
Epoch [14/30], Batch [3300/6000], Loss: 0.1285
Epoch [14/30], Batch [3400/6000], Loss: 0.7216
Epoch [14/30], Batch [3500/6000], Loss: 0.1852
Epoch [14/30], Batch [3600/6000], Loss: 0.1831
Epoch [14/30], Batch [3700/6000], Loss: 0.1269
Epoch [14/30], Batch [3800/6000], Loss: 0.8411
Epoch [14/30], Batch [3900/6000], Loss: 0.2566
Epoch [14/30], Batch [4000/6000], Loss: 0.2091
Epoch [14/30], Batch [4100/6000], Loss: 0.1360
Epoch [14/30], Batch [4200/6000], Loss: 0.1280
Epoch [14/30], Batch [4300/6000], Loss: 0.1375
Epoch [14/30], Batch [4400/6000], Loss: 0.1157
Epoch [14/30], Batch [4500/6000], Loss: 0.1333
Epoch [14/30], Batch [4600/6000], Loss: 0.1403
Epoch [14/30], Batch [4700/6000], Loss: 0.1179
Epoch [14/30], Batch [4800/6000], Loss: 0.1655
Epoch [14/30], Batch [4900/6000], Loss: 0.1453
Epoch [14/30], Batch [5000/6000], Loss: 0.1833
Epoch [14/30], Batch [5100/6000], Loss: 0.1442
Epoch [14/30], Batch [5200/6000], Loss: 0.2140
Epoch [14/30], Batch [5300/6000], Loss: 0.1362
Epoch [14/30], Batch [5400/6000], Loss: 0.1340
Epoch [14/30], Batch [5500/6000], Loss: 0.1056
Epoch [14/30], Batch [5600/6000], Loss: 0.1567
Epoch [14/30], Batch [5700/6000], Loss: 0.1412
Epoch [14/30], Batch [5800/6000], Loss: 0.1316
Epoch [14/30], Batch [5900/6000], Loss: 0.1528
Epoch [14/30], Loss: 0.3023
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.1518
Epoch [15/30], Batch [100/6000], Loss: 0.1497
Epoch [15/30], Batch [200/6000], Loss: 0.1454
Epoch [15/30], Batch [300/6000], Loss: 0.1618
Epoch [15/30], Batch [400/6000], Loss: 0.1733
Epoch [15/30], Batch [500/6000], Loss: 0.1280
Epoch [15/30], Batch [600/6000], Loss: 0.2066
Epoch [15/30], Batch [700/6000], Loss: 0.1549
Epoch [15/30], Batch [800/6000], Loss: 0.1477
Epoch [15/30], Batch [900/6000], Loss: 0.1393
Epoch [15/30], Batch [1000/6000], Loss: 0.1995
Epoch [15/30], Batch [1100/6000], Loss: 0.1376
Epoch [15/30], Batch [1200/6000], Loss: 0.3785
Epoch [15/30], Batch [1300/6000], Loss: 1.0295
Epoch [15/30], Batch [1400/6000], Loss: 0.1540
Epoch [15/30], Batch [1500/6000], Loss: 0.7528
Epoch [15/30], Batch [1600/6000], Loss: 0.2109
Epoch [15/30], Batch [1700/6000], Loss: 0.1751
Epoch [15/30], Batch [1800/6000], Loss: 0.1690
Epoch [15/30], Batch [1900/6000], Loss: 0.1513
Epoch [15/30], Batch [2000/6000], Loss: 0.1664
Epoch [15/30], Batch [2100/6000], Loss: 1.3458
Epoch [15/30], Batch [2200/6000], Loss: 0.1462
Epoch [15/30], Batch [2300/6000], Loss: 0.7514
Epoch [15/30], Batch [2400/6000], Loss: 0.1414
Epoch [15/30], Batch [2500/6000], Loss: 0.1145
Epoch [15/30], Batch [2600/6000], Loss: 0.1480
Epoch [15/30], Batch [2700/6000], Loss: 0.1256
Epoch [15/30], Batch [2800/6000], Loss: 0.3128
Epoch [15/30], Batch [2900/6000], Loss: 0.0967
Epoch [15/30], Batch [3000/6000], Loss: 0.1417
Epoch [15/30], Batch [3100/6000], Loss: 0.1273
Epoch [15/30], Batch [3200/6000], Loss: 0.1593
Epoch [15/30], Batch [3300/6000], Loss: 0.1639
Epoch [15/30], Batch [3400/6000], Loss: 0.1554
Epoch [15/30], Batch [3500/6000], Loss: 1.0621
Epoch [15/30], Batch [3600/6000], Loss: 0.1745
Epoch [15/30], Batch [3700/6000], Loss: 0.7795
Epoch [15/30], Batch [3800/6000], Loss: 0.1514
Epoch [15/30], Batch [3900/6000], Loss: 2.2876
Epoch [15/30], Batch [4000/6000], Loss: 0.1712
Epoch [15/30], Batch [4100/6000], Loss: 0.1325
Epoch [15/30], Batch [4200/6000], Loss: 0.1334
Epoch [15/30], Batch [4300/6000], Loss: 0.1521
Epoch [15/30], Batch [4400/6000], Loss: 0.2168
Epoch [15/30], Batch [4500/6000], Loss: 0.3580
Epoch [15/30], Batch [4600/6000], Loss: 0.1539
Epoch [15/30], Batch [4700/6000], Loss: 0.1437
Epoch [15/30], Batch [4800/6000], Loss: 0.1173
Epoch [15/30], Batch [4900/6000], Loss: 0.4090
Epoch [15/30], Batch [5000/6000], Loss: 0.1473
Epoch [15/30], Batch [5100/6000], Loss: 0.1326
Epoch [15/30], Batch [5200/6000], Loss: 0.2104
Epoch [15/30], Batch [5300/6000], Loss: 0.4075
Epoch [15/30], Batch [5400/6000], Loss: 0.1054
Epoch [15/30], Batch [5500/6000], Loss: 0.1493
Epoch [15/30], Batch [5600/6000], Loss: 0.6721
Epoch [15/30], Batch [5700/6000], Loss: 1.5000
Epoch [15/30], Batch [5800/6000], Loss: 0.3527
Epoch [15/30], Batch [5900/6000], Loss: 0.1662
Epoch [15/30], Loss: 0.2918
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.1332
Epoch [16/30], Batch [100/6000], Loss: 0.1905
Epoch [16/30], Batch [200/6000], Loss: 0.1854
Epoch [16/30], Batch [300/6000], Loss: 0.1241
Epoch [16/30], Batch [400/6000], Loss: 1.4473
Epoch [16/30], Batch [500/6000], Loss: 0.1266
Epoch [16/30], Batch [600/6000], Loss: 0.2495
Epoch [16/30], Batch [700/6000], Loss: 0.1339
Epoch [16/30], Batch [800/6000], Loss: 0.2368
Epoch [16/30], Batch [900/6000], Loss: 0.1906
Epoch [16/30], Batch [1000/6000], Loss: 0.4988
Epoch [16/30], Batch [1100/6000], Loss: 0.1744
Epoch [16/30], Batch [1200/6000], Loss: 0.1252
Epoch [16/30], Batch [1300/6000], Loss: 0.3781
Epoch [16/30], Batch [1400/6000], Loss: 0.1743
Epoch [16/30], Batch [1500/6000], Loss: 0.1685
Epoch [16/30], Batch [1600/6000], Loss: 0.2054
Epoch [16/30], Batch [1700/6000], Loss: 0.7375
Epoch [16/30], Batch [1800/6000], Loss: 0.8662
Epoch [16/30], Batch [1900/6000], Loss: 0.6089
Epoch [16/30], Batch [2000/6000], Loss: 0.1292
Epoch [16/30], Batch [2100/6000], Loss: 0.1875
Epoch [16/30], Batch [2200/6000], Loss: 0.1645
Epoch [16/30], Batch [2300/6000], Loss: 0.6456
Epoch [16/30], Batch [2400/6000], Loss: 0.4034
Epoch [16/30], Batch [2500/6000], Loss: 0.1244
Epoch [16/30], Batch [2600/6000], Loss: 0.1497
Epoch [16/30], Batch [2700/6000], Loss: 0.1400
Epoch [16/30], Batch [2800/6000], Loss: 0.3028
Epoch [16/30], Batch [2900/6000], Loss: 0.1534
Epoch [16/30], Batch [3000/6000], Loss: 0.1694
Epoch [16/30], Batch [3100/6000], Loss: 0.1472
Epoch [16/30], Batch [3200/6000], Loss: 0.1356
Epoch [16/30], Batch [3300/6000], Loss: 0.1387
Epoch [16/30], Batch [3400/6000], Loss: 0.3805
Epoch [16/30], Batch [3500/6000], Loss: 0.1516
Epoch [16/30], Batch [3600/6000], Loss: 0.2302
Epoch [16/30], Batch [3700/6000], Loss: 0.1326
Epoch [16/30], Batch [3800/6000], Loss: 0.1641
Epoch [16/30], Batch [3900/6000], Loss: 0.1331
Epoch [16/30], Batch [4000/6000], Loss: 0.4233
Epoch [16/30], Batch [4100/6000], Loss: 0.1368
Epoch [16/30], Batch [4200/6000], Loss: 0.1226
Epoch [16/30], Batch [4300/6000], Loss: 0.7359
Epoch [16/30], Batch [4400/6000], Loss: 2.0639
Epoch [16/30], Batch [4500/6000], Loss: 0.1706
Epoch [16/30], Batch [4600/6000], Loss: 0.1522
Epoch [16/30], Batch [4700/6000], Loss: 0.1691
Epoch [16/30], Batch [4800/6000], Loss: 0.1170
Epoch [16/30], Batch [4900/6000], Loss: 0.1244
Epoch [16/30], Batch [5000/6000], Loss: 0.1571
Epoch [16/30], Batch [5100/6000], Loss: 0.1188
Epoch [16/30], Batch [5200/6000], Loss: 0.3422
Epoch [16/30], Batch [5300/6000], Loss: 0.2011
Epoch [16/30], Batch [5400/6000], Loss: 0.5494
Epoch [16/30], Batch [5500/6000], Loss: 0.2494
Epoch [16/30], Batch [5600/6000], Loss: 0.1672
Epoch [16/30], Batch [5700/6000], Loss: 0.3048
Epoch [16/30], Batch [5800/6000], Loss: 0.1189
Epoch [16/30], Batch [5900/6000], Loss: 0.1393
Epoch [16/30], Loss: 0.2718
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.1459
Epoch [17/30], Batch [100/6000], Loss: 0.1397
Epoch [17/30], Batch [200/6000], Loss: 0.1297
Epoch [17/30], Batch [300/6000], Loss: 0.1232
Epoch [17/30], Batch [400/6000], Loss: 0.7818
Epoch [17/30], Batch [500/6000], Loss: 0.1257
Epoch [17/30], Batch [600/6000], Loss: 0.1429
Epoch [17/30], Batch [700/6000], Loss: 0.1364
Epoch [17/30], Batch [800/6000], Loss: 0.1305
Epoch [17/30], Batch [900/6000], Loss: 1.3143
Epoch [17/30], Batch [1000/6000], Loss: 0.2095
Epoch [17/30], Batch [1100/6000], Loss: 0.2161
Epoch [17/30], Batch [1200/6000], Loss: 0.1402
Epoch [17/30], Batch [1300/6000], Loss: 2.1294
Epoch [17/30], Batch [1400/6000], Loss: 0.3691
Epoch [17/30], Batch [1500/6000], Loss: 0.2380
Epoch [17/30], Batch [1600/6000], Loss: 0.1471
Epoch [17/30], Batch [1700/6000], Loss: 0.1267
Epoch [17/30], Batch [1800/6000], Loss: 0.1260
Epoch [17/30], Batch [1900/6000], Loss: 0.1250
Epoch [17/30], Batch [2000/6000], Loss: 0.1545
Epoch [17/30], Batch [2100/6000], Loss: 0.6193
Epoch [17/30], Batch [2200/6000], Loss: 1.1165
Epoch [17/30], Batch [2300/6000], Loss: 0.1249
Epoch [17/30], Batch [2400/6000], Loss: 0.1118
Epoch [17/30], Batch [2500/6000], Loss: 0.1760
Epoch [17/30], Batch [2600/6000], Loss: 0.5259
Epoch [17/30], Batch [2700/6000], Loss: 0.1333
Epoch [17/30], Batch [2800/6000], Loss: 0.1422
Epoch [17/30], Batch [2900/6000], Loss: 0.1330
Epoch [17/30], Batch [3000/6000], Loss: 0.2647
Epoch [17/30], Batch [3100/6000], Loss: 0.1142
Epoch [17/30], Batch [3200/6000], Loss: 0.1501
Epoch [17/30], Batch [3300/6000], Loss: 0.1346
Epoch [17/30], Batch [3400/6000], Loss: 0.1038
Epoch [17/30], Batch [3500/6000], Loss: 0.1280
Epoch [17/30], Batch [3600/6000], Loss: 0.3028
Epoch [17/30], Batch [3700/6000], Loss: 1.3199
Epoch [17/30], Batch [3800/6000], Loss: 0.2468
Epoch [17/30], Batch [3900/6000], Loss: 0.1289
Epoch [17/30], Batch [4000/6000], Loss: 0.1469
Epoch [17/30], Batch [4100/6000], Loss: 0.1229
Epoch [17/30], Batch [4200/6000], Loss: 0.2622
Epoch [17/30], Batch [4300/6000], Loss: 0.1208
Epoch [17/30], Batch [4400/6000], Loss: 0.1805
Epoch [17/30], Batch [4500/6000], Loss: 0.1739
Epoch [17/30], Batch [4600/6000], Loss: 0.2897
Epoch [17/30], Batch [4700/6000], Loss: 0.1204
Epoch [17/30], Batch [4800/6000], Loss: 0.7181
Epoch [17/30], Batch [4900/6000], Loss: 0.3262
Epoch [17/30], Batch [5000/6000], Loss: 0.3588
Epoch [17/30], Batch [5100/6000], Loss: 0.1330
Epoch [17/30], Batch [5200/6000], Loss: 0.1295
Epoch [17/30], Batch [5300/6000], Loss: 0.1345
Epoch [17/30], Batch [5400/6000], Loss: 0.1742
Epoch [17/30], Batch [5500/6000], Loss: 0.1134
Epoch [17/30], Batch [5600/6000], Loss: 0.1297
Epoch [17/30], Batch [5700/6000], Loss: 0.1540
Epoch [17/30], Batch [5800/6000], Loss: 0.1285
Epoch [17/30], Batch [5900/6000], Loss: 0.1468
Epoch [17/30], Loss: 0.2602
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.1352
Epoch [18/30], Batch [100/6000], Loss: 0.0998
Epoch [18/30], Batch [200/6000], Loss: 0.1889
Epoch [18/30], Batch [300/6000], Loss: 0.3061
Epoch [18/30], Batch [400/6000], Loss: 0.1216
Epoch [18/30], Batch [500/6000], Loss: 0.1739
Epoch [18/30], Batch [600/6000], Loss: 0.1572
Epoch [18/30], Batch [700/6000], Loss: 0.1220
Epoch [18/30], Batch [800/6000], Loss: 0.5935
Epoch [18/30], Batch [900/6000], Loss: 0.1619
Epoch [18/30], Batch [1000/6000], Loss: 0.1177
Epoch [18/30], Batch [1100/6000], Loss: 1.0220
Epoch [18/30], Batch [1200/6000], Loss: 1.0145
Epoch [18/30], Batch [1300/6000], Loss: 0.1691
Epoch [18/30], Batch [1400/6000], Loss: 0.1101
Epoch [18/30], Batch [1500/6000], Loss: 0.1747
Epoch [18/30], Batch [1600/6000], Loss: 0.1048
Epoch [18/30], Batch [1700/6000], Loss: 0.1989
Epoch [18/30], Batch [1800/6000], Loss: 0.1234
Epoch [18/30], Batch [1900/6000], Loss: 0.3864
Epoch [18/30], Batch [2000/6000], Loss: 0.2188
Epoch [18/30], Batch [2100/6000], Loss: 0.1627
Epoch [18/30], Batch [2200/6000], Loss: 0.1216
Epoch [18/30], Batch [2300/6000], Loss: 0.1469
Epoch [18/30], Batch [2400/6000], Loss: 0.1197
Epoch [18/30], Batch [2500/6000], Loss: 0.7951
Epoch [18/30], Batch [2600/6000], Loss: 0.1288
Epoch [18/30], Batch [2700/6000], Loss: 0.1176
Epoch [18/30], Batch [2800/6000], Loss: 0.1166
Epoch [18/30], Batch [2900/6000], Loss: 0.1377
Epoch [18/30], Batch [3000/6000], Loss: 0.1476
Epoch [18/30], Batch [3100/6000], Loss: 0.1496
Epoch [18/30], Batch [3200/6000], Loss: 1.1760
Epoch [18/30], Batch [3300/6000], Loss: 0.1683
Epoch [18/30], Batch [3400/6000], Loss: 0.1515
Epoch [18/30], Batch [3500/6000], Loss: 1.5215
Epoch [18/30], Batch [3600/6000], Loss: 0.9211
Epoch [18/30], Batch [3700/6000], Loss: 0.1281
Epoch [18/30], Batch [3800/6000], Loss: 0.1343
Epoch [18/30], Batch [3900/6000], Loss: 0.1079
Epoch [18/30], Batch [4000/6000], Loss: 0.1153
Epoch [18/30], Batch [4100/6000], Loss: 0.2909
Epoch [18/30], Batch [4200/6000], Loss: 0.3446
Epoch [18/30], Batch [4300/6000], Loss: 0.1082
Epoch [18/30], Batch [4400/6000], Loss: 0.1659
Epoch [18/30], Batch [4500/6000], Loss: 0.2692
Epoch [18/30], Batch [4600/6000], Loss: 0.1282
Epoch [18/30], Batch [4700/6000], Loss: 0.1182
Epoch [18/30], Batch [4800/6000], Loss: 0.1468
Epoch [18/30], Batch [4900/6000], Loss: 0.1114
Epoch [18/30], Batch [5000/6000], Loss: 0.1495
Epoch [18/30], Batch [5100/6000], Loss: 0.1710
Epoch [18/30], Batch [5200/6000], Loss: 0.1200
Epoch [18/30], Batch [5300/6000], Loss: 0.2920
Epoch [18/30], Batch [5400/6000], Loss: 0.1013
Epoch [18/30], Batch [5500/6000], Loss: 0.1390
Epoch [18/30], Batch [5600/6000], Loss: 0.1320
Epoch [18/30], Batch [5700/6000], Loss: 0.2204
Epoch [18/30], Batch [5800/6000], Loss: 0.1527
Epoch [18/30], Batch [5900/6000], Loss: 0.1203
Epoch [18/30], Loss: 0.2419
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.1301
Epoch [19/30], Batch [100/6000], Loss: 0.1326
Epoch [19/30], Batch [200/6000], Loss: 0.5494
Epoch [19/30], Batch [300/6000], Loss: 0.1622
Epoch [19/30], Batch [400/6000], Loss: 0.2115
Epoch [19/30], Batch [500/6000], Loss: 0.1125
Epoch [19/30], Batch [600/6000], Loss: 0.1460
Epoch [19/30], Batch [700/6000], Loss: 0.1407
Epoch [19/30], Batch [800/6000], Loss: 0.1425
Epoch [19/30], Batch [900/6000], Loss: 0.1881
Epoch [19/30], Batch [1000/6000], Loss: 0.1263
Epoch [19/30], Batch [1100/6000], Loss: 0.2285
Epoch [19/30], Batch [1200/6000], Loss: 0.2477
Epoch [19/30], Batch [1300/6000], Loss: 0.2159
Epoch [19/30], Batch [1400/6000], Loss: 0.1170
Epoch [19/30], Batch [1500/6000], Loss: 0.1334
Epoch [19/30], Batch [1600/6000], Loss: 0.1475
Epoch [19/30], Batch [1700/6000], Loss: 0.4706
Epoch [19/30], Batch [1800/6000], Loss: 0.1285
Epoch [19/30], Batch [1900/6000], Loss: 0.1392
Epoch [19/30], Batch [2000/6000], Loss: 0.7549
Epoch [19/30], Batch [2100/6000], Loss: 0.1020
Epoch [19/30], Batch [2200/6000], Loss: 0.1591
Epoch [19/30], Batch [2300/6000], Loss: 0.1576
Epoch [19/30], Batch [2400/6000], Loss: 0.1265
Epoch [19/30], Batch [2500/6000], Loss: 0.2265
Epoch [19/30], Batch [2600/6000], Loss: 0.1251
Epoch [19/30], Batch [2700/6000], Loss: 0.1135
Epoch [19/30], Batch [2800/6000], Loss: 0.1606
Epoch [19/30], Batch [2900/6000], Loss: 0.1500
Epoch [19/30], Batch [3000/6000], Loss: 0.1067
Epoch [19/30], Batch [3100/6000], Loss: 0.1246
Epoch [19/30], Batch [3200/6000], Loss: 0.1139
Epoch [19/30], Batch [3300/6000], Loss: 0.5790
Epoch [19/30], Batch [3400/6000], Loss: 0.2475
Epoch [19/30], Batch [3500/6000], Loss: 0.1198
Epoch [19/30], Batch [3600/6000], Loss: 0.1177
Epoch [19/30], Batch [3700/6000], Loss: 0.1713
Epoch [19/30], Batch [3800/6000], Loss: 0.1701
Epoch [19/30], Batch [3900/6000], Loss: 0.1127
Epoch [19/30], Batch [4000/6000], Loss: 0.1207
Epoch [19/30], Batch [4100/6000], Loss: 0.1079
Epoch [19/30], Batch [4200/6000], Loss: 0.1245
Epoch [19/30], Batch [4300/6000], Loss: 0.1690
Epoch [19/30], Batch [4400/6000], Loss: 0.2016
Epoch [19/30], Batch [4500/6000], Loss: 0.1309
Epoch [19/30], Batch [4600/6000], Loss: 0.1252
Epoch [19/30], Batch [4700/6000], Loss: 0.1524
Epoch [19/30], Batch [4800/6000], Loss: 0.0870
Epoch [19/30], Batch [4900/6000], Loss: 0.1353
Epoch [19/30], Batch [5000/6000], Loss: 0.1484
Epoch [19/30], Batch [5100/6000], Loss: 0.1383
Epoch [19/30], Batch [5200/6000], Loss: 0.3262
Epoch [19/30], Batch [5300/6000], Loss: 0.1210
Epoch [19/30], Batch [5400/6000], Loss: 0.4275
Epoch [19/30], Batch [5500/6000], Loss: 0.3295
Epoch [19/30], Batch [5600/6000], Loss: 1.8035
Epoch [19/30], Batch [5700/6000], Loss: 0.1221
Epoch [19/30], Batch [5800/6000], Loss: 0.1326
Epoch [19/30], Batch [5900/6000], Loss: 0.1223
Epoch [19/30], Loss: 0.2371
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.1502
Epoch [20/30], Batch [100/6000], Loss: 0.1504
Epoch [20/30], Batch [200/6000], Loss: 0.1067
Epoch [20/30], Batch [300/6000], Loss: 0.1306
Epoch [20/30], Batch [400/6000], Loss: 0.1563
Epoch [20/30], Batch [500/6000], Loss: 0.1197
Epoch [20/30], Batch [600/6000], Loss: 0.1179
Epoch [20/30], Batch [700/6000], Loss: 0.4439
Epoch [20/30], Batch [800/6000], Loss: 0.1971
Epoch [20/30], Batch [900/6000], Loss: 0.1655
Epoch [20/30], Batch [1000/6000], Loss: 0.1178
Epoch [20/30], Batch [1100/6000], Loss: 0.1409
Epoch [20/30], Batch [1200/6000], Loss: 0.6647
Epoch [20/30], Batch [1300/6000], Loss: 0.1335
Epoch [20/30], Batch [1400/6000], Loss: 0.1768
Epoch [20/30], Batch [1500/6000], Loss: 0.4327
Epoch [20/30], Batch [1600/6000], Loss: 0.3012
Epoch [20/30], Batch [1700/6000], Loss: 0.1416
Epoch [20/30], Batch [1800/6000], Loss: 0.1442
Epoch [20/30], Batch [1900/6000], Loss: 0.1305
Epoch [20/30], Batch [2000/6000], Loss: 0.1631
Epoch [20/30], Batch [2100/6000], Loss: 0.1296
Epoch [20/30], Batch [2200/6000], Loss: 0.1164
Epoch [20/30], Batch [2300/6000], Loss: 0.1284
Epoch [20/30], Batch [2400/6000], Loss: 0.0965
Epoch [20/30], Batch [2500/6000], Loss: 0.3923
Epoch [20/30], Batch [2600/6000], Loss: 0.1200
Epoch [20/30], Batch [2700/6000], Loss: 0.1058
Epoch [20/30], Batch [2800/6000], Loss: 0.1984
Epoch [20/30], Batch [2900/6000], Loss: 0.2264
Epoch [20/30], Batch [3000/6000], Loss: 0.1578
Epoch [20/30], Batch [3100/6000], Loss: 0.1263
Epoch [20/30], Batch [3200/6000], Loss: 0.1360
Epoch [20/30], Batch [3300/6000], Loss: 0.1335
Epoch [20/30], Batch [3400/6000], Loss: 0.1719
Epoch [20/30], Batch [3500/6000], Loss: 0.1328
Epoch [20/30], Batch [3600/6000], Loss: 1.4175
Epoch [20/30], Batch [3700/6000], Loss: 0.3108
Epoch [20/30], Batch [3800/6000], Loss: 0.1151
Epoch [20/30], Batch [3900/6000], Loss: 0.1146
Epoch [20/30], Batch [4000/6000], Loss: 0.1287
Epoch [20/30], Batch [4100/6000], Loss: 0.1096
Epoch [20/30], Batch [4200/6000], Loss: 0.1373
Epoch [20/30], Batch [4300/6000], Loss: 0.1251
Epoch [20/30], Batch [4400/6000], Loss: 0.7406
Epoch [20/30], Batch [4500/6000], Loss: 0.1496
Epoch [20/30], Batch [4600/6000], Loss: 0.1268
Epoch [20/30], Batch [4700/6000], Loss: 0.1495
Epoch [20/30], Batch [4800/6000], Loss: 0.0955
Epoch [20/30], Batch [4900/6000], Loss: 0.1740
Epoch [20/30], Batch [5000/6000], Loss: 0.1229
Epoch [20/30], Batch [5100/6000], Loss: 0.1296
Epoch [20/30], Batch [5200/6000], Loss: 0.1623
Epoch [20/30], Batch [5300/6000], Loss: 0.5394
Epoch [20/30], Batch [5400/6000], Loss: 0.1452
Epoch [20/30], Batch [5500/6000], Loss: 0.1155
Epoch [20/30], Batch [5600/6000], Loss: 0.1280
Epoch [20/30], Batch [5700/6000], Loss: 0.8114
Epoch [20/30], Batch [5800/6000], Loss: 0.2378
Epoch [20/30], Batch [5900/6000], Loss: 0.1865
Epoch [20/30], Loss: 0.2271
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.1305
Epoch [21/30], Batch [100/6000], Loss: 0.1216
Epoch [21/30], Batch [200/6000], Loss: 0.2103
Epoch [21/30], Batch [300/6000], Loss: 0.1404
Epoch [21/30], Batch [400/6000], Loss: 0.1428
Epoch [21/30], Batch [500/6000], Loss: 0.1373
Epoch [21/30], Batch [600/6000], Loss: 0.1090
Epoch [21/30], Batch [700/6000], Loss: 0.1231
Epoch [21/30], Batch [800/6000], Loss: 0.1460
Epoch [21/30], Batch [900/6000], Loss: 0.2524
Epoch [21/30], Batch [1000/6000], Loss: 0.3333
Epoch [21/30], Batch [1100/6000], Loss: 0.1305
Epoch [21/30], Batch [1200/6000], Loss: 0.1305
Epoch [21/30], Batch [1300/6000], Loss: 0.8632
Epoch [21/30], Batch [1400/6000], Loss: 0.1421
Epoch [21/30], Batch [1500/6000], Loss: 0.1234
Epoch [21/30], Batch [1600/6000], Loss: 0.5112
Epoch [21/30], Batch [1700/6000], Loss: 0.3472
Epoch [21/30], Batch [1800/6000], Loss: 0.1199
Epoch [21/30], Batch [1900/6000], Loss: 0.1149
Epoch [21/30], Batch [2000/6000], Loss: 0.1145
Epoch [21/30], Batch [2100/6000], Loss: 0.2574
Epoch [21/30], Batch [2200/6000], Loss: 0.1165
Epoch [21/30], Batch [2300/6000], Loss: 0.0947
Epoch [21/30], Batch [2400/6000], Loss: 0.1310
Epoch [21/30], Batch [2500/6000], Loss: 0.1210
Epoch [21/30], Batch [2600/6000], Loss: 0.1085
Epoch [21/30], Batch [2700/6000], Loss: 0.1185
Epoch [21/30], Batch [2800/6000], Loss: 0.1500
Epoch [21/30], Batch [2900/6000], Loss: 0.1461
Epoch [21/30], Batch [3000/6000], Loss: 0.4512
Epoch [21/30], Batch [3100/6000], Loss: 0.1193
Epoch [21/30], Batch [3200/6000], Loss: 0.1210
Epoch [21/30], Batch [3300/6000], Loss: 0.1367
Epoch [21/30], Batch [3400/6000], Loss: 0.1277
Epoch [21/30], Batch [3500/6000], Loss: 0.1093
Epoch [21/30], Batch [3600/6000], Loss: 0.1449
Epoch [21/30], Batch [3700/6000], Loss: 0.1102
Epoch [21/30], Batch [3800/6000], Loss: 0.1992
Epoch [21/30], Batch [3900/6000], Loss: 0.1236
Epoch [21/30], Batch [4000/6000], Loss: 0.1292
Epoch [21/30], Batch [4100/6000], Loss: 0.2692
Epoch [21/30], Batch [4200/6000], Loss: 0.1281
Epoch [21/30], Batch [4300/6000], Loss: 0.2757
Epoch [21/30], Batch [4400/6000], Loss: 0.7185
Epoch [21/30], Batch [4500/6000], Loss: 0.1087
Epoch [21/30], Batch [4600/6000], Loss: 0.2520
Epoch [21/30], Batch [4700/6000], Loss: 0.1793
Epoch [21/30], Batch [4800/6000], Loss: 0.1029
Epoch [21/30], Batch [4900/6000], Loss: 0.1539
Epoch [21/30], Batch [5000/6000], Loss: 0.1294
Epoch [21/30], Batch [5100/6000], Loss: 0.1296
Epoch [21/30], Batch [5200/6000], Loss: 0.1433
Epoch [21/30], Batch [5300/6000], Loss: 0.1191
Epoch [21/30], Batch [5400/6000], Loss: 0.1002
Epoch [21/30], Batch [5500/6000], Loss: 0.3141
Epoch [21/30], Batch [5600/6000], Loss: 0.1463
Epoch [21/30], Batch [5700/6000], Loss: 0.1546
Epoch [21/30], Batch [5800/6000], Loss: 0.1352
Epoch [21/30], Batch [5900/6000], Loss: 0.1355
Epoch [21/30], Loss: 0.2181
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.1660
Epoch [22/30], Batch [100/6000], Loss: 0.1179
Epoch [22/30], Batch [200/6000], Loss: 0.1500
Epoch [22/30], Batch [300/6000], Loss: 0.1233
Epoch [22/30], Batch [400/6000], Loss: 0.1207
Epoch [22/30], Batch [500/6000], Loss: 0.1271
Epoch [22/30], Batch [600/6000], Loss: 0.1215
Epoch [22/30], Batch [700/6000], Loss: 0.1169
Epoch [22/30], Batch [800/6000], Loss: 1.2616
Epoch [22/30], Batch [900/6000], Loss: 0.1214
Epoch [22/30], Batch [1000/6000], Loss: 2.5019
Epoch [22/30], Batch [1100/6000], Loss: 0.0932
Epoch [22/30], Batch [1200/6000], Loss: 1.7260
Epoch [22/30], Batch [1300/6000], Loss: 0.1455
Epoch [22/30], Batch [1400/6000], Loss: 0.1294
Epoch [22/30], Batch [1500/6000], Loss: 0.1293
Epoch [22/30], Batch [1600/6000], Loss: 0.1292
Epoch [22/30], Batch [1700/6000], Loss: 0.1330
Epoch [22/30], Batch [1800/6000], Loss: 0.1498
Epoch [22/30], Batch [1900/6000], Loss: 0.1229
Epoch [22/30], Batch [2000/6000], Loss: 0.1205
Epoch [22/30], Batch [2100/6000], Loss: 0.1330
Epoch [22/30], Batch [2200/6000], Loss: 0.9011
Epoch [22/30], Batch [2300/6000], Loss: 0.1307
Epoch [22/30], Batch [2400/6000], Loss: 0.1432
Epoch [22/30], Batch [2500/6000], Loss: 0.6752
Epoch [22/30], Batch [2600/6000], Loss: 1.5152
Epoch [22/30], Batch [2700/6000], Loss: 0.1155
Epoch [22/30], Batch [2800/6000], Loss: 0.8958
Epoch [22/30], Batch [2900/6000], Loss: 0.1399
Epoch [22/30], Batch [3000/6000], Loss: 0.1643
Epoch [22/30], Batch [3100/6000], Loss: 0.1782
Epoch [22/30], Batch [3200/6000], Loss: 0.1060
Epoch [22/30], Batch [3300/6000], Loss: 0.1917
Epoch [22/30], Batch [3400/6000], Loss: 0.1207
Epoch [22/30], Batch [3500/6000], Loss: 0.0842
Epoch [22/30], Batch [3600/6000], Loss: 0.1246
Epoch [22/30], Batch [3700/6000], Loss: 0.1249
Epoch [22/30], Batch [3800/6000], Loss: 0.3228
Epoch [22/30], Batch [3900/6000], Loss: 0.1248
Epoch [22/30], Batch [4000/6000], Loss: 0.1168
Epoch [22/30], Batch [4100/6000], Loss: 0.1057
Epoch [22/30], Batch [4200/6000], Loss: 0.1344
Epoch [22/30], Batch [4300/6000], Loss: 0.1210
Epoch [22/30], Batch [4400/6000], Loss: 0.1328
Epoch [22/30], Batch [4500/6000], Loss: 0.5869
Epoch [22/30], Batch [4600/6000], Loss: 0.2144
Epoch [22/30], Batch [4700/6000], Loss: 0.0889
Epoch [22/30], Batch [4800/6000], Loss: 0.0943
Epoch [22/30], Batch [4900/6000], Loss: 0.1093
Epoch [22/30], Batch [5000/6000], Loss: 0.1318
Epoch [22/30], Batch [5100/6000], Loss: 0.1518
Epoch [22/30], Batch [5200/6000], Loss: 0.1145
Epoch [22/30], Batch [5300/6000], Loss: 0.1404
Epoch [22/30], Batch [5400/6000], Loss: 0.1371
Epoch [22/30], Batch [5500/6000], Loss: 0.1121
Epoch [22/30], Batch [5600/6000], Loss: 0.1077
Epoch [22/30], Batch [5700/6000], Loss: 0.1097
Epoch [22/30], Batch [5800/6000], Loss: 0.1610
Epoch [22/30], Batch [5900/6000], Loss: 0.1168
Epoch [22/30], Loss: 0.2075
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.1137
Epoch [23/30], Batch [100/6000], Loss: 0.0938
Epoch [23/30], Batch [200/6000], Loss: 0.1329
Epoch [23/30], Batch [300/6000], Loss: 0.0965
Epoch [23/30], Batch [400/6000], Loss: 0.1206
Epoch [23/30], Batch [500/6000], Loss: 0.1306
Epoch [23/30], Batch [600/6000], Loss: 0.1136
Epoch [23/30], Batch [700/6000], Loss: 0.1244
Epoch [23/30], Batch [800/6000], Loss: 0.1724
Epoch [23/30], Batch [900/6000], Loss: 0.1533
Epoch [23/30], Batch [1000/6000], Loss: 0.1066
Epoch [23/30], Batch [1100/6000], Loss: 0.1334
Epoch [23/30], Batch [1200/6000], Loss: 0.1206
Epoch [23/30], Batch [1300/6000], Loss: 0.1250
Epoch [23/30], Batch [1400/6000], Loss: 0.2306
Epoch [23/30], Batch [1500/6000], Loss: 0.1359
Epoch [23/30], Batch [1600/6000], Loss: 0.5240
Epoch [23/30], Batch [1700/6000], Loss: 0.1158
Epoch [23/30], Batch [1800/6000], Loss: 0.1196
Epoch [23/30], Batch [1900/6000], Loss: 0.4360
Epoch [23/30], Batch [2000/6000], Loss: 0.2309
Epoch [23/30], Batch [2100/6000], Loss: 0.1918
Epoch [23/30], Batch [2200/6000], Loss: 0.1748
Epoch [23/30], Batch [2300/6000], Loss: 0.1363
Epoch [23/30], Batch [2400/6000], Loss: 0.1803
Epoch [23/30], Batch [2500/6000], Loss: 0.0937
Epoch [23/30], Batch [2600/6000], Loss: 0.1210
Epoch [23/30], Batch [2700/6000], Loss: 0.1259
Epoch [23/30], Batch [2800/6000], Loss: 0.1096
Epoch [23/30], Batch [2900/6000], Loss: 0.1162
Epoch [23/30], Batch [3000/6000], Loss: 0.1406
Epoch [23/30], Batch [3100/6000], Loss: 0.1060
Epoch [23/30], Batch [3200/6000], Loss: 0.1484
Epoch [23/30], Batch [3300/6000], Loss: 0.1093
Epoch [23/30], Batch [3400/6000], Loss: 0.1030
Epoch [23/30], Batch [3500/6000], Loss: 0.1011
Epoch [23/30], Batch [3600/6000], Loss: 0.1264
Epoch [23/30], Batch [3700/6000], Loss: 0.0973
Epoch [23/30], Batch [3800/6000], Loss: 0.1397
Epoch [23/30], Batch [3900/6000], Loss: 0.1028
Epoch [23/30], Batch [4000/6000], Loss: 0.1065
Epoch [23/30], Batch [4100/6000], Loss: 0.1145
Epoch [23/30], Batch [4200/6000], Loss: 0.1068
Epoch [23/30], Batch [4300/6000], Loss: 0.1384
Epoch [23/30], Batch [4400/6000], Loss: 0.1385
Epoch [23/30], Batch [4500/6000], Loss: 0.1101
Epoch [23/30], Batch [4600/6000], Loss: 0.1275
Epoch [23/30], Batch [4700/6000], Loss: 0.1049
Epoch [23/30], Batch [4800/6000], Loss: 0.1431
Epoch [23/30], Batch [4900/6000], Loss: 0.1300
Epoch [23/30], Batch [5000/6000], Loss: 0.1353
Epoch [23/30], Batch [5100/6000], Loss: 0.1285
Epoch [23/30], Batch [5200/6000], Loss: 0.1444
Epoch [23/30], Batch [5300/6000], Loss: 0.1201
Epoch [23/30], Batch [5400/6000], Loss: 0.0847
Epoch [23/30], Batch [5500/6000], Loss: 0.7452
Epoch [23/30], Batch [5600/6000], Loss: 0.1730
Epoch [23/30], Batch [5700/6000], Loss: 0.1021
Epoch [23/30], Batch [5800/6000], Loss: 0.1269
Epoch [23/30], Batch [5900/6000], Loss: 0.3024
Epoch [23/30], Loss: 0.2036
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 1.0975
Epoch [24/30], Batch [100/6000], Loss: 0.1045
Epoch [24/30], Batch [200/6000], Loss: 0.1240
Epoch [24/30], Batch [300/6000], Loss: 0.0847
Epoch [24/30], Batch [400/6000], Loss: 0.1074
Epoch [24/30], Batch [500/6000], Loss: 0.0829
Epoch [24/30], Batch [600/6000], Loss: 0.1002
Epoch [24/30], Batch [700/6000], Loss: 0.1267
Epoch [24/30], Batch [800/6000], Loss: 0.1354
Epoch [24/30], Batch [900/6000], Loss: 0.1655
Epoch [24/30], Batch [1000/6000], Loss: 0.1265
Epoch [24/30], Batch [1100/6000], Loss: 0.1230
Epoch [24/30], Batch [1200/6000], Loss: 0.1354
Epoch [24/30], Batch [1300/6000], Loss: 0.0877
Epoch [24/30], Batch [1400/6000], Loss: 0.1056
Epoch [24/30], Batch [1500/6000], Loss: 0.1858
Epoch [24/30], Batch [1600/6000], Loss: 0.1122
Epoch [24/30], Batch [1700/6000], Loss: 0.1122
Epoch [24/30], Batch [1800/6000], Loss: 0.9562
Epoch [24/30], Batch [1900/6000], Loss: 0.1117
Epoch [24/30], Batch [2000/6000], Loss: 0.1232
Epoch [24/30], Batch [2100/6000], Loss: 0.1039
Epoch [24/30], Batch [2200/6000], Loss: 0.1445
Epoch [24/30], Batch [2300/6000], Loss: 0.1182
Epoch [24/30], Batch [2400/6000], Loss: 0.1167
Epoch [24/30], Batch [2500/6000], Loss: 0.1736
Epoch [24/30], Batch [2600/6000], Loss: 0.1136
Epoch [24/30], Batch [2700/6000], Loss: 0.1942
Epoch [24/30], Batch [2800/6000], Loss: 0.4020
Epoch [24/30], Batch [2900/6000], Loss: 0.1242
Epoch [24/30], Batch [3000/6000], Loss: 0.0979
Epoch [24/30], Batch [3100/6000], Loss: 0.1305
Epoch [24/30], Batch [3200/6000], Loss: 0.1202
Epoch [24/30], Batch [3300/6000], Loss: 0.1104
Epoch [24/30], Batch [3400/6000], Loss: 0.0908
Epoch [24/30], Batch [3500/6000], Loss: 0.1278
Epoch [24/30], Batch [3600/6000], Loss: 0.1071
Epoch [24/30], Batch [3700/6000], Loss: 0.1199
Epoch [24/30], Batch [3800/6000], Loss: 0.1113
Epoch [24/30], Batch [3900/6000], Loss: 0.1462
Epoch [24/30], Batch [4000/6000], Loss: 0.2017
Epoch [24/30], Batch [4100/6000], Loss: 0.1320
Epoch [24/30], Batch [4200/6000], Loss: 0.1529
Epoch [24/30], Batch [4300/6000], Loss: 0.0900
Epoch [24/30], Batch [4400/6000], Loss: 0.1171
Epoch [24/30], Batch [4500/6000], Loss: 0.1168
Epoch [24/30], Batch [4600/6000], Loss: 0.1379
Epoch [24/30], Batch [4700/6000], Loss: 0.1358
Epoch [24/30], Batch [4800/6000], Loss: 0.1218
Epoch [24/30], Batch [4900/6000], Loss: 0.1408
Epoch [24/30], Batch [5000/6000], Loss: 0.1299
Epoch [24/30], Batch [5100/6000], Loss: 0.1298
Epoch [24/30], Batch [5200/6000], Loss: 0.1534
Epoch [24/30], Batch [5300/6000], Loss: 2.1409
Epoch [24/30], Batch [5400/6000], Loss: 0.1447
Epoch [24/30], Batch [5500/6000], Loss: 0.1346
Epoch [24/30], Batch [5600/6000], Loss: 0.1492
Epoch [24/30], Batch [5700/6000], Loss: 2.7263
Epoch [24/30], Batch [5800/6000], Loss: 0.5312
Epoch [24/30], Batch [5900/6000], Loss: 0.3553
Epoch [24/30], Loss: 0.1948
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.1609
Epoch [25/30], Batch [100/6000], Loss: 0.0921
Epoch [25/30], Batch [200/6000], Loss: 0.1366
Epoch [25/30], Batch [300/6000], Loss: 0.1598
Epoch [25/30], Batch [400/6000], Loss: 0.1110
Epoch [25/30], Batch [500/6000], Loss: 0.0908
Epoch [25/30], Batch [600/6000], Loss: 0.1299
Epoch [25/30], Batch [700/6000], Loss: 0.1164
Epoch [25/30], Batch [800/6000], Loss: 0.0994
Epoch [25/30], Batch [900/6000], Loss: 0.1483
Epoch [25/30], Batch [1000/6000], Loss: 0.1360
Epoch [25/30], Batch [1100/6000], Loss: 0.0957
Epoch [25/30], Batch [1200/6000], Loss: 0.1006
Epoch [25/30], Batch [1300/6000], Loss: 0.1441
Epoch [25/30], Batch [1400/6000], Loss: 0.1167
Epoch [25/30], Batch [1500/6000], Loss: 0.1211
Epoch [25/30], Batch [1600/6000], Loss: 0.1502
Epoch [25/30], Batch [1700/6000], Loss: 0.1243
Epoch [25/30], Batch [1800/6000], Loss: 0.1042
Epoch [25/30], Batch [1900/6000], Loss: 2.9362
Epoch [25/30], Batch [2000/6000], Loss: 0.1347
Epoch [25/30], Batch [2100/6000], Loss: 0.0870
Epoch [25/30], Batch [2200/6000], Loss: 0.3291
Epoch [25/30], Batch [2300/6000], Loss: 0.1796
Epoch [25/30], Batch [2400/6000], Loss: 0.1219
Epoch [25/30], Batch [2500/6000], Loss: 0.0997
Epoch [25/30], Batch [2600/6000], Loss: 0.1107
Epoch [25/30], Batch [2700/6000], Loss: 0.1385
Epoch [25/30], Batch [2800/6000], Loss: 0.9813
Epoch [25/30], Batch [2900/6000], Loss: 0.1303
Epoch [25/30], Batch [3000/6000], Loss: 0.1358
Epoch [25/30], Batch [3100/6000], Loss: 0.1320
Epoch [25/30], Batch [3200/6000], Loss: 0.1662
Epoch [25/30], Batch [3300/6000], Loss: 0.1334
Epoch [25/30], Batch [3400/6000], Loss: 0.1120
Epoch [25/30], Batch [3500/6000], Loss: 0.1320
Epoch [25/30], Batch [3600/6000], Loss: 0.0896
Epoch [25/30], Batch [3700/6000], Loss: 0.0919
Epoch [25/30], Batch [3800/6000], Loss: 0.1331
Epoch [25/30], Batch [3900/6000], Loss: 0.1244
Epoch [25/30], Batch [4000/6000], Loss: 0.1098
Epoch [25/30], Batch [4100/6000], Loss: 0.1334
Epoch [25/30], Batch [4200/6000], Loss: 0.1970
Epoch [25/30], Batch [4300/6000], Loss: 0.1098
Epoch [25/30], Batch [4400/6000], Loss: 0.0997
Epoch [25/30], Batch [4500/6000], Loss: 1.1887
Epoch [25/30], Batch [4600/6000], Loss: 0.1592
Epoch [25/30], Batch [4700/6000], Loss: 0.1130
Epoch [25/30], Batch [4800/6000], Loss: 0.0973
Epoch [25/30], Batch [4900/6000], Loss: 0.0868
Epoch [25/30], Batch [5000/6000], Loss: 0.2227
Epoch [25/30], Batch [5100/6000], Loss: 0.1237
Epoch [25/30], Batch [5200/6000], Loss: 0.1078
Epoch [25/30], Batch [5300/6000], Loss: 0.0994
Epoch [25/30], Batch [5400/6000], Loss: 0.1171
Epoch [25/30], Batch [5500/6000], Loss: 0.1505
Epoch [25/30], Batch [5600/6000], Loss: 0.1240
Epoch [25/30], Batch [5700/6000], Loss: 0.1362
Epoch [25/30], Batch [5800/6000], Loss: 0.1172
Epoch [25/30], Batch [5900/6000], Loss: 0.0939
Epoch [25/30], Loss: 0.1868
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.2190
Epoch [26/30], Batch [100/6000], Loss: 0.1012
Epoch [26/30], Batch [200/6000], Loss: 0.0900
Epoch [26/30], Batch [300/6000], Loss: 0.1334
Epoch [26/30], Batch [400/6000], Loss: 1.4605
Epoch [26/30], Batch [500/6000], Loss: 0.1480
Epoch [26/30], Batch [600/6000], Loss: 0.0993
Epoch [26/30], Batch [700/6000], Loss: 0.1837
Epoch [26/30], Batch [800/6000], Loss: 0.1744
Epoch [26/30], Batch [900/6000], Loss: 0.1293
Epoch [26/30], Batch [1000/6000], Loss: 0.1034
Epoch [26/30], Batch [1100/6000], Loss: 0.2736
Epoch [26/30], Batch [1200/6000], Loss: 0.0970
Epoch [26/30], Batch [1300/6000], Loss: 0.1096
Epoch [26/30], Batch [1400/6000], Loss: 0.1342
Epoch [26/30], Batch [1500/6000], Loss: 0.1304
Epoch [26/30], Batch [1600/6000], Loss: 0.1259
Epoch [26/30], Batch [1700/6000], Loss: 0.0999
Epoch [26/30], Batch [1800/6000], Loss: 0.1069
Epoch [26/30], Batch [1900/6000], Loss: 0.1239
Epoch [26/30], Batch [2000/6000], Loss: 0.4594
Epoch [26/30], Batch [2100/6000], Loss: 0.1065
Epoch [26/30], Batch [2200/6000], Loss: 0.1041
Epoch [26/30], Batch [2300/6000], Loss: 0.1275
Epoch [26/30], Batch [2400/6000], Loss: 0.1425
Epoch [26/30], Batch [2500/6000], Loss: 0.1569
Epoch [26/30], Batch [2600/6000], Loss: 0.0919
Epoch [26/30], Batch [2700/6000], Loss: 0.1231
Epoch [26/30], Batch [2800/6000], Loss: 0.4788
Epoch [26/30], Batch [2900/6000], Loss: 0.1371
Epoch [26/30], Batch [3000/6000], Loss: 0.2252
Epoch [26/30], Batch [3100/6000], Loss: 0.1173
Epoch [26/30], Batch [3200/6000], Loss: 0.1345
Epoch [26/30], Batch [3300/6000], Loss: 0.1695
Epoch [26/30], Batch [3400/6000], Loss: 0.1013
Epoch [26/30], Batch [3500/6000], Loss: 0.1086
Epoch [26/30], Batch [3600/6000], Loss: 0.0940
Epoch [26/30], Batch [3700/6000], Loss: 0.0906
Epoch [26/30], Batch [3800/6000], Loss: 0.0941
Epoch [26/30], Batch [3900/6000], Loss: 0.0966
Epoch [26/30], Batch [4000/6000], Loss: 0.1357
Epoch [26/30], Batch [4100/6000], Loss: 0.1405
Epoch [26/30], Batch [4200/6000], Loss: 0.1001
Epoch [26/30], Batch [4300/6000], Loss: 0.1066
Epoch [26/30], Batch [4400/6000], Loss: 0.1014
Epoch [26/30], Batch [4500/6000], Loss: 0.1457
Epoch [26/30], Batch [4600/6000], Loss: 0.1029
Epoch [26/30], Batch [4700/6000], Loss: 0.1222
Epoch [26/30], Batch [4800/6000], Loss: 0.1053
Epoch [26/30], Batch [4900/6000], Loss: 0.1311
Epoch [26/30], Batch [5000/6000], Loss: 0.1343
Epoch [26/30], Batch [5100/6000], Loss: 0.1182
Epoch [26/30], Batch [5200/6000], Loss: 0.1194
Epoch [26/30], Batch [5300/6000], Loss: 0.0952
Epoch [26/30], Batch [5400/6000], Loss: 0.1229
Epoch [26/30], Batch [5500/6000], Loss: 0.1301
Epoch [26/30], Batch [5600/6000], Loss: 0.1213
Epoch [26/30], Batch [5700/6000], Loss: 0.1335
Epoch [26/30], Batch [5800/6000], Loss: 0.1466
Epoch [26/30], Batch [5900/6000], Loss: 0.1008
Epoch [26/30], Loss: 0.1840
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.1404
Epoch [27/30], Batch [100/6000], Loss: 0.0987
Epoch [27/30], Batch [200/6000], Loss: 0.0953
Epoch [27/30], Batch [300/6000], Loss: 0.0865
Epoch [27/30], Batch [400/6000], Loss: 0.1300
Epoch [27/30], Batch [500/6000], Loss: 1.4276
Epoch [27/30], Batch [600/6000], Loss: 0.1025
Epoch [27/30], Batch [700/6000], Loss: 0.1045
Epoch [27/30], Batch [800/6000], Loss: 0.1036
Epoch [27/30], Batch [900/6000], Loss: 0.1283
Epoch [27/30], Batch [1000/6000], Loss: 0.1159
Epoch [27/30], Batch [1100/6000], Loss: 0.1206
Epoch [27/30], Batch [1200/6000], Loss: 0.1335
Epoch [27/30], Batch [1300/6000], Loss: 0.1381
Epoch [27/30], Batch [1400/6000], Loss: 0.0994
Epoch [27/30], Batch [1500/6000], Loss: 0.1138
Epoch [27/30], Batch [1600/6000], Loss: 0.1122
Epoch [27/30], Batch [1700/6000], Loss: 0.1062
Epoch [27/30], Batch [1800/6000], Loss: 0.1488
Epoch [27/30], Batch [1900/6000], Loss: 0.0845
Epoch [27/30], Batch [2000/6000], Loss: 0.1050
Epoch [27/30], Batch [2100/6000], Loss: 0.1440
Epoch [27/30], Batch [2200/6000], Loss: 0.0919
Epoch [27/30], Batch [2300/6000], Loss: 0.1130
Epoch [27/30], Batch [2400/6000], Loss: 0.1126
Epoch [27/30], Batch [2500/6000], Loss: 0.1447
Epoch [27/30], Batch [2600/6000], Loss: 0.1871
Epoch [27/30], Batch [2700/6000], Loss: 0.1604
Epoch [27/30], Batch [2800/6000], Loss: 0.0815
Epoch [27/30], Batch [2900/6000], Loss: 0.1325
Epoch [27/30], Batch [3000/6000], Loss: 0.0928
Epoch [27/30], Batch [3100/6000], Loss: 0.1171
Epoch [27/30], Batch [3200/6000], Loss: 0.1248
Epoch [27/30], Batch [3300/6000], Loss: 0.1074
Epoch [27/30], Batch [3400/6000], Loss: 0.1276
Epoch [27/30], Batch [3500/6000], Loss: 0.1112
Epoch [27/30], Batch [3600/6000], Loss: 0.1112
Epoch [27/30], Batch [3700/6000], Loss: 0.1033
Epoch [27/30], Batch [3800/6000], Loss: 0.1173
Epoch [27/30], Batch [3900/6000], Loss: 0.0988
Epoch [27/30], Batch [4000/6000], Loss: 0.1016
Epoch [27/30], Batch [4100/6000], Loss: 0.1109
Epoch [27/30], Batch [4200/6000], Loss: 0.1057
Epoch [27/30], Batch [4300/6000], Loss: 0.1220
Epoch [27/30], Batch [4400/6000], Loss: 0.1353
Epoch [27/30], Batch [4500/6000], Loss: 0.1159
Epoch [27/30], Batch [4600/6000], Loss: 0.0987
Epoch [27/30], Batch [4700/6000], Loss: 0.1202
Epoch [27/30], Batch [4800/6000], Loss: 0.1511
Epoch [27/30], Batch [4900/6000], Loss: 0.1168
Epoch [27/30], Batch [5000/6000], Loss: 0.1256
Epoch [27/30], Batch [5100/6000], Loss: 0.6087
Epoch [27/30], Batch [5200/6000], Loss: 0.1058
Epoch [27/30], Batch [5300/6000], Loss: 0.1089
Epoch [27/30], Batch [5400/6000], Loss: 0.0971
Epoch [27/30], Batch [5500/6000], Loss: 0.1059
Epoch [27/30], Batch [5600/6000], Loss: 0.1486
Epoch [27/30], Batch [5700/6000], Loss: 0.0993
Epoch [27/30], Batch [5800/6000], Loss: 0.1485
Epoch [27/30], Batch [5900/6000], Loss: 0.1039
Epoch [27/30], Loss: 0.1777
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.1914
Epoch [28/30], Batch [100/6000], Loss: 0.0946
Epoch [28/30], Batch [200/6000], Loss: 0.0889
Epoch [28/30], Batch [300/6000], Loss: 0.1326
Epoch [28/30], Batch [400/6000], Loss: 0.1216
Epoch [28/30], Batch [500/6000], Loss: 0.0938
Epoch [28/30], Batch [600/6000], Loss: 0.1037
Epoch [28/30], Batch [700/6000], Loss: 0.1139
Epoch [28/30], Batch [800/6000], Loss: 0.1116
Epoch [28/30], Batch [900/6000], Loss: 0.3966
Epoch [28/30], Batch [1000/6000], Loss: 0.4201
Epoch [28/30], Batch [1100/6000], Loss: 0.1097
Epoch [28/30], Batch [1200/6000], Loss: 0.0985
Epoch [28/30], Batch [1300/6000], Loss: 0.1175
Epoch [28/30], Batch [1400/6000], Loss: 0.0951
Epoch [28/30], Batch [1500/6000], Loss: 0.0983
Epoch [28/30], Batch [1600/6000], Loss: 0.1380
Epoch [28/30], Batch [1700/6000], Loss: 0.0937
Epoch [28/30], Batch [1800/6000], Loss: 0.1093
Epoch [28/30], Batch [1900/6000], Loss: 0.1092
Epoch [28/30], Batch [2000/6000], Loss: 0.1128
Epoch [28/30], Batch [2100/6000], Loss: 0.0866
Epoch [28/30], Batch [2200/6000], Loss: 0.0921
Epoch [28/30], Batch [2300/6000], Loss: 0.1558
Epoch [28/30], Batch [2400/6000], Loss: 0.1011
Epoch [28/30], Batch [2500/6000], Loss: 0.0964
Epoch [28/30], Batch [2600/6000], Loss: 0.1859
Epoch [28/30], Batch [2700/6000], Loss: 0.1229
Epoch [28/30], Batch [2800/6000], Loss: 0.3760
Epoch [28/30], Batch [2900/6000], Loss: 0.1168
Epoch [28/30], Batch [3000/6000], Loss: 0.1344
Epoch [28/30], Batch [3100/6000], Loss: 0.1222
Epoch [28/30], Batch [3200/6000], Loss: 0.1521
Epoch [28/30], Batch [3300/6000], Loss: 0.1211
Epoch [28/30], Batch [3400/6000], Loss: 0.1136
Epoch [28/30], Batch [3500/6000], Loss: 0.1420
Epoch [28/30], Batch [3600/6000], Loss: 0.1125
Epoch [28/30], Batch [3700/6000], Loss: 0.1232
Epoch [28/30], Batch [3800/6000], Loss: 0.1255
Epoch [28/30], Batch [3900/6000], Loss: 0.1191
Epoch [28/30], Batch [4000/6000], Loss: 0.1063
Epoch [28/30], Batch [4100/6000], Loss: 0.7125
Epoch [28/30], Batch [4200/6000], Loss: 0.1380
Epoch [28/30], Batch [4300/6000], Loss: 0.0879
Epoch [28/30], Batch [4400/6000], Loss: 0.6126
Epoch [28/30], Batch [4500/6000], Loss: 0.1128
Epoch [28/30], Batch [4600/6000], Loss: 0.1644
Epoch [28/30], Batch [4700/6000], Loss: 0.1103
Epoch [28/30], Batch [4800/6000], Loss: 0.1177
Epoch [28/30], Batch [4900/6000], Loss: 0.1194
Epoch [28/30], Batch [5000/6000], Loss: 0.1092
Epoch [28/30], Batch [5100/6000], Loss: 0.1079
Epoch [28/30], Batch [5200/6000], Loss: 0.1062
Epoch [28/30], Batch [5300/6000], Loss: 0.0940
Epoch [28/30], Batch [5400/6000], Loss: 0.1125
Epoch [28/30], Batch [5500/6000], Loss: 0.0996
Epoch [28/30], Batch [5600/6000], Loss: 0.1132
Epoch [28/30], Batch [5700/6000], Loss: 0.1024
Epoch [28/30], Batch [5800/6000], Loss: 1.5226
Epoch [28/30], Batch [5900/6000], Loss: 0.0826
Epoch [28/30], Loss: 0.1704
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.1310
Epoch [29/30], Batch [100/6000], Loss: 0.1195
Epoch [29/30], Batch [200/6000], Loss: 0.1065
Epoch [29/30], Batch [300/6000], Loss: 0.1003
Epoch [29/30], Batch [400/6000], Loss: 0.1115
Epoch [29/30], Batch [500/6000], Loss: 0.1050
Epoch [29/30], Batch [600/6000], Loss: 0.1015
Epoch [29/30], Batch [700/6000], Loss: 0.2442
Epoch [29/30], Batch [800/6000], Loss: 0.1146
Epoch [29/30], Batch [900/6000], Loss: 0.0954
Epoch [29/30], Batch [1000/6000], Loss: 0.1124
Epoch [29/30], Batch [1100/6000], Loss: 0.1057
Epoch [29/30], Batch [1200/6000], Loss: 0.1132
Epoch [29/30], Batch [1300/6000], Loss: 0.1374
Epoch [29/30], Batch [1400/6000], Loss: 0.1296
Epoch [29/30], Batch [1500/6000], Loss: 0.1130
Epoch [29/30], Batch [1600/6000], Loss: 0.1087
Epoch [29/30], Batch [1700/6000], Loss: 0.1254
Epoch [29/30], Batch [1800/6000], Loss: 0.1398
Epoch [29/30], Batch [1900/6000], Loss: 0.1584
Epoch [29/30], Batch [2000/6000], Loss: 0.1360
Epoch [29/30], Batch [2100/6000], Loss: 0.1044
Epoch [29/30], Batch [2200/6000], Loss: 0.1197
Epoch [29/30], Batch [2300/6000], Loss: 0.0839
Epoch [29/30], Batch [2400/6000], Loss: 0.1816
Epoch [29/30], Batch [2500/6000], Loss: 0.3875
Epoch [29/30], Batch [2600/6000], Loss: 0.1062
Epoch [29/30], Batch [2700/6000], Loss: 0.1106
Epoch [29/30], Batch [2800/6000], Loss: 0.0987
Epoch [29/30], Batch [2900/6000], Loss: 0.0913
Epoch [29/30], Batch [3000/6000], Loss: 0.1076
Epoch [29/30], Batch [3100/6000], Loss: 0.1009
Epoch [29/30], Batch [3200/6000], Loss: 0.1129
Epoch [29/30], Batch [3300/6000], Loss: 0.0995
Epoch [29/30], Batch [3400/6000], Loss: 0.0920
Epoch [29/30], Batch [3500/6000], Loss: 0.1092
Epoch [29/30], Batch [3600/6000], Loss: 0.1077
Epoch [29/30], Batch [3700/6000], Loss: 0.1250
Epoch [29/30], Batch [3800/6000], Loss: 0.1061
Epoch [29/30], Batch [3900/6000], Loss: 0.1153
Epoch [29/30], Batch [4000/6000], Loss: 0.1245
Epoch [29/30], Batch [4100/6000], Loss: 0.1301
Epoch [29/30], Batch [4200/6000], Loss: 0.0956
Epoch [29/30], Batch [4300/6000], Loss: 0.1134
Epoch [29/30], Batch [4400/6000], Loss: 0.0934
Epoch [29/30], Batch [4500/6000], Loss: 0.1553
Epoch [29/30], Batch [4600/6000], Loss: 0.0744
Epoch [29/30], Batch [4700/6000], Loss: 0.1160
Epoch [29/30], Batch [4800/6000], Loss: 1.5283
Epoch [29/30], Batch [4900/6000], Loss: 0.1092
Epoch [29/30], Batch [5000/6000], Loss: 0.1423
Epoch [29/30], Batch [5100/6000], Loss: 0.1136
Epoch [29/30], Batch [5200/6000], Loss: 0.1138
Epoch [29/30], Batch [5300/6000], Loss: 0.1105
Epoch [29/30], Batch [5400/6000], Loss: 0.0885
Epoch [29/30], Batch [5500/6000], Loss: 0.1163
Epoch [29/30], Batch [5600/6000], Loss: 0.1161
Epoch [29/30], Batch [5700/6000], Loss: 0.1061
Epoch [29/30], Batch [5800/6000], Loss: 0.1085
Epoch [29/30], Batch [5900/6000], Loss: 0.1000
Epoch [29/30], Loss: 0.1681
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.1117
Epoch [30/30], Batch [100/6000], Loss: 0.1012
Epoch [30/30], Batch [200/6000], Loss: 0.1185
Epoch [30/30], Batch [300/6000], Loss: 0.1221
Epoch [30/30], Batch [400/6000], Loss: 0.0884
Epoch [30/30], Batch [500/6000], Loss: 0.1354
Epoch [30/30], Batch [600/6000], Loss: 0.1004
Epoch [30/30], Batch [700/6000], Loss: 0.1255
Epoch [30/30], Batch [800/6000], Loss: 0.0931
Epoch [30/30], Batch [900/6000], Loss: 0.0972
Epoch [30/30], Batch [1000/6000], Loss: 0.0832
Epoch [30/30], Batch [1100/6000], Loss: 0.1743
Epoch [30/30], Batch [1200/6000], Loss: 0.2038
Epoch [30/30], Batch [1300/6000], Loss: 0.1245
Epoch [30/30], Batch [1400/6000], Loss: 0.1389
Epoch [30/30], Batch [1500/6000], Loss: 0.1565
Epoch [30/30], Batch [1600/6000], Loss: 0.1215
Epoch [30/30], Batch [1700/6000], Loss: 0.1306
Epoch [30/30], Batch [1800/6000], Loss: 0.1038
Epoch [30/30], Batch [1900/6000], Loss: 0.0920
Epoch [30/30], Batch [2000/6000], Loss: 0.1079
Epoch [30/30], Batch [2100/6000], Loss: 0.1211
Epoch [30/30], Batch [2200/6000], Loss: 0.1596
Epoch [30/30], Batch [2300/6000], Loss: 0.1160
Epoch [30/30], Batch [2400/6000], Loss: 0.1328
Epoch [30/30], Batch [2500/6000], Loss: 0.1075
Epoch [30/30], Batch [2600/6000], Loss: 0.0953
Epoch [30/30], Batch [2700/6000], Loss: 0.1351
Epoch [30/30], Batch [2800/6000], Loss: 0.1295
Epoch [30/30], Batch [2900/6000], Loss: 0.0854
Epoch [30/30], Batch [3000/6000], Loss: 0.0927
Epoch [30/30], Batch [3100/6000], Loss: 0.1134
Epoch [30/30], Batch [3200/6000], Loss: 0.1019
Epoch [30/30], Batch [3300/6000], Loss: 0.1272
Epoch [30/30], Batch [3400/6000], Loss: 0.1212
Epoch [30/30], Batch [3500/6000], Loss: 0.1069
Epoch [30/30], Batch [3600/6000], Loss: 0.1054
Epoch [30/30], Batch [3700/6000], Loss: 0.1055
Epoch [30/30], Batch [3800/6000], Loss: 0.0924
Epoch [30/30], Batch [3900/6000], Loss: 0.1062
Epoch [30/30], Batch [4000/6000], Loss: 0.1064
Epoch [30/30], Batch [4100/6000], Loss: 0.1003
Epoch [30/30], Batch [4200/6000], Loss: 0.3551
Epoch [30/30], Batch [4300/6000], Loss: 0.5633
Epoch [30/30], Batch [4400/6000], Loss: 0.3475
Epoch [30/30], Batch [4500/6000], Loss: 0.0977
Epoch [30/30], Batch [4600/6000], Loss: 0.1038
Epoch [30/30], Batch [4700/6000], Loss: 0.1043
Epoch [30/30], Batch [4800/6000], Loss: 0.1257
Epoch [30/30], Batch [4900/6000], Loss: 0.1256
Epoch [30/30], Batch [5000/6000], Loss: 0.1261
Epoch [30/30], Batch [5100/6000], Loss: 0.1248
Epoch [30/30], Batch [5200/6000], Loss: 0.1244
Epoch [30/30], Batch [5300/6000], Loss: 0.1228
Epoch [30/30], Batch [5400/6000], Loss: 0.2201
Epoch [30/30], Batch [5500/6000], Loss: 0.1254
Epoch [30/30], Batch [5600/6000], Loss: 0.1047
Epoch [30/30], Batch [5700/6000], Loss: 0.0999
Epoch [30/30], Batch [5800/6000], Loss: 0.1147
Epoch [30/30], Batch [5900/6000], Loss: 0.8300
Epoch [30/30], Loss: 0.1676
Visualization saved to figures/visualization_0.png
Test Loss: 0.1149, Accuracy: 98.12%
Visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 2.2819
  Image Loss: 0.0496
  Total Loss: 22.8684
  Image grad max: 1.5817725658416748
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 2/300:
  Label Loss: 2.1845
  Image Loss: 0.0494
  Total Loss: 21.8944
  Image grad max: 1.6154134273529053
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 3/300:
  Label Loss: 2.0891
  Image Loss: 0.0491
  Total Loss: 20.9402
  Image grad max: 1.6543304920196533
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 4/300:
  Label Loss: 1.9924
  Image Loss: 0.0488
  Total Loss: 19.9731
  Image grad max: 1.687076210975647
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 5/300:
  Label Loss: 1.8937
  Image Loss: 0.0486
  Total Loss: 18.9860
  Image grad max: 1.726147174835205
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 6/300:
  Label Loss: 1.7910
  Image Loss: 0.0486
  Total Loss: 17.9585
  Image grad max: 1.7778469324111938
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 7/300:
  Label Loss: 1.6831
  Image Loss: 0.0486
  Total Loss: 16.8793
  Image grad max: 1.8192481994628906
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 8/300:
  Label Loss: 1.5706
  Image Loss: 0.0488
  Total Loss: 15.7550
  Image grad max: 1.79409921169281
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 9/300:
  Label Loss: 1.4539
  Image Loss: 0.0490
  Total Loss: 14.5879
  Image grad max: 1.8003243207931519
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 10/300:
  Label Loss: 1.3288
  Image Loss: 0.0493
  Total Loss: 13.3369
  Image grad max: 1.8143322467803955
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 11/300:
  Label Loss: 1.1912
  Image Loss: 0.0497
  Total Loss: 11.9622
  Image grad max: 1.840664029121399
  Output probs: [[0.    0.    0.    0.999 0.    0.001 0.    0.    0.    0.   ]]
Adversarial Training Loop 12/300:
  Label Loss: 1.0406
  Image Loss: 0.0502
  Total Loss: 10.4567
  Image grad max: 1.863623857498169
  Output probs: [[0.    0.    0.    0.993 0.    0.006 0.    0.    0.    0.   ]]
Adversarial Training Loop 13/300:
  Label Loss: 0.8820
  Image Loss: 0.0508
  Total Loss: 8.8704
  Image grad max: 1.8432071208953857
  Output probs: [[0.    0.    0.001 0.964 0.    0.034 0.    0.    0.001 0.   ]]
Adversarial Training Loop 14/300:
  Label Loss: 0.7202
  Image Loss: 0.0514
  Total Loss: 7.2537
  Image grad max: 1.7572120428085327
  Output probs: [[0.    0.    0.004 0.844 0.    0.15  0.    0.    0.002 0.   ]]
Adversarial Training Loop 15/300:
  Label Loss: 0.5660
  Image Loss: 0.0521
  Total Loss: 5.7121
  Image grad max: 1.8356261253356934
  Output probs: [[0.001 0.    0.012 0.57  0.    0.41  0.    0.    0.007 0.   ]]
Adversarial Training Loop 16/300:
  Label Loss: 0.4347
  Image Loss: 0.0528
  Total Loss: 4.3995
  Image grad max: 1.7976434230804443
  Output probs: [[0.018 0.    0.031 0.318 0.    0.615 0.001 0.    0.016 0.   ]]
Adversarial Training Loop 17/300:
  Label Loss: 0.3357
  Image Loss: 0.0536
  Total Loss: 3.4111
  Image grad max: 1.5079628229141235
  Output probs: [[0.179 0.    0.06  0.156 0.    0.572 0.007 0.    0.025 0.   ]]
Adversarial Training Loop 18/300:
  Label Loss: 0.2750
  Image Loss: 0.0545
  Total Loss: 2.8047
  Image grad max: 0.7116164565086365
  Output probs: [[0.619 0.    0.054 0.046 0.    0.245 0.016 0.    0.019 0.001]]
Adversarial Training Loop 19/300:
  Label Loss: 0.2921
  Image Loss: 0.0554
  Total Loss: 2.9766
  Image grad max: 0.8819891810417175
  Output probs: [[0.848 0.    0.03  0.013 0.    0.081 0.017 0.    0.011 0.   ]]
Adversarial Training Loop 20/300:
  Label Loss: 0.3524
  Image Loss: 0.0561
  Total Loss: 3.5803
  Image grad max: 1.4091932773590088
  Output probs: [[0.909 0.    0.02  0.006 0.    0.04  0.017 0.    0.007 0.   ]]
Adversarial Training Loop 21/300:
  Label Loss: 0.3919
  Image Loss: 0.0567
  Total Loss: 3.9754
  Image grad max: 1.4215763807296753
  Output probs: [[0.922 0.    0.018 0.004 0.    0.03  0.019 0.    0.007 0.   ]]
Adversarial Training Loop 22/300:
  Label Loss: 0.4049
  Image Loss: 0.0572
  Total Loss: 4.1059
  Image grad max: 1.3591415882110596
  Output probs: [[0.916 0.    0.019 0.004 0.    0.03  0.022 0.    0.009 0.   ]]
Adversarial Training Loop 23/300:
  Label Loss: 0.3972
  Image Loss: 0.0575
  Total Loss: 4.0294
  Image grad max: 1.3063377141952515
  Output probs: [[0.892 0.    0.023 0.005 0.    0.039 0.027 0.    0.013 0.   ]]
Adversarial Training Loop 24/300:
  Label Loss: 0.3738
  Image Loss: 0.0576
  Total Loss: 3.7952
  Image grad max: 1.2570796012878418
  Output probs: [[0.841 0.    0.032 0.008 0.    0.06  0.035 0.    0.023 0.001]]
Adversarial Training Loop 25/300:
  Label Loss: 0.3390
  Image Loss: 0.0577
  Total Loss: 3.4473
  Image grad max: 1.1798702478408813
  Output probs: [[0.744 0.    0.047 0.015 0.    0.105 0.044 0.    0.044 0.001]]
Adversarial Training Loop 26/300:
  Label Loss: 0.2978
  Image Loss: 0.0577
  Total Loss: 3.0359
  Image grad max: 1.017541766166687
  Output probs: [[0.579 0.    0.065 0.028 0.    0.192 0.05  0.    0.084 0.001]]
Adversarial Training Loop 27/300:
  Label Loss: 0.2596
  Image Loss: 0.0577
  Total Loss: 2.6538
  Image grad max: 0.6791514754295349
  Output probs: [[0.372 0.    0.077 0.046 0.    0.315 0.048 0.    0.14  0.002]]
Adversarial Training Loop 28/300:
  Label Loss: 0.2360
  Image Loss: 0.0576
  Total Loss: 2.4176
  Image grad max: 0.28839483857154846
  Output probs: [[0.204 0.    0.075 0.064 0.    0.427 0.039 0.    0.189 0.002]]
Adversarial Training Loop 29/300:
  Label Loss: 0.2310
  Image Loss: 0.0575
  Total Loss: 2.3675
  Image grad max: 0.25783079862594604
  Output probs: [[0.112 0.    0.067 0.076 0.    0.493 0.029 0.    0.221 0.002]]
Adversarial Training Loop 30/300:
  Label Loss: 0.2367
  Image Loss: 0.0574
  Total Loss: 2.4243
  Image grad max: 0.5157354474067688
  Output probs: [[0.069 0.    0.06  0.085 0.    0.517 0.023 0.    0.245 0.001]]
Adversarial Training Loop 31/300:
  Label Loss: 0.2437
  Image Loss: 0.0574
  Total Loss: 2.4946
  Image grad max: 0.6227142214775085
  Output probs: [[0.05  0.    0.058 0.091 0.    0.512 0.019 0.    0.268 0.001]]
Adversarial Training Loop 32/300:
  Label Loss: 0.2479
  Image Loss: 0.0575
  Total Loss: 2.5363
  Image grad max: 0.6468043327331543
  Output probs: [[0.042 0.    0.061 0.097 0.    0.489 0.018 0.    0.292 0.001]]
Adversarial Training Loop 33/300:
  Label Loss: 0.2482
  Image Loss: 0.0576
  Total Loss: 2.5396
  Image grad max: 0.625069797039032
  Output probs: [[0.041 0.    0.067 0.101 0.    0.456 0.018 0.    0.315 0.001]]
Adversarial Training Loop 34/300:
  Label Loss: 0.2452
  Image Loss: 0.0577
  Total Loss: 2.5095
  Image grad max: 0.5789375901222229
  Output probs: [[0.044 0.    0.077 0.104 0.    0.419 0.02  0.    0.334 0.002]]
Adversarial Training Loop 35/300:
  Label Loss: 0.2397
  Image Loss: 0.0578
  Total Loss: 2.4546
  Image grad max: 0.5426159501075745
  Output probs: [[0.051 0.    0.09  0.107 0.    0.385 0.023 0.    0.342 0.002]]
Adversarial Training Loop 36/300:
  Label Loss: 0.2322
  Image Loss: 0.0580
  Total Loss: 2.3805
  Image grad max: 0.5148170590400696
  Output probs: [[0.063 0.    0.106 0.107 0.    0.356 0.027 0.    0.338 0.002]]
Adversarial Training Loop 37/300:
  Label Loss: 0.2236
  Image Loss: 0.0582
  Total Loss: 2.2939
  Image grad max: 0.47493377327919006
  Output probs: [[0.081 0.001 0.124 0.107 0.    0.333 0.032 0.    0.321 0.002]]
Adversarial Training Loop 38/300:
  Label Loss: 0.2146
  Image Loss: 0.0584
  Total Loss: 2.2041
  Image grad max: 0.428644061088562
  Output probs: [[0.104 0.001 0.142 0.104 0.    0.312 0.039 0.    0.296 0.002]]
Adversarial Training Loop 39/300:
  Label Loss: 0.2061
  Image Loss: 0.0586
  Total Loss: 2.1197
  Image grad max: 0.36515769362449646
  Output probs: [[0.133 0.001 0.158 0.101 0.    0.294 0.046 0.    0.265 0.002]]
Adversarial Training Loop 40/300:
  Label Loss: 0.1988
  Image Loss: 0.0588
  Total Loss: 2.0464
  Image grad max: 0.29104506969451904
  Output probs: [[0.166 0.001 0.17  0.096 0.    0.279 0.055 0.    0.231 0.002]]
Adversarial Training Loop 41/300:
  Label Loss: 0.1928
  Image Loss: 0.0590
  Total Loss: 1.9870
  Image grad max: 0.2080940157175064
  Output probs: [[0.201 0.001 0.178 0.091 0.    0.266 0.063 0.    0.197 0.002]]
Adversarial Training Loop 42/300:
  Label Loss: 0.1883
  Image Loss: 0.0591
  Total Loss: 1.9419
  Image grad max: 0.1440458744764328
  Output probs: [[0.236 0.001 0.18  0.086 0.    0.257 0.072 0.    0.166 0.002]]
Adversarial Training Loop 43/300:
  Label Loss: 0.1851
  Image Loss: 0.0593
  Total Loss: 1.9099
  Image grad max: 0.15880167484283447
  Output probs: [[0.266 0.001 0.179 0.082 0.    0.25  0.079 0.    0.141 0.002]]
Adversarial Training Loop 44/300:
  Label Loss: 0.1828
  Image Loss: 0.0594
  Total Loss: 1.8870
  Image grad max: 0.17630015313625336
  Output probs: [[0.289 0.001 0.176 0.078 0.    0.248 0.086 0.    0.12  0.001]]
Adversarial Training Loop 45/300:
  Label Loss: 0.1808
  Image Loss: 0.0595
  Total Loss: 1.8680
  Image grad max: 0.18443971872329712
  Output probs: [[0.304 0.001 0.171 0.076 0.    0.25  0.092 0.    0.105 0.001]]
Adversarial Training Loop 46/300:
  Label Loss: 0.1790
  Image Loss: 0.0596
  Total Loss: 1.8492
  Image grad max: 0.18412555754184723
  Output probs: [[0.311 0.001 0.166 0.076 0.    0.256 0.096 0.    0.093 0.001]]
Adversarial Training Loop 47/300:
  Label Loss: 0.1768
  Image Loss: 0.0597
  Total Loss: 1.8280
  Image grad max: 0.18000268936157227
  Output probs: [[0.31  0.001 0.161 0.076 0.    0.266 0.1   0.    0.084 0.001]]
Adversarial Training Loop 48/300:
  Label Loss: 0.1744
  Image Loss: 0.0598
  Total Loss: 1.8035
  Image grad max: 0.1866326481103897
  Output probs: [[0.302 0.001 0.157 0.078 0.    0.28  0.103 0.    0.077 0.001]]
Adversarial Training Loop 49/300:
  Label Loss: 0.1716
  Image Loss: 0.0599
  Total Loss: 1.7758
  Image grad max: 0.1851429045200348
  Output probs: [[0.288 0.001 0.153 0.081 0.    0.297 0.106 0.    0.072 0.001]]
Adversarial Training Loop 50/300:
  Label Loss: 0.1686
  Image Loss: 0.0599
  Total Loss: 1.7460
  Image grad max: 0.177581787109375
  Output probs: [[0.271 0.001 0.149 0.085 0.    0.315 0.107 0.    0.069 0.001]]
Adversarial Training Loop 51/300:
  Label Loss: 0.1656
  Image Loss: 0.0600
  Total Loss: 1.7155
  Image grad max: 0.1660504788160324
  Output probs: [[0.252 0.001 0.147 0.09  0.    0.334 0.108 0.    0.067 0.001]]
Adversarial Training Loop 52/300:
  Label Loss: 0.1626
  Image Loss: 0.0600
  Total Loss: 1.6856
  Image grad max: 0.15188603103160858
  Output probs: [[0.232 0.001 0.144 0.095 0.    0.351 0.108 0.    0.066 0.001]]
Adversarial Training Loop 53/300:
  Label Loss: 0.1597
  Image Loss: 0.0600
  Total Loss: 1.6568
  Image grad max: 0.14284475147724152
  Output probs: [[0.214 0.001 0.143 0.101 0.    0.366 0.108 0.    0.066 0.001]]
Adversarial Training Loop 54/300:
  Label Loss: 0.1569
  Image Loss: 0.0601
  Total Loss: 1.6292
  Image grad max: 0.13754931092262268
  Output probs: [[0.197 0.001 0.143 0.106 0.    0.376 0.108 0.    0.067 0.001]]
Adversarial Training Loop 55/300:
  Label Loss: 0.1542
  Image Loss: 0.0601
  Total Loss: 1.6022
  Image grad max: 0.13123241066932678
  Output probs: [[0.183 0.001 0.144 0.111 0.    0.382 0.109 0.    0.069 0.001]]
Adversarial Training Loop 56/300:
  Label Loss: 0.1515
  Image Loss: 0.0602
  Total Loss: 1.5754
  Image grad max: 0.14000873267650604
  Output probs: [[0.171 0.002 0.146 0.116 0.    0.382 0.11  0.    0.072 0.001]]
Adversarial Training Loop 57/300:
  Label Loss: 0.1488
  Image Loss: 0.0603
  Total Loss: 1.5482
  Image grad max: 0.14600956439971924
  Output probs: [[0.162 0.002 0.15  0.12  0.    0.378 0.112 0.    0.075 0.001]]
Adversarial Training Loop 58/300:
  Label Loss: 0.1460
  Image Loss: 0.0604
  Total Loss: 1.5204
  Image grad max: 0.14460620284080505
  Output probs: [[0.155 0.002 0.154 0.123 0.    0.369 0.116 0.    0.08  0.001]]
Adversarial Training Loop 59/300:
  Label Loss: 0.1432
  Image Loss: 0.0604
  Total Loss: 1.4925
  Image grad max: 0.13730661571025848
  Output probs: [[0.149 0.002 0.159 0.126 0.    0.358 0.12  0.    0.085 0.001]]
Adversarial Training Loop 60/300:
  Label Loss: 0.1404
  Image Loss: 0.0605
  Total Loss: 1.4649
  Image grad max: 0.12979383766651154
  Output probs: [[0.145 0.002 0.164 0.127 0.    0.345 0.125 0.    0.09  0.001]]
Adversarial Training Loop 61/300:
  Label Loss: 0.1377
  Image Loss: 0.0606
  Total Loss: 1.4380
  Image grad max: 0.11986815929412842
  Output probs: [[0.142 0.002 0.168 0.127 0.    0.333 0.131 0.    0.095 0.001]]
Adversarial Training Loop 62/300:
  Label Loss: 0.1352
  Image Loss: 0.0607
  Total Loss: 1.4123
  Image grad max: 0.10824264585971832
  Output probs: [[0.139 0.002 0.171 0.127 0.    0.32  0.139 0.    0.1   0.001]]
Adversarial Training Loop 63/300:
  Label Loss: 0.1327
  Image Loss: 0.0608
  Total Loss: 1.3876
  Image grad max: 0.09501902759075165
  Output probs: [[0.136 0.003 0.174 0.125 0.    0.309 0.147 0.    0.104 0.001]]
Adversarial Training Loop 64/300:
  Label Loss: 0.1303
  Image Loss: 0.0610
  Total Loss: 1.3639
  Image grad max: 0.0895451009273529
  Output probs: [[0.133 0.003 0.174 0.124 0.    0.301 0.156 0.    0.108 0.001]]
Adversarial Training Loop 65/300:
  Label Loss: 0.1280
  Image Loss: 0.0611
  Total Loss: 1.3410
  Image grad max: 0.08382702618837357
  Output probs: [[0.13  0.003 0.174 0.122 0.    0.294 0.166 0.    0.11  0.001]]
Adversarial Training Loop 66/300:
  Label Loss: 0.1258
  Image Loss: 0.0612
  Total Loss: 1.3189
  Image grad max: 0.07775897532701492
  Output probs: [[0.127 0.003 0.172 0.119 0.    0.29  0.177 0.    0.111 0.001]]
Adversarial Training Loop 67/300:
  Label Loss: 0.1236
  Image Loss: 0.0613
  Total Loss: 1.2972
  Image grad max: 0.0736837163567543
  Output probs: [[0.123 0.003 0.169 0.118 0.    0.288 0.187 0.    0.111 0.001]]
Adversarial Training Loop 68/300:
  Label Loss: 0.1215
  Image Loss: 0.0614
  Total Loss: 1.2761
  Image grad max: 0.07359300553798676
  Output probs: [[0.119 0.003 0.165 0.116 0.    0.288 0.198 0.    0.11  0.001]]
Adversarial Training Loop 69/300:
  Label Loss: 0.1194
  Image Loss: 0.0615
  Total Loss: 1.2557
  Image grad max: 0.07342144101858139
  Output probs: [[0.114 0.003 0.161 0.115 0.    0.29  0.207 0.    0.109 0.001]]
Adversarial Training Loop 70/300:
  Label Loss: 0.1174
  Image Loss: 0.0616
  Total Loss: 1.2357
  Image grad max: 0.07306589186191559
  Output probs: [[0.109 0.003 0.157 0.115 0.    0.293 0.215 0.    0.107 0.001]]
Adversarial Training Loop 71/300:
  Label Loss: 0.1155
  Image Loss: 0.0617
  Total Loss: 1.2162
  Image grad max: 0.07247527688741684
  Output probs: [[0.104 0.003 0.153 0.116 0.    0.297 0.222 0.    0.104 0.001]]
Adversarial Training Loop 72/300:
  Label Loss: 0.1135
  Image Loss: 0.0618
  Total Loss: 1.1972
  Image grad max: 0.07674717158079147
  Output probs: [[0.098 0.004 0.149 0.118 0.    0.302 0.227 0.    0.101 0.001]]
Adversarial Training Loop 73/300:
  Label Loss: 0.1117
  Image Loss: 0.0619
  Total Loss: 1.1785
  Image grad max: 0.08218478411436081
  Output probs: [[0.093 0.004 0.146 0.121 0.    0.307 0.23  0.    0.099 0.001]]
Adversarial Training Loop 74/300:
  Label Loss: 0.1098
  Image Loss: 0.0620
  Total Loss: 1.1601
  Image grad max: 0.08531031757593155
  Output probs: [[0.088 0.004 0.144 0.125 0.    0.311 0.231 0.    0.096 0.001]]
Adversarial Training Loop 75/300:
  Label Loss: 0.1080
  Image Loss: 0.0621
  Total Loss: 1.1420
  Image grad max: 0.08553215116262436
  Output probs: [[0.084 0.004 0.143 0.13  0.    0.315 0.23  0.    0.094 0.001]]
Adversarial Training Loop 76/300:
  Label Loss: 0.1062
  Image Loss: 0.0622
  Total Loss: 1.1241
  Image grad max: 0.08256803452968597
  Output probs: [[0.08  0.004 0.143 0.136 0.    0.317 0.228 0.    0.092 0.   ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.1044
  Image Loss: 0.0623
  Total Loss: 1.1065
  Image grad max: 0.07731276005506516
  Output probs: [[0.077 0.004 0.144 0.143 0.    0.317 0.224 0.    0.09  0.   ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.1027
  Image Loss: 0.0624
  Total Loss: 1.0893
  Image grad max: 0.07520608603954315
  Output probs: [[0.074 0.004 0.146 0.15  0.    0.314 0.221 0.    0.089 0.   ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.1010
  Image Loss: 0.0625
  Total Loss: 1.0725
  Image grad max: 0.07789164781570435
  Output probs: [[0.071 0.004 0.149 0.157 0.    0.311 0.218 0.    0.089 0.   ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0994
  Image Loss: 0.0626
  Total Loss: 1.0564
  Image grad max: 0.08001582324504852
  Output probs: [[0.069 0.004 0.152 0.164 0.    0.306 0.215 0.    0.088 0.   ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0978
  Image Loss: 0.0627
  Total Loss: 1.0406
  Image grad max: 0.08380294591188431
  Output probs: [[0.068 0.005 0.155 0.17  0.    0.3   0.214 0.    0.088 0.   ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0962
  Image Loss: 0.0628
  Total Loss: 1.0252
  Image grad max: 0.08560103178024292
  Output probs: [[0.066 0.005 0.157 0.175 0.    0.294 0.214 0.    0.087 0.   ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0947
  Image Loss: 0.0630
  Total Loss: 1.0101
  Image grad max: 0.08518841117620468
  Output probs: [[0.065 0.005 0.159 0.179 0.    0.289 0.216 0.    0.086 0.   ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0932
  Image Loss: 0.0631
  Total Loss: 0.9950
  Image grad max: 0.082853302359581
  Output probs: [[0.064 0.005 0.159 0.181 0.    0.284 0.22  0.    0.085 0.   ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0917
  Image Loss: 0.0632
  Total Loss: 0.9800
  Image grad max: 0.07858278602361679
  Output probs: [[0.064 0.005 0.159 0.182 0.    0.28  0.226 0.    0.084 0.   ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0902
  Image Loss: 0.0633
  Total Loss: 0.9651
  Image grad max: 0.07277043163776398
  Output probs: [[0.063 0.005 0.157 0.182 0.    0.276 0.233 0.    0.083 0.   ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0887
  Image Loss: 0.0634
  Total Loss: 0.9503
  Image grad max: 0.0660264641046524
  Output probs: [[0.062 0.005 0.155 0.181 0.    0.274 0.241 0.    0.081 0.   ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0872
  Image Loss: 0.0635
  Total Loss: 0.9358
  Image grad max: 0.058894623070955276
  Output probs: [[0.061 0.005 0.152 0.181 0.    0.272 0.249 0.    0.08  0.   ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0858
  Image Loss: 0.0636
  Total Loss: 0.9216
  Image grad max: 0.0595291368663311
  Output probs: [[0.06  0.005 0.148 0.181 0.    0.271 0.256 0.    0.078 0.   ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0844
  Image Loss: 0.0637
  Total Loss: 0.9078
  Image grad max: 0.06409651041030884
  Output probs: [[0.059 0.005 0.145 0.182 0.    0.271 0.261 0.    0.076 0.   ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0830
  Image Loss: 0.0638
  Total Loss: 0.8942
  Image grad max: 0.06811422854661942
  Output probs: [[0.057 0.006 0.142 0.184 0.    0.27  0.265 0.    0.075 0.   ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0817
  Image Loss: 0.0639
  Total Loss: 0.8806
  Image grad max: 0.07112671434879303
  Output probs: [[0.055 0.006 0.14  0.188 0.    0.27  0.267 0.    0.074 0.   ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0803
  Image Loss: 0.0640
  Total Loss: 0.8669
  Image grad max: 0.07280897349119186
  Output probs: [[0.054 0.006 0.138 0.193 0.    0.269 0.267 0.    0.073 0.   ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0789
  Image Loss: 0.0641
  Total Loss: 0.8531
  Image grad max: 0.07302457094192505
  Output probs: [[0.052 0.006 0.137 0.2   0.    0.268 0.266 0.    0.072 0.   ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0775
  Image Loss: 0.0642
  Total Loss: 0.8394
  Image grad max: 0.07196686416864395
  Output probs: [[0.05  0.006 0.135 0.207 0.    0.266 0.264 0.    0.071 0.   ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0761
  Image Loss: 0.0643
  Total Loss: 0.8257
  Image grad max: 0.06983238458633423
  Output probs: [[0.049 0.006 0.134 0.214 0.    0.264 0.262 0.    0.071 0.   ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0748
  Image Loss: 0.0645
  Total Loss: 0.8123
  Image grad max: 0.06720416992902756
  Output probs: [[0.047 0.006 0.133 0.221 0.    0.262 0.261 0.    0.07  0.   ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0734
  Image Loss: 0.0646
  Total Loss: 0.7989
  Image grad max: 0.07072459906339645
  Output probs: [[0.046 0.006 0.132 0.226 0.    0.259 0.262 0.    0.07  0.   ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0721
  Image Loss: 0.0647
  Total Loss: 0.7855
  Image grad max: 0.07256247103214264
  Output probs: [[0.045 0.006 0.13  0.229 0.    0.255 0.265 0.    0.07  0.   ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0707
  Image Loss: 0.0648
  Total Loss: 0.7721
  Image grad max: 0.07136386632919312
  Output probs: [[0.044 0.006 0.127 0.231 0.    0.252 0.27  0.    0.069 0.   ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0694
  Image Loss: 0.0649
  Total Loss: 0.7585
  Image grad max: 0.0673120990395546
  Output probs: [[0.043 0.006 0.125 0.232 0.    0.248 0.277 0.    0.07  0.   ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0680
  Image Loss: 0.0650
  Total Loss: 0.7451
  Image grad max: 0.06528361886739731
  Output probs: [[0.042 0.006 0.122 0.232 0.    0.244 0.284 0.    0.07  0.   ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0667
  Image Loss: 0.0651
  Total Loss: 0.7318
  Image grad max: 0.06749088317155838
  Output probs: [[0.041 0.006 0.119 0.233 0.    0.24  0.291 0.    0.07  0.   ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0653
  Image Loss: 0.0652
  Total Loss: 0.7186
  Image grad max: 0.06936267018318176
  Output probs: [[0.04  0.006 0.117 0.235 0.    0.236 0.296 0.    0.07  0.   ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0640
  Image Loss: 0.0653
  Total Loss: 0.7055
  Image grad max: 0.07010696083307266
  Output probs: [[0.039 0.006 0.115 0.238 0.    0.231 0.3   0.    0.07  0.   ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0627
  Image Loss: 0.0654
  Total Loss: 0.6923
  Image grad max: 0.06928931176662445
  Output probs: [[0.038 0.006 0.113 0.243 0.    0.227 0.301 0.    0.071 0.   ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0614
  Image Loss: 0.0655
  Total Loss: 0.6792
  Image grad max: 0.068272165954113
  Output probs: [[0.037 0.006 0.111 0.25  0.    0.224 0.301 0.    0.071 0.   ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0600
  Image Loss: 0.0656
  Total Loss: 0.6660
  Image grad max: 0.06616929173469543
  Output probs: [[0.036 0.006 0.109 0.257 0.    0.22  0.3   0.    0.072 0.   ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0587
  Image Loss: 0.0658
  Total Loss: 0.6529
  Image grad max: 0.06312552094459534
  Output probs: [[0.035 0.006 0.106 0.264 0.    0.217 0.301 0.    0.072 0.   ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0574
  Image Loss: 0.0659
  Total Loss: 0.6399
  Image grad max: 0.06110088899731636
  Output probs: [[0.034 0.006 0.103 0.269 0.    0.214 0.302 0.    0.072 0.   ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0561
  Image Loss: 0.0660
  Total Loss: 0.6271
  Image grad max: 0.059365611523389816
  Output probs: [[0.033 0.006 0.099 0.273 0.    0.211 0.306 0.    0.072 0.   ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0548
  Image Loss: 0.0661
  Total Loss: 0.6143
  Image grad max: 0.05760977044701576
  Output probs: [[0.032 0.006 0.096 0.276 0.    0.208 0.311 0.    0.072 0.   ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0535
  Image Loss: 0.0662
  Total Loss: 0.6017
  Image grad max: 0.05862043425440788
  Output probs: [[0.031 0.006 0.092 0.277 0.    0.205 0.317 0.    0.072 0.   ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0523
  Image Loss: 0.0664
  Total Loss: 0.5893
  Image grad max: 0.06063122674822807
  Output probs: [[0.03  0.006 0.089 0.279 0.    0.201 0.322 0.    0.072 0.   ]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0511
  Image Loss: 0.0665
  Total Loss: 0.5772
  Image grad max: 0.06207919120788574
  Output probs: [[0.03  0.006 0.086 0.283 0.    0.198 0.326 0.    0.072 0.   ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0499
  Image Loss: 0.0666
  Total Loss: 0.5653
  Image grad max: 0.06207292526960373
  Output probs: [[0.029 0.006 0.083 0.287 0.    0.193 0.328 0.    0.072 0.   ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0487
  Image Loss: 0.0668
  Total Loss: 0.5536
  Image grad max: 0.060214247554540634
  Output probs: [[0.028 0.006 0.081 0.294 0.    0.189 0.329 0.    0.073 0.   ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0475
  Image Loss: 0.0669
  Total Loss: 0.5421
  Image grad max: 0.05687079578638077
  Output probs: [[0.028 0.006 0.079 0.3   0.    0.185 0.329 0.    0.073 0.   ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0464
  Image Loss: 0.0670
  Total Loss: 0.5309
  Image grad max: 0.052908264100551605
  Output probs: [[0.027 0.006 0.078 0.306 0.    0.18  0.33  0.    0.073 0.   ]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0453
  Image Loss: 0.0672
  Total Loss: 0.5200
  Image grad max: 0.0494641438126564
  Output probs: [[0.027 0.006 0.075 0.31  0.    0.175 0.333 0.    0.073 0.   ]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0442
  Image Loss: 0.0673
  Total Loss: 0.5093
  Image grad max: 0.04747191444039345
  Output probs: [[0.026 0.006 0.073 0.313 0.    0.171 0.337 0.    0.073 0.   ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0431
  Image Loss: 0.0675
  Total Loss: 0.4989
  Image grad max: 0.04715494439005852
  Output probs: [[0.026 0.006 0.071 0.314 0.    0.167 0.343 0.    0.073 0.   ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0421
  Image Loss: 0.0676
  Total Loss: 0.4888
  Image grad max: 0.04786469414830208
  Output probs: [[0.026 0.006 0.068 0.316 0.    0.164 0.348 0.    0.072 0.   ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0411
  Image Loss: 0.0677
  Total Loss: 0.4790
  Image grad max: 0.048807088285684586
  Output probs: [[0.025 0.006 0.066 0.318 0.    0.16  0.352 0.    0.072 0.   ]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0402
  Image Loss: 0.0679
  Total Loss: 0.4694
  Image grad max: 0.04850843548774719
  Output probs: [[0.025 0.006 0.064 0.323 0.    0.157 0.353 0.    0.071 0.   ]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0392
  Image Loss: 0.0680
  Total Loss: 0.4601
  Image grad max: 0.04651613533496857
  Output probs: [[0.025 0.006 0.062 0.329 0.    0.154 0.353 0.    0.071 0.   ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0383
  Image Loss: 0.0682
  Total Loss: 0.4511
  Image grad max: 0.04340362548828125
  Output probs: [[0.024 0.006 0.06  0.335 0.    0.151 0.354 0.    0.07  0.   ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0374
  Image Loss: 0.0683
  Total Loss: 0.4423
  Image grad max: 0.04197393357753754
  Output probs: [[0.024 0.006 0.059 0.339 0.    0.148 0.355 0.    0.069 0.   ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0365
  Image Loss: 0.0685
  Total Loss: 0.4339
  Image grad max: 0.041835807263851166
  Output probs: [[0.024 0.006 0.057 0.341 0.    0.145 0.359 0.    0.068 0.   ]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0357
  Image Loss: 0.0686
  Total Loss: 0.4256
  Image grad max: 0.0397811196744442
  Output probs: [[0.024 0.007 0.055 0.342 0.    0.142 0.364 0.    0.067 0.   ]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0349
  Image Loss: 0.0688
  Total Loss: 0.4177
  Image grad max: 0.040195532143116
  Output probs: [[0.024 0.007 0.053 0.344 0.    0.139 0.368 0.    0.066 0.   ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0341
  Image Loss: 0.0689
  Total Loss: 0.4100
  Image grad max: 0.04084249585866928
  Output probs: [[0.023 0.007 0.051 0.347 0.    0.137 0.37  0.    0.065 0.   ]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0334
  Image Loss: 0.0691
  Total Loss: 0.4026
  Image grad max: 0.039862487465143204
  Output probs: [[0.023 0.007 0.05  0.351 0.    0.134 0.371 0.    0.064 0.   ]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0326
  Image Loss: 0.0693
  Total Loss: 0.3955
  Image grad max: 0.03738122060894966
  Output probs: [[0.023 0.007 0.049 0.356 0.    0.131 0.371 0.    0.063 0.   ]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0319
  Image Loss: 0.0694
  Total Loss: 0.3886
  Image grad max: 0.0354563407599926
  Output probs: [[0.023 0.007 0.048 0.36  0.    0.128 0.372 0.    0.062 0.   ]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0312
  Image Loss: 0.0696
  Total Loss: 0.3820
  Image grad max: 0.036152806133031845
  Output probs: [[0.023 0.007 0.046 0.362 0.    0.126 0.375 0.    0.061 0.   ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0306
  Image Loss: 0.0697
  Total Loss: 0.3757
  Image grad max: 0.03498055785894394
  Output probs: [[0.023 0.008 0.045 0.363 0.    0.123 0.379 0.    0.06  0.   ]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0300
  Image Loss: 0.0698
  Total Loss: 0.3695
  Image grad max: 0.032760217785835266
  Output probs: [[0.023 0.008 0.044 0.364 0.    0.121 0.382 0.    0.059 0.   ]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0294
  Image Loss: 0.0700
  Total Loss: 0.3636
  Image grad max: 0.033180560916662216
  Output probs: [[0.022 0.008 0.043 0.366 0.    0.118 0.384 0.    0.058 0.   ]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0288
  Image Loss: 0.0701
  Total Loss: 0.3579
  Image grad max: 0.032946426421403885
  Output probs: [[0.022 0.008 0.042 0.369 0.    0.116 0.385 0.    0.057 0.   ]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0282
  Image Loss: 0.0703
  Total Loss: 0.3524
  Image grad max: 0.031870804727077484
  Output probs: [[0.022 0.008 0.041 0.373 0.    0.114 0.385 0.    0.056 0.   ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0277
  Image Loss: 0.0704
  Total Loss: 0.3471
  Image grad max: 0.03049219772219658
  Output probs: [[0.022 0.009 0.04  0.376 0.    0.112 0.386 0.    0.055 0.   ]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0271
  Image Loss: 0.0706
  Total Loss: 0.3420
  Image grad max: 0.030210068449378014
  Output probs: [[0.022 0.009 0.039 0.377 0.    0.11  0.389 0.    0.054 0.   ]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0266
  Image Loss: 0.0707
  Total Loss: 0.3371
  Image grad max: 0.029381824657320976
  Output probs: [[0.022 0.009 0.038 0.378 0.    0.108 0.392 0.    0.053 0.   ]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0262
  Image Loss: 0.0709
  Total Loss: 0.3324
  Image grad max: 0.029447434470057487
  Output probs: [[0.022 0.009 0.037 0.379 0.    0.106 0.395 0.    0.052 0.   ]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0257
  Image Loss: 0.0710
  Total Loss: 0.3279
  Image grad max: 0.02919715642929077
  Output probs: [[0.022 0.01  0.036 0.381 0.    0.104 0.396 0.    0.051 0.   ]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0252
  Image Loss: 0.0711
  Total Loss: 0.3236
  Image grad max: 0.028418656438589096
  Output probs: [[0.021 0.01  0.036 0.385 0.    0.103 0.396 0.    0.05  0.   ]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0248
  Image Loss: 0.0713
  Total Loss: 0.3194
  Image grad max: 0.02739042602479458
  Output probs: [[0.021 0.01  0.035 0.387 0.    0.101 0.396 0.    0.049 0.   ]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0244
  Image Loss: 0.0714
  Total Loss: 0.3154
  Image grad max: 0.02662271074950695
  Output probs: [[0.021 0.01  0.034 0.389 0.    0.099 0.398 0.    0.048 0.   ]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0240
  Image Loss: 0.0716
  Total Loss: 0.3116
  Image grad max: 0.02633626200258732
  Output probs: [[0.021 0.011 0.033 0.389 0.    0.098 0.401 0.    0.047 0.   ]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0236
  Image Loss: 0.0717
  Total Loss: 0.3079
  Image grad max: 0.026481110602617264
  Output probs: [[0.021 0.011 0.033 0.39  0.    0.096 0.403 0.    0.046 0.   ]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0233
  Image Loss: 0.0718
  Total Loss: 0.3044
  Image grad max: 0.02654835395514965
  Output probs: [[0.021 0.011 0.032 0.391 0.    0.095 0.404 0.    0.046 0.   ]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0229
  Image Loss: 0.0719
  Total Loss: 0.3010
  Image grad max: 0.026249028742313385
  Output probs: [[0.021 0.011 0.032 0.393 0.    0.094 0.405 0.    0.045 0.   ]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0226
  Image Loss: 0.0721
  Total Loss: 0.2977
  Image grad max: 0.025843420997262
  Output probs: [[0.02  0.012 0.031 0.395 0.    0.092 0.406 0.    0.044 0.   ]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0222
  Image Loss: 0.0722
  Total Loss: 0.2946
  Image grad max: 0.0254001934081316
  Output probs: [[0.02  0.012 0.03  0.397 0.    0.091 0.406 0.    0.043 0.   ]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0219
  Image Loss: 0.0723
  Total Loss: 0.2916
  Image grad max: 0.02500438503921032
  Output probs: [[0.02  0.012 0.03  0.398 0.    0.089 0.408 0.    0.043 0.   ]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0216
  Image Loss: 0.0724
  Total Loss: 0.2888
  Image grad max: 0.024817725643515587
  Output probs: [[0.02  0.012 0.029 0.399 0.    0.088 0.409 0.    0.042 0.   ]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0214
  Image Loss: 0.0725
  Total Loss: 0.2861
  Image grad max: 0.024596640840172768
  Output probs: [[0.02  0.013 0.029 0.4   0.    0.087 0.41  0.    0.041 0.   ]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0211
  Image Loss: 0.0727
  Total Loss: 0.2835
  Image grad max: 0.024436913430690765
  Output probs: [[0.02  0.013 0.028 0.401 0.    0.086 0.411 0.    0.041 0.   ]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0208
  Image Loss: 0.0728
  Total Loss: 0.2810
  Image grad max: 0.024197259917855263
  Output probs: [[0.02  0.013 0.028 0.402 0.    0.085 0.412 0.    0.04  0.   ]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0206
  Image Loss: 0.0729
  Total Loss: 0.2786
  Image grad max: 0.023839807137846947
  Output probs: [[0.019 0.013 0.028 0.403 0.    0.084 0.413 0.    0.04  0.   ]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0203
  Image Loss: 0.0730
  Total Loss: 0.2763
  Image grad max: 0.023460006341338158
  Output probs: [[0.019 0.013 0.027 0.404 0.    0.083 0.414 0.    0.039 0.   ]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0201
  Image Loss: 0.0731
  Total Loss: 0.2741
  Image grad max: 0.02316865511238575
  Output probs: [[0.019 0.014 0.027 0.405 0.    0.082 0.415 0.    0.039 0.   ]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0199
  Image Loss: 0.0732
  Total Loss: 0.2719
  Image grad max: 0.02297721803188324
  Output probs: [[0.019 0.014 0.026 0.406 0.    0.081 0.416 0.    0.038 0.   ]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0197
  Image Loss: 0.0733
  Total Loss: 0.2699
  Image grad max: 0.022792449221014977
  Output probs: [[0.019 0.014 0.026 0.407 0.    0.08  0.417 0.    0.037 0.   ]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0194
  Image Loss: 0.0734
  Total Loss: 0.2678
  Image grad max: 0.02253609150648117
  Output probs: [[0.018 0.014 0.025 0.408 0.    0.08  0.417 0.    0.037 0.   ]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0192
  Image Loss: 0.0735
  Total Loss: 0.2659
  Image grad max: 0.022228747606277466
  Output probs: [[0.018 0.014 0.025 0.409 0.    0.079 0.418 0.    0.036 0.   ]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0190
  Image Loss: 0.0736
  Total Loss: 0.2640
  Image grad max: 0.021969040855765343
  Output probs: [[0.018 0.015 0.025 0.409 0.    0.078 0.419 0.    0.036 0.   ]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0189
  Image Loss: 0.0736
  Total Loss: 0.2622
  Image grad max: 0.02181035839021206
  Output probs: [[0.018 0.015 0.025 0.41  0.    0.077 0.42  0.    0.036 0.   ]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0187
  Image Loss: 0.0737
  Total Loss: 0.2604
  Image grad max: 0.021686695516109467
  Output probs: [[0.018 0.015 0.024 0.411 0.    0.076 0.421 0.    0.035 0.   ]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0185
  Image Loss: 0.0738
  Total Loss: 0.2587
  Image grad max: 0.021529581397771835
  Output probs: [[0.018 0.015 0.024 0.412 0.    0.076 0.421 0.    0.035 0.   ]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0183
  Image Loss: 0.0739
  Total Loss: 0.2570
  Image grad max: 0.02132618986070156
  Output probs: [[0.018 0.015 0.024 0.412 0.    0.075 0.422 0.    0.034 0.   ]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0181
  Image Loss: 0.0740
  Total Loss: 0.2554
  Image grad max: 0.02113174833357334
  Output probs: [[0.017 0.015 0.023 0.413 0.    0.074 0.422 0.    0.034 0.   ]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0180
  Image Loss: 0.0741
  Total Loss: 0.2538
  Image grad max: 0.020986733958125114
  Output probs: [[0.017 0.015 0.023 0.414 0.    0.073 0.423 0.    0.033 0.   ]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0178
  Image Loss: 0.0742
  Total Loss: 0.2522
  Image grad max: 0.020859455689787865
  Output probs: [[0.017 0.016 0.023 0.414 0.    0.073 0.424 0.    0.033 0.   ]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0176
  Image Loss: 0.0742
  Total Loss: 0.2507
  Image grad max: 0.02070946991443634
  Output probs: [[0.017 0.016 0.023 0.415 0.    0.072 0.424 0.    0.033 0.   ]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0175
  Image Loss: 0.0743
  Total Loss: 0.2493
  Image grad max: 0.020531553775072098
  Output probs: [[0.017 0.016 0.023 0.416 0.    0.071 0.425 0.    0.032 0.   ]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0173
  Image Loss: 0.0744
  Total Loss: 0.2479
  Image grad max: 0.020358141511678696
  Output probs: [[0.017 0.016 0.022 0.416 0.    0.071 0.426 0.    0.032 0.   ]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0172
  Image Loss: 0.0745
  Total Loss: 0.2465
  Image grad max: 0.020216695964336395
  Output probs: [[0.017 0.016 0.022 0.417 0.    0.07  0.426 0.    0.032 0.   ]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0171
  Image Loss: 0.0745
  Total Loss: 0.2451
  Image grad max: 0.020068692043423653
  Output probs: [[0.017 0.016 0.022 0.418 0.    0.07  0.427 0.    0.031 0.   ]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0169
  Image Loss: 0.0746
  Total Loss: 0.2438
  Image grad max: 0.01990523934364319
  Output probs: [[0.017 0.016 0.022 0.418 0.    0.069 0.427 0.    0.031 0.   ]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0168
  Image Loss: 0.0747
  Total Loss: 0.2425
  Image grad max: 0.019724799320101738
  Output probs: [[0.016 0.016 0.022 0.419 0.    0.068 0.428 0.    0.031 0.   ]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0166
  Image Loss: 0.0748
  Total Loss: 0.2413
  Image grad max: 0.019553249701857567
  Output probs: [[0.016 0.016 0.022 0.419 0.    0.068 0.428 0.    0.03  0.   ]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0165
  Image Loss: 0.0748
  Total Loss: 0.2400
  Image grad max: 0.019424255937337875
  Output probs: [[0.016 0.016 0.021 0.42  0.    0.067 0.429 0.    0.03  0.   ]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0164
  Image Loss: 0.0749
  Total Loss: 0.2388
  Image grad max: 0.019311122596263885
  Output probs: [[0.016 0.016 0.021 0.42  0.    0.067 0.43  0.    0.03  0.   ]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0163
  Image Loss: 0.0750
  Total Loss: 0.2376
  Image grad max: 0.01919245906174183
  Output probs: [[0.016 0.016 0.021 0.421 0.    0.066 0.43  0.    0.029 0.   ]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0161
  Image Loss: 0.0750
  Total Loss: 0.2365
  Image grad max: 0.019110068678855896
  Output probs: [[0.016 0.016 0.021 0.421 0.    0.066 0.431 0.    0.029 0.   ]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0160
  Image Loss: 0.0751
  Total Loss: 0.2353
  Image grad max: 0.01897556334733963
  Output probs: [[0.016 0.016 0.021 0.422 0.    0.065 0.431 0.    0.029 0.   ]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0159
  Image Loss: 0.0752
  Total Loss: 0.2342
  Image grad max: 0.01882200315594673
  Output probs: [[0.016 0.016 0.021 0.423 0.    0.065 0.431 0.    0.028 0.   ]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0158
  Image Loss: 0.0752
  Total Loss: 0.2331
  Image grad max: 0.018709354102611542
  Output probs: [[0.016 0.016 0.021 0.423 0.    0.064 0.432 0.    0.028 0.   ]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0157
  Image Loss: 0.0753
  Total Loss: 0.2321
  Image grad max: 0.018643038347363472
  Output probs: [[0.016 0.016 0.021 0.423 0.    0.064 0.433 0.    0.028 0.   ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0156
  Image Loss: 0.0753
  Total Loss: 0.2310
  Image grad max: 0.018587004393339157
  Output probs: [[0.016 0.016 0.021 0.424 0.    0.063 0.433 0.    0.028 0.   ]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0155
  Image Loss: 0.0754
  Total Loss: 0.2300
  Image grad max: 0.018482021987438202
  Output probs: [[0.015 0.016 0.02  0.424 0.    0.063 0.433 0.    0.027 0.   ]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0154
  Image Loss: 0.0754
  Total Loss: 0.2290
  Image grad max: 0.018351417034864426
  Output probs: [[0.015 0.016 0.02  0.425 0.    0.062 0.434 0.    0.027 0.   ]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0153
  Image Loss: 0.0755
  Total Loss: 0.2280
  Image grad max: 0.018222033977508545
  Output probs: [[0.015 0.016 0.02  0.425 0.    0.062 0.434 0.    0.027 0.   ]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0151
  Image Loss: 0.0755
  Total Loss: 0.2270
  Image grad max: 0.018130656331777573
  Output probs: [[0.015 0.016 0.02  0.426 0.    0.062 0.435 0.    0.027 0.   ]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0150
  Image Loss: 0.0756
  Total Loss: 0.2261
  Image grad max: 0.018101023510098457
  Output probs: [[0.015 0.016 0.02  0.426 0.    0.061 0.435 0.    0.026 0.   ]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0149
  Image Loss: 0.0756
  Total Loss: 0.2251
  Image grad max: 0.01787826418876648
  Output probs: [[0.015 0.016 0.02  0.427 0.    0.061 0.435 0.    0.026 0.   ]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0149
  Image Loss: 0.0757
  Total Loss: 0.2242
  Image grad max: 0.017731904983520508
  Output probs: [[0.015 0.016 0.02  0.427 0.    0.06  0.436 0.    0.026 0.   ]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0148
  Image Loss: 0.0757
  Total Loss: 0.2233
  Image grad max: 0.01761695370078087
  Output probs: [[0.015 0.016 0.02  0.427 0.    0.06  0.436 0.    0.026 0.   ]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0147
  Image Loss: 0.0758
  Total Loss: 0.2224
  Image grad max: 0.017567357048392296
  Output probs: [[0.015 0.015 0.02  0.428 0.    0.06  0.437 0.    0.026 0.   ]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0146
  Image Loss: 0.0758
  Total Loss: 0.2215
  Image grad max: 0.01740253157913685
  Output probs: [[0.015 0.015 0.02  0.428 0.    0.059 0.437 0.    0.025 0.   ]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0145
  Image Loss: 0.0759
  Total Loss: 0.2206
  Image grad max: 0.01723005250096321
  Output probs: [[0.015 0.015 0.019 0.429 0.    0.059 0.437 0.    0.025 0.   ]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0144
  Image Loss: 0.0759
  Total Loss: 0.2197
  Image grad max: 0.01710871420800686
  Output probs: [[0.015 0.015 0.019 0.429 0.    0.059 0.438 0.    0.025 0.   ]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0143
  Image Loss: 0.0760
  Total Loss: 0.2189
  Image grad max: 0.017000790685415268
  Output probs: [[0.014 0.015 0.019 0.429 0.    0.058 0.438 0.    0.025 0.   ]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0142
  Image Loss: 0.0760
  Total Loss: 0.2180
  Image grad max: 0.016812438145279884
  Output probs: [[0.014 0.015 0.019 0.43  0.    0.058 0.439 0.    0.025 0.   ]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0141
  Image Loss: 0.0761
  Total Loss: 0.2172
  Image grad max: 0.01664869487285614
  Output probs: [[0.014 0.015 0.019 0.43  0.    0.058 0.439 0.    0.025 0.   ]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0140
  Image Loss: 0.0761
  Total Loss: 0.2163
  Image grad max: 0.01651817560195923
  Output probs: [[0.014 0.015 0.019 0.431 0.    0.057 0.439 0.    0.024 0.   ]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0139
  Image Loss: 0.0761
  Total Loss: 0.2155
  Image grad max: 0.016407852992415428
  Output probs: [[0.014 0.015 0.019 0.431 0.    0.057 0.44  0.    0.024 0.   ]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0139
  Image Loss: 0.0762
  Total Loss: 0.2147
  Image grad max: 0.016314877197146416
  Output probs: [[0.014 0.015 0.019 0.431 0.    0.057 0.44  0.    0.024 0.   ]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0138
  Image Loss: 0.0762
  Total Loss: 0.2139
  Image grad max: 0.016233352944254875
  Output probs: [[0.014 0.015 0.019 0.432 0.    0.056 0.44  0.    0.024 0.   ]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0137
  Image Loss: 0.0763
  Total Loss: 0.2131
  Image grad max: 0.016162795946002007
  Output probs: [[0.014 0.015 0.019 0.432 0.    0.056 0.441 0.    0.024 0.   ]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0136
  Image Loss: 0.0763
  Total Loss: 0.2123
  Image grad max: 0.016103634610772133
  Output probs: [[0.014 0.015 0.019 0.432 0.    0.055 0.441 0.    0.024 0.   ]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0135
  Image Loss: 0.0763
  Total Loss: 0.2116
  Image grad max: 0.016071151942014694
  Output probs: [[0.014 0.015 0.019 0.433 0.    0.055 0.442 0.    0.023 0.   ]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0134
  Image Loss: 0.0764
  Total Loss: 0.2108
  Image grad max: 0.015986667945981026
  Output probs: [[0.014 0.014 0.019 0.433 0.    0.055 0.442 0.    0.023 0.   ]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0134
  Image Loss: 0.0764
  Total Loss: 0.2101
  Image grad max: 0.015927061438560486
  Output probs: [[0.014 0.014 0.019 0.433 0.    0.054 0.442 0.    0.023 0.   ]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0133
  Image Loss: 0.0764
  Total Loss: 0.2093
  Image grad max: 0.015867887064814568
  Output probs: [[0.014 0.014 0.018 0.434 0.    0.054 0.443 0.    0.023 0.   ]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0132
  Image Loss: 0.0765
  Total Loss: 0.2086
  Image grad max: 0.0158198494464159
  Output probs: [[0.014 0.014 0.018 0.434 0.    0.054 0.443 0.    0.023 0.   ]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0131
  Image Loss: 0.0765
  Total Loss: 0.2078
  Image grad max: 0.015734564512968063
  Output probs: [[0.013 0.014 0.018 0.434 0.    0.053 0.443 0.    0.023 0.   ]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0131
  Image Loss: 0.0766
  Total Loss: 0.2071
  Image grad max: 0.015637902542948723
  Output probs: [[0.013 0.014 0.018 0.435 0.    0.053 0.443 0.    0.023 0.   ]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0130
  Image Loss: 0.0766
  Total Loss: 0.2064
  Image grad max: 0.015540925785899162
  Output probs: [[0.013 0.014 0.018 0.435 0.    0.053 0.444 0.    0.022 0.   ]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0129
  Image Loss: 0.0766
  Total Loss: 0.2057
  Image grad max: 0.015432046726346016
  Output probs: [[0.013 0.014 0.018 0.435 0.    0.053 0.444 0.    0.022 0.   ]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0128
  Image Loss: 0.0767
  Total Loss: 0.2050
  Image grad max: 0.015317387878894806
  Output probs: [[0.013 0.014 0.018 0.436 0.    0.052 0.444 0.    0.022 0.   ]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0128
  Image Loss: 0.0767
  Total Loss: 0.2043
  Image grad max: 0.015200072899460793
  Output probs: [[0.013 0.014 0.018 0.436 0.    0.052 0.445 0.    0.022 0.   ]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0127
  Image Loss: 0.0767
  Total Loss: 0.2036
  Image grad max: 0.015082338824868202
  Output probs: [[0.013 0.014 0.018 0.436 0.    0.052 0.445 0.    0.022 0.   ]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0126
  Image Loss: 0.0768
  Total Loss: 0.2030
  Image grad max: 0.014967081137001514
  Output probs: [[0.013 0.014 0.018 0.437 0.    0.051 0.445 0.    0.022 0.   ]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0126
  Image Loss: 0.0768
  Total Loss: 0.2023
  Image grad max: 0.014855882152915001
  Output probs: [[0.013 0.014 0.018 0.437 0.    0.051 0.446 0.    0.022 0.   ]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0125
  Image Loss: 0.0768
  Total Loss: 0.2016
  Image grad max: 0.0147491954267025
  Output probs: [[0.013 0.014 0.018 0.438 0.    0.051 0.446 0.    0.022 0.   ]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0124
  Image Loss: 0.0768
  Total Loss: 0.2010
  Image grad max: 0.014630272053182125
  Output probs: [[0.013 0.013 0.018 0.438 0.    0.051 0.446 0.    0.021 0.   ]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0123
  Image Loss: 0.0769
  Total Loss: 0.2003
  Image grad max: 0.014509876258671284
  Output probs: [[0.013 0.013 0.018 0.438 0.    0.05  0.447 0.    0.021 0.   ]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0123
  Image Loss: 0.0769
  Total Loss: 0.1997
  Image grad max: 0.014487719163298607
  Output probs: [[0.013 0.013 0.017 0.438 0.    0.05  0.447 0.    0.021 0.   ]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0122
  Image Loss: 0.0769
  Total Loss: 0.1991
  Image grad max: 0.014318873174488544
  Output probs: [[0.013 0.013 0.017 0.439 0.    0.05  0.447 0.    0.021 0.   ]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0122
  Image Loss: 0.0770
  Total Loss: 0.1985
  Image grad max: 0.014245614409446716
  Output probs: [[0.013 0.013 0.017 0.439 0.    0.05  0.447 0.    0.021 0.   ]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0121
  Image Loss: 0.0770
  Total Loss: 0.1979
  Image grad max: 0.014163008891046047
  Output probs: [[0.013 0.013 0.017 0.439 0.    0.049 0.448 0.    0.021 0.   ]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0120
  Image Loss: 0.0770
  Total Loss: 0.1973
  Image grad max: 0.014176493510603905
  Output probs: [[0.012 0.013 0.017 0.439 0.    0.049 0.448 0.    0.021 0.   ]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0120
  Image Loss: 0.0770
  Total Loss: 0.1967
  Image grad max: 0.014120612293481827
  Output probs: [[0.012 0.013 0.017 0.44  0.    0.049 0.448 0.    0.021 0.   ]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0119
  Image Loss: 0.0771
  Total Loss: 0.1961
  Image grad max: 0.013975434005260468
  Output probs: [[0.012 0.013 0.017 0.44  0.    0.048 0.448 0.    0.021 0.   ]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0118
  Image Loss: 0.0771
  Total Loss: 0.1955
  Image grad max: 0.013915750198066235
  Output probs: [[0.012 0.013 0.017 0.44  0.    0.048 0.449 0.    0.02  0.   ]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0118
  Image Loss: 0.0771
  Total Loss: 0.1949
  Image grad max: 0.013907467015087605
  Output probs: [[0.012 0.013 0.017 0.44  0.    0.048 0.449 0.    0.02  0.   ]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0117
  Image Loss: 0.0771
  Total Loss: 0.1944
  Image grad max: 0.013854260556399822
  Output probs: [[0.012 0.013 0.017 0.441 0.    0.048 0.449 0.    0.02  0.   ]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0117
  Image Loss: 0.0772
  Total Loss: 0.1938
  Image grad max: 0.013682762160897255
  Output probs: [[0.012 0.013 0.017 0.441 0.    0.047 0.449 0.    0.02  0.   ]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0116
  Image Loss: 0.0772
  Total Loss: 0.1932
  Image grad max: 0.013591965660452843
  Output probs: [[0.012 0.013 0.017 0.441 0.    0.047 0.45  0.    0.02  0.   ]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0115
  Image Loss: 0.0772
  Total Loss: 0.1927
  Image grad max: 0.013540436513721943
  Output probs: [[0.012 0.012 0.017 0.441 0.    0.047 0.45  0.    0.02  0.   ]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0115
  Image Loss: 0.0772
  Total Loss: 0.1921
  Image grad max: 0.013445105403661728
  Output probs: [[0.012 0.012 0.017 0.442 0.    0.047 0.45  0.    0.02  0.   ]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0114
  Image Loss: 0.0772
  Total Loss: 0.1916
  Image grad max: 0.013286994770169258
  Output probs: [[0.012 0.012 0.016 0.442 0.    0.047 0.45  0.    0.02  0.   ]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0114
  Image Loss: 0.0773
  Total Loss: 0.1911
  Image grad max: 0.013187561184167862
  Output probs: [[0.012 0.012 0.016 0.442 0.    0.046 0.451 0.    0.02  0.   ]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0113
  Image Loss: 0.0773
  Total Loss: 0.1905
  Image grad max: 0.013124139048159122
  Output probs: [[0.012 0.012 0.016 0.443 0.    0.046 0.451 0.    0.02  0.   ]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0113
  Image Loss: 0.0773
  Total Loss: 0.1900
  Image grad max: 0.01302625983953476
  Output probs: [[0.012 0.012 0.016 0.443 0.    0.046 0.451 0.    0.02  0.   ]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0112
  Image Loss: 0.0773
  Total Loss: 0.1895
  Image grad max: 0.012900429777801037
  Output probs: [[0.012 0.012 0.016 0.443 0.    0.046 0.451 0.    0.019 0.   ]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0112
  Image Loss: 0.0773
  Total Loss: 0.1890
  Image grad max: 0.012770108878612518
  Output probs: [[0.012 0.012 0.016 0.443 0.    0.045 0.451 0.    0.019 0.   ]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0111
  Image Loss: 0.0774
  Total Loss: 0.1885
  Image grad max: 0.012658818624913692
  Output probs: [[0.012 0.012 0.016 0.444 0.    0.045 0.452 0.    0.019 0.   ]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0111
  Image Loss: 0.0774
  Total Loss: 0.1880
  Image grad max: 0.01255333237349987
  Output probs: [[0.012 0.012 0.016 0.444 0.    0.045 0.452 0.    0.019 0.   ]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0110
  Image Loss: 0.0774
  Total Loss: 0.1875
  Image grad max: 0.012451504357159138
  Output probs: [[0.012 0.012 0.016 0.444 0.    0.045 0.452 0.    0.019 0.   ]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0110
  Image Loss: 0.0774
  Total Loss: 0.1870
  Image grad max: 0.012373635545372963
  Output probs: [[0.012 0.012 0.016 0.444 0.    0.045 0.452 0.    0.019 0.   ]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0109
  Image Loss: 0.0774
  Total Loss: 0.1865
  Image grad max: 0.01229315996170044
  Output probs: [[0.012 0.012 0.016 0.444 0.    0.044 0.453 0.    0.019 0.   ]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0109
  Image Loss: 0.0774
  Total Loss: 0.1860
  Image grad max: 0.012216591276228428
  Output probs: [[0.012 0.012 0.016 0.445 0.    0.044 0.453 0.    0.019 0.   ]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0108
  Image Loss: 0.0775
  Total Loss: 0.1855
  Image grad max: 0.012144900858402252
  Output probs: [[0.012 0.012 0.016 0.445 0.    0.044 0.453 0.    0.019 0.   ]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0108
  Image Loss: 0.0775
  Total Loss: 0.1851
  Image grad max: 0.012099454179406166
  Output probs: [[0.012 0.012 0.016 0.445 0.    0.044 0.453 0.    0.019 0.   ]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0107
  Image Loss: 0.0775
  Total Loss: 0.1846
  Image grad max: 0.012033331207931042
  Output probs: [[0.012 0.012 0.016 0.445 0.    0.044 0.454 0.    0.019 0.   ]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0107
  Image Loss: 0.0775
  Total Loss: 0.1842
  Image grad max: 0.011980750598013401
  Output probs: [[0.012 0.011 0.016 0.446 0.    0.043 0.454 0.    0.019 0.   ]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0106
  Image Loss: 0.0775
  Total Loss: 0.1837
  Image grad max: 0.011917405761778355
  Output probs: [[0.012 0.011 0.015 0.446 0.    0.043 0.454 0.    0.018 0.   ]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0106
  Image Loss: 0.0775
  Total Loss: 0.1832
  Image grad max: 0.011879945173859596
  Output probs: [[0.011 0.011 0.015 0.446 0.    0.043 0.454 0.    0.018 0.   ]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0105
  Image Loss: 0.0775
  Total Loss: 0.1828
  Image grad max: 0.01180531270802021
  Output probs: [[0.011 0.011 0.015 0.446 0.    0.043 0.454 0.    0.018 0.   ]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0105
  Image Loss: 0.0775
  Total Loss: 0.1824
  Image grad max: 0.011758387088775635
  Output probs: [[0.011 0.011 0.015 0.446 0.    0.043 0.454 0.    0.018 0.   ]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0104
  Image Loss: 0.0776
  Total Loss: 0.1819
  Image grad max: 0.011668271385133266
  Output probs: [[0.011 0.011 0.015 0.447 0.    0.042 0.455 0.    0.018 0.   ]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0104
  Image Loss: 0.0776
  Total Loss: 0.1815
  Image grad max: 0.011606840416789055
  Output probs: [[0.011 0.011 0.015 0.447 0.    0.042 0.455 0.    0.018 0.   ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0103
  Image Loss: 0.0776
  Total Loss: 0.1810
  Image grad max: 0.01149057038128376
  Output probs: [[0.011 0.011 0.015 0.447 0.    0.042 0.455 0.    0.018 0.   ]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0103
  Image Loss: 0.0776
  Total Loss: 0.1806
  Image grad max: 0.011391093023121357
  Output probs: [[0.011 0.011 0.015 0.447 0.    0.042 0.455 0.    0.018 0.   ]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0103
  Image Loss: 0.0776
  Total Loss: 0.1802
  Image grad max: 0.01131256204098463
  Output probs: [[0.011 0.011 0.015 0.448 0.    0.042 0.455 0.    0.018 0.   ]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0102
  Image Loss: 0.0776
  Total Loss: 0.1798
  Image grad max: 0.011218798346817493
  Output probs: [[0.011 0.011 0.015 0.448 0.    0.041 0.456 0.    0.018 0.   ]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0102
  Image Loss: 0.0776
  Total Loss: 0.1794
  Image grad max: 0.011115049012005329
  Output probs: [[0.011 0.011 0.015 0.448 0.    0.041 0.456 0.    0.018 0.   ]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0101
  Image Loss: 0.0776
  Total Loss: 0.1790
  Image grad max: 0.011036848649382591
  Output probs: [[0.011 0.011 0.015 0.448 0.    0.041 0.456 0.    0.018 0.   ]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0101
  Image Loss: 0.0776
  Total Loss: 0.1786
  Image grad max: 0.01097188051789999
  Output probs: [[0.011 0.011 0.015 0.448 0.    0.041 0.456 0.    0.018 0.   ]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0101
  Image Loss: 0.0776
  Total Loss: 0.1782
  Image grad max: 0.010892653837800026
  Output probs: [[0.011 0.011 0.015 0.449 0.    0.041 0.456 0.    0.017 0.   ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0100
  Image Loss: 0.0777
  Total Loss: 0.1778
  Image grad max: 0.010812697000801563
  Output probs: [[0.011 0.011 0.015 0.449 0.    0.041 0.456 0.    0.017 0.   ]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0100
  Image Loss: 0.0777
  Total Loss: 0.1774
  Image grad max: 0.010756864212453365
  Output probs: [[0.011 0.011 0.015 0.449 0.    0.04  0.457 0.    0.017 0.   ]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0099
  Image Loss: 0.0777
  Total Loss: 0.1770
  Image grad max: 0.010701967403292656
  Output probs: [[0.011 0.011 0.014 0.449 0.    0.04  0.457 0.    0.017 0.   ]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0099
  Image Loss: 0.0777
  Total Loss: 0.1766
  Image grad max: 0.010642305947840214
  Output probs: [[0.011 0.011 0.014 0.449 0.    0.04  0.457 0.    0.017 0.   ]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0099
  Image Loss: 0.0777
  Total Loss: 0.1762
  Image grad max: 0.010563605464994907
  Output probs: [[0.011 0.011 0.014 0.449 0.    0.04  0.457 0.    0.017 0.   ]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0098
  Image Loss: 0.0777
  Total Loss: 0.1758
  Image grad max: 0.010458109900355339
  Output probs: [[0.011 0.011 0.014 0.45  0.    0.04  0.457 0.    0.017 0.   ]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0098
  Image Loss: 0.0777
  Total Loss: 0.1755
  Image grad max: 0.010352362878620625
  Output probs: [[0.011 0.011 0.014 0.45  0.    0.04  0.457 0.    0.017 0.   ]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0097
  Image Loss: 0.0777
  Total Loss: 0.1751
  Image grad max: 0.010276351124048233
  Output probs: [[0.011 0.011 0.014 0.45  0.    0.039 0.457 0.    0.017 0.   ]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0097
  Image Loss: 0.0777
  Total Loss: 0.1747
  Image grad max: 0.01017676666378975
  Output probs: [[0.011 0.011 0.014 0.45  0.    0.039 0.458 0.    0.017 0.   ]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0097
  Image Loss: 0.0777
  Total Loss: 0.1744
  Image grad max: 0.010061739012598991
  Output probs: [[0.011 0.011 0.014 0.45  0.    0.039 0.458 0.    0.017 0.   ]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0096
  Image Loss: 0.0777
  Total Loss: 0.1740
  Image grad max: 0.009979956783354282
  Output probs: [[0.011 0.011 0.014 0.451 0.    0.039 0.458 0.    0.017 0.   ]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0096
  Image Loss: 0.0777
  Total Loss: 0.1737
  Image grad max: 0.009907779283821583
  Output probs: [[0.011 0.011 0.014 0.451 0.    0.039 0.458 0.    0.017 0.   ]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0096
  Image Loss: 0.0777
  Total Loss: 0.1733
  Image grad max: 0.009806912392377853
  Output probs: [[0.011 0.011 0.014 0.451 0.    0.039 0.458 0.    0.017 0.   ]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0095
  Image Loss: 0.0777
  Total Loss: 0.1730
  Image grad max: 0.009708509780466557
  Output probs: [[0.011 0.011 0.014 0.451 0.    0.039 0.458 0.    0.017 0.   ]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0095
  Image Loss: 0.0777
  Total Loss: 0.1727
  Image grad max: 0.009637748822569847
  Output probs: [[0.011 0.011 0.014 0.451 0.    0.038 0.458 0.    0.017 0.   ]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0095
  Image Loss: 0.0777
  Total Loss: 0.1723
  Image grad max: 0.009559535421431065
  Output probs: [[0.011 0.01  0.014 0.451 0.    0.038 0.459 0.    0.017 0.   ]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0094
  Image Loss: 0.0777
  Total Loss: 0.1720
  Image grad max: 0.009461556561291218
  Output probs: [[0.011 0.01  0.014 0.452 0.    0.038 0.459 0.    0.016 0.   ]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0094
  Image Loss: 0.0777
  Total Loss: 0.1717
  Image grad max: 0.00938273873180151
  Output probs: [[0.011 0.01  0.014 0.452 0.    0.038 0.459 0.    0.016 0.   ]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0094
  Image Loss: 0.0777
  Total Loss: 0.1714
  Image grad max: 0.009317485615611076
  Output probs: [[0.011 0.01  0.014 0.452 0.    0.038 0.459 0.    0.016 0.   ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0093
  Image Loss: 0.0777
  Total Loss: 0.1710
  Image grad max: 0.009235776960849762
  Output probs: [[0.011 0.01  0.013 0.452 0.    0.038 0.459 0.    0.016 0.   ]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0093
  Image Loss: 0.0777
  Total Loss: 0.1707
  Image grad max: 0.009148563258349895
  Output probs: [[0.011 0.01  0.013 0.452 0.    0.038 0.459 0.    0.016 0.   ]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0093
  Image Loss: 0.0777
  Total Loss: 0.1704
  Image grad max: 0.009083673357963562
  Output probs: [[0.011 0.01  0.013 0.452 0.    0.037 0.459 0.    0.016 0.   ]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0092
  Image Loss: 0.0777
  Total Loss: 0.1701
  Image grad max: 0.009013297036290169
  Output probs: [[0.011 0.01  0.013 0.452 0.    0.037 0.46  0.    0.016 0.   ]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0092
  Image Loss: 0.0777
  Total Loss: 0.1698
  Image grad max: 0.0089345658197999
  Output probs: [[0.01  0.01  0.013 0.453 0.    0.037 0.46  0.    0.016 0.   ]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0092
  Image Loss: 0.0777
  Total Loss: 0.1695
  Image grad max: 0.008868255652487278
  Output probs: [[0.01  0.01  0.013 0.453 0.    0.037 0.46  0.    0.016 0.   ]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0092
  Image Loss: 0.0777
  Total Loss: 0.1692
  Image grad max: 0.008806408382952213
Visualization saved to adversarial_figures/adversarial_training.png
Visualization saved to adversarial_figures/adversarial_testing.png
