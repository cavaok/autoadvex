running dynamical.py
Epoch [1/30], Batch [0/6000], Loss: 0.8861
Epoch [1/30], Batch [100/6000], Loss: 0.7002
Epoch [1/30], Batch [200/6000], Loss: 0.7167
Epoch [1/30], Batch [300/6000], Loss: 0.6822
Epoch [1/30], Batch [400/6000], Loss: 0.5850
Epoch [1/30], Batch [500/6000], Loss: 0.7288
Epoch [1/30], Batch [600/6000], Loss: 0.5963
Epoch [1/30], Batch [700/6000], Loss: 0.5494
Epoch [1/30], Batch [800/6000], Loss: 0.6068
Epoch [1/30], Batch [900/6000], Loss: 0.5423
Epoch [1/30], Batch [1000/6000], Loss: 0.4648
Epoch [1/30], Batch [1100/6000], Loss: 0.5708
Epoch [1/30], Batch [1200/6000], Loss: 0.4983
Epoch [1/30], Batch [1300/6000], Loss: 0.4653
Epoch [1/30], Batch [1400/6000], Loss: 0.4887
Epoch [1/30], Batch [1500/6000], Loss: 0.5092
Epoch [1/30], Batch [1600/6000], Loss: 0.5740
Epoch [1/30], Batch [1700/6000], Loss: 0.4939
Epoch [1/30], Batch [1800/6000], Loss: 0.5700
Epoch [1/30], Batch [1900/6000], Loss: 0.3795
Epoch [1/30], Batch [2000/6000], Loss: 0.4884
Epoch [1/30], Batch [2100/6000], Loss: 0.5435
Epoch [1/30], Batch [2200/6000], Loss: 0.3659
Epoch [1/30], Batch [2300/6000], Loss: 0.3847
Epoch [1/30], Batch [2400/6000], Loss: 0.4044
Epoch [1/30], Batch [2500/6000], Loss: 0.3640
Epoch [1/30], Batch [2600/6000], Loss: 0.3883
Epoch [1/30], Batch [2700/6000], Loss: 0.3175
Epoch [1/30], Batch [2800/6000], Loss: 0.3645
Epoch [1/30], Batch [2900/6000], Loss: 0.4338
Epoch [1/30], Batch [3000/6000], Loss: 0.3221
Epoch [1/30], Batch [3100/6000], Loss: 0.3899
Epoch [1/30], Batch [3200/6000], Loss: 0.2728
Epoch [1/30], Batch [3300/6000], Loss: 0.4679
Epoch [1/30], Batch [3400/6000], Loss: 0.3217
Epoch [1/30], Batch [3500/6000], Loss: 0.2955
Epoch [1/30], Batch [3600/6000], Loss: 0.4165
Epoch [1/30], Batch [3700/6000], Loss: 0.2802
Epoch [1/30], Batch [3800/6000], Loss: 0.3881
Epoch [1/30], Batch [3900/6000], Loss: 0.4402
Epoch [1/30], Batch [4000/6000], Loss: 0.2938
Epoch [1/30], Batch [4100/6000], Loss: 0.4623
Epoch [1/30], Batch [4200/6000], Loss: 0.3089
Epoch [1/30], Batch [4300/6000], Loss: 0.3657
Epoch [1/30], Batch [4400/6000], Loss: 0.2786
Epoch [1/30], Batch [4500/6000], Loss: 0.4329
Epoch [1/30], Batch [4600/6000], Loss: 0.3691
Epoch [1/30], Batch [4700/6000], Loss: 0.2614
Epoch [1/30], Batch [4800/6000], Loss: 0.2823
Epoch [1/30], Batch [4900/6000], Loss: 0.3364
Epoch [1/30], Batch [5000/6000], Loss: 0.4117
Epoch [1/30], Batch [5100/6000], Loss: 0.4621
Epoch [1/30], Batch [5200/6000], Loss: 0.3570
Epoch [1/30], Batch [5300/6000], Loss: 0.2790
Epoch [1/30], Batch [5400/6000], Loss: 0.2951
Epoch [1/30], Batch [5500/6000], Loss: 0.2683
Epoch [1/30], Batch [5600/6000], Loss: 0.2526
Epoch [1/30], Batch [5700/6000], Loss: 0.3336
Epoch [1/30], Batch [5800/6000], Loss: 0.3305
Epoch [1/30], Batch [5900/6000], Loss: 0.4054
Epoch [1/30], Loss: 0.4191
Epoch [2/30], Batch [0/6000], Loss: 0.2914
Epoch [2/30], Batch [100/6000], Loss: 0.2918
Epoch [2/30], Batch [200/6000], Loss: 0.2339
Epoch [2/30], Batch [300/6000], Loss: 0.3693
Epoch [2/30], Batch [400/6000], Loss: 0.2328
Epoch [2/30], Batch [500/6000], Loss: 0.2684
Epoch [2/30], Batch [600/6000], Loss: 0.3195
Epoch [2/30], Batch [700/6000], Loss: 0.2064
Epoch [2/30], Batch [800/6000], Loss: 0.3845
Epoch [2/30], Batch [900/6000], Loss: 0.2458
Epoch [2/30], Batch [1000/6000], Loss: 0.2634
Epoch [2/30], Batch [1100/6000], Loss: 0.2607
Epoch [2/30], Batch [1200/6000], Loss: 0.3141
Epoch [2/30], Batch [1300/6000], Loss: 0.3005
Epoch [2/30], Batch [1400/6000], Loss: 0.4564
Epoch [2/30], Batch [1500/6000], Loss: 0.2473
Epoch [2/30], Batch [1600/6000], Loss: 0.3354
Epoch [2/30], Batch [1700/6000], Loss: 0.3312
Epoch [2/30], Batch [1800/6000], Loss: 0.3791
Epoch [2/30], Batch [1900/6000], Loss: 0.2585
Epoch [2/30], Batch [2000/6000], Loss: 0.2467
Epoch [2/30], Batch [2100/6000], Loss: 0.3430
Epoch [2/30], Batch [2200/6000], Loss: 0.3041
Epoch [2/30], Batch [2300/6000], Loss: 0.3089
Epoch [2/30], Batch [2400/6000], Loss: 0.2390
Epoch [2/30], Batch [2500/6000], Loss: 0.2213
Epoch [2/30], Batch [2600/6000], Loss: 0.3401
Epoch [2/30], Batch [2700/6000], Loss: 0.2211
Epoch [2/30], Batch [2800/6000], Loss: 0.2349
Epoch [2/30], Batch [2900/6000], Loss: 0.2786
Epoch [2/30], Batch [3000/6000], Loss: 0.2595
Epoch [2/30], Batch [3100/6000], Loss: 0.2849
Epoch [2/30], Batch [3200/6000], Loss: 0.2951
Epoch [2/30], Batch [3300/6000], Loss: 0.2564
Epoch [2/30], Batch [3400/6000], Loss: 0.2492
Epoch [2/30], Batch [3500/6000], Loss: 0.2592
Epoch [2/30], Batch [3600/6000], Loss: 0.2127
Epoch [2/30], Batch [3700/6000], Loss: 0.2041
Epoch [2/30], Batch [3800/6000], Loss: 0.2318
Epoch [2/30], Batch [3900/6000], Loss: 0.2808
Epoch [2/30], Batch [4000/6000], Loss: 0.2647
Epoch [2/30], Batch [4100/6000], Loss: 0.3095
Epoch [2/30], Batch [4200/6000], Loss: 0.3314
Epoch [2/30], Batch [4300/6000], Loss: 0.2695
Epoch [2/30], Batch [4400/6000], Loss: 0.2991
Epoch [2/30], Batch [4500/6000], Loss: 0.2419
Epoch [2/30], Batch [4600/6000], Loss: 0.3300
Epoch [2/30], Batch [4700/6000], Loss: 0.2754
Epoch [2/30], Batch [4800/6000], Loss: 0.3098
Epoch [2/30], Batch [4900/6000], Loss: 0.2977
Epoch [2/30], Batch [5000/6000], Loss: 0.2725
Epoch [2/30], Batch [5100/6000], Loss: 0.1729
Epoch [2/30], Batch [5200/6000], Loss: 0.2080
Epoch [2/30], Batch [5300/6000], Loss: 0.2563
Epoch [2/30], Batch [5400/6000], Loss: 0.5526
Epoch [2/30], Batch [5500/6000], Loss: 0.2060
Epoch [2/30], Batch [5600/6000], Loss: 0.2187
Epoch [2/30], Batch [5700/6000], Loss: 0.2285
Epoch [2/30], Batch [5800/6000], Loss: 0.1851
Epoch [2/30], Batch [5900/6000], Loss: 0.2407
Epoch [2/30], Loss: 0.2868
Epoch [3/30], Batch [0/6000], Loss: 0.3046
Epoch [3/30], Batch [100/6000], Loss: 0.3211
Epoch [3/30], Batch [200/6000], Loss: 0.2591
Epoch [3/30], Batch [300/6000], Loss: 0.3449
Epoch [3/30], Batch [400/6000], Loss: 0.2939
Epoch [3/30], Batch [500/6000], Loss: 0.1944
Epoch [3/30], Batch [600/6000], Loss: 0.2174
Epoch [3/30], Batch [700/6000], Loss: 0.2541
Epoch [3/30], Batch [800/6000], Loss: 0.1750
Epoch [3/30], Batch [900/6000], Loss: 0.2471
Epoch [3/30], Batch [1000/6000], Loss: 0.1791
Epoch [3/30], Batch [1100/6000], Loss: 0.1956
Epoch [3/30], Batch [1200/6000], Loss: 0.3335
Epoch [3/30], Batch [1300/6000], Loss: 0.2774
Epoch [3/30], Batch [1400/6000], Loss: 0.2107
Epoch [3/30], Batch [1500/6000], Loss: 0.2131
Epoch [3/30], Batch [1600/6000], Loss: 0.3507
Epoch [3/30], Batch [1700/6000], Loss: 0.3784
Epoch [3/30], Batch [1800/6000], Loss: 0.3277
Epoch [3/30], Batch [1900/6000], Loss: 0.4055
Epoch [3/30], Batch [2000/6000], Loss: 0.2409
Epoch [3/30], Batch [2100/6000], Loss: 0.2917
Epoch [3/30], Batch [2200/6000], Loss: 0.3499
Epoch [3/30], Batch [2300/6000], Loss: 0.2508
Epoch [3/30], Batch [2400/6000], Loss: 0.2587
Epoch [3/30], Batch [2500/6000], Loss: 0.2464
Epoch [3/30], Batch [2600/6000], Loss: 0.1983
Epoch [3/30], Batch [2700/6000], Loss: 0.2442
Epoch [3/30], Batch [2800/6000], Loss: 0.2386
Epoch [3/30], Batch [2900/6000], Loss: 0.1971
Epoch [3/30], Batch [3000/6000], Loss: 0.3195
Epoch [3/30], Batch [3100/6000], Loss: 0.2450
Epoch [3/30], Batch [3200/6000], Loss: 0.2311
Epoch [3/30], Batch [3300/6000], Loss: 0.2494
Epoch [3/30], Batch [3400/6000], Loss: 0.2133
Epoch [3/30], Batch [3500/6000], Loss: 0.2124
Epoch [3/30], Batch [3600/6000], Loss: 0.3595
Epoch [3/30], Batch [3700/6000], Loss: 0.1754
Epoch [3/30], Batch [3800/6000], Loss: 0.3028
Epoch [3/30], Batch [3900/6000], Loss: 0.1685
Epoch [3/30], Batch [4000/6000], Loss: 0.2707
Epoch [3/30], Batch [4100/6000], Loss: 0.1818
Epoch [3/30], Batch [4200/6000], Loss: 0.1894
Epoch [3/30], Batch [4300/6000], Loss: 0.1880
Epoch [3/30], Batch [4400/6000], Loss: 0.2493
Epoch [3/30], Batch [4500/6000], Loss: 0.3783
Epoch [3/30], Batch [4600/6000], Loss: 0.2783
Epoch [3/30], Batch [4700/6000], Loss: 0.1969
Epoch [3/30], Batch [4800/6000], Loss: 0.4047
Epoch [3/30], Batch [4900/6000], Loss: 0.2241
Epoch [3/30], Batch [5000/6000], Loss: 0.2515
Epoch [3/30], Batch [5100/6000], Loss: 0.2050
Epoch [3/30], Batch [5200/6000], Loss: 0.1962
Epoch [3/30], Batch [5300/6000], Loss: 0.2475
Epoch [3/30], Batch [5400/6000], Loss: 0.1917
Epoch [3/30], Batch [5500/6000], Loss: 0.2323
Epoch [3/30], Batch [5600/6000], Loss: 0.2146
Epoch [3/30], Batch [5700/6000], Loss: 0.3473
Epoch [3/30], Batch [5800/6000], Loss: 0.3889
Epoch [3/30], Batch [5900/6000], Loss: 0.2424
Epoch [3/30], Loss: 0.2498
Epoch [4/30], Batch [0/6000], Loss: 0.3692
Epoch [4/30], Batch [100/6000], Loss: 0.3610
Epoch [4/30], Batch [200/6000], Loss: 0.1772
Epoch [4/30], Batch [300/6000], Loss: 0.2058
Epoch [4/30], Batch [400/6000], Loss: 0.1725
Epoch [4/30], Batch [500/6000], Loss: 0.1889
Epoch [4/30], Batch [600/6000], Loss: 0.1824
Epoch [4/30], Batch [700/6000], Loss: 0.2270
Epoch [4/30], Batch [800/6000], Loss: 0.1752
Epoch [4/30], Batch [900/6000], Loss: 0.2465
Epoch [4/30], Batch [1000/6000], Loss: 0.1903
Epoch [4/30], Batch [1100/6000], Loss: 0.2224
Epoch [4/30], Batch [1200/6000], Loss: 0.2334
Epoch [4/30], Batch [1300/6000], Loss: 0.1908
Epoch [4/30], Batch [1400/6000], Loss: 0.1687
Epoch [4/30], Batch [1500/6000], Loss: 0.1730
Epoch [4/30], Batch [1600/6000], Loss: 0.3527
Epoch [4/30], Batch [1700/6000], Loss: 0.1808
Epoch [4/30], Batch [1800/6000], Loss: 0.1964
Epoch [4/30], Batch [1900/6000], Loss: 0.2352
Epoch [4/30], Batch [2000/6000], Loss: 0.1654
Epoch [4/30], Batch [2100/6000], Loss: 0.2909
Epoch [4/30], Batch [2200/6000], Loss: 0.2703
Epoch [4/30], Batch [2300/6000], Loss: 0.2024
Epoch [4/30], Batch [2400/6000], Loss: 0.2460
Epoch [4/30], Batch [2500/6000], Loss: 0.1629
Epoch [4/30], Batch [2600/6000], Loss: 0.2261
Epoch [4/30], Batch [2700/6000], Loss: 0.1646
Epoch [4/30], Batch [2800/6000], Loss: 0.1970
Epoch [4/30], Batch [2900/6000], Loss: 0.1649
Epoch [4/30], Batch [3000/6000], Loss: 0.1736
Epoch [4/30], Batch [3100/6000], Loss: 0.1513
Epoch [4/30], Batch [3200/6000], Loss: 0.1813
Epoch [4/30], Batch [3300/6000], Loss: 0.2341
Epoch [4/30], Batch [3400/6000], Loss: 0.1899
Epoch [4/30], Batch [3500/6000], Loss: 0.2706
Epoch [4/30], Batch [3600/6000], Loss: 0.2467
Epoch [4/30], Batch [3700/6000], Loss: 0.1823
Epoch [4/30], Batch [3800/6000], Loss: 0.2166
Epoch [4/30], Batch [3900/6000], Loss: 0.2059
Epoch [4/30], Batch [4000/6000], Loss: 0.2327
Epoch [4/30], Batch [4100/6000], Loss: 0.1896
Epoch [4/30], Batch [4200/6000], Loss: 0.2050
Epoch [4/30], Batch [4300/6000], Loss: 0.1767
Epoch [4/30], Batch [4400/6000], Loss: 0.1848
Epoch [4/30], Batch [4500/6000], Loss: 0.1738
Epoch [4/30], Batch [4600/6000], Loss: 0.1694
Epoch [4/30], Batch [4700/6000], Loss: 0.1777
Epoch [4/30], Batch [4800/6000], Loss: 0.1659
Epoch [4/30], Batch [4900/6000], Loss: 0.1801
Epoch [4/30], Batch [5000/6000], Loss: 0.1562
Epoch [4/30], Batch [5100/6000], Loss: 0.2352
Epoch [4/30], Batch [5200/6000], Loss: 0.2066
Epoch [4/30], Batch [5300/6000], Loss: 0.2338
Epoch [4/30], Batch [5400/6000], Loss: 0.1830
Epoch [4/30], Batch [5500/6000], Loss: 0.1459
Epoch [4/30], Batch [5600/6000], Loss: 0.2794
Epoch [4/30], Batch [5700/6000], Loss: 0.1990
Epoch [4/30], Batch [5800/6000], Loss: 0.1908
Epoch [4/30], Batch [5900/6000], Loss: 0.3037
Epoch [4/30], Loss: 0.2220
Epoch [5/30], Batch [0/6000], Loss: 0.1873
Epoch [5/30], Batch [100/6000], Loss: 0.2348
Epoch [5/30], Batch [200/6000], Loss: 0.1901
Epoch [5/30], Batch [300/6000], Loss: 0.2123
Epoch [5/30], Batch [400/6000], Loss: 0.2097
Epoch [5/30], Batch [500/6000], Loss: 0.1482
Epoch [5/30], Batch [600/6000], Loss: 0.1763
Epoch [5/30], Batch [700/6000], Loss: 0.2240
Epoch [5/30], Batch [800/6000], Loss: 0.2739
Epoch [5/30], Batch [900/6000], Loss: 0.1449
Epoch [5/30], Batch [1000/6000], Loss: 0.1773
Epoch [5/30], Batch [1100/6000], Loss: 0.2354
Epoch [5/30], Batch [1200/6000], Loss: 0.3359
Epoch [5/30], Batch [1300/6000], Loss: 0.1809
Epoch [5/30], Batch [1400/6000], Loss: 0.1976
Epoch [5/30], Batch [1500/6000], Loss: 0.2449
Epoch [5/30], Batch [1600/6000], Loss: 0.3042
Epoch [5/30], Batch [1700/6000], Loss: 0.2092
Epoch [5/30], Batch [1800/6000], Loss: 0.3614
Epoch [5/30], Batch [1900/6000], Loss: 0.2529
Epoch [5/30], Batch [2000/6000], Loss: 0.1833
Epoch [5/30], Batch [2100/6000], Loss: 0.1537
Epoch [5/30], Batch [2200/6000], Loss: 0.2392
Epoch [5/30], Batch [2300/6000], Loss: 0.2720
Epoch [5/30], Batch [2400/6000], Loss: 0.1572
Epoch [5/30], Batch [2500/6000], Loss: 0.1621
Epoch [5/30], Batch [2600/6000], Loss: 0.1666
Epoch [5/30], Batch [2700/6000], Loss: 0.2056
Epoch [5/30], Batch [2800/6000], Loss: 0.2050
Epoch [5/30], Batch [2900/6000], Loss: 0.1492
Epoch [5/30], Batch [3000/6000], Loss: 0.1290
Epoch [5/30], Batch [3100/6000], Loss: 0.1835
Epoch [5/30], Batch [3200/6000], Loss: 0.2051
Epoch [5/30], Batch [3300/6000], Loss: 0.2032
Epoch [5/30], Batch [3400/6000], Loss: 0.1715
Epoch [5/30], Batch [3500/6000], Loss: 0.2123
Epoch [5/30], Batch [3600/6000], Loss: 0.1676
Epoch [5/30], Batch [3700/6000], Loss: 0.1359
Epoch [5/30], Batch [3800/6000], Loss: 0.1697
Epoch [5/30], Batch [3900/6000], Loss: 0.1674
Epoch [5/30], Batch [4000/6000], Loss: 0.1409
Epoch [5/30], Batch [4100/6000], Loss: 0.3487
Epoch [5/30], Batch [4200/6000], Loss: 0.1644
Epoch [5/30], Batch [4300/6000], Loss: 0.1651
Epoch [5/30], Batch [4400/6000], Loss: 0.2491
Epoch [5/30], Batch [4500/6000], Loss: 0.1795
Epoch [5/30], Batch [4600/6000], Loss: 0.1443
Epoch [5/30], Batch [4700/6000], Loss: 0.1785
Epoch [5/30], Batch [4800/6000], Loss: 0.1403
Epoch [5/30], Batch [4900/6000], Loss: 0.2221
Epoch [5/30], Batch [5000/6000], Loss: 0.2120
Epoch [5/30], Batch [5100/6000], Loss: 0.2229
Epoch [5/30], Batch [5200/6000], Loss: 0.1948
Epoch [5/30], Batch [5300/6000], Loss: 0.2349
Epoch [5/30], Batch [5400/6000], Loss: 0.2081
Epoch [5/30], Batch [5500/6000], Loss: 0.1301
Epoch [5/30], Batch [5600/6000], Loss: 0.1950
Epoch [5/30], Batch [5700/6000], Loss: 0.1750
Epoch [5/30], Batch [5800/6000], Loss: 0.1645
Epoch [5/30], Batch [5900/6000], Loss: 0.2321
Epoch [5/30], Loss: 0.2006
Epoch [6/30], Batch [0/6000], Loss: 0.1709
Epoch [6/30], Batch [100/6000], Loss: 0.1877
Epoch [6/30], Batch [200/6000], Loss: 0.1614
Epoch [6/30], Batch [300/6000], Loss: 0.1334
Epoch [6/30], Batch [400/6000], Loss: 0.2024
Epoch [6/30], Batch [500/6000], Loss: 0.2801
Epoch [6/30], Batch [600/6000], Loss: 0.1941
Epoch [6/30], Batch [700/6000], Loss: 0.1746
Epoch [6/30], Batch [800/6000], Loss: 0.2528
Epoch [6/30], Batch [900/6000], Loss: 0.1626
Epoch [6/30], Batch [1000/6000], Loss: 0.2437
Epoch [6/30], Batch [1100/6000], Loss: 0.1486
Epoch [6/30], Batch [1200/6000], Loss: 0.1600
Epoch [6/30], Batch [1300/6000], Loss: 0.1828
Epoch [6/30], Batch [1400/6000], Loss: 0.1575
Epoch [6/30], Batch [1500/6000], Loss: 0.1668
Epoch [6/30], Batch [1600/6000], Loss: 0.1327
Epoch [6/30], Batch [1700/6000], Loss: 0.1877
Epoch [6/30], Batch [1800/6000], Loss: 0.2982
Epoch [6/30], Batch [1900/6000], Loss: 0.1639
Epoch [6/30], Batch [2000/6000], Loss: 0.1446
Epoch [6/30], Batch [2100/6000], Loss: 0.1878
Epoch [6/30], Batch [2200/6000], Loss: 0.1540
Epoch [6/30], Batch [2300/6000], Loss: 0.1712
Epoch [6/30], Batch [2400/6000], Loss: 0.2803
Epoch [6/30], Batch [2500/6000], Loss: 0.1711
Epoch [6/30], Batch [2600/6000], Loss: 0.1165
Epoch [6/30], Batch [2700/6000], Loss: 0.1752
Epoch [6/30], Batch [2800/6000], Loss: 0.1505
Epoch [6/30], Batch [2900/6000], Loss: 0.1857
Epoch [6/30], Batch [3000/6000], Loss: 0.2214
Epoch [6/30], Batch [3100/6000], Loss: 0.1936
Epoch [6/30], Batch [3200/6000], Loss: 0.1355
Epoch [6/30], Batch [3300/6000], Loss: 0.2212
Epoch [6/30], Batch [3400/6000], Loss: 0.1695
Epoch [6/30], Batch [3500/6000], Loss: 0.1950
Epoch [6/30], Batch [3600/6000], Loss: 0.1575
Epoch [6/30], Batch [3700/6000], Loss: 0.1375
Epoch [6/30], Batch [3800/6000], Loss: 0.3109
Epoch [6/30], Batch [3900/6000], Loss: 0.1642
Epoch [6/30], Batch [4000/6000], Loss: 0.2589
Epoch [6/30], Batch [4100/6000], Loss: 0.1305
Epoch [6/30], Batch [4200/6000], Loss: 0.1464
Epoch [6/30], Batch [4300/6000], Loss: 0.2233
Epoch [6/30], Batch [4400/6000], Loss: 0.2218
Epoch [6/30], Batch [4500/6000], Loss: 0.1333
Epoch [6/30], Batch [4600/6000], Loss: 0.1654
Epoch [6/30], Batch [4700/6000], Loss: 0.2344
Epoch [6/30], Batch [4800/6000], Loss: 0.1736
Epoch [6/30], Batch [4900/6000], Loss: 0.1137
Epoch [6/30], Batch [5000/6000], Loss: 0.1515
Epoch [6/30], Batch [5100/6000], Loss: 0.1667
Epoch [6/30], Batch [5200/6000], Loss: 0.1643
Epoch [6/30], Batch [5300/6000], Loss: 0.1973
Epoch [6/30], Batch [5400/6000], Loss: 0.1536
Epoch [6/30], Batch [5500/6000], Loss: 0.1788
Epoch [6/30], Batch [5600/6000], Loss: 0.1502
Epoch [6/30], Batch [5700/6000], Loss: 0.2456
Epoch [6/30], Batch [5800/6000], Loss: 0.1415
Epoch [6/30], Batch [5900/6000], Loss: 0.2303
Epoch [6/30], Loss: 0.1865
Epoch [7/30], Batch [0/6000], Loss: 0.1895
Epoch [7/30], Batch [100/6000], Loss: 0.1797
Epoch [7/30], Batch [200/6000], Loss: 0.1988
Epoch [7/30], Batch [300/6000], Loss: 0.1432
Epoch [7/30], Batch [400/6000], Loss: 0.1445
Epoch [7/30], Batch [500/6000], Loss: 0.1524
Epoch [7/30], Batch [600/6000], Loss: 0.2082
Epoch [7/30], Batch [700/6000], Loss: 0.1388
Epoch [7/30], Batch [800/6000], Loss: 0.2690
Epoch [7/30], Batch [900/6000], Loss: 0.1420
Epoch [7/30], Batch [1000/6000], Loss: 0.1574
Epoch [7/30], Batch [1100/6000], Loss: 0.1869
Epoch [7/30], Batch [1200/6000], Loss: 0.1497
Epoch [7/30], Batch [1300/6000], Loss: 0.1219
Epoch [7/30], Batch [1400/6000], Loss: 0.1823
Epoch [7/30], Batch [1500/6000], Loss: 0.1524
Epoch [7/30], Batch [1600/6000], Loss: 0.2271
Epoch [7/30], Batch [1700/6000], Loss: 0.1498
Epoch [7/30], Batch [1800/6000], Loss: 0.1691
Epoch [7/30], Batch [1900/6000], Loss: 0.1644
Epoch [7/30], Batch [2000/6000], Loss: 0.1258
Epoch [7/30], Batch [2100/6000], Loss: 0.1314
Epoch [7/30], Batch [2200/6000], Loss: 0.2366
Epoch [7/30], Batch [2300/6000], Loss: 0.1306
Epoch [7/30], Batch [2400/6000], Loss: 0.1366
Epoch [7/30], Batch [2500/6000], Loss: 0.1512
Epoch [7/30], Batch [2600/6000], Loss: 0.1648
Epoch [7/30], Batch [2700/6000], Loss: 0.1438
Epoch [7/30], Batch [2800/6000], Loss: 0.1898
Epoch [7/30], Batch [2900/6000], Loss: 0.1531
Epoch [7/30], Batch [3000/6000], Loss: 0.2155
Epoch [7/30], Batch [3100/6000], Loss: 0.1330
Epoch [7/30], Batch [3200/6000], Loss: 0.2647
Epoch [7/30], Batch [3300/6000], Loss: 0.1660
Epoch [7/30], Batch [3400/6000], Loss: 0.1601
Epoch [7/30], Batch [3500/6000], Loss: 0.1523
Epoch [7/30], Batch [3600/6000], Loss: 0.1506
Epoch [7/30], Batch [3700/6000], Loss: 0.1662
Epoch [7/30], Batch [3800/6000], Loss: 0.1026
Epoch [7/30], Batch [3900/6000], Loss: 0.1675
Epoch [7/30], Batch [4000/6000], Loss: 0.1463
Epoch [7/30], Batch [4100/6000], Loss: 0.1505
Epoch [7/30], Batch [4200/6000], Loss: 0.1110
Epoch [7/30], Batch [4300/6000], Loss: 0.1361
Epoch [7/30], Batch [4400/6000], Loss: 0.1529
Epoch [7/30], Batch [4500/6000], Loss: 0.1441
Epoch [7/30], Batch [4600/6000], Loss: 0.1233
Epoch [7/30], Batch [4700/6000], Loss: 0.1756
Epoch [7/30], Batch [4800/6000], Loss: 0.1813
Epoch [7/30], Batch [4900/6000], Loss: 0.1545
Epoch [7/30], Batch [5000/6000], Loss: 0.1855
Epoch [7/30], Batch [5100/6000], Loss: 0.1225
Epoch [7/30], Batch [5200/6000], Loss: 0.1487
Epoch [7/30], Batch [5300/6000], Loss: 0.1499
Epoch [7/30], Batch [5400/6000], Loss: 0.1417
Epoch [7/30], Batch [5500/6000], Loss: 0.1482
Epoch [7/30], Batch [5600/6000], Loss: 0.1490
Epoch [7/30], Batch [5700/6000], Loss: 0.1709
Epoch [7/30], Batch [5800/6000], Loss: 0.2668
Epoch [7/30], Batch [5900/6000], Loss: 0.1516
Epoch [7/30], Loss: 0.1755
Epoch [8/30], Batch [0/6000], Loss: 0.1266
Epoch [8/30], Batch [100/6000], Loss: 0.1578
Epoch [8/30], Batch [200/6000], Loss: 0.1531
Epoch [8/30], Batch [300/6000], Loss: 0.1433
Epoch [8/30], Batch [400/6000], Loss: 0.1399
Epoch [8/30], Batch [500/6000], Loss: 0.1873
Epoch [8/30], Batch [600/6000], Loss: 0.1526
Epoch [8/30], Batch [700/6000], Loss: 0.1529
Epoch [8/30], Batch [800/6000], Loss: 0.1907
Epoch [8/30], Batch [900/6000], Loss: 0.1169
Epoch [8/30], Batch [1000/6000], Loss: 0.1320
Epoch [8/30], Batch [1100/6000], Loss: 0.1575
Epoch [8/30], Batch [1200/6000], Loss: 0.1287
Epoch [8/30], Batch [1300/6000], Loss: 0.1336
Epoch [8/30], Batch [1400/6000], Loss: 0.2162
Epoch [8/30], Batch [1500/6000], Loss: 0.2386
Epoch [8/30], Batch [1600/6000], Loss: 0.2127
Epoch [8/30], Batch [1700/6000], Loss: 0.1188
Epoch [8/30], Batch [1800/6000], Loss: 0.1573
Epoch [8/30], Batch [1900/6000], Loss: 0.1462
Epoch [8/30], Batch [2000/6000], Loss: 0.2156
Epoch [8/30], Batch [2100/6000], Loss: 0.1513
Epoch [8/30], Batch [2200/6000], Loss: 0.1592
Epoch [8/30], Batch [2300/6000], Loss: 0.1443
Epoch [8/30], Batch [2400/6000], Loss: 0.1344
Epoch [8/30], Batch [2500/6000], Loss: 0.2778
Epoch [8/30], Batch [2600/6000], Loss: 0.1130
Epoch [8/30], Batch [2700/6000], Loss: 0.1499
Epoch [8/30], Batch [2800/6000], Loss: 0.1754
Epoch [8/30], Batch [2900/6000], Loss: 0.1467
Epoch [8/30], Batch [3000/6000], Loss: 0.1528
Epoch [8/30], Batch [3100/6000], Loss: 0.1313
Epoch [8/30], Batch [3200/6000], Loss: 0.1236
Epoch [8/30], Batch [3300/6000], Loss: 0.1829
Epoch [8/30], Batch [3400/6000], Loss: 0.1561
Epoch [8/30], Batch [3500/6000], Loss: 0.1407
Epoch [8/30], Batch [3600/6000], Loss: 0.1443
Epoch [8/30], Batch [3700/6000], Loss: 0.2304
Epoch [8/30], Batch [3800/6000], Loss: 0.1264
Epoch [8/30], Batch [3900/6000], Loss: 0.1305
Epoch [8/30], Batch [4000/6000], Loss: 0.1580
Epoch [8/30], Batch [4100/6000], Loss: 0.1589
Epoch [8/30], Batch [4200/6000], Loss: 0.1591
Epoch [8/30], Batch [4300/6000], Loss: 0.1758
Epoch [8/30], Batch [4400/6000], Loss: 0.1744
Epoch [8/30], Batch [4500/6000], Loss: 0.1131
Epoch [8/30], Batch [4600/6000], Loss: 0.1835
Epoch [8/30], Batch [4700/6000], Loss: 0.3160
Epoch [8/30], Batch [4800/6000], Loss: 0.1832
Epoch [8/30], Batch [4900/6000], Loss: 0.1416
Epoch [8/30], Batch [5000/6000], Loss: 0.2258
Epoch [8/30], Batch [5100/6000], Loss: 0.1186
Epoch [8/30], Batch [5200/6000], Loss: 0.1681
Epoch [8/30], Batch [5300/6000], Loss: 0.2504
Epoch [8/30], Batch [5400/6000], Loss: 0.1386
Epoch [8/30], Batch [5500/6000], Loss: 0.1140
Epoch [8/30], Batch [5600/6000], Loss: 0.1283
Epoch [8/30], Batch [5700/6000], Loss: 0.1289
Epoch [8/30], Batch [5800/6000], Loss: 0.1688
Epoch [8/30], Batch [5900/6000], Loss: 0.1912
Epoch [8/30], Loss: 0.1674
Epoch [9/30], Batch [0/6000], Loss: 0.2851
Epoch [9/30], Batch [100/6000], Loss: 0.1438
Epoch [9/30], Batch [200/6000], Loss: 0.1189
Epoch [9/30], Batch [300/6000], Loss: 0.1846
Epoch [9/30], Batch [400/6000], Loss: 0.1951
Epoch [9/30], Batch [500/6000], Loss: 0.1502
Epoch [9/30], Batch [600/6000], Loss: 0.1970
Epoch [9/30], Batch [700/6000], Loss: 0.2747
Epoch [9/30], Batch [800/6000], Loss: 0.1428
Epoch [9/30], Batch [900/6000], Loss: 0.1499
Epoch [9/30], Batch [1000/6000], Loss: 0.1341
Epoch [9/30], Batch [1100/6000], Loss: 0.1891
Epoch [9/30], Batch [1200/6000], Loss: 0.1960
Epoch [9/30], Batch [1300/6000], Loss: 0.2298
Epoch [9/30], Batch [1400/6000], Loss: 0.1391
Epoch [9/30], Batch [1500/6000], Loss: 0.2320
Epoch [9/30], Batch [1600/6000], Loss: 0.1645
Epoch [9/30], Batch [1700/6000], Loss: 0.1436
Epoch [9/30], Batch [1800/6000], Loss: 0.1684
Epoch [9/30], Batch [1900/6000], Loss: 0.1515
Epoch [9/30], Batch [2000/6000], Loss: 0.1141
Epoch [9/30], Batch [2100/6000], Loss: 0.1549
Epoch [9/30], Batch [2200/6000], Loss: 0.2117
Epoch [9/30], Batch [2300/6000], Loss: 0.1616
Epoch [9/30], Batch [2400/6000], Loss: 0.1964
Epoch [9/30], Batch [2500/6000], Loss: 0.2221
Epoch [9/30], Batch [2600/6000], Loss: 0.1399
Epoch [9/30], Batch [2700/6000], Loss: 0.1797
Epoch [9/30], Batch [2800/6000], Loss: 0.2208
Epoch [9/30], Batch [2900/6000], Loss: 0.1224
Epoch [9/30], Batch [3000/6000], Loss: 0.1467
Epoch [9/30], Batch [3100/6000], Loss: 0.2150
Epoch [9/30], Batch [3200/6000], Loss: 0.2268
Epoch [9/30], Batch [3300/6000], Loss: 0.1232
Epoch [9/30], Batch [3400/6000], Loss: 0.1921
Epoch [9/30], Batch [3500/6000], Loss: 0.1475
Epoch [9/30], Batch [3600/6000], Loss: 0.1204
Epoch [9/30], Batch [3700/6000], Loss: 0.2149
Epoch [9/30], Batch [3800/6000], Loss: 0.1227
Epoch [9/30], Batch [3900/6000], Loss: 0.1225
Epoch [9/30], Batch [4000/6000], Loss: 0.0990
Epoch [9/30], Batch [4100/6000], Loss: 0.1107
Epoch [9/30], Batch [4200/6000], Loss: 0.1445
Epoch [9/30], Batch [4300/6000], Loss: 0.1432
Epoch [9/30], Batch [4400/6000], Loss: 0.1665
Epoch [9/30], Batch [4500/6000], Loss: 0.1426
Epoch [9/30], Batch [4600/6000], Loss: 0.1652
Epoch [9/30], Batch [4700/6000], Loss: 0.1125
Epoch [9/30], Batch [4800/6000], Loss: 0.1256
Epoch [9/30], Batch [4900/6000], Loss: 0.1335
Epoch [9/30], Batch [5000/6000], Loss: 0.1206
Epoch [9/30], Batch [5100/6000], Loss: 0.1506
Epoch [9/30], Batch [5200/6000], Loss: 0.2305
Epoch [9/30], Batch [5300/6000], Loss: 0.1755
Epoch [9/30], Batch [5400/6000], Loss: 0.2140
Epoch [9/30], Batch [5500/6000], Loss: 0.1107
Epoch [9/30], Batch [5600/6000], Loss: 0.1219
Epoch [9/30], Batch [5700/6000], Loss: 0.1270
Epoch [9/30], Batch [5800/6000], Loss: 0.1406
Epoch [9/30], Batch [5900/6000], Loss: 0.1292
Epoch [9/30], Loss: 0.1597
Epoch [10/30], Batch [0/6000], Loss: 0.1707
Epoch [10/30], Batch [100/6000], Loss: 0.1419
Epoch [10/30], Batch [200/6000], Loss: 0.1095
Epoch [10/30], Batch [300/6000], Loss: 0.1280
Epoch [10/30], Batch [400/6000], Loss: 0.2028
Epoch [10/30], Batch [500/6000], Loss: 0.1062
Epoch [10/30], Batch [600/6000], Loss: 0.1517
Epoch [10/30], Batch [700/6000], Loss: 0.1305
Epoch [10/30], Batch [800/6000], Loss: 0.1369
Epoch [10/30], Batch [900/6000], Loss: 0.2790
Epoch [10/30], Batch [1000/6000], Loss: 0.1342
Epoch [10/30], Batch [1100/6000], Loss: 0.1494
Epoch [10/30], Batch [1200/6000], Loss: 0.1974
Epoch [10/30], Batch [1300/6000], Loss: 0.1518
Epoch [10/30], Batch [1400/6000], Loss: 0.1686
Epoch [10/30], Batch [1500/6000], Loss: 0.1451
Epoch [10/30], Batch [1600/6000], Loss: 0.2009
Epoch [10/30], Batch [1700/6000], Loss: 0.1818
Epoch [10/30], Batch [1800/6000], Loss: 0.1943
Epoch [10/30], Batch [1900/6000], Loss: 0.1351
Epoch [10/30], Batch [2000/6000], Loss: 0.1302
Epoch [10/30], Batch [2100/6000], Loss: 0.0999
Epoch [10/30], Batch [2200/6000], Loss: 0.1355
Epoch [10/30], Batch [2300/6000], Loss: 0.1421
Epoch [10/30], Batch [2400/6000], Loss: 0.1365
Epoch [10/30], Batch [2500/6000], Loss: 0.1138
Epoch [10/30], Batch [2600/6000], Loss: 0.1448
Epoch [10/30], Batch [2700/6000], Loss: 0.1239
Epoch [10/30], Batch [2800/6000], Loss: 0.1346
Epoch [10/30], Batch [2900/6000], Loss: 0.1502
Epoch [10/30], Batch [3000/6000], Loss: 0.2477
Epoch [10/30], Batch [3100/6000], Loss: 0.1476
Epoch [10/30], Batch [3200/6000], Loss: 0.1372
Epoch [10/30], Batch [3300/6000], Loss: 0.1347
Epoch [10/30], Batch [3400/6000], Loss: 0.1767
Epoch [10/30], Batch [3500/6000], Loss: 0.1709
Epoch [10/30], Batch [3600/6000], Loss: 0.1463
Epoch [10/30], Batch [3700/6000], Loss: 0.1418
Epoch [10/30], Batch [3800/6000], Loss: 0.1389
Epoch [10/30], Batch [3900/6000], Loss: 0.1790
Epoch [10/30], Batch [4000/6000], Loss: 0.1201
Epoch [10/30], Batch [4100/6000], Loss: 0.1334
Epoch [10/30], Batch [4200/6000], Loss: 0.1415
Epoch [10/30], Batch [4300/6000], Loss: 0.1205
Epoch [10/30], Batch [4400/6000], Loss: 0.1373
Epoch [10/30], Batch [4500/6000], Loss: 0.1204
Epoch [10/30], Batch [4600/6000], Loss: 0.1395
Epoch [10/30], Batch [4700/6000], Loss: 0.1597
Epoch [10/30], Batch [4800/6000], Loss: 0.2302
Epoch [10/30], Batch [4900/6000], Loss: 0.2355
Epoch [10/30], Batch [5000/6000], Loss: 0.1430
Epoch [10/30], Batch [5100/6000], Loss: 0.1168
Epoch [10/30], Batch [5200/6000], Loss: 0.1350
Epoch [10/30], Batch [5300/6000], Loss: 0.1344
Epoch [10/30], Batch [5400/6000], Loss: 0.1191
Epoch [10/30], Batch [5500/6000], Loss: 0.1405
Epoch [10/30], Batch [5600/6000], Loss: 0.1332
Epoch [10/30], Batch [5700/6000], Loss: 0.1463
Epoch [10/30], Batch [5800/6000], Loss: 0.1161
Epoch [10/30], Batch [5900/6000], Loss: 0.2049
Epoch [10/30], Loss: 0.1524
Epoch [11/30], Batch [0/6000], Loss: 0.0989
Epoch [11/30], Batch [100/6000], Loss: 0.1621
Epoch [11/30], Batch [200/6000], Loss: 0.1061
Epoch [11/30], Batch [300/6000], Loss: 0.2034
Epoch [11/30], Batch [400/6000], Loss: 0.1248
Epoch [11/30], Batch [500/6000], Loss: 0.0838
Epoch [11/30], Batch [600/6000], Loss: 0.2147
Epoch [11/30], Batch [700/6000], Loss: 0.1104
Epoch [11/30], Batch [800/6000], Loss: 0.1892
Epoch [11/30], Batch [900/6000], Loss: 0.1209
Epoch [11/30], Batch [1000/6000], Loss: 0.1928
Epoch [11/30], Batch [1100/6000], Loss: 0.1903
Epoch [11/30], Batch [1200/6000], Loss: 0.1366
Epoch [11/30], Batch [1300/6000], Loss: 0.1061
Epoch [11/30], Batch [1400/6000], Loss: 0.1581
Epoch [11/30], Batch [1500/6000], Loss: 0.1002
Epoch [11/30], Batch [1600/6000], Loss: 0.1223
Epoch [11/30], Batch [1700/6000], Loss: 0.1308
Epoch [11/30], Batch [1800/6000], Loss: 0.1332
Epoch [11/30], Batch [1900/6000], Loss: 0.1400
Epoch [11/30], Batch [2000/6000], Loss: 0.1184
Epoch [11/30], Batch [2100/6000], Loss: 0.1028
Epoch [11/30], Batch [2200/6000], Loss: 0.1564
Epoch [11/30], Batch [2300/6000], Loss: 0.1495
Epoch [11/30], Batch [2400/6000], Loss: 0.1162
Epoch [11/30], Batch [2500/6000], Loss: 0.1553
Epoch [11/30], Batch [2600/6000], Loss: 0.1601
Epoch [11/30], Batch [2700/6000], Loss: 0.1965
Epoch [11/30], Batch [2800/6000], Loss: 0.1404
Epoch [11/30], Batch [2900/6000], Loss: 0.1548
Epoch [11/30], Batch [3000/6000], Loss: 0.1673
Epoch [11/30], Batch [3100/6000], Loss: 0.1439
Epoch [11/30], Batch [3200/6000], Loss: 0.1484
Epoch [11/30], Batch [3300/6000], Loss: 0.1479
Epoch [11/30], Batch [3400/6000], Loss: 0.1342
Epoch [11/30], Batch [3500/6000], Loss: 0.1147
Epoch [11/30], Batch [3600/6000], Loss: 0.1553
Epoch [11/30], Batch [3700/6000], Loss: 0.1474
Epoch [11/30], Batch [3800/6000], Loss: 0.1568
Epoch [11/30], Batch [3900/6000], Loss: 0.1610
Epoch [11/30], Batch [4000/6000], Loss: 0.1031
Epoch [11/30], Batch [4100/6000], Loss: 0.1018
Epoch [11/30], Batch [4200/6000], Loss: 0.1572
Epoch [11/30], Batch [4300/6000], Loss: 0.1070
Epoch [11/30], Batch [4400/6000], Loss: 0.1258
Epoch [11/30], Batch [4500/6000], Loss: 0.1630
Epoch [11/30], Batch [4600/6000], Loss: 0.1731
Epoch [11/30], Batch [4700/6000], Loss: 0.1399
Epoch [11/30], Batch [4800/6000], Loss: 0.2128
Epoch [11/30], Batch [4900/6000], Loss: 0.1743
Epoch [11/30], Batch [5000/6000], Loss: 0.1975
Epoch [11/30], Batch [5100/6000], Loss: 0.1238
Epoch [11/30], Batch [5200/6000], Loss: 0.1382
Epoch [11/30], Batch [5300/6000], Loss: 0.1578
Epoch [11/30], Batch [5400/6000], Loss: 0.1270
Epoch [11/30], Batch [5500/6000], Loss: 0.1608
Epoch [11/30], Batch [5600/6000], Loss: 0.1377
Epoch [11/30], Batch [5700/6000], Loss: 0.1588
Epoch [11/30], Batch [5800/6000], Loss: 0.1259
Epoch [11/30], Batch [5900/6000], Loss: 0.1669
Epoch [11/30], Loss: 0.1461
Epoch [12/30], Batch [0/6000], Loss: 0.1107
Epoch [12/30], Batch [100/6000], Loss: 0.1833
Epoch [12/30], Batch [200/6000], Loss: 0.1993
Epoch [12/30], Batch [300/6000], Loss: 0.1070
Epoch [12/30], Batch [400/6000], Loss: 0.1937
Epoch [12/30], Batch [500/6000], Loss: 0.1066
Epoch [12/30], Batch [600/6000], Loss: 0.1034
Epoch [12/30], Batch [700/6000], Loss: 0.1019
Epoch [12/30], Batch [800/6000], Loss: 0.1741
Epoch [12/30], Batch [900/6000], Loss: 0.1148
Epoch [12/30], Batch [1000/6000], Loss: 0.1308
Epoch [12/30], Batch [1100/6000], Loss: 0.1129
Epoch [12/30], Batch [1200/6000], Loss: 0.1073
Epoch [12/30], Batch [1300/6000], Loss: 0.1878
Epoch [12/30], Batch [1400/6000], Loss: 0.2120
Epoch [12/30], Batch [1500/6000], Loss: 0.1154
Epoch [12/30], Batch [1600/6000], Loss: 0.1168
Epoch [12/30], Batch [1700/6000], Loss: 0.1646
Epoch [12/30], Batch [1800/6000], Loss: 0.1863
Epoch [12/30], Batch [1900/6000], Loss: 0.2409
Epoch [12/30], Batch [2000/6000], Loss: 0.2073
Epoch [12/30], Batch [2100/6000], Loss: 0.1152
Epoch [12/30], Batch [2200/6000], Loss: 0.1191
Epoch [12/30], Batch [2300/6000], Loss: 0.1051
Epoch [12/30], Batch [2400/6000], Loss: 0.1321
Epoch [12/30], Batch [2500/6000], Loss: 0.1180
Epoch [12/30], Batch [2600/6000], Loss: 0.2801
Epoch [12/30], Batch [2700/6000], Loss: 0.1558
Epoch [12/30], Batch [2800/6000], Loss: 0.1145
Epoch [12/30], Batch [2900/6000], Loss: 0.1130
Epoch [12/30], Batch [3000/6000], Loss: 0.1132
Epoch [12/30], Batch [3100/6000], Loss: 0.1143
Epoch [12/30], Batch [3200/6000], Loss: 0.1133
Epoch [12/30], Batch [3300/6000], Loss: 0.1191
Epoch [12/30], Batch [3400/6000], Loss: 0.1298
Epoch [12/30], Batch [3500/6000], Loss: 0.2242
Epoch [12/30], Batch [3600/6000], Loss: 0.1260
Epoch [12/30], Batch [3700/6000], Loss: 0.1461
Epoch [12/30], Batch [3800/6000], Loss: 0.1931
Epoch [12/30], Batch [3900/6000], Loss: 0.1305
Epoch [12/30], Batch [4000/6000], Loss: 0.1394
Epoch [12/30], Batch [4100/6000], Loss: 0.1340
Epoch [12/30], Batch [4200/6000], Loss: 0.1370
Epoch [12/30], Batch [4300/6000], Loss: 0.1138
Epoch [12/30], Batch [4400/6000], Loss: 0.1136
Epoch [12/30], Batch [4500/6000], Loss: 0.2497
Epoch [12/30], Batch [4600/6000], Loss: 0.0896
Epoch [12/30], Batch [4700/6000], Loss: 0.1657
Epoch [12/30], Batch [4800/6000], Loss: 0.1309
Epoch [12/30], Batch [4900/6000], Loss: 0.1001
Epoch [12/30], Batch [5000/6000], Loss: 0.1041
Epoch [12/30], Batch [5100/6000], Loss: 0.1416
Epoch [12/30], Batch [5200/6000], Loss: 0.1466
Epoch [12/30], Batch [5300/6000], Loss: 0.1357
Epoch [12/30], Batch [5400/6000], Loss: 0.1847
Epoch [12/30], Batch [5500/6000], Loss: 0.2964
Epoch [12/30], Batch [5600/6000], Loss: 0.1126
Epoch [12/30], Batch [5700/6000], Loss: 0.2033
Epoch [12/30], Batch [5800/6000], Loss: 0.1355
Epoch [12/30], Batch [5900/6000], Loss: 0.1328
Epoch [12/30], Loss: 0.1409
Epoch [13/30], Batch [0/6000], Loss: 0.1401
Epoch [13/30], Batch [100/6000], Loss: 0.1256
Epoch [13/30], Batch [200/6000], Loss: 0.2391
Epoch [13/30], Batch [300/6000], Loss: 0.1127
Epoch [13/30], Batch [400/6000], Loss: 0.1976
Epoch [13/30], Batch [500/6000], Loss: 0.1150
Epoch [13/30], Batch [600/6000], Loss: 0.1786
Epoch [13/30], Batch [700/6000], Loss: 0.1170
Epoch [13/30], Batch [800/6000], Loss: 0.2915
Epoch [13/30], Batch [900/6000], Loss: 0.1345
Epoch [13/30], Batch [1000/6000], Loss: 0.2198
Epoch [13/30], Batch [1100/6000], Loss: 0.1176
Epoch [13/30], Batch [1200/6000], Loss: 0.1024
Epoch [13/30], Batch [1300/6000], Loss: 0.1128
Epoch [13/30], Batch [1400/6000], Loss: 0.1771
Epoch [13/30], Batch [1500/6000], Loss: 0.1222
Epoch [13/30], Batch [1600/6000], Loss: 0.1673
Epoch [13/30], Batch [1700/6000], Loss: 0.1885
Epoch [13/30], Batch [1800/6000], Loss: 0.1037
Epoch [13/30], Batch [1900/6000], Loss: 0.1482
Epoch [13/30], Batch [2000/6000], Loss: 0.1471
Epoch [13/30], Batch [2100/6000], Loss: 0.1117
Epoch [13/30], Batch [2200/6000], Loss: 0.2473
Epoch [13/30], Batch [2300/6000], Loss: 0.1825
Epoch [13/30], Batch [2400/6000], Loss: 0.1211
Epoch [13/30], Batch [2500/6000], Loss: 0.1072
Epoch [13/30], Batch [2600/6000], Loss: 0.2269
Epoch [13/30], Batch [2700/6000], Loss: 0.1563
Epoch [13/30], Batch [2800/6000], Loss: 0.1247
Epoch [13/30], Batch [2900/6000], Loss: 0.1201
Epoch [13/30], Batch [3000/6000], Loss: 0.1292
Epoch [13/30], Batch [3100/6000], Loss: 0.1996
Epoch [13/30], Batch [3200/6000], Loss: 0.0927
Epoch [13/30], Batch [3300/6000], Loss: 0.1462
Epoch [13/30], Batch [3400/6000], Loss: 0.1771
Epoch [13/30], Batch [3500/6000], Loss: 0.0980
Epoch [13/30], Batch [3600/6000], Loss: 0.1288
Epoch [13/30], Batch [3700/6000], Loss: 0.2222
Epoch [13/30], Batch [3800/6000], Loss: 0.1089
Epoch [13/30], Batch [3900/6000], Loss: 0.1299
Epoch [13/30], Batch [4000/6000], Loss: 0.1649
Epoch [13/30], Batch [4100/6000], Loss: 0.1358
Epoch [13/30], Batch [4200/6000], Loss: 0.1338
Epoch [13/30], Batch [4300/6000], Loss: 0.1300
Epoch [13/30], Batch [4400/6000], Loss: 0.1105
Epoch [13/30], Batch [4500/6000], Loss: 0.1375
Epoch [13/30], Batch [4600/6000], Loss: 0.1158
Epoch [13/30], Batch [4700/6000], Loss: 0.1116
Epoch [13/30], Batch [4800/6000], Loss: 0.1290
Epoch [13/30], Batch [4900/6000], Loss: 0.1617
Epoch [13/30], Batch [5000/6000], Loss: 0.1622
Epoch [13/30], Batch [5100/6000], Loss: 0.2633
Epoch [13/30], Batch [5200/6000], Loss: 0.1163
Epoch [13/30], Batch [5300/6000], Loss: 0.1104
Epoch [13/30], Batch [5400/6000], Loss: 0.1155
Epoch [13/30], Batch [5500/6000], Loss: 0.1286
Epoch [13/30], Batch [5600/6000], Loss: 0.1638
Epoch [13/30], Batch [5700/6000], Loss: 0.1008
Epoch [13/30], Batch [5800/6000], Loss: 0.1190
Epoch [13/30], Batch [5900/6000], Loss: 0.1341
Epoch [13/30], Loss: 0.1359
Epoch [14/30], Batch [0/6000], Loss: 0.1380
Epoch [14/30], Batch [100/6000], Loss: 0.1519
Epoch [14/30], Batch [200/6000], Loss: 0.1471
Epoch [14/30], Batch [300/6000], Loss: 0.1397
Epoch [14/30], Batch [400/6000], Loss: 0.1284
Epoch [14/30], Batch [500/6000], Loss: 0.0999
Epoch [14/30], Batch [600/6000], Loss: 0.1326
Epoch [14/30], Batch [700/6000], Loss: 0.0905
Epoch [14/30], Batch [800/6000], Loss: 0.1158
Epoch [14/30], Batch [900/6000], Loss: 0.1088
Epoch [14/30], Batch [1000/6000], Loss: 0.1183
Epoch [14/30], Batch [1100/6000], Loss: 0.1166
Epoch [14/30], Batch [1200/6000], Loss: 0.1309
Epoch [14/30], Batch [1300/6000], Loss: 0.0993
Epoch [14/30], Batch [1400/6000], Loss: 0.1045
Epoch [14/30], Batch [1500/6000], Loss: 0.1133
Epoch [14/30], Batch [1600/6000], Loss: 0.0950
Epoch [14/30], Batch [1700/6000], Loss: 0.1509
Epoch [14/30], Batch [1800/6000], Loss: 0.1185
Epoch [14/30], Batch [1900/6000], Loss: 0.1638
Epoch [14/30], Batch [2000/6000], Loss: 0.1271
Epoch [14/30], Batch [2100/6000], Loss: 0.1318
Epoch [14/30], Batch [2200/6000], Loss: 0.1327
Epoch [14/30], Batch [2300/6000], Loss: 0.1427
Epoch [14/30], Batch [2400/6000], Loss: 0.1218
Epoch [14/30], Batch [2500/6000], Loss: 0.1439
Epoch [14/30], Batch [2600/6000], Loss: 0.1337
Epoch [14/30], Batch [2700/6000], Loss: 0.2411
Epoch [14/30], Batch [2800/6000], Loss: 0.1448
Epoch [14/30], Batch [2900/6000], Loss: 0.1562
Epoch [14/30], Batch [3000/6000], Loss: 0.0935
Epoch [14/30], Batch [3100/6000], Loss: 0.1302
Epoch [14/30], Batch [3200/6000], Loss: 0.0771
Epoch [14/30], Batch [3300/6000], Loss: 0.1400
Epoch [14/30], Batch [3400/6000], Loss: 0.1534
Epoch [14/30], Batch [3500/6000], Loss: 0.1026
Epoch [14/30], Batch [3600/6000], Loss: 0.1413
Epoch [14/30], Batch [3700/6000], Loss: 0.1045
Epoch [14/30], Batch [3800/6000], Loss: 0.1397
Epoch [14/30], Batch [3900/6000], Loss: 0.1155
Epoch [14/30], Batch [4000/6000], Loss: 0.1091
Epoch [14/30], Batch [4100/6000], Loss: 0.1080
Epoch [14/30], Batch [4200/6000], Loss: 0.1113
Epoch [14/30], Batch [4300/6000], Loss: 0.1711
Epoch [14/30], Batch [4400/6000], Loss: 0.0875
Epoch [14/30], Batch [4500/6000], Loss: 0.1382
Epoch [14/30], Batch [4600/6000], Loss: 0.1230
Epoch [14/30], Batch [4700/6000], Loss: 0.1251
Epoch [14/30], Batch [4800/6000], Loss: 0.1371
Epoch [14/30], Batch [4900/6000], Loss: 0.1108
Epoch [14/30], Batch [5000/6000], Loss: 0.1577
Epoch [14/30], Batch [5100/6000], Loss: 0.0997
Epoch [14/30], Batch [5200/6000], Loss: 0.1115
Epoch [14/30], Batch [5300/6000], Loss: 0.1765
Epoch [14/30], Batch [5400/6000], Loss: 0.1334
Epoch [14/30], Batch [5500/6000], Loss: 0.1137
Epoch [14/30], Batch [5600/6000], Loss: 0.0832
Epoch [14/30], Batch [5700/6000], Loss: 0.1276
Epoch [14/30], Batch [5800/6000], Loss: 0.1062
Epoch [14/30], Batch [5900/6000], Loss: 0.2717
Epoch [14/30], Loss: 0.1321
Epoch [15/30], Batch [0/6000], Loss: 0.1017
Epoch [15/30], Batch [100/6000], Loss: 0.1050
Epoch [15/30], Batch [200/6000], Loss: 0.1409
Epoch [15/30], Batch [300/6000], Loss: 0.1191
Epoch [15/30], Batch [400/6000], Loss: 0.1033
Epoch [15/30], Batch [500/6000], Loss: 0.1122
Epoch [15/30], Batch [600/6000], Loss: 0.1252
Epoch [15/30], Batch [700/6000], Loss: 0.1028
Epoch [15/30], Batch [800/6000], Loss: 0.1340
Epoch [15/30], Batch [900/6000], Loss: 0.1909
Epoch [15/30], Batch [1000/6000], Loss: 0.0927
Epoch [15/30], Batch [1100/6000], Loss: 0.0985
Epoch [15/30], Batch [1200/6000], Loss: 0.0913
Epoch [15/30], Batch [1300/6000], Loss: 0.1024
Epoch [15/30], Batch [1400/6000], Loss: 0.1271
Epoch [15/30], Batch [1500/6000], Loss: 0.1248
Epoch [15/30], Batch [1600/6000], Loss: 0.1138
Epoch [15/30], Batch [1700/6000], Loss: 0.1525
Epoch [15/30], Batch [1800/6000], Loss: 0.1096
Epoch [15/30], Batch [1900/6000], Loss: 0.0974
Epoch [15/30], Batch [2000/6000], Loss: 0.1198
Epoch [15/30], Batch [2100/6000], Loss: 0.1695
Epoch [15/30], Batch [2200/6000], Loss: 0.1900
Epoch [15/30], Batch [2300/6000], Loss: 0.1423
Epoch [15/30], Batch [2400/6000], Loss: 0.1262
Epoch [15/30], Batch [2500/6000], Loss: 0.1462
Epoch [15/30], Batch [2600/6000], Loss: 0.1371
Epoch [15/30], Batch [2700/6000], Loss: 0.2044
Epoch [15/30], Batch [2800/6000], Loss: 0.1073
Epoch [15/30], Batch [2900/6000], Loss: 0.0947
Epoch [15/30], Batch [3000/6000], Loss: 0.1479
Epoch [15/30], Batch [3100/6000], Loss: 0.1003
Epoch [15/30], Batch [3200/6000], Loss: 0.1110
Epoch [15/30], Batch [3300/6000], Loss: 0.1755
Epoch [15/30], Batch [3400/6000], Loss: 0.1109
Epoch [15/30], Batch [3500/6000], Loss: 0.1340
Epoch [15/30], Batch [3600/6000], Loss: 0.1624
Epoch [15/30], Batch [3700/6000], Loss: 0.1092
Epoch [15/30], Batch [3800/6000], Loss: 0.1268
Epoch [15/30], Batch [3900/6000], Loss: 0.1329
Epoch [15/30], Batch [4000/6000], Loss: 0.1334
Epoch [15/30], Batch [4100/6000], Loss: 0.0909
Epoch [15/30], Batch [4200/6000], Loss: 0.1897
Epoch [15/30], Batch [4300/6000], Loss: 0.1291
Epoch [15/30], Batch [4400/6000], Loss: 0.1844
Epoch [15/30], Batch [4500/6000], Loss: 0.1317
Epoch [15/30], Batch [4600/6000], Loss: 0.1066
Epoch [15/30], Batch [4700/6000], Loss: 0.1061
Epoch [15/30], Batch [4800/6000], Loss: 0.1262
Epoch [15/30], Batch [4900/6000], Loss: 0.1007
Epoch [15/30], Batch [5000/6000], Loss: 0.1143
Epoch [15/30], Batch [5100/6000], Loss: 0.1200
Epoch [15/30], Batch [5200/6000], Loss: 0.1275
Epoch [15/30], Batch [5300/6000], Loss: 0.1033
Epoch [15/30], Batch [5400/6000], Loss: 0.1143
Epoch [15/30], Batch [5500/6000], Loss: 0.1363
Epoch [15/30], Batch [5600/6000], Loss: 0.1157
Epoch [15/30], Batch [5700/6000], Loss: 0.1935
Epoch [15/30], Batch [5800/6000], Loss: 0.1452
Epoch [15/30], Batch [5900/6000], Loss: 0.1059
Epoch [15/30], Loss: 0.1279
Epoch [16/30], Batch [0/6000], Loss: 0.1227
Epoch [16/30], Batch [100/6000], Loss: 0.1180
Epoch [16/30], Batch [200/6000], Loss: 0.0997
Epoch [16/30], Batch [300/6000], Loss: 0.1219
Epoch [16/30], Batch [400/6000], Loss: 0.1198
Epoch [16/30], Batch [500/6000], Loss: 0.1186
Epoch [16/30], Batch [600/6000], Loss: 0.0936
Epoch [16/30], Batch [700/6000], Loss: 0.1187
Epoch [16/30], Batch [800/6000], Loss: 0.1063
Epoch [16/30], Batch [900/6000], Loss: 0.1275
Epoch [16/30], Batch [1000/6000], Loss: 0.1310
Epoch [16/30], Batch [1100/6000], Loss: 0.1326
Epoch [16/30], Batch [1200/6000], Loss: 0.1200
Epoch [16/30], Batch [1300/6000], Loss: 0.1340
Epoch [16/30], Batch [1400/6000], Loss: 0.1593
Epoch [16/30], Batch [1500/6000], Loss: 0.0940
Epoch [16/30], Batch [1600/6000], Loss: 0.2586
Epoch [16/30], Batch [1700/6000], Loss: 0.1147
Epoch [16/30], Batch [1800/6000], Loss: 0.1140
Epoch [16/30], Batch [1900/6000], Loss: 0.1094
Epoch [16/30], Batch [2000/6000], Loss: 0.1931
Epoch [16/30], Batch [2100/6000], Loss: 0.1104
Epoch [16/30], Batch [2200/6000], Loss: 0.3717
Epoch [16/30], Batch [2300/6000], Loss: 0.1594
Epoch [16/30], Batch [2400/6000], Loss: 0.1612
Epoch [16/30], Batch [2500/6000], Loss: 0.0936
Epoch [16/30], Batch [2600/6000], Loss: 0.1130
Epoch [16/30], Batch [2700/6000], Loss: 0.1015
Epoch [16/30], Batch [2800/6000], Loss: 0.1512
Epoch [16/30], Batch [2900/6000], Loss: 0.1245
Epoch [16/30], Batch [3000/6000], Loss: 0.1138
Epoch [16/30], Batch [3100/6000], Loss: 0.1270
Epoch [16/30], Batch [3200/6000], Loss: 0.1062
Epoch [16/30], Batch [3300/6000], Loss: 0.0725
Epoch [16/30], Batch [3400/6000], Loss: 0.1376
Epoch [16/30], Batch [3500/6000], Loss: 0.1624
Epoch [16/30], Batch [3600/6000], Loss: 0.1006
Epoch [16/30], Batch [3700/6000], Loss: 0.1113
Epoch [16/30], Batch [3800/6000], Loss: 0.1125
Epoch [16/30], Batch [3900/6000], Loss: 0.1203
Epoch [16/30], Batch [4000/6000], Loss: 0.1417
Epoch [16/30], Batch [4100/6000], Loss: 0.1032
Epoch [16/30], Batch [4200/6000], Loss: 0.1030
Epoch [16/30], Batch [4300/6000], Loss: 0.0988
Epoch [16/30], Batch [4400/6000], Loss: 0.1749
Epoch [16/30], Batch [4500/6000], Loss: 0.1182
Epoch [16/30], Batch [4600/6000], Loss: 0.0920
Epoch [16/30], Batch [4700/6000], Loss: 0.0790
Epoch [16/30], Batch [4800/6000], Loss: 0.1066
Epoch [16/30], Batch [4900/6000], Loss: 0.2187
Epoch [16/30], Batch [5000/6000], Loss: 0.1185
Epoch [16/30], Batch [5100/6000], Loss: 0.1043
Epoch [16/30], Batch [5200/6000], Loss: 0.1308
Epoch [16/30], Batch [5300/6000], Loss: 0.0958
Epoch [16/30], Batch [5400/6000], Loss: 0.0958
Epoch [16/30], Batch [5500/6000], Loss: 0.1080
Epoch [16/30], Batch [5600/6000], Loss: 0.1675
Epoch [16/30], Batch [5700/6000], Loss: 0.1040
Epoch [16/30], Batch [5800/6000], Loss: 0.0913
Epoch [16/30], Batch [5900/6000], Loss: 0.0861
Epoch [16/30], Loss: 0.1249
Epoch [17/30], Batch [0/6000], Loss: 0.1886
Epoch [17/30], Batch [100/6000], Loss: 0.1163
Epoch [17/30], Batch [200/6000], Loss: 0.1149
Epoch [17/30], Batch [300/6000], Loss: 0.1151
Epoch [17/30], Batch [400/6000], Loss: 0.2277
Epoch [17/30], Batch [500/6000], Loss: 0.0860
Epoch [17/30], Batch [600/6000], Loss: 0.1130
Epoch [17/30], Batch [700/6000], Loss: 0.0982
Epoch [17/30], Batch [800/6000], Loss: 0.1047
Epoch [17/30], Batch [900/6000], Loss: 0.1105
Epoch [17/30], Batch [1000/6000], Loss: 0.1492
Epoch [17/30], Batch [1100/6000], Loss: 0.0893
Epoch [17/30], Batch [1200/6000], Loss: 0.1304
Epoch [17/30], Batch [1300/6000], Loss: 0.1075
Epoch [17/30], Batch [1400/6000], Loss: 0.1019
Epoch [17/30], Batch [1500/6000], Loss: 0.0775
Epoch [17/30], Batch [1600/6000], Loss: 0.1264
Epoch [17/30], Batch [1700/6000], Loss: 0.0911
Epoch [17/30], Batch [1800/6000], Loss: 0.1020
Epoch [17/30], Batch [1900/6000], Loss: 0.1064
Epoch [17/30], Batch [2000/6000], Loss: 0.1025
Epoch [17/30], Batch [2100/6000], Loss: 0.1718
Epoch [17/30], Batch [2200/6000], Loss: 0.1499
Epoch [17/30], Batch [2300/6000], Loss: 0.1100
Epoch [17/30], Batch [2400/6000], Loss: 0.1031
Epoch [17/30], Batch [2500/6000], Loss: 0.1205
Epoch [17/30], Batch [2600/6000], Loss: 0.0896
Epoch [17/30], Batch [2700/6000], Loss: 0.1367
Epoch [17/30], Batch [2800/6000], Loss: 0.1074
Epoch [17/30], Batch [2900/6000], Loss: 0.1239
Epoch [17/30], Batch [3000/6000], Loss: 0.1147
Epoch [17/30], Batch [3100/6000], Loss: 0.1955
Epoch [17/30], Batch [3200/6000], Loss: 0.1031
Epoch [17/30], Batch [3300/6000], Loss: 0.1937
Epoch [17/30], Batch [3400/6000], Loss: 0.1435
Epoch [17/30], Batch [3500/6000], Loss: 0.1009
Epoch [17/30], Batch [3600/6000], Loss: 0.1104
Epoch [17/30], Batch [3700/6000], Loss: 0.2036
Epoch [17/30], Batch [3800/6000], Loss: 0.0870
Epoch [17/30], Batch [3900/6000], Loss: 0.0947
Epoch [17/30], Batch [4000/6000], Loss: 0.1175
Epoch [17/30], Batch [4100/6000], Loss: 0.0810
Epoch [17/30], Batch [4200/6000], Loss: 0.0606
Epoch [17/30], Batch [4300/6000], Loss: 0.1459
Epoch [17/30], Batch [4400/6000], Loss: 0.0940
Epoch [17/30], Batch [4500/6000], Loss: 0.1232
Epoch [17/30], Batch [4600/6000], Loss: 0.0853
Epoch [17/30], Batch [4700/6000], Loss: 0.1032
Epoch [17/30], Batch [4800/6000], Loss: 0.1488
Epoch [17/30], Batch [4900/6000], Loss: 0.1059
Epoch [17/30], Batch [5000/6000], Loss: 0.1442
Epoch [17/30], Batch [5100/6000], Loss: 0.1148
Epoch [17/30], Batch [5200/6000], Loss: 0.1208
Epoch [17/30], Batch [5300/6000], Loss: 0.1038
Epoch [17/30], Batch [5400/6000], Loss: 0.0924
Epoch [17/30], Batch [5500/6000], Loss: 0.1154
Epoch [17/30], Batch [5600/6000], Loss: 0.1336
Epoch [17/30], Batch [5700/6000], Loss: 0.1097
Epoch [17/30], Batch [5800/6000], Loss: 0.0966
Epoch [17/30], Batch [5900/6000], Loss: 0.1484
Epoch [17/30], Loss: 0.1213
Epoch [18/30], Batch [0/6000], Loss: 0.1754
Epoch [18/30], Batch [100/6000], Loss: 0.1334
Epoch [18/30], Batch [200/6000], Loss: 0.1068
Epoch [18/30], Batch [300/6000], Loss: 0.2060
Epoch [18/30], Batch [400/6000], Loss: 0.0942
Epoch [18/30], Batch [500/6000], Loss: 0.1222
Epoch [18/30], Batch [600/6000], Loss: 0.0985
Epoch [18/30], Batch [700/6000], Loss: 0.1023
Epoch [18/30], Batch [800/6000], Loss: 0.0892
Epoch [18/30], Batch [900/6000], Loss: 0.0969
Epoch [18/30], Batch [1000/6000], Loss: 0.1289
Epoch [18/30], Batch [1100/6000], Loss: 0.1059
Epoch [18/30], Batch [1200/6000], Loss: 0.1230
Epoch [18/30], Batch [1300/6000], Loss: 0.0721
Epoch [18/30], Batch [1400/6000], Loss: 0.1023
Epoch [18/30], Batch [1500/6000], Loss: 0.0929
Epoch [18/30], Batch [1600/6000], Loss: 0.1120
Epoch [18/30], Batch [1700/6000], Loss: 0.1444
Epoch [18/30], Batch [1800/6000], Loss: 0.1316
Epoch [18/30], Batch [1900/6000], Loss: 0.0937
Epoch [18/30], Batch [2000/6000], Loss: 0.1226
Epoch [18/30], Batch [2100/6000], Loss: 0.1453
Epoch [18/30], Batch [2200/6000], Loss: 0.1898
Epoch [18/30], Batch [2300/6000], Loss: 0.2305
Epoch [18/30], Batch [2400/6000], Loss: 0.0937
Epoch [18/30], Batch [2500/6000], Loss: 0.1468
Epoch [18/30], Batch [2600/6000], Loss: 0.1192
Epoch [18/30], Batch [2700/6000], Loss: 0.1735
Epoch [18/30], Batch [2800/6000], Loss: 0.2755
Epoch [18/30], Batch [2900/6000], Loss: 0.1113
Epoch [18/30], Batch [3000/6000], Loss: 0.1192
Epoch [18/30], Batch [3100/6000], Loss: 0.1197
Epoch [18/30], Batch [3200/6000], Loss: 0.1555
Epoch [18/30], Batch [3300/6000], Loss: 0.2026
Epoch [18/30], Batch [3400/6000], Loss: 0.1005
Epoch [18/30], Batch [3500/6000], Loss: 0.0786
Epoch [18/30], Batch [3600/6000], Loss: 0.0989
Epoch [18/30], Batch [3700/6000], Loss: 0.1167
Epoch [18/30], Batch [3800/6000], Loss: 0.2173
Epoch [18/30], Batch [3900/6000], Loss: 0.1729
Epoch [18/30], Batch [4000/6000], Loss: 0.0981
Epoch [18/30], Batch [4100/6000], Loss: 0.1278
Epoch [18/30], Batch [4200/6000], Loss: 0.1170
Epoch [18/30], Batch [4300/6000], Loss: 0.1444
Epoch [18/30], Batch [4400/6000], Loss: 0.0988
Epoch [18/30], Batch [4500/6000], Loss: 0.1456
Epoch [18/30], Batch [4600/6000], Loss: 0.1071
Epoch [18/30], Batch [4700/6000], Loss: 0.1265
Epoch [18/30], Batch [4800/6000], Loss: 0.2020
Epoch [18/30], Batch [4900/6000], Loss: 0.1015
Epoch [18/30], Batch [5000/6000], Loss: 0.1105
Epoch [18/30], Batch [5100/6000], Loss: 0.0851
Epoch [18/30], Batch [5200/6000], Loss: 0.1622
Epoch [18/30], Batch [5300/6000], Loss: 0.1239
Epoch [18/30], Batch [5400/6000], Loss: 0.1263
Epoch [18/30], Batch [5500/6000], Loss: 0.0902
Epoch [18/30], Batch [5600/6000], Loss: 0.1344
Epoch [18/30], Batch [5700/6000], Loss: 0.1538
Epoch [18/30], Batch [5800/6000], Loss: 0.0969
Epoch [18/30], Batch [5900/6000], Loss: 0.0860
Epoch [18/30], Loss: 0.1186
Epoch [19/30], Batch [0/6000], Loss: 0.1054
Epoch [19/30], Batch [100/6000], Loss: 0.1264
Epoch [19/30], Batch [200/6000], Loss: 0.1099
Epoch [19/30], Batch [300/6000], Loss: 0.1037
Epoch [19/30], Batch [400/6000], Loss: 0.0885
Epoch [19/30], Batch [500/6000], Loss: 0.1394
Epoch [19/30], Batch [600/6000], Loss: 0.0999
Epoch [19/30], Batch [700/6000], Loss: 0.1338
Epoch [19/30], Batch [800/6000], Loss: 0.1095
Epoch [19/30], Batch [900/6000], Loss: 0.1177
Epoch [19/30], Batch [1000/6000], Loss: 0.1124
Epoch [19/30], Batch [1100/6000], Loss: 0.1007
Epoch [19/30], Batch [1200/6000], Loss: 0.1223
Epoch [19/30], Batch [1300/6000], Loss: 0.1051
Epoch [19/30], Batch [1400/6000], Loss: 0.2567
Epoch [19/30], Batch [1500/6000], Loss: 0.1326
Epoch [19/30], Batch [1600/6000], Loss: 0.1293
Epoch [19/30], Batch [1700/6000], Loss: 0.1059
Epoch [19/30], Batch [1800/6000], Loss: 0.1343
Epoch [19/30], Batch [1900/6000], Loss: 0.0818
Epoch [19/30], Batch [2000/6000], Loss: 0.0976
Epoch [19/30], Batch [2100/6000], Loss: 0.0958
Epoch [19/30], Batch [2200/6000], Loss: 0.0912
Epoch [19/30], Batch [2300/6000], Loss: 0.1142
Epoch [19/30], Batch [2400/6000], Loss: 0.1449
Epoch [19/30], Batch [2500/6000], Loss: 0.1093
Epoch [19/30], Batch [2600/6000], Loss: 0.0762
Epoch [19/30], Batch [2700/6000], Loss: 0.1142
Epoch [19/30], Batch [2800/6000], Loss: 0.0931
Epoch [19/30], Batch [2900/6000], Loss: 0.1151
Epoch [19/30], Batch [3000/6000], Loss: 0.0874
Epoch [19/30], Batch [3100/6000], Loss: 0.1466
Epoch [19/30], Batch [3200/6000], Loss: 0.1441
Epoch [19/30], Batch [3300/6000], Loss: 0.1031
Epoch [19/30], Batch [3400/6000], Loss: 0.1254
Epoch [19/30], Batch [3500/6000], Loss: 0.1608
Epoch [19/30], Batch [3600/6000], Loss: 0.1003
Epoch [19/30], Batch [3700/6000], Loss: 0.1031
Epoch [19/30], Batch [3800/6000], Loss: 0.0862
Epoch [19/30], Batch [3900/6000], Loss: 0.1044
Epoch [19/30], Batch [4000/6000], Loss: 0.0998
Epoch [19/30], Batch [4100/6000], Loss: 0.1232
Epoch [19/30], Batch [4200/6000], Loss: 0.1268
Epoch [19/30], Batch [4300/6000], Loss: 0.0957
Epoch [19/30], Batch [4400/6000], Loss: 0.0911
Epoch [19/30], Batch [4500/6000], Loss: 0.0971
Epoch [19/30], Batch [4600/6000], Loss: 0.1528
Epoch [19/30], Batch [4700/6000], Loss: 0.1021
Epoch [19/30], Batch [4800/6000], Loss: 0.0969
Epoch [19/30], Batch [4900/6000], Loss: 0.1283
Epoch [19/30], Batch [5000/6000], Loss: 0.1342
Epoch [19/30], Batch [5100/6000], Loss: 0.0886
Epoch [19/30], Batch [5200/6000], Loss: 0.1009
Epoch [19/30], Batch [5300/6000], Loss: 0.1096
Epoch [19/30], Batch [5400/6000], Loss: 0.0912
Epoch [19/30], Batch [5500/6000], Loss: 0.1236
Epoch [19/30], Batch [5600/6000], Loss: 0.1293
Epoch [19/30], Batch [5700/6000], Loss: 0.1058
Epoch [19/30], Batch [5800/6000], Loss: 0.1220
Epoch [19/30], Batch [5900/6000], Loss: 0.1124
Epoch [19/30], Loss: 0.1164
Epoch [20/30], Batch [0/6000], Loss: 0.1232
Epoch [20/30], Batch [100/6000], Loss: 0.0884
Epoch [20/30], Batch [200/6000], Loss: 0.1074
Epoch [20/30], Batch [300/6000], Loss: 0.1786
Epoch [20/30], Batch [400/6000], Loss: 0.1126
Epoch [20/30], Batch [500/6000], Loss: 0.1235
Epoch [20/30], Batch [600/6000], Loss: 0.1209
Epoch [20/30], Batch [700/6000], Loss: 0.0855
Epoch [20/30], Batch [800/6000], Loss: 0.0697
Epoch [20/30], Batch [900/6000], Loss: 0.0882
Epoch [20/30], Batch [1000/6000], Loss: 0.1113
Epoch [20/30], Batch [1100/6000], Loss: 0.1752
Epoch [20/30], Batch [1200/6000], Loss: 0.1166
Epoch [20/30], Batch [1300/6000], Loss: 0.1076
Epoch [20/30], Batch [1400/6000], Loss: 0.0898
Epoch [20/30], Batch [1500/6000], Loss: 0.0897
Epoch [20/30], Batch [1600/6000], Loss: 0.1127
Epoch [20/30], Batch [1700/6000], Loss: 0.1270
Epoch [20/30], Batch [1800/6000], Loss: 0.1523
Epoch [20/30], Batch [1900/6000], Loss: 0.1179
Epoch [20/30], Batch [2000/6000], Loss: 0.1192
Epoch [20/30], Batch [2100/6000], Loss: 0.1171
Epoch [20/30], Batch [2200/6000], Loss: 0.1234
Epoch [20/30], Batch [2300/6000], Loss: 0.1216
Epoch [20/30], Batch [2400/6000], Loss: 0.1150
Epoch [20/30], Batch [2500/6000], Loss: 0.1072
Epoch [20/30], Batch [2600/6000], Loss: 0.0753
Epoch [20/30], Batch [2700/6000], Loss: 0.0851
Epoch [20/30], Batch [2800/6000], Loss: 0.1297
Epoch [20/30], Batch [2900/6000], Loss: 0.2478
Epoch [20/30], Batch [3000/6000], Loss: 0.0788
Epoch [20/30], Batch [3100/6000], Loss: 0.1480
Epoch [20/30], Batch [3200/6000], Loss: 0.1664
Epoch [20/30], Batch [3300/6000], Loss: 0.1190
Epoch [20/30], Batch [3400/6000], Loss: 0.1219
Epoch [20/30], Batch [3500/6000], Loss: 0.0800
Epoch [20/30], Batch [3600/6000], Loss: 0.0985
Epoch [20/30], Batch [3700/6000], Loss: 0.1393
Epoch [20/30], Batch [3800/6000], Loss: 0.0976
Epoch [20/30], Batch [3900/6000], Loss: 0.0850
Epoch [20/30], Batch [4000/6000], Loss: 0.1228
Epoch [20/30], Batch [4100/6000], Loss: 0.1128
Epoch [20/30], Batch [4200/6000], Loss: 0.3228
Epoch [20/30], Batch [4300/6000], Loss: 0.1383
Epoch [20/30], Batch [4400/6000], Loss: 0.0907
Epoch [20/30], Batch [4500/6000], Loss: 0.1173
Epoch [20/30], Batch [4600/6000], Loss: 0.1845
Epoch [20/30], Batch [4700/6000], Loss: 0.0985
Epoch [20/30], Batch [4800/6000], Loss: 0.1287
Epoch [20/30], Batch [4900/6000], Loss: 0.0975
Epoch [20/30], Batch [5000/6000], Loss: 0.0925
Epoch [20/30], Batch [5100/6000], Loss: 0.1340
Epoch [20/30], Batch [5200/6000], Loss: 0.1486
Epoch [20/30], Batch [5300/6000], Loss: 0.0872
Epoch [20/30], Batch [5400/6000], Loss: 0.0801
Epoch [20/30], Batch [5500/6000], Loss: 0.1102
Epoch [20/30], Batch [5600/6000], Loss: 0.0806
Epoch [20/30], Batch [5700/6000], Loss: 0.1287
Epoch [20/30], Batch [5800/6000], Loss: 0.1314
Epoch [20/30], Batch [5900/6000], Loss: 0.0916
Epoch [20/30], Loss: 0.1141
Epoch [21/30], Batch [0/6000], Loss: 0.1083
Epoch [21/30], Batch [100/6000], Loss: 0.1187
Epoch [21/30], Batch [200/6000], Loss: 0.0921
Epoch [21/30], Batch [300/6000], Loss: 0.1622
Epoch [21/30], Batch [400/6000], Loss: 0.1559
Epoch [21/30], Batch [500/6000], Loss: 0.1126
Epoch [21/30], Batch [600/6000], Loss: 0.1108
Epoch [21/30], Batch [700/6000], Loss: 0.0883
Epoch [21/30], Batch [800/6000], Loss: 0.1429
Epoch [21/30], Batch [900/6000], Loss: 0.0888
Epoch [21/30], Batch [1000/6000], Loss: 0.0885
Epoch [21/30], Batch [1100/6000], Loss: 0.1025
Epoch [21/30], Batch [1200/6000], Loss: 0.0768
Epoch [21/30], Batch [1300/6000], Loss: 0.1064
Epoch [21/30], Batch [1400/6000], Loss: 0.0881
Epoch [21/30], Batch [1500/6000], Loss: 0.1169
Epoch [21/30], Batch [1600/6000], Loss: 0.1031
Epoch [21/30], Batch [1700/6000], Loss: 0.1106
Epoch [21/30], Batch [1800/6000], Loss: 0.1370
Epoch [21/30], Batch [1900/6000], Loss: 0.0865
Epoch [21/30], Batch [2000/6000], Loss: 0.1038
Epoch [21/30], Batch [2100/6000], Loss: 0.1556
Epoch [21/30], Batch [2200/6000], Loss: 0.1246
Epoch [21/30], Batch [2300/6000], Loss: 0.1157
Epoch [21/30], Batch [2400/6000], Loss: 0.0925
Epoch [21/30], Batch [2500/6000], Loss: 0.1026
Epoch [21/30], Batch [2600/6000], Loss: 0.1706
Epoch [21/30], Batch [2700/6000], Loss: 0.1373
Epoch [21/30], Batch [2800/6000], Loss: 0.1225
Epoch [21/30], Batch [2900/6000], Loss: 0.0980
Epoch [21/30], Batch [3000/6000], Loss: 0.1253
Epoch [21/30], Batch [3100/6000], Loss: 0.1183
Epoch [21/30], Batch [3200/6000], Loss: 0.1045
Epoch [21/30], Batch [3300/6000], Loss: 0.1188
Epoch [21/30], Batch [3400/6000], Loss: 0.0991
Epoch [21/30], Batch [3500/6000], Loss: 0.1897
Epoch [21/30], Batch [3600/6000], Loss: 0.0780
Epoch [21/30], Batch [3700/6000], Loss: 0.1000
Epoch [21/30], Batch [3800/6000], Loss: 0.0928
Epoch [21/30], Batch [3900/6000], Loss: 0.1290
Epoch [21/30], Batch [4000/6000], Loss: 0.2195
Epoch [21/30], Batch [4100/6000], Loss: 0.0974
Epoch [21/30], Batch [4200/6000], Loss: 0.1216
Epoch [21/30], Batch [4300/6000], Loss: 0.0839
Epoch [21/30], Batch [4400/6000], Loss: 0.1212
Epoch [21/30], Batch [4500/6000], Loss: 0.1927
Epoch [21/30], Batch [4600/6000], Loss: 0.1303
Epoch [21/30], Batch [4700/6000], Loss: 0.1287
Epoch [21/30], Batch [4800/6000], Loss: 0.1085
Epoch [21/30], Batch [4900/6000], Loss: 0.0850
Epoch [21/30], Batch [5000/6000], Loss: 0.0936
Epoch [21/30], Batch [5100/6000], Loss: 0.1031
Epoch [21/30], Batch [5200/6000], Loss: 0.1881
Epoch [21/30], Batch [5300/6000], Loss: 0.0803
Epoch [21/30], Batch [5400/6000], Loss: 0.1966
Epoch [21/30], Batch [5500/6000], Loss: 0.0917
Epoch [21/30], Batch [5600/6000], Loss: 0.0886
Epoch [21/30], Batch [5700/6000], Loss: 0.0844
Epoch [21/30], Batch [5800/6000], Loss: 0.0833
Epoch [21/30], Batch [5900/6000], Loss: 0.0846
Epoch [21/30], Loss: 0.1115
Epoch [22/30], Batch [0/6000], Loss: 0.0864
Epoch [22/30], Batch [100/6000], Loss: 0.1266
Epoch [22/30], Batch [200/6000], Loss: 0.0833
Epoch [22/30], Batch [300/6000], Loss: 0.1014
Epoch [22/30], Batch [400/6000], Loss: 0.1161
Epoch [22/30], Batch [500/6000], Loss: 0.1202
Epoch [22/30], Batch [600/6000], Loss: 0.1474
Epoch [22/30], Batch [700/6000], Loss: 0.0941
Epoch [22/30], Batch [800/6000], Loss: 0.1078
Epoch [22/30], Batch [900/6000], Loss: 0.0997
Epoch [22/30], Batch [1000/6000], Loss: 0.0953
Epoch [22/30], Batch [1100/6000], Loss: 0.2003
Epoch [22/30], Batch [1200/6000], Loss: 0.0819
Epoch [22/30], Batch [1300/6000], Loss: 0.0947
Epoch [22/30], Batch [1400/6000], Loss: 0.1227
Epoch [22/30], Batch [1500/6000], Loss: 0.1089
Epoch [22/30], Batch [1600/6000], Loss: 0.0875
Epoch [22/30], Batch [1700/6000], Loss: 0.1202
Epoch [22/30], Batch [1800/6000], Loss: 0.2231
Epoch [22/30], Batch [1900/6000], Loss: 0.0982
Epoch [22/30], Batch [2000/6000], Loss: 0.1600
Epoch [22/30], Batch [2100/6000], Loss: 0.0907
Epoch [22/30], Batch [2200/6000], Loss: 0.0970
Epoch [22/30], Batch [2300/6000], Loss: 0.1072
Epoch [22/30], Batch [2400/6000], Loss: 0.1096
Epoch [22/30], Batch [2500/6000], Loss: 0.0747
Epoch [22/30], Batch [2600/6000], Loss: 0.0884
Epoch [22/30], Batch [2700/6000], Loss: 0.1073
Epoch [22/30], Batch [2800/6000], Loss: 0.1120
Epoch [22/30], Batch [2900/6000], Loss: 0.1087
Epoch [22/30], Batch [3000/6000], Loss: 0.1041
Epoch [22/30], Batch [3100/6000], Loss: 0.0950
Epoch [22/30], Batch [3200/6000], Loss: 0.1140
Epoch [22/30], Batch [3300/6000], Loss: 0.1134
Epoch [22/30], Batch [3400/6000], Loss: 0.1009
Epoch [22/30], Batch [3500/6000], Loss: 0.0866
Epoch [22/30], Batch [3600/6000], Loss: 0.1274
Epoch [22/30], Batch [3700/6000], Loss: 0.1329
Epoch [22/30], Batch [3800/6000], Loss: 0.1086
Epoch [22/30], Batch [3900/6000], Loss: 0.0845
Epoch [22/30], Batch [4000/6000], Loss: 0.0961
Epoch [22/30], Batch [4100/6000], Loss: 0.1527
Epoch [22/30], Batch [4200/6000], Loss: 0.2059
Epoch [22/30], Batch [4300/6000], Loss: 0.0959
Epoch [22/30], Batch [4400/6000], Loss: 0.1244
Epoch [22/30], Batch [4500/6000], Loss: 0.0949
Epoch [22/30], Batch [4600/6000], Loss: 0.0907
Epoch [22/30], Batch [4700/6000], Loss: 0.1312
Epoch [22/30], Batch [4800/6000], Loss: 0.1258
Epoch [22/30], Batch [4900/6000], Loss: 0.1481
Epoch [22/30], Batch [5000/6000], Loss: 0.0618
Epoch [22/30], Batch [5100/6000], Loss: 0.1154
Epoch [22/30], Batch [5200/6000], Loss: 0.1031
Epoch [22/30], Batch [5300/6000], Loss: 0.0843
Epoch [22/30], Batch [5400/6000], Loss: 0.1267
Epoch [22/30], Batch [5500/6000], Loss: 0.0908
Epoch [22/30], Batch [5600/6000], Loss: 0.1137
Epoch [22/30], Batch [5700/6000], Loss: 0.1048
Epoch [22/30], Batch [5800/6000], Loss: 0.0989
Epoch [22/30], Batch [5900/6000], Loss: 0.1350
Epoch [22/30], Loss: 0.1096
Epoch [23/30], Batch [0/6000], Loss: 0.1516
Epoch [23/30], Batch [100/6000], Loss: 0.1194
Epoch [23/30], Batch [200/6000], Loss: 0.0827
Epoch [23/30], Batch [300/6000], Loss: 0.0995
Epoch [23/30], Batch [400/6000], Loss: 0.1028
Epoch [23/30], Batch [500/6000], Loss: 0.1014
Epoch [23/30], Batch [600/6000], Loss: 0.1403
Epoch [23/30], Batch [700/6000], Loss: 0.1091
Epoch [23/30], Batch [800/6000], Loss: 0.1340
Epoch [23/30], Batch [900/6000], Loss: 0.0801
Epoch [23/30], Batch [1000/6000], Loss: 0.0969
Epoch [23/30], Batch [1100/6000], Loss: 0.1009
Epoch [23/30], Batch [1200/6000], Loss: 0.1032
Epoch [23/30], Batch [1300/6000], Loss: 0.1137
Epoch [23/30], Batch [1400/6000], Loss: 0.1025
Epoch [23/30], Batch [1500/6000], Loss: 0.0981
Epoch [23/30], Batch [1600/6000], Loss: 0.1172
Epoch [23/30], Batch [1700/6000], Loss: 0.1178
Epoch [23/30], Batch [1800/6000], Loss: 0.0890
Epoch [23/30], Batch [1900/6000], Loss: 0.1037
Epoch [23/30], Batch [2000/6000], Loss: 0.0932
Epoch [23/30], Batch [2100/6000], Loss: 0.1023
Epoch [23/30], Batch [2200/6000], Loss: 0.2409
Epoch [23/30], Batch [2300/6000], Loss: 0.1043
Epoch [23/30], Batch [2400/6000], Loss: 0.1053
Epoch [23/30], Batch [2500/6000], Loss: 0.1073
Epoch [23/30], Batch [2600/6000], Loss: 0.3015
Epoch [23/30], Batch [2700/6000], Loss: 0.0897
Epoch [23/30], Batch [2800/6000], Loss: 0.0995
Epoch [23/30], Batch [2900/6000], Loss: 0.1035
Epoch [23/30], Batch [3000/6000], Loss: 0.0953
Epoch [23/30], Batch [3100/6000], Loss: 0.0939
Epoch [23/30], Batch [3200/6000], Loss: 0.1218
Epoch [23/30], Batch [3300/6000], Loss: 0.0846
Epoch [23/30], Batch [3400/6000], Loss: 0.0937
Epoch [23/30], Batch [3500/6000], Loss: 0.1031
Epoch [23/30], Batch [3600/6000], Loss: 0.0727
Epoch [23/30], Batch [3700/6000], Loss: 0.1060
Epoch [23/30], Batch [3800/6000], Loss: 0.0964
Epoch [23/30], Batch [3900/6000], Loss: 0.1088
Epoch [23/30], Batch [4000/6000], Loss: 0.0917
Epoch [23/30], Batch [4100/6000], Loss: 0.1398
Epoch [23/30], Batch [4200/6000], Loss: 0.1154
Epoch [23/30], Batch [4300/6000], Loss: 0.1169
Epoch [23/30], Batch [4400/6000], Loss: 0.1076
Epoch [23/30], Batch [4500/6000], Loss: 0.1133
Epoch [23/30], Batch [4600/6000], Loss: 0.1050
Epoch [23/30], Batch [4700/6000], Loss: 0.1301
Epoch [23/30], Batch [4800/6000], Loss: 0.1185
Epoch [23/30], Batch [4900/6000], Loss: 0.1011
Epoch [23/30], Batch [5000/6000], Loss: 0.0652
Epoch [23/30], Batch [5100/6000], Loss: 0.1018
Epoch [23/30], Batch [5200/6000], Loss: 0.1048
Epoch [23/30], Batch [5300/6000], Loss: 0.0891
Epoch [23/30], Batch [5400/6000], Loss: 0.0813
Epoch [23/30], Batch [5500/6000], Loss: 0.0865
Epoch [23/30], Batch [5600/6000], Loss: 0.1067
Epoch [23/30], Batch [5700/6000], Loss: 0.0856
Epoch [23/30], Batch [5800/6000], Loss: 0.1019
Epoch [23/30], Batch [5900/6000], Loss: 0.0896
Epoch [23/30], Loss: 0.1072
Epoch [24/30], Batch [0/6000], Loss: 0.0887
Epoch [24/30], Batch [100/6000], Loss: 0.0890
Epoch [24/30], Batch [200/6000], Loss: 0.0902
Epoch [24/30], Batch [300/6000], Loss: 0.0899
Epoch [24/30], Batch [400/6000], Loss: 0.1887
Epoch [24/30], Batch [500/6000], Loss: 0.1051
Epoch [24/30], Batch [600/6000], Loss: 0.0944
Epoch [24/30], Batch [700/6000], Loss: 0.1211
Epoch [24/30], Batch [800/6000], Loss: 0.0931
Epoch [24/30], Batch [900/6000], Loss: 0.1021
Epoch [24/30], Batch [1000/6000], Loss: 0.0853
Epoch [24/30], Batch [1100/6000], Loss: 0.1094
Epoch [24/30], Batch [1200/6000], Loss: 0.0967
Epoch [24/30], Batch [1300/6000], Loss: 0.1854
Epoch [24/30], Batch [1400/6000], Loss: 0.0824
Epoch [24/30], Batch [1500/6000], Loss: 0.0999
Epoch [24/30], Batch [1600/6000], Loss: 0.1148
Epoch [24/30], Batch [1700/6000], Loss: 0.0949
Epoch [24/30], Batch [1800/6000], Loss: 0.1183
Epoch [24/30], Batch [1900/6000], Loss: 0.1030
Epoch [24/30], Batch [2000/6000], Loss: 0.1425
Epoch [24/30], Batch [2100/6000], Loss: 0.0922
Epoch [24/30], Batch [2200/6000], Loss: 0.0945
Epoch [24/30], Batch [2300/6000], Loss: 0.1113
Epoch [24/30], Batch [2400/6000], Loss: 0.0990
Epoch [24/30], Batch [2500/6000], Loss: 0.0791
Epoch [24/30], Batch [2600/6000], Loss: 0.0970
Epoch [24/30], Batch [2700/6000], Loss: 0.0988
Epoch [24/30], Batch [2800/6000], Loss: 0.1002
Epoch [24/30], Batch [2900/6000], Loss: 0.0726
Epoch [24/30], Batch [3000/6000], Loss: 0.1209
Epoch [24/30], Batch [3100/6000], Loss: 0.0896
Epoch [24/30], Batch [3200/6000], Loss: 0.1213
Epoch [24/30], Batch [3300/6000], Loss: 0.0941
Epoch [24/30], Batch [3400/6000], Loss: 0.1162
Epoch [24/30], Batch [3500/6000], Loss: 0.0895
Epoch [24/30], Batch [3600/6000], Loss: 0.1347
Epoch [24/30], Batch [3700/6000], Loss: 0.1044
Epoch [24/30], Batch [3800/6000], Loss: 0.1011
Epoch [24/30], Batch [3900/6000], Loss: 0.0931
Epoch [24/30], Batch [4000/6000], Loss: 0.0923
Epoch [24/30], Batch [4100/6000], Loss: 0.0762
Epoch [24/30], Batch [4200/6000], Loss: 0.0926
Epoch [24/30], Batch [4300/6000], Loss: 0.0742
Epoch [24/30], Batch [4400/6000], Loss: 0.0883
Epoch [24/30], Batch [4500/6000], Loss: 0.1354
Epoch [24/30], Batch [4600/6000], Loss: 0.0849
Epoch [24/30], Batch [4700/6000], Loss: 0.1154
Epoch [24/30], Batch [4800/6000], Loss: 0.0944
Epoch [24/30], Batch [4900/6000], Loss: 0.1059
Epoch [24/30], Batch [5000/6000], Loss: 0.1325
Epoch [24/30], Batch [5100/6000], Loss: 0.0727
Epoch [24/30], Batch [5200/6000], Loss: 0.1092
Epoch [24/30], Batch [5300/6000], Loss: 0.0775
Epoch [24/30], Batch [5400/6000], Loss: 0.0874
Epoch [24/30], Batch [5500/6000], Loss: 0.0918
Epoch [24/30], Batch [5600/6000], Loss: 0.1051
Epoch [24/30], Batch [5700/6000], Loss: 0.0995
Epoch [24/30], Batch [5800/6000], Loss: 0.0645
Epoch [24/30], Batch [5900/6000], Loss: 0.0716
Epoch [24/30], Loss: 0.1060
Epoch [25/30], Batch [0/6000], Loss: 0.0881
Epoch [25/30], Batch [100/6000], Loss: 0.0913
Epoch [25/30], Batch [200/6000], Loss: 0.0621
Epoch [25/30], Batch [300/6000], Loss: 0.0803
Epoch [25/30], Batch [400/6000], Loss: 0.0926
Epoch [25/30], Batch [500/6000], Loss: 0.0986
Epoch [25/30], Batch [600/6000], Loss: 0.1512
Epoch [25/30], Batch [700/6000], Loss: 0.1213
Epoch [25/30], Batch [800/6000], Loss: 0.1264
Epoch [25/30], Batch [900/6000], Loss: 0.1068
Epoch [25/30], Batch [1000/6000], Loss: 0.0956
Epoch [25/30], Batch [1100/6000], Loss: 0.1119
Epoch [25/30], Batch [1200/6000], Loss: 0.1445
Epoch [25/30], Batch [1300/6000], Loss: 0.0870
Epoch [25/30], Batch [1400/6000], Loss: 0.0963
Epoch [25/30], Batch [1500/6000], Loss: 0.0810
Epoch [25/30], Batch [1600/6000], Loss: 0.1233
Epoch [25/30], Batch [1700/6000], Loss: 0.0972
Epoch [25/30], Batch [1800/6000], Loss: 0.1030
Epoch [25/30], Batch [1900/6000], Loss: 0.1060
Epoch [25/30], Batch [2000/6000], Loss: 0.0932
Epoch [25/30], Batch [2100/6000], Loss: 0.1021
Epoch [25/30], Batch [2200/6000], Loss: 0.1002
Epoch [25/30], Batch [2300/6000], Loss: 0.0953
Epoch [25/30], Batch [2400/6000], Loss: 0.1157
Epoch [25/30], Batch [2500/6000], Loss: 0.1014
Epoch [25/30], Batch [2600/6000], Loss: 0.0927
Epoch [25/30], Batch [2700/6000], Loss: 0.1070
Epoch [25/30], Batch [2800/6000], Loss: 0.0679
Epoch [25/30], Batch [2900/6000], Loss: 0.1241
Epoch [25/30], Batch [3000/6000], Loss: 0.0801
Epoch [25/30], Batch [3100/6000], Loss: 0.0943
Epoch [25/30], Batch [3200/6000], Loss: 0.0875
Epoch [25/30], Batch [3300/6000], Loss: 0.0759
Epoch [25/30], Batch [3400/6000], Loss: 0.1029
Epoch [25/30], Batch [3500/6000], Loss: 0.0931
Epoch [25/30], Batch [3600/6000], Loss: 0.0811
Epoch [25/30], Batch [3700/6000], Loss: 0.0911
Epoch [25/30], Batch [3800/6000], Loss: 0.1422
Epoch [25/30], Batch [3900/6000], Loss: 0.0747
Epoch [25/30], Batch [4000/6000], Loss: 0.0897
Epoch [25/30], Batch [4100/6000], Loss: 0.0784
Epoch [25/30], Batch [4200/6000], Loss: 0.1041
Epoch [25/30], Batch [4300/6000], Loss: 0.1110
Epoch [25/30], Batch [4400/6000], Loss: 0.1109
Epoch [25/30], Batch [4500/6000], Loss: 0.0870
Epoch [25/30], Batch [4600/6000], Loss: 0.0890
Epoch [25/30], Batch [4700/6000], Loss: 0.0985
Epoch [25/30], Batch [4800/6000], Loss: 0.1098
Epoch [25/30], Batch [4900/6000], Loss: 0.0978
Epoch [25/30], Batch [5000/6000], Loss: 0.1109
Epoch [25/30], Batch [5100/6000], Loss: 0.1332
Epoch [25/30], Batch [5200/6000], Loss: 0.1068
Epoch [25/30], Batch [5300/6000], Loss: 0.1029
Epoch [25/30], Batch [5400/6000], Loss: 0.1260
Epoch [25/30], Batch [5500/6000], Loss: 0.0923
Epoch [25/30], Batch [5600/6000], Loss: 0.0835
Epoch [25/30], Batch [5700/6000], Loss: 0.0924
Epoch [25/30], Batch [5800/6000], Loss: 0.0987
Epoch [25/30], Batch [5900/6000], Loss: 0.1195
Epoch [25/30], Loss: 0.1041
Epoch [26/30], Batch [0/6000], Loss: 0.0729
Epoch [26/30], Batch [100/6000], Loss: 0.0812
Epoch [26/30], Batch [200/6000], Loss: 0.0871
Epoch [26/30], Batch [300/6000], Loss: 0.0783
Epoch [26/30], Batch [400/6000], Loss: 0.0922
Epoch [26/30], Batch [500/6000], Loss: 0.1138
Epoch [26/30], Batch [600/6000], Loss: 0.1053
Epoch [26/30], Batch [700/6000], Loss: 0.1021
Epoch [26/30], Batch [800/6000], Loss: 0.1152
Epoch [26/30], Batch [900/6000], Loss: 0.1136
Epoch [26/30], Batch [1000/6000], Loss: 0.0786
Epoch [26/30], Batch [1100/6000], Loss: 0.1200
Epoch [26/30], Batch [1200/6000], Loss: 0.1014
Epoch [26/30], Batch [1300/6000], Loss: 0.1617
Epoch [26/30], Batch [1400/6000], Loss: 0.0679
Epoch [26/30], Batch [1500/6000], Loss: 0.1006
Epoch [26/30], Batch [1600/6000], Loss: 0.1255
Epoch [26/30], Batch [1700/6000], Loss: 0.1225
Epoch [26/30], Batch [1800/6000], Loss: 0.1116
Epoch [26/30], Batch [1900/6000], Loss: 0.0993
Epoch [26/30], Batch [2000/6000], Loss: 0.0900
Epoch [26/30], Batch [2100/6000], Loss: 0.0906
Epoch [26/30], Batch [2200/6000], Loss: 0.1156
Epoch [26/30], Batch [2300/6000], Loss: 0.0964
Epoch [26/30], Batch [2400/6000], Loss: 0.1028
Epoch [26/30], Batch [2500/6000], Loss: 0.1365
Epoch [26/30], Batch [2600/6000], Loss: 0.0720
Epoch [26/30], Batch [2700/6000], Loss: 0.0803
Epoch [26/30], Batch [2800/6000], Loss: 0.1242
Epoch [26/30], Batch [2900/6000], Loss: 0.0788
Epoch [26/30], Batch [3000/6000], Loss: 0.0819
Epoch [26/30], Batch [3100/6000], Loss: 0.0987
Epoch [26/30], Batch [3200/6000], Loss: 0.1011
Epoch [26/30], Batch [3300/6000], Loss: 0.0875
Epoch [26/30], Batch [3400/6000], Loss: 0.0819
Epoch [26/30], Batch [3500/6000], Loss: 0.0791
Epoch [26/30], Batch [3600/6000], Loss: 0.0919
Epoch [26/30], Batch [3700/6000], Loss: 0.1143
Epoch [26/30], Batch [3800/6000], Loss: 0.0643
Epoch [26/30], Batch [3900/6000], Loss: 0.1074
Epoch [26/30], Batch [4000/6000], Loss: 0.0934
Epoch [26/30], Batch [4100/6000], Loss: 0.1138
Epoch [26/30], Batch [4200/6000], Loss: 0.0823
Epoch [26/30], Batch [4300/6000], Loss: 0.1094
Epoch [26/30], Batch [4400/6000], Loss: 0.0779
Epoch [26/30], Batch [4500/6000], Loss: 0.1220
Epoch [26/30], Batch [4600/6000], Loss: 0.1129
Epoch [26/30], Batch [4700/6000], Loss: 0.0958
Epoch [26/30], Batch [4800/6000], Loss: 0.1098
Epoch [26/30], Batch [4900/6000], Loss: 0.1043
Epoch [26/30], Batch [5000/6000], Loss: 0.1005
Epoch [26/30], Batch [5100/6000], Loss: 0.0879
Epoch [26/30], Batch [5200/6000], Loss: 0.1740
Epoch [26/30], Batch [5300/6000], Loss: 0.0942
Epoch [26/30], Batch [5400/6000], Loss: 0.0858
Epoch [26/30], Batch [5500/6000], Loss: 0.0933
Epoch [26/30], Batch [5600/6000], Loss: 0.1365
Epoch [26/30], Batch [5700/6000], Loss: 0.1057
Epoch [26/30], Batch [5800/6000], Loss: 0.1056
Epoch [26/30], Batch [5900/6000], Loss: 0.1039
Epoch [26/30], Loss: 0.1023
Epoch [27/30], Batch [0/6000], Loss: 0.0799
Epoch [27/30], Batch [100/6000], Loss: 0.1335
Epoch [27/30], Batch [200/6000], Loss: 0.0993
Epoch [27/30], Batch [300/6000], Loss: 0.0992
Epoch [27/30], Batch [400/6000], Loss: 0.0852
Epoch [27/30], Batch [500/6000], Loss: 0.1024
Epoch [27/30], Batch [600/6000], Loss: 0.0923
Epoch [27/30], Batch [700/6000], Loss: 0.0770
Epoch [27/30], Batch [800/6000], Loss: 0.0569
Epoch [27/30], Batch [900/6000], Loss: 0.0786
Epoch [27/30], Batch [1000/6000], Loss: 0.0994
Epoch [27/30], Batch [1100/6000], Loss: 0.0975
Epoch [27/30], Batch [1200/6000], Loss: 0.1015
Epoch [27/30], Batch [1300/6000], Loss: 0.1066
Epoch [27/30], Batch [1400/6000], Loss: 0.0960
Epoch [27/30], Batch [1500/6000], Loss: 0.0958
Epoch [27/30], Batch [1600/6000], Loss: 0.1027
Epoch [27/30], Batch [1700/6000], Loss: 0.0955
Epoch [27/30], Batch [1800/6000], Loss: 0.0915
Epoch [27/30], Batch [1900/6000], Loss: 0.0916
Epoch [27/30], Batch [2000/6000], Loss: 0.0991
Epoch [27/30], Batch [2100/6000], Loss: 0.0911
Epoch [27/30], Batch [2200/6000], Loss: 0.1585
Epoch [27/30], Batch [2300/6000], Loss: 0.0897
Epoch [27/30], Batch [2400/6000], Loss: 0.0716
Epoch [27/30], Batch [2500/6000], Loss: 0.0851
Epoch [27/30], Batch [2600/6000], Loss: 0.1187
Epoch [27/30], Batch [2700/6000], Loss: 0.1051
Epoch [27/30], Batch [2800/6000], Loss: 0.1224
Epoch [27/30], Batch [2900/6000], Loss: 0.0977
Epoch [27/30], Batch [3000/6000], Loss: 0.0884
Epoch [27/30], Batch [3100/6000], Loss: 0.0978
Epoch [27/30], Batch [3200/6000], Loss: 0.1101
Epoch [27/30], Batch [3300/6000], Loss: 0.0837
Epoch [27/30], Batch [3400/6000], Loss: 0.1115
Epoch [27/30], Batch [3500/6000], Loss: 0.0650
Epoch [27/30], Batch [3600/6000], Loss: 0.0990
Epoch [27/30], Batch [3700/6000], Loss: 0.1039
Epoch [27/30], Batch [3800/6000], Loss: 0.0765
Epoch [27/30], Batch [3900/6000], Loss: 0.0740
Epoch [27/30], Batch [4000/6000], Loss: 0.1187
Epoch [27/30], Batch [4100/6000], Loss: 0.0860
Epoch [27/30], Batch [4200/6000], Loss: 0.0894
Epoch [27/30], Batch [4300/6000], Loss: 0.0888
Epoch [27/30], Batch [4400/6000], Loss: 0.0690
Epoch [27/30], Batch [4500/6000], Loss: 0.0913
Epoch [27/30], Batch [4600/6000], Loss: 0.0780
Epoch [27/30], Batch [4700/6000], Loss: 0.1015
Epoch [27/30], Batch [4800/6000], Loss: 0.1211
Epoch [27/30], Batch [4900/6000], Loss: 0.1086
Epoch [27/30], Batch [5000/6000], Loss: 0.0811
Epoch [27/30], Batch [5100/6000], Loss: 0.0864
Epoch [27/30], Batch [5200/6000], Loss: 0.0853
Epoch [27/30], Batch [5300/6000], Loss: 0.0773
Epoch [27/30], Batch [5400/6000], Loss: 0.1832
Epoch [27/30], Batch [5500/6000], Loss: 0.1086
Epoch [27/30], Batch [5600/6000], Loss: 0.1043
Epoch [27/30], Batch [5700/6000], Loss: 0.1447
Epoch [27/30], Batch [5800/6000], Loss: 0.0790
Epoch [27/30], Batch [5900/6000], Loss: 0.0960
Epoch [27/30], Loss: 0.1013
Epoch [28/30], Batch [0/6000], Loss: 0.0696
Epoch [28/30], Batch [100/6000], Loss: 0.1478
Epoch [28/30], Batch [200/6000], Loss: 0.0994
Epoch [28/30], Batch [300/6000], Loss: 0.1203
Epoch [28/30], Batch [400/6000], Loss: 0.1372
Epoch [28/30], Batch [500/6000], Loss: 0.0923
Epoch [28/30], Batch [600/6000], Loss: 0.1000
Epoch [28/30], Batch [700/6000], Loss: 0.0929
Epoch [28/30], Batch [800/6000], Loss: 0.0733
Epoch [28/30], Batch [900/6000], Loss: 0.0878
Epoch [28/30], Batch [1000/6000], Loss: 0.0866
Epoch [28/30], Batch [1100/6000], Loss: 0.0807
Epoch [28/30], Batch [1200/6000], Loss: 0.1248
Epoch [28/30], Batch [1300/6000], Loss: 0.1041
Epoch [28/30], Batch [1400/6000], Loss: 0.0849
Epoch [28/30], Batch [1500/6000], Loss: 0.1014
Epoch [28/30], Batch [1600/6000], Loss: 0.0715
Epoch [28/30], Batch [1700/6000], Loss: 0.0975
Epoch [28/30], Batch [1800/6000], Loss: 0.1383
Epoch [28/30], Batch [1900/6000], Loss: 0.0986
Epoch [28/30], Batch [2000/6000], Loss: 0.0800
Epoch [28/30], Batch [2100/6000], Loss: 0.0933
Epoch [28/30], Batch [2200/6000], Loss: 0.1210
Epoch [28/30], Batch [2300/6000], Loss: 0.1094
Epoch [28/30], Batch [2400/6000], Loss: 0.0962
Epoch [28/30], Batch [2500/6000], Loss: 0.0866
Epoch [28/30], Batch [2600/6000], Loss: 0.0963
Epoch [28/30], Batch [2700/6000], Loss: 0.0865
Epoch [28/30], Batch [2800/6000], Loss: 0.0825
Epoch [28/30], Batch [2900/6000], Loss: 0.1847
Epoch [28/30], Batch [3000/6000], Loss: 0.0884
Epoch [28/30], Batch [3100/6000], Loss: 0.1103
Epoch [28/30], Batch [3200/6000], Loss: 0.0821
Epoch [28/30], Batch [3300/6000], Loss: 0.1512
Epoch [28/30], Batch [3400/6000], Loss: 0.0820
Epoch [28/30], Batch [3500/6000], Loss: 0.0896
Epoch [28/30], Batch [3600/6000], Loss: 0.1103
Epoch [28/30], Batch [3700/6000], Loss: 0.0824
Epoch [28/30], Batch [3800/6000], Loss: 0.1257
Epoch [28/30], Batch [3900/6000], Loss: 0.0886
Epoch [28/30], Batch [4000/6000], Loss: 0.0884
Epoch [28/30], Batch [4100/6000], Loss: 0.1039
Epoch [28/30], Batch [4200/6000], Loss: 0.0944
Epoch [28/30], Batch [4300/6000], Loss: 0.0743
Epoch [28/30], Batch [4400/6000], Loss: 0.1033
Epoch [28/30], Batch [4500/6000], Loss: 0.1137
Epoch [28/30], Batch [4600/6000], Loss: 0.1065
Epoch [28/30], Batch [4700/6000], Loss: 0.0892
Epoch [28/30], Batch [4800/6000], Loss: 0.0884
Epoch [28/30], Batch [4900/6000], Loss: 0.0997
Epoch [28/30], Batch [5000/6000], Loss: 0.0837
Epoch [28/30], Batch [5100/6000], Loss: 0.0931
Epoch [28/30], Batch [5200/6000], Loss: 0.1257
Epoch [28/30], Batch [5300/6000], Loss: 0.0809
Epoch [28/30], Batch [5400/6000], Loss: 0.0842
Epoch [28/30], Batch [5500/6000], Loss: 0.0965
Epoch [28/30], Batch [5600/6000], Loss: 0.1127
Epoch [28/30], Batch [5700/6000], Loss: 0.0976
Epoch [28/30], Batch [5800/6000], Loss: 0.0939
Epoch [28/30], Batch [5900/6000], Loss: 0.1034
Epoch [28/30], Loss: 0.0997
Epoch [29/30], Batch [0/6000], Loss: 0.1877
Epoch [29/30], Batch [100/6000], Loss: 0.0896
Epoch [29/30], Batch [200/6000], Loss: 0.1048
Epoch [29/30], Batch [300/6000], Loss: 0.2643
Epoch [29/30], Batch [400/6000], Loss: 0.0907
Epoch [29/30], Batch [500/6000], Loss: 0.0849
Epoch [29/30], Batch [600/6000], Loss: 0.0907
Epoch [29/30], Batch [700/6000], Loss: 0.0950
Epoch [29/30], Batch [800/6000], Loss: 0.0783
Epoch [29/30], Batch [900/6000], Loss: 0.0929
Epoch [29/30], Batch [1000/6000], Loss: 0.1412
Epoch [29/30], Batch [1100/6000], Loss: 0.1186
Epoch [29/30], Batch [1200/6000], Loss: 0.0874
Epoch [29/30], Batch [1300/6000], Loss: 0.0797
Epoch [29/30], Batch [1400/6000], Loss: 0.1116
Epoch [29/30], Batch [1500/6000], Loss: 0.1085
Epoch [29/30], Batch [1600/6000], Loss: 0.0970
Epoch [29/30], Batch [1700/6000], Loss: 0.0975
Epoch [29/30], Batch [1800/6000], Loss: 0.0871
Epoch [29/30], Batch [1900/6000], Loss: 0.0855
Epoch [29/30], Batch [2000/6000], Loss: 0.0947
Epoch [29/30], Batch [2100/6000], Loss: 0.0964
Epoch [29/30], Batch [2200/6000], Loss: 0.0850
Epoch [29/30], Batch [2300/6000], Loss: 0.1045
Epoch [29/30], Batch [2400/6000], Loss: 0.1335
Epoch [29/30], Batch [2500/6000], Loss: 0.1451
Epoch [29/30], Batch [2600/6000], Loss: 0.1028
Epoch [29/30], Batch [2700/6000], Loss: 0.1055
Epoch [29/30], Batch [2800/6000], Loss: 0.1046
Epoch [29/30], Batch [2900/6000], Loss: 0.0920
Epoch [29/30], Batch [3000/6000], Loss: 0.1058
Epoch [29/30], Batch [3100/6000], Loss: 0.0904
Epoch [29/30], Batch [3200/6000], Loss: 0.0965
Epoch [29/30], Batch [3300/6000], Loss: 0.0847
Epoch [29/30], Batch [3400/6000], Loss: 0.0984
Epoch [29/30], Batch [3500/6000], Loss: 0.1069
Epoch [29/30], Batch [3600/6000], Loss: 0.0969
Epoch [29/30], Batch [3700/6000], Loss: 0.0779
Epoch [29/30], Batch [3800/6000], Loss: 0.0946
Epoch [29/30], Batch [3900/6000], Loss: 0.0952
Epoch [29/30], Batch [4000/6000], Loss: 0.0779
Epoch [29/30], Batch [4100/6000], Loss: 0.0727
Epoch [29/30], Batch [4200/6000], Loss: 0.0953
Epoch [29/30], Batch [4300/6000], Loss: 0.0654
Epoch [29/30], Batch [4400/6000], Loss: 0.0811
Epoch [29/30], Batch [4500/6000], Loss: 0.0716
Epoch [29/30], Batch [4600/6000], Loss: 0.0849
Epoch [29/30], Batch [4700/6000], Loss: 0.1115
Epoch [29/30], Batch [4800/6000], Loss: 0.1057
Epoch [29/30], Batch [4900/6000], Loss: 0.1430
Epoch [29/30], Batch [5000/6000], Loss: 0.1131
Epoch [29/30], Batch [5100/6000], Loss: 0.1158
Epoch [29/30], Batch [5200/6000], Loss: 0.1279
Epoch [29/30], Batch [5300/6000], Loss: 0.0798
Epoch [29/30], Batch [5400/6000], Loss: 0.0725
Epoch [29/30], Batch [5500/6000], Loss: 0.0776
Epoch [29/30], Batch [5600/6000], Loss: 0.0924
Epoch [29/30], Batch [5700/6000], Loss: 0.0619
Epoch [29/30], Batch [5800/6000], Loss: 0.1552
Epoch [29/30], Batch [5900/6000], Loss: 0.1259
Epoch [29/30], Loss: 0.0985
Epoch [30/30], Batch [0/6000], Loss: 0.0821
Epoch [30/30], Batch [100/6000], Loss: 0.0888
Epoch [30/30], Batch [200/6000], Loss: 0.0764
Epoch [30/30], Batch [300/6000], Loss: 0.1004
Epoch [30/30], Batch [400/6000], Loss: 0.0726
Epoch [30/30], Batch [500/6000], Loss: 0.1011
Epoch [30/30], Batch [600/6000], Loss: 0.0851
Epoch [30/30], Batch [700/6000], Loss: 0.0844
Epoch [30/30], Batch [800/6000], Loss: 0.1040
Epoch [30/30], Batch [900/6000], Loss: 0.1557
Epoch [30/30], Batch [1000/6000], Loss: 0.0831
Epoch [30/30], Batch [1100/6000], Loss: 0.0802
Epoch [30/30], Batch [1200/6000], Loss: 0.0643
Epoch [30/30], Batch [1300/6000], Loss: 0.1375
Epoch [30/30], Batch [1400/6000], Loss: 0.0801
Epoch [30/30], Batch [1500/6000], Loss: 0.0836
Epoch [30/30], Batch [1600/6000], Loss: 0.0870
Epoch [30/30], Batch [1700/6000], Loss: 0.0823
Epoch [30/30], Batch [1800/6000], Loss: 0.0648
Epoch [30/30], Batch [1900/6000], Loss: 0.0927
Epoch [30/30], Batch [2000/6000], Loss: 0.0798
Epoch [30/30], Batch [2100/6000], Loss: 0.0890
Epoch [30/30], Batch [2200/6000], Loss: 0.1069
Epoch [30/30], Batch [2300/6000], Loss: 0.0664
Epoch [30/30], Batch [2400/6000], Loss: 0.0879
Epoch [30/30], Batch [2500/6000], Loss: 0.1724
Epoch [30/30], Batch [2600/6000], Loss: 0.1040
Epoch [30/30], Batch [2700/6000], Loss: 0.0755
Epoch [30/30], Batch [2800/6000], Loss: 0.1056
Epoch [30/30], Batch [2900/6000], Loss: 0.0895
Epoch [30/30], Batch [3000/6000], Loss: 0.1020
Epoch [30/30], Batch [3100/6000], Loss: 0.0830
Epoch [30/30], Batch [3200/6000], Loss: 0.0956
Epoch [30/30], Batch [3300/6000], Loss: 0.0773
Epoch [30/30], Batch [3400/6000], Loss: 0.0957
Epoch [30/30], Batch [3500/6000], Loss: 0.0979
Epoch [30/30], Batch [3600/6000], Loss: 0.0727
Epoch [30/30], Batch [3700/6000], Loss: 0.1284
Epoch [30/30], Batch [3800/6000], Loss: 0.0819
Epoch [30/30], Batch [3900/6000], Loss: 0.0949
Epoch [30/30], Batch [4000/6000], Loss: 0.0709
Epoch [30/30], Batch [4100/6000], Loss: 0.0727
Epoch [30/30], Batch [4200/6000], Loss: 0.0972
Epoch [30/30], Batch [4300/6000], Loss: 0.0681
Epoch [30/30], Batch [4400/6000], Loss: 0.1710
Epoch [30/30], Batch [4500/6000], Loss: 0.0756
Epoch [30/30], Batch [4600/6000], Loss: 0.0749
Epoch [30/30], Batch [4700/6000], Loss: 0.1072
Epoch [30/30], Batch [4800/6000], Loss: 0.1079
Epoch [30/30], Batch [4900/6000], Loss: 0.0748
Epoch [30/30], Batch [5000/6000], Loss: 0.0904
Epoch [30/30], Batch [5100/6000], Loss: 0.0942
Epoch [30/30], Batch [5200/6000], Loss: 0.1087
Epoch [30/30], Batch [5300/6000], Loss: 0.0829
Epoch [30/30], Batch [5400/6000], Loss: 0.0596
Epoch [30/30], Batch [5500/6000], Loss: 0.0709
Epoch [30/30], Batch [5600/6000], Loss: 0.0738
Epoch [30/30], Batch [5700/6000], Loss: 0.0919
Epoch [30/30], Batch [5800/6000], Loss: 0.1304
Epoch [30/30], Batch [5900/6000], Loss: 0.0802
Epoch [30/30], Loss: 0.0974
Test Loss: 0.0222, Accuracy: 97.92%
Visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0.    0.    0.    0.    0.    0.996 0.    0.    0.003 0.001]]
Adversarial Training Loop 1/300:
  Label Loss: 0.4574
  Image Loss: 0.0243
  Total Loss: 0.2530
  Image grad max: 0.010796008631587029
  Output probs: [[0.    0.    0.    0.    0.    0.994 0.    0.    0.004 0.001]]
Adversarial Training Loop 2/300:
  Label Loss: 0.4397
  Image Loss: 0.0242
  Total Loss: 0.2440
  Image grad max: 0.011950967833399773
  Output probs: [[0.    0.    0.    0.    0.    0.992 0.    0.    0.005 0.002]]
Adversarial Training Loop 3/300:
  Label Loss: 0.4221
  Image Loss: 0.0239
  Total Loss: 0.2350
  Image grad max: 0.013153515756130219
  Output probs: [[0.    0.    0.    0.    0.    0.989 0.    0.    0.008 0.002]]
Adversarial Training Loop 4/300:
  Label Loss: 0.4029
  Image Loss: 0.0238
  Total Loss: 0.2253
  Image grad max: 0.014423780143260956
  Output probs: [[0.    0.    0.    0.    0.    0.984 0.    0.    0.012 0.003]]
Adversarial Training Loop 5/300:
  Label Loss: 0.3816
  Image Loss: 0.0238
  Total Loss: 0.2146
  Image grad max: 0.015788717195391655
  Output probs: [[0.    0.    0.    0.001 0.    0.974 0.    0.    0.02  0.004]]
Adversarial Training Loop 6/300:
  Label Loss: 0.3580
  Image Loss: 0.0239
  Total Loss: 0.2029
  Image grad max: 0.017313145101070404
  Output probs: [[0.    0.    0.    0.001 0.    0.957 0.    0.    0.035 0.006]]
Adversarial Training Loop 7/300:
  Label Loss: 0.3325
  Image Loss: 0.0240
  Total Loss: 0.1903
  Image grad max: 0.018363865092396736
  Output probs: [[0.001 0.    0.    0.001 0.    0.92  0.001 0.    0.068 0.01 ]]
Adversarial Training Loop 8/300:
  Label Loss: 0.3056
  Image Loss: 0.0242
  Total Loss: 0.1770
  Image grad max: 0.018914511427283287
  Output probs: [[0.001 0.    0.    0.002 0.    0.845 0.001 0.    0.135 0.015]]
Adversarial Training Loop 9/300:
  Label Loss: 0.2798
  Image Loss: 0.0245
  Total Loss: 0.1644
  Image grad max: 0.016418855637311935
  Output probs: [[0.002 0.    0.    0.003 0.    0.711 0.001 0.    0.261 0.023]]
Adversarial Training Loop 10/300:
  Label Loss: 0.2607
  Image Loss: 0.0249
  Total Loss: 0.1553
  Image grad max: 0.012930911034345627
  Output probs: [[0.003 0.    0.    0.004 0.    0.546 0.001 0.    0.418 0.029]]
Adversarial Training Loop 11/300:
  Label Loss: 0.2522
  Image Loss: 0.0254
  Total Loss: 0.1515
  Image grad max: 0.019461065530776978
  Output probs: [[0.004 0.    0.    0.004 0.    0.444 0.001 0.    0.514 0.032]]
Adversarial Training Loop 12/300:
  Label Loss: 0.2460
  Image Loss: 0.0257
  Total Loss: 0.1487
  Image grad max: 0.028275247663259506
  Output probs: [[0.006 0.    0.    0.004 0.    0.425 0.001 0.    0.529 0.034]]
Adversarial Training Loop 13/300:
  Label Loss: 0.2328
  Image Loss: 0.0260
  Total Loss: 0.1424
  Image grad max: 0.02913292497396469
  Output probs: [[0.008 0.    0.    0.004 0.    0.465 0.001 0.    0.485 0.036]]
Adversarial Training Loop 14/300:
  Label Loss: 0.2134
  Image Loss: 0.0261
  Total Loss: 0.1328
  Image grad max: 0.02376183122396469
  Output probs: [[0.01  0.    0.    0.004 0.    0.535 0.002 0.    0.412 0.037]]
Adversarial Training Loop 15/300:
  Label Loss: 0.1926
  Image Loss: 0.0262
  Total Loss: 0.1225
  Image grad max: 0.019064294174313545
  Output probs: [[0.013 0.    0.    0.004 0.    0.608 0.002 0.    0.336 0.037]]
Adversarial Training Loop 16/300:
  Label Loss: 0.1734
  Image Loss: 0.0264
  Total Loss: 0.1131
  Image grad max: 0.01584937795996666
  Output probs: [[0.017 0.    0.    0.003 0.    0.665 0.002 0.    0.276 0.037]]
Adversarial Training Loop 17/300:
  Label Loss: 0.1559
  Image Loss: 0.0266
  Total Loss: 0.1045
  Image grad max: 0.016087472438812256
  Output probs: [[0.022 0.    0.    0.003 0.    0.697 0.002 0.    0.237 0.037]]
Adversarial Training Loop 18/300:
  Label Loss: 0.1386
  Image Loss: 0.0268
  Total Loss: 0.0961
  Image grad max: 0.017430579289793968
  Output probs: [[0.032 0.    0.    0.003 0.    0.707 0.003 0.    0.216 0.038]]
Adversarial Training Loop 19/300:
  Label Loss: 0.1196
  Image Loss: 0.0272
  Total Loss: 0.0870
  Image grad max: 0.019346779212355614
  Output probs: [[0.05  0.    0.    0.003 0.    0.695 0.003 0.    0.207 0.041]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0983
  Image Loss: 0.0276
  Total Loss: 0.0767
  Image grad max: 0.0200539268553257
  Output probs: [[0.083 0.    0.    0.003 0.    0.659 0.004 0.    0.205 0.045]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0757
  Image Loss: 0.0281
  Total Loss: 0.0660
  Image grad max: 0.018735602498054504
  Output probs: [[0.139 0.    0.    0.003 0.    0.6   0.005 0.    0.203 0.049]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0547
  Image Loss: 0.0287
  Total Loss: 0.0561
  Image grad max: 0.01548925880342722
  Output probs: [[0.226 0.    0.    0.003 0.    0.519 0.006 0.    0.195 0.051]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0378
  Image Loss: 0.0294
  Total Loss: 0.0483
  Image grad max: 0.010932394303381443
  Output probs: [[0.342 0.    0.    0.003 0.    0.423 0.006 0.    0.176 0.05 ]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0274
  Image Loss: 0.0300
  Total Loss: 0.0437
  Image grad max: 0.008224192075431347
  Output probs: [[0.471 0.    0.    0.003 0.    0.329 0.006 0.    0.146 0.045]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0240
  Image Loss: 0.0307
  Total Loss: 0.0427
  Image grad max: 0.007806376554071903
  Output probs: [[0.583 0.    0.    0.002 0.    0.255 0.006 0.    0.115 0.039]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0260
  Image Loss: 0.0311
  Total Loss: 0.0442
  Image grad max: 0.012649829499423504
  Output probs: [[0.661 0.    0.    0.002 0.    0.209 0.006 0.    0.09  0.033]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0296
  Image Loss: 0.0314
  Total Loss: 0.0463
  Image grad max: 0.016107838600873947
  Output probs: [[0.704 0.    0.    0.001 0.    0.189 0.006 0.    0.071 0.028]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0316
  Image Loss: 0.0315
  Total Loss: 0.0473
  Image grad max: 0.017831260338425636
  Output probs: [[0.717 0.    0.    0.001 0.    0.191 0.006 0.    0.06  0.025]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0302
  Image Loss: 0.0314
  Total Loss: 0.0465
  Image grad max: 0.018042491748929024
  Output probs: [[0.702 0.    0.    0.001 0.    0.215 0.006 0.    0.052 0.024]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0253
  Image Loss: 0.0311
  Total Loss: 0.0437
  Image grad max: 0.016554784029722214
  Output probs: [[0.66  0.    0.    0.001 0.    0.262 0.007 0.    0.048 0.022]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0185
  Image Loss: 0.0306
  Total Loss: 0.0398
  Image grad max: 0.013363770209252834
  Output probs: [[0.592 0.    0.    0.001 0.    0.333 0.008 0.    0.044 0.022]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0119
  Image Loss: 0.0300
  Total Loss: 0.0360
  Image grad max: 0.008728835731744766
  Output probs: [[0.507 0.    0.    0.001 0.    0.423 0.008 0.    0.041 0.021]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0078
  Image Loss: 0.0294
  Total Loss: 0.0333
  Image grad max: 0.003251246176660061
  Output probs: [[0.419 0.    0.    0.001 0.    0.516 0.009 0.    0.037 0.019]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0073
  Image Loss: 0.0288
  Total Loss: 0.0325
  Image grad max: 0.003197824815288186
  Output probs: [[0.347 0.    0.    0.001 0.    0.593 0.009 0.    0.033 0.017]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0097
  Image Loss: 0.0283
  Total Loss: 0.0331
  Image grad max: 0.007818810641765594
  Output probs: [[0.3   0.    0.    0.001 0.    0.645 0.009 0.    0.029 0.016]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0128
  Image Loss: 0.0278
  Total Loss: 0.0342
  Image grad max: 0.011018031276762486
  Output probs: [[0.278 0.    0.    0.001 0.    0.672 0.009 0.    0.027 0.015]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0147
  Image Loss: 0.0275
  Total Loss: 0.0348
  Image grad max: 0.012569062411785126
  Output probs: [[0.277 0.    0.    0.001 0.    0.675 0.009 0.    0.025 0.014]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0146
  Image Loss: 0.0272
  Total Loss: 0.0345
  Image grad max: 0.012672539800405502
  Output probs: [[0.296 0.    0.    0.001 0.    0.657 0.009 0.    0.024 0.014]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0126
  Image Loss: 0.0271
  Total Loss: 0.0334
  Image grad max: 0.01149272732436657
  Output probs: [[0.33  0.    0.    0.001 0.    0.623 0.009 0.    0.023 0.013]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0097
  Image Loss: 0.0270
  Total Loss: 0.0319
  Image grad max: 0.009225866757333279
  Output probs: [[0.377 0.    0.    0.001 0.    0.577 0.01  0.    0.022 0.013]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0070
  Image Loss: 0.0270
  Total Loss: 0.0305
  Image grad max: 0.006172012537717819
  Output probs: [[0.43  0.    0.    0.001 0.    0.525 0.01  0.    0.022 0.013]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0052
  Image Loss: 0.0270
  Total Loss: 0.0295
  Image grad max: 0.002866868395358324
  Output probs: [[0.481 0.    0.    0.001 0.    0.474 0.01  0.    0.021 0.013]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0045
  Image Loss: 0.0269
  Total Loss: 0.0292
  Image grad max: 0.0019965667743235826
  Output probs: [[0.525 0.    0.    0.001 0.    0.431 0.01  0.    0.02  0.012]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0049
  Image Loss: 0.0269
  Total Loss: 0.0293
  Image grad max: 0.00365600548684597
  Output probs: [[0.556 0.    0.    0.001 0.    0.402 0.01  0.    0.019 0.012]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0056
  Image Loss: 0.0267
  Total Loss: 0.0295
  Image grad max: 0.005610159132629633
  Output probs: [[0.571 0.    0.    0.001 0.    0.388 0.01  0.    0.019 0.012]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0060
  Image Loss: 0.0265
  Total Loss: 0.0296
  Image grad max: 0.006604492664337158
  Output probs: [[0.571 0.    0.    0.001 0.    0.389 0.01  0.    0.018 0.011]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0059
  Image Loss: 0.0263
  Total Loss: 0.0292
  Image grad max: 0.006556336767971516
  Output probs: [[0.556 0.    0.    0.001 0.    0.404 0.01  0.    0.018 0.011]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0053
  Image Loss: 0.0259
  Total Loss: 0.0286
  Image grad max: 0.0055649057030677795
  Output probs: [[0.529 0.    0.    0.001 0.    0.431 0.01  0.    0.018 0.011]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0046
  Image Loss: 0.0255
  Total Loss: 0.0278
  Image grad max: 0.003823012113571167
  Output probs: [[0.494 0.    0.    0.001 0.    0.466 0.01  0.    0.018 0.011]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0041
  Image Loss: 0.0251
  Total Loss: 0.0272
  Image grad max: 0.0024190417025238276
  Output probs: [[0.458 0.    0.    0.001 0.    0.502 0.01  0.    0.018 0.011]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0042
  Image Loss: 0.0247
  Total Loss: 0.0268
  Image grad max: 0.001756989280693233
  Output probs: [[0.426 0.    0.    0.001 0.    0.535 0.01  0.    0.018 0.011]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0047
  Image Loss: 0.0243
  Total Loss: 0.0266
  Image grad max: 0.0031177508644759655
  Output probs: [[0.402 0.    0.    0.001 0.    0.559 0.01  0.    0.017 0.011]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0053
  Image Loss: 0.0239
  Total Loss: 0.0266
  Image grad max: 0.004674260504543781
  Output probs: [[0.39  0.    0.    0.001 0.    0.572 0.01  0.    0.017 0.011]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0058
  Image Loss: 0.0236
  Total Loss: 0.0265
  Image grad max: 0.00552802300080657
  Output probs: [[0.388 0.    0.    0.001 0.    0.573 0.01  0.    0.017 0.011]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0058
  Image Loss: 0.0233
  Total Loss: 0.0262
  Image grad max: 0.00562641303986311
  Output probs: [[0.398 0.    0.    0.001 0.    0.564 0.01  0.    0.017 0.011]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0054
  Image Loss: 0.0231
  Total Loss: 0.0258
  Image grad max: 0.005025106016546488
  Output probs: [[0.415 0.    0.    0.001 0.    0.547 0.01  0.    0.017 0.011]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0049
  Image Loss: 0.0230
  Total Loss: 0.0254
  Image grad max: 0.0038682392332702875
  Output probs: [[0.437 0.    0.    0.001 0.    0.525 0.01  0.    0.017 0.011]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0043
  Image Loss: 0.0228
  Total Loss: 0.0250
  Image grad max: 0.0026636957190930843
  Output probs: [[0.461 0.    0.    0.001 0.    0.501 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0040
  Image Loss: 0.0227
  Total Loss: 0.0247
  Image grad max: 0.0016361502930521965
  Output probs: [[0.482 0.    0.    0.001 0.    0.48  0.01  0.    0.016 0.01 ]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0039
  Image Loss: 0.0225
  Total Loss: 0.0245
  Image grad max: 0.0018812590278685093
  Output probs: [[0.499 0.    0.    0.001 0.    0.464 0.01  0.    0.016 0.01 ]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0039
  Image Loss: 0.0224
  Total Loss: 0.0243
  Image grad max: 0.002572485711425543
  Output probs: [[0.508 0.    0.    0.001 0.    0.454 0.01  0.    0.016 0.01 ]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0040
  Image Loss: 0.0222
  Total Loss: 0.0242
  Image grad max: 0.0029699308797717094
  Output probs: [[0.51  0.    0.    0.001 0.    0.453 0.01  0.    0.016 0.01 ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0040
  Image Loss: 0.0220
  Total Loss: 0.0240
  Image grad max: 0.003036693437024951
  Output probs: [[0.504 0.    0.    0.001 0.    0.459 0.01  0.    0.016 0.01 ]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0039
  Image Loss: 0.0217
  Total Loss: 0.0237
  Image grad max: 0.0027841422706842422
  Output probs: [[0.492 0.    0.    0.001 0.    0.471 0.01  0.    0.016 0.01 ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0038
  Image Loss: 0.0214
  Total Loss: 0.0234
  Image grad max: 0.002266799798235297
  Output probs: [[0.476 0.    0.    0.001 0.    0.486 0.01  0.    0.016 0.01 ]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0038
  Image Loss: 0.0212
  Total Loss: 0.0231
  Image grad max: 0.001658847788348794
  Output probs: [[0.46  0.    0.    0.001 0.    0.503 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0039
  Image Loss: 0.0209
  Total Loss: 0.0228
  Image grad max: 0.0016679690452292562
  Output probs: [[0.445 0.    0.    0.001 0.    0.517 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0041
  Image Loss: 0.0206
  Total Loss: 0.0227
  Image grad max: 0.0022919627372175455
  Output probs: [[0.436 0.    0.    0.001 0.    0.527 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0043
  Image Loss: 0.0204
  Total Loss: 0.0225
  Image grad max: 0.0027265644166618586
  Output probs: [[0.431 0.    0.    0.001 0.    0.531 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0044
  Image Loss: 0.0202
  Total Loss: 0.0223
  Image grad max: 0.0029181824065744877
  Output probs: [[0.433 0.    0.    0.001 0.    0.53  0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0044
  Image Loss: 0.0200
  Total Loss: 0.0221
  Image grad max: 0.0028550249990075827
  Output probs: [[0.439 0.    0.    0.    0.    0.523 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0042
  Image Loss: 0.0198
  Total Loss: 0.0219
  Image grad max: 0.00256471149623394
  Output probs: [[0.449 0.    0.    0.    0.    0.513 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0041
  Image Loss: 0.0197
  Total Loss: 0.0217
  Image grad max: 0.002113733906298876
  Output probs: [[0.461 0.    0.    0.    0.    0.501 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0039
  Image Loss: 0.0195
  Total Loss: 0.0215
  Image grad max: 0.001592810731381178
  Output probs: [[0.472 0.    0.    0.    0.    0.49  0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0039
  Image Loss: 0.0194
  Total Loss: 0.0213
  Image grad max: 0.0014794666785746813
  Output probs: [[0.481 0.    0.    0.    0.    0.482 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0038
  Image Loss: 0.0192
  Total Loss: 0.0212
  Image grad max: 0.0017326672095805407
  Output probs: [[0.485 0.    0.    0.    0.    0.477 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0038
  Image Loss: 0.0191
  Total Loss: 0.0210
  Image grad max: 0.0019349639769643545
  Output probs: [[0.486 0.    0.    0.    0.    0.477 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0038
  Image Loss: 0.0189
  Total Loss: 0.0208
  Image grad max: 0.0019433606648817658
  Output probs: [[0.482 0.    0.    0.    0.    0.481 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0038
  Image Loss: 0.0187
  Total Loss: 0.0206
  Image grad max: 0.0017729330575093627
  Output probs: [[0.475 0.    0.    0.    0.    0.487 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0039
  Image Loss: 0.0185
  Total Loss: 0.0205
  Image grad max: 0.0014675905695185065
  Output probs: [[0.467 0.    0.    0.    0.    0.495 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0039
  Image Loss: 0.0183
  Total Loss: 0.0203
  Image grad max: 0.0013209409080445766
  Output probs: [[0.459 0.    0.    0.    0.    0.503 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0040
  Image Loss: 0.0181
  Total Loss: 0.0201
  Image grad max: 0.0016715929377824068
  Output probs: [[0.453 0.    0.    0.    0.    0.509 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0040
  Image Loss: 0.0180
  Total Loss: 0.0200
  Image grad max: 0.001947843935340643
  Output probs: [[0.449 0.    0.    0.    0.    0.513 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0041
  Image Loss: 0.0178
  Total Loss: 0.0198
  Image grad max: 0.0020973249338567257
  Output probs: [[0.449 0.    0.    0.    0.    0.513 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0041
  Image Loss: 0.0177
  Total Loss: 0.0197
  Image grad max: 0.0021009864285588264
  Output probs: [[0.452 0.    0.    0.    0.    0.51  0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0040
  Image Loss: 0.0175
  Total Loss: 0.0195
  Image grad max: 0.0019693979993462563
  Output probs: [[0.457 0.    0.    0.    0.    0.505 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0040
  Image Loss: 0.0174
  Total Loss: 0.0194
  Image grad max: 0.001737687038257718
  Output probs: [[0.463 0.    0.    0.    0.    0.499 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0039
  Image Loss: 0.0173
  Total Loss: 0.0192
  Image grad max: 0.0014587017940357327
  Output probs: [[0.469 0.    0.    0.    0.    0.493 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0039
  Image Loss: 0.0172
  Total Loss: 0.0191
  Image grad max: 0.0012521572643890977
  Output probs: [[0.474 0.    0.    0.    0.    0.489 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0039
  Image Loss: 0.0170
  Total Loss: 0.0190
  Image grad max: 0.0013404532801359892
  Output probs: [[0.476 0.    0.    0.    0.    0.487 0.01  0.    0.017 0.01 ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0039
  Image Loss: 0.0169
  Total Loss: 0.0188
  Image grad max: 0.0014328648103401065
  Output probs: [[0.475 0.    0.    0.    0.    0.487 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0039
  Image Loss: 0.0168
  Total Loss: 0.0187
  Image grad max: 0.001413289224728942
  Output probs: [[0.473 0.    0.    0.    0.    0.489 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0039
  Image Loss: 0.0166
  Total Loss: 0.0186
  Image grad max: 0.0012935304548591375
  Output probs: [[0.469 0.    0.    0.    0.    0.493 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0039
  Image Loss: 0.0165
  Total Loss: 0.0184
  Image grad max: 0.0011863504769280553
  Output probs: [[0.465 0.    0.    0.    0.    0.498 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0039
  Image Loss: 0.0164
  Total Loss: 0.0183
  Image grad max: 0.0013827778166159987
  Output probs: [[0.461 0.    0.    0.    0.    0.501 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0039
  Image Loss: 0.0162
  Total Loss: 0.0182
  Image grad max: 0.001555829425342381
  Output probs: [[0.458 0.    0.    0.    0.    0.504 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0040
  Image Loss: 0.0161
  Total Loss: 0.0181
  Image grad max: 0.0016677994281053543
  Output probs: [[0.458 0.    0.    0.    0.    0.505 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0040
  Image Loss: 0.0160
  Total Loss: 0.0179
  Image grad max: 0.0016945843817666173
  Output probs: [[0.459 0.    0.    0.    0.    0.503 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0040
  Image Loss: 0.0159
  Total Loss: 0.0178
  Image grad max: 0.0016382279573008418
  Output probs: [[0.461 0.    0.    0.    0.    0.501 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0039
  Image Loss: 0.0157
  Total Loss: 0.0177
  Image grad max: 0.0015174788422882557
  Output probs: [[0.465 0.    0.    0.    0.    0.498 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0039
  Image Loss: 0.0156
  Total Loss: 0.0176
  Image grad max: 0.0013631503097712994
  Output probs: [[0.468 0.    0.    0.    0.    0.494 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0039
  Image Loss: 0.0155
  Total Loss: 0.0175
  Image grad max: 0.001211668481118977
  Output probs: [[0.47  0.    0.    0.    0.    0.492 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0039
  Image Loss: 0.0155
  Total Loss: 0.0174
  Image grad max: 0.0011300663463771343
  Output probs: [[0.472 0.    0.    0.    0.    0.491 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0038
  Image Loss: 0.0153
  Total Loss: 0.0173
  Image grad max: 0.0011594975367188454
  Output probs: [[0.471 0.    0.    0.    0.    0.491 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0038
  Image Loss: 0.0152
  Total Loss: 0.0172
  Image grad max: 0.001141347922384739
  Output probs: [[0.47  0.    0.    0.    0.    0.493 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0038
  Image Loss: 0.0151
  Total Loss: 0.0171
  Image grad max: 0.0011223296169191599
  Output probs: [[0.468 0.    0.    0.    0.    0.495 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0039
  Image Loss: 0.0150
  Total Loss: 0.0170
  Image grad max: 0.0012115456629544497
  Output probs: [[0.465 0.    0.    0.    0.    0.497 0.01  0.    0.018 0.01 ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0039
  Image Loss: 0.0149
  Total Loss: 0.0169
  Image grad max: 0.0013178422814235091
  Output probs: [[0.463 0.    0.    0.    0.    0.499 0.009 0.    0.018 0.01 ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0039
  Image Loss: 0.0148
  Total Loss: 0.0168
  Image grad max: 0.0014028615551069379
  Output probs: [[0.462 0.    0.    0.    0.    0.5   0.009 0.    0.018 0.01 ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0039
  Image Loss: 0.0147
  Total Loss: 0.0167
  Image grad max: 0.001446090522222221
  Output probs: [[0.463 0.    0.    0.    0.    0.5   0.009 0.    0.018 0.01 ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0039
  Image Loss: 0.0146
  Total Loss: 0.0166
  Image grad max: 0.0014376724138855934
  Output probs: [[0.464 0.    0.    0.    0.    0.499 0.009 0.    0.018 0.01 ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0039
  Image Loss: 0.0145
  Total Loss: 0.0165
  Image grad max: 0.0013802062021568418
  Output probs: [[0.466 0.    0.    0.    0.    0.497 0.009 0.    0.018 0.01 ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0038
  Image Loss: 0.0145
  Total Loss: 0.0164
  Image grad max: 0.0012927890056744218
  Output probs: [[0.467 0.    0.    0.    0.    0.495 0.009 0.    0.018 0.009]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0038
  Image Loss: 0.0144
  Total Loss: 0.0163
  Image grad max: 0.0011981343850493431
  Output probs: [[0.469 0.    0.    0.    0.    0.494 0.009 0.    0.018 0.009]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0038
  Image Loss: 0.0143
  Total Loss: 0.0162
  Image grad max: 0.0011198192369192839
  Output probs: [[0.47  0.    0.    0.    0.    0.493 0.009 0.    0.018 0.009]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0038
  Image Loss: 0.0142
  Total Loss: 0.0161
  Image grad max: 0.0010751544032245874
  Output probs: [[0.47  0.    0.    0.    0.    0.493 0.009 0.    0.018 0.009]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0038
  Image Loss: 0.0141
  Total Loss: 0.0160
  Image grad max: 0.0010700093116611242
  Output probs: [[0.469 0.    0.    0.    0.    0.494 0.009 0.    0.018 0.009]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0038
  Image Loss: 0.0140
  Total Loss: 0.0159
  Image grad max: 0.0011005119886249304
  Output probs: [[0.468 0.    0.    0.    0.    0.495 0.009 0.    0.018 0.009]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0038
  Image Loss: 0.0140
  Total Loss: 0.0159
  Image grad max: 0.0011548098409548402
  Output probs: [[0.467 0.    0.    0.    0.    0.497 0.009 0.    0.018 0.009]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0038
  Image Loss: 0.0139
  Total Loss: 0.0158
  Image grad max: 0.0012142567429691553
  Output probs: [[0.466 0.    0.    0.    0.    0.498 0.009 0.    0.018 0.009]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0038
  Image Loss: 0.0138
  Total Loss: 0.0157
  Image grad max: 0.001260755816474557
  Output probs: [[0.465 0.    0.    0.    0.    0.498 0.009 0.    0.018 0.009]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0038
  Image Loss: 0.0137
  Total Loss: 0.0156
  Image grad max: 0.00128131581004709
  Output probs: [[0.465 0.    0.    0.    0.    0.498 0.009 0.    0.018 0.009]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0038
  Image Loss: 0.0136
  Total Loss: 0.0155
  Image grad max: 0.0012706697452813387
  Output probs: [[0.466 0.    0.    0.    0.    0.498 0.009 0.    0.018 0.009]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0038
  Image Loss: 0.0136
  Total Loss: 0.0155
  Image grad max: 0.0012325524585321546
  Output probs: [[0.467 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0038
  Image Loss: 0.0135
  Total Loss: 0.0154
  Image grad max: 0.0011779464548453689
  Output probs: [[0.468 0.    0.    0.    0.    0.495 0.009 0.    0.017 0.009]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0037
  Image Loss: 0.0134
  Total Loss: 0.0153
  Image grad max: 0.001121125416830182
  Output probs: [[0.469 0.    0.    0.    0.    0.495 0.009 0.    0.017 0.009]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0037
  Image Loss: 0.0134
  Total Loss: 0.0152
  Image grad max: 0.0010760213481262326
  Output probs: [[0.469 0.    0.    0.    0.    0.494 0.009 0.    0.017 0.009]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0037
  Image Loss: 0.0133
  Total Loss: 0.0152
  Image grad max: 0.0010521417716518044
  Output probs: [[0.469 0.    0.    0.    0.    0.494 0.009 0.    0.017 0.009]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0037
  Image Loss: 0.0132
  Total Loss: 0.0151
  Image grad max: 0.0010524034732952714
  Output probs: [[0.469 0.    0.    0.    0.    0.495 0.009 0.    0.017 0.009]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0037
  Image Loss: 0.0132
  Total Loss: 0.0150
  Image grad max: 0.0010725451866164804
  Output probs: [[0.468 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0037
  Image Loss: 0.0131
  Total Loss: 0.0149
  Image grad max: 0.0011032792972400784
  Output probs: [[0.467 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0037
  Image Loss: 0.0130
  Total Loss: 0.0149
  Image grad max: 0.0011329271364957094
  Output probs: [[0.467 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0037
  Image Loss: 0.0130
  Total Loss: 0.0148
  Image grad max: 0.0011510761687532067
  Output probs: [[0.467 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0037
  Image Loss: 0.0129
  Total Loss: 0.0147
  Image grad max: 0.001151494449004531
  Output probs: [[0.467 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0037
  Image Loss: 0.0128
  Total Loss: 0.0147
  Image grad max: 0.001133742625825107
  Output probs: [[0.468 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0037
  Image Loss: 0.0128
  Total Loss: 0.0146
  Image grad max: 0.0011026979191228747
  Output probs: [[0.468 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0037
  Image Loss: 0.0127
  Total Loss: 0.0146
  Image grad max: 0.001066115451976657
  Output probs: [[0.469 0.    0.    0.    0.    0.495 0.009 0.    0.017 0.009]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0037
  Image Loss: 0.0127
  Total Loss: 0.0145
  Image grad max: 0.0010336486157029867
  Output probs: [[0.469 0.    0.    0.    0.    0.495 0.009 0.    0.017 0.009]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0037
  Image Loss: 0.0126
  Total Loss: 0.0144
  Image grad max: 0.0010126970009878278
  Output probs: [[0.469 0.    0.    0.    0.    0.495 0.009 0.    0.017 0.009]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0036
  Image Loss: 0.0125
  Total Loss: 0.0144
  Image grad max: 0.0010059730848297477
  Output probs: [[0.469 0.    0.    0.    0.    0.495 0.009 0.    0.017 0.009]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0036
  Image Loss: 0.0125
  Total Loss: 0.0143
  Image grad max: 0.0010124101536348462
  Output probs: [[0.469 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0036
  Image Loss: 0.0124
  Total Loss: 0.0143
  Image grad max: 0.001027365680783987
  Output probs: [[0.468 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0036
  Image Loss: 0.0124
  Total Loss: 0.0142
  Image grad max: 0.0010432939743623137
  Output probs: [[0.468 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0036
  Image Loss: 0.0123
  Total Loss: 0.0141
  Image grad max: 0.0010533288586884737
  Output probs: [[0.468 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0036
  Image Loss: 0.0123
  Total Loss: 0.0141
  Image grad max: 0.0010529706487432122
  Output probs: [[0.468 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.009]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0036
  Image Loss: 0.0122
  Total Loss: 0.0140
  Image grad max: 0.001041211187839508
  Output probs: [[0.469 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0036
  Image Loss: 0.0122
  Total Loss: 0.0140
  Image grad max: 0.001020984724164009
  Output probs: [[0.469 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0036
  Image Loss: 0.0121
  Total Loss: 0.0139
  Image grad max: 0.0010033791186288
  Output probs: [[0.469 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0036
  Image Loss: 0.0121
  Total Loss: 0.0139
  Image grad max: 0.000987850595265627
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0036
  Image Loss: 0.0120
  Total Loss: 0.0138
  Image grad max: 0.000978633644990623
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0036
  Image Loss: 0.0120
  Total Loss: 0.0138
  Image grad max: 0.0009776236256584525
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0036
  Image Loss: 0.0119
  Total Loss: 0.0137
  Image grad max: 0.0009839521953836083
  Output probs: [[0.469 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.009]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0036
  Image Loss: 0.0119
  Total Loss: 0.0137
  Image grad max: 0.0009945377241820097
  Output probs: [[0.469 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.008]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0036
  Image Loss: 0.0119
  Total Loss: 0.0136
  Image grad max: 0.0010050969431176782
  Output probs: [[0.469 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.008]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0036
  Image Loss: 0.0118
  Total Loss: 0.0136
  Image grad max: 0.0010118618374690413
  Output probs: [[0.469 0.    0.    0.    0.    0.497 0.009 0.    0.017 0.008]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0036
  Image Loss: 0.0118
  Total Loss: 0.0135
  Image grad max: 0.0010126057313755155
  Output probs: [[0.469 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.008]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0035
  Image Loss: 0.0117
  Total Loss: 0.0135
  Image grad max: 0.001007132581435144
  Output probs: [[0.469 0.    0.    0.    0.    0.496 0.009 0.    0.017 0.008]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0035
  Image Loss: 0.0117
  Total Loss: 0.0135
  Image grad max: 0.000997409108094871
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.017 0.008]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0035
  Image Loss: 0.0116
  Total Loss: 0.0134
  Image grad max: 0.0009865626925602555
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0035
  Image Loss: 0.0116
  Total Loss: 0.0134
  Image grad max: 0.0009779114043340087
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0035
  Image Loss: 0.0116
  Total Loss: 0.0133
  Image grad max: 0.0009737883228808641
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0035
  Image Loss: 0.0115
  Total Loss: 0.0133
  Image grad max: 0.0009747559670358896
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0035
  Image Loss: 0.0115
  Total Loss: 0.0132
  Image grad max: 0.0009797126986086369
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0035
  Image Loss: 0.0115
  Total Loss: 0.0132
  Image grad max: 0.0009862296283245087
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0035
  Image Loss: 0.0114
  Total Loss: 0.0132
  Image grad max: 0.0009915258269757032
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0035
  Image Loss: 0.0114
  Total Loss: 0.0131
  Image grad max: 0.0009937240974977612
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0035
  Image Loss: 0.0114
  Total Loss: 0.0131
  Image grad max: 0.0009919238509610295
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0035
  Image Loss: 0.0113
  Total Loss: 0.0131
  Image grad max: 0.0009865855099633336
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0035
  Image Loss: 0.0113
  Total Loss: 0.0130
  Image grad max: 0.000979628530330956
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0035
  Image Loss: 0.0113
  Total Loss: 0.0130
  Image grad max: 0.0009733225451782346
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0035
  Image Loss: 0.0112
  Total Loss: 0.0129
  Image grad max: 0.0009693868923932314
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0035
  Image Loss: 0.0112
  Total Loss: 0.0129
  Image grad max: 0.0009686520788818598
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0034
  Image Loss: 0.0112
  Total Loss: 0.0129
  Image grad max: 0.0009707321878522635
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0034
  Image Loss: 0.0111
  Total Loss: 0.0128
  Image grad max: 0.0009742933325469494
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0034
  Image Loss: 0.0111
  Total Loss: 0.0128
  Image grad max: 0.0009776023216545582
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0034
  Image Loss: 0.0111
  Total Loss: 0.0128
  Image grad max: 0.0009790904587134719
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0034
  Image Loss: 0.0110
  Total Loss: 0.0127
  Image grad max: 0.0009779593674466014
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0034
  Image Loss: 0.0110
  Total Loss: 0.0127
  Image grad max: 0.0009748568991199136
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0034
  Image Loss: 0.0110
  Total Loss: 0.0127
  Image grad max: 0.000970697496086359
  Output probs: [[0.47  0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0034
  Image Loss: 0.0109
  Total Loss: 0.0127
  Image grad max: 0.0009668683633208275
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0034
  Image Loss: 0.0109
  Total Loss: 0.0126
  Image grad max: 0.0009644144447520375
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0034
  Image Loss: 0.0109
  Total Loss: 0.0126
  Image grad max: 0.000963856466114521
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0034
  Image Loss: 0.0109
  Total Loss: 0.0126
  Image grad max: 0.0009646222461014986
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0034
  Image Loss: 0.0108
  Total Loss: 0.0125
  Image grad max: 0.0009661148069426417
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0034
  Image Loss: 0.0108
  Total Loss: 0.0125
  Image grad max: 0.0009673594031482935
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0034
  Image Loss: 0.0108
  Total Loss: 0.0125
  Image grad max: 0.0009675329783931375
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0034
  Image Loss: 0.0108
  Total Loss: 0.0125
  Image grad max: 0.0009663786040619016
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0034
  Image Loss: 0.0107
  Total Loss: 0.0124
  Image grad max: 0.0009640667121857405
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0034
  Image Loss: 0.0107
  Total Loss: 0.0124
  Image grad max: 0.0009611553978174925
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0034
  Image Loss: 0.0107
  Total Loss: 0.0124
  Image grad max: 0.0009582506027072668
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0034
  Image Loss: 0.0107
  Total Loss: 0.0124
  Image grad max: 0.0009563346393406391
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0034
  Image Loss: 0.0107
  Total Loss: 0.0123
  Image grad max: 0.0009555205469951034
  Output probs: [[0.471 0.    0.    0.    0.    0.496 0.009 0.    0.016 0.008]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0034
  Image Loss: 0.0106
  Total Loss: 0.0123
  Image grad max: 0.0009555902797728777
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0033
  Image Loss: 0.0106
  Total Loss: 0.0123
  Image grad max: 0.0009560304461047053
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0033
  Image Loss: 0.0106
  Total Loss: 0.0123
  Image grad max: 0.0009562302147969604
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0033
  Image Loss: 0.0106
  Total Loss: 0.0122
  Image grad max: 0.0009556765435263515
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0033
  Image Loss: 0.0105
  Total Loss: 0.0122
  Image grad max: 0.0009544495260342956
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0033
  Image Loss: 0.0105
  Total Loss: 0.0122
  Image grad max: 0.0009526632493361831
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0033
  Image Loss: 0.0105
  Total Loss: 0.0122
  Image grad max: 0.0009504994377493858
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.016 0.008]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0033
  Image Loss: 0.0105
  Total Loss: 0.0122
  Image grad max: 0.0009484715992584825
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0033
  Image Loss: 0.0105
  Total Loss: 0.0121
  Image grad max: 0.0009469492360949516
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0033
  Image Loss: 0.0105
  Total Loss: 0.0121
  Image grad max: 0.0009459758875891566
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0033
  Image Loss: 0.0104
  Total Loss: 0.0121
  Image grad max: 0.0009456448024138808
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0033
  Image Loss: 0.0104
  Total Loss: 0.0121
  Image grad max: 0.0009455535328015685
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0033
  Image Loss: 0.0104
  Total Loss: 0.0120
  Image grad max: 0.0009453402599319816
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0033
  Image Loss: 0.0104
  Total Loss: 0.0120
  Image grad max: 0.000944728497415781
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0033
  Image Loss: 0.0104
  Total Loss: 0.0120
  Image grad max: 0.0009436745895072818
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0033
  Image Loss: 0.0103
  Total Loss: 0.0120
  Image grad max: 0.0009421835420653224
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0033
  Image Loss: 0.0103
  Total Loss: 0.0120
  Image grad max: 0.0009406566387042403
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0033
  Image Loss: 0.0103
  Total Loss: 0.0120
  Image grad max: 0.000939215999096632
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0033
  Image Loss: 0.0103
  Total Loss: 0.0119
  Image grad max: 0.0009381003910675645
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0033
  Image Loss: 0.0103
  Total Loss: 0.0119
  Image grad max: 0.0009374016663059592
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0033
  Image Loss: 0.0103
  Total Loss: 0.0119
  Image grad max: 0.000937002943828702
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0033
  Image Loss: 0.0102
  Total Loss: 0.0119
  Image grad max: 0.0009367122547701001
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0033
  Image Loss: 0.0102
  Total Loss: 0.0119
  Image grad max: 0.0009362783748656511
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0033
  Image Loss: 0.0102
  Total Loss: 0.0119
  Image grad max: 0.0009355056099593639
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0033
  Image Loss: 0.0102
  Total Loss: 0.0118
  Image grad max: 0.0009345019934698939
  Output probs: [[0.471 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0033
  Image Loss: 0.0102
  Total Loss: 0.0118
  Image grad max: 0.0009333083871752024
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0033
  Image Loss: 0.0102
  Total Loss: 0.0118
  Image grad max: 0.0009321486577391624
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0033
  Image Loss: 0.0102
  Total Loss: 0.0118
  Image grad max: 0.0009311430621892214
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0033
  Image Loss: 0.0101
  Total Loss: 0.0118
  Image grad max: 0.0009303814731538296
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0033
  Image Loss: 0.0101
  Total Loss: 0.0118
  Image grad max: 0.0009298495715484023
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0033
  Image Loss: 0.0101
  Total Loss: 0.0118
  Image grad max: 0.0009293794864788651
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0033
  Image Loss: 0.0101
  Total Loss: 0.0117
  Image grad max: 0.0009288586443290114
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0033
  Image Loss: 0.0101
  Total Loss: 0.0117
  Image grad max: 0.0009282547980546951
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0033
  Image Loss: 0.0101
  Total Loss: 0.0117
  Image grad max: 0.0009274741169065237
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0032
  Image Loss: 0.0101
  Total Loss: 0.0117
  Image grad max: 0.0009265390690416098
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0032
  Image Loss: 0.0101
  Total Loss: 0.0117
  Image grad max: 0.0009255748009309173
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0117
  Image grad max: 0.0009246900444850326
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0117
  Image grad max: 0.0009240639628842473
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0116
  Image grad max: 0.0009234878234565258
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0116
  Image grad max: 0.0009229525458067656
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0116
  Image grad max: 0.0009224243694916368
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0116
  Image grad max: 0.0009217928163707256
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0116
  Image grad max: 0.0009210869902744889
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0116
  Image grad max: 0.0009202902438119054
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0116
  Image grad max: 0.000919542508199811
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0032
  Image Loss: 0.0100
  Total Loss: 0.0116
  Image grad max: 0.0009187698597088456
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.008]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0116
  Image grad max: 0.0009180946508422494
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009175612358376384
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009170351549983025
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009164930088445544
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009159435285255313
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009152622660622001
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009146095253527164
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009139103349298239
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009132381528615952
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.000912667834199965
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0032
  Image Loss: 0.0099
  Total Loss: 0.0115
  Image grad max: 0.0009121656185016036
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.000911695184186101
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009112383704632521
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.000910714385099709
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009101275354623795
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009095172863453627
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.000908880727365613
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009083733893930912
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009078277507796884
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009073453256860375
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009069107472896576
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009064233163371682
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.0009059043368324637
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0114
  Image grad max: 0.000905386870726943
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0032
  Image Loss: 0.0098
  Total Loss: 0.0113
  Image grad max: 0.0009048734791576862
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009043681202456355
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009038321441039443
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009033606620505452
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009028824279084802
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009024592582136393
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009020367870107293
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.000901563442312181
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009011198999360204
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009006495820358396
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0009001307189464569
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0008996581891551614
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0008992187213152647
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.009 0.    0.015 0.007]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0008988233748823404
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0008983932202681899
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0008985074236989021
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0113
  Image grad max: 0.0008984816959127784
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0112
  Image grad max: 0.0008980325656011701
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0112
  Image grad max: 0.0008971797069534659
  Output probs: [[0.472 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0112
  Image grad max: 0.000896263518370688
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0032
  Image Loss: 0.0097
  Total Loss: 0.0112
  Image grad max: 0.0008955110097303987
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008950942428782582
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008950306801125407
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008950363844633102
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008949752664193511
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008946552406996489
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.000894143246114254
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008934754878282547
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008928170427680016
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008923428831622005
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.000892081530764699
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0032
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008919311221688986
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0031
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008918426465243101
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0031
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008909698808565736
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0031
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008902063127607107
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0031
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008898464730009437
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0031
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.0008898460073396564
  Output probs: [[0.473 0.    0.    0.    0.    0.497 0.008 0.    0.015 0.007]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0031
  Image Loss: 0.0096
  Total Loss: 0.0112
  Image grad max: 0.000890007708221674
Visualization saved to adversarial_figures/adversarial_testing.png
