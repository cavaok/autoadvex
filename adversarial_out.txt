running dynamical.py
Epoch [1/30], Batch [0/6000], Loss: 7.2903
Epoch [1/30], Batch [100/6000], Loss: 4.0539
Epoch [1/30], Batch [200/6000], Loss: 3.6675
Epoch [1/30], Batch [300/6000], Loss: 2.2957
Epoch [1/30], Batch [400/6000], Loss: 2.6747
Epoch [1/30], Batch [500/6000], Loss: 3.5504
Epoch [1/30], Batch [600/6000], Loss: 5.0288
Epoch [1/30], Batch [700/6000], Loss: 1.1786
Epoch [1/30], Batch [800/6000], Loss: 2.1067
Epoch [1/30], Batch [900/6000], Loss: 2.6216
Epoch [1/30], Batch [1000/6000], Loss: 2.3834
Epoch [1/30], Batch [1100/6000], Loss: 2.7916
Epoch [1/30], Batch [1200/6000], Loss: 2.5930
Epoch [1/30], Batch [1300/6000], Loss: 0.9092
Epoch [1/30], Batch [1400/6000], Loss: 1.7433
Epoch [1/30], Batch [1500/6000], Loss: 1.3072
Epoch [1/30], Batch [1600/6000], Loss: 1.9774
Epoch [1/30], Batch [1700/6000], Loss: 2.1508
Epoch [1/30], Batch [1800/6000], Loss: 2.3907
Epoch [1/30], Batch [1900/6000], Loss: 3.0417
Epoch [1/30], Batch [2000/6000], Loss: 2.1982
Epoch [1/30], Batch [2100/6000], Loss: 1.4493
Epoch [1/30], Batch [2200/6000], Loss: 0.9663
Epoch [1/30], Batch [2300/6000], Loss: 1.3689
Epoch [1/30], Batch [2400/6000], Loss: 0.4435
Epoch [1/30], Batch [2500/6000], Loss: 0.8242
Epoch [1/30], Batch [2600/6000], Loss: 4.0855
Epoch [1/30], Batch [2700/6000], Loss: 1.0349
Epoch [1/30], Batch [2800/6000], Loss: 1.7653
Epoch [1/30], Batch [2900/6000], Loss: 1.4075
Epoch [1/30], Batch [3000/6000], Loss: 1.9157
Epoch [1/30], Batch [3100/6000], Loss: 1.4883
Epoch [1/30], Batch [3200/6000], Loss: 1.3392
Epoch [1/30], Batch [3300/6000], Loss: 0.3891
Epoch [1/30], Batch [3400/6000], Loss: 3.2728
Epoch [1/30], Batch [3500/6000], Loss: 2.0049
Epoch [1/30], Batch [3600/6000], Loss: 2.5664
Epoch [1/30], Batch [3700/6000], Loss: 0.8489
Epoch [1/30], Batch [3800/6000], Loss: 0.8172
Epoch [1/30], Batch [3900/6000], Loss: 0.8980
Epoch [1/30], Batch [4000/6000], Loss: 1.2325
Epoch [1/30], Batch [4100/6000], Loss: 0.5455
Epoch [1/30], Batch [4200/6000], Loss: 1.0937
Epoch [1/30], Batch [4300/6000], Loss: 0.4969
Epoch [1/30], Batch [4400/6000], Loss: 0.7871
Epoch [1/30], Batch [4500/6000], Loss: 1.6090
Epoch [1/30], Batch [4600/6000], Loss: 0.5343
Epoch [1/30], Batch [4700/6000], Loss: 1.6664
Epoch [1/30], Batch [4800/6000], Loss: 1.2236
Epoch [1/30], Batch [4900/6000], Loss: 0.5560
Epoch [1/30], Batch [5000/6000], Loss: 1.0011
Epoch [1/30], Batch [5100/6000], Loss: 1.2956
Epoch [1/30], Batch [5200/6000], Loss: 1.2737
Epoch [1/30], Batch [5300/6000], Loss: 2.3016
Epoch [1/30], Batch [5400/6000], Loss: 0.6830
Epoch [1/30], Batch [5500/6000], Loss: 0.6166
Epoch [1/30], Batch [5600/6000], Loss: 0.7103
Epoch [1/30], Batch [5700/6000], Loss: 0.4939
Epoch [1/30], Batch [5800/6000], Loss: 1.8028
Epoch [1/30], Batch [5900/6000], Loss: 0.8185
Epoch [1/30], Loss: 1.8177
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.5157
Epoch [2/30], Batch [100/6000], Loss: 0.7260
Epoch [2/30], Batch [200/6000], Loss: 0.7173
Epoch [2/30], Batch [300/6000], Loss: 1.3847
Epoch [2/30], Batch [400/6000], Loss: 0.8971
Epoch [2/30], Batch [500/6000], Loss: 1.2586
Epoch [2/30], Batch [600/6000], Loss: 0.6490
Epoch [2/30], Batch [700/6000], Loss: 0.4144
Epoch [2/30], Batch [800/6000], Loss: 1.8751
Epoch [2/30], Batch [900/6000], Loss: 0.6187
Epoch [2/30], Batch [1000/6000], Loss: 2.7721
Epoch [2/30], Batch [1100/6000], Loss: 0.5226
Epoch [2/30], Batch [1200/6000], Loss: 2.3623
Epoch [2/30], Batch [1300/6000], Loss: 0.3355
Epoch [2/30], Batch [1400/6000], Loss: 0.4244
Epoch [2/30], Batch [1500/6000], Loss: 2.2738
Epoch [2/30], Batch [1600/6000], Loss: 1.5870
Epoch [2/30], Batch [1700/6000], Loss: 0.6362
Epoch [2/30], Batch [1800/6000], Loss: 3.5721
Epoch [2/30], Batch [1900/6000], Loss: 2.3249
Epoch [2/30], Batch [2000/6000], Loss: 1.9219
Epoch [2/30], Batch [2100/6000], Loss: 0.7886
Epoch [2/30], Batch [2200/6000], Loss: 0.6742
Epoch [2/30], Batch [2300/6000], Loss: 0.5054
Epoch [2/30], Batch [2400/6000], Loss: 1.3158
Epoch [2/30], Batch [2500/6000], Loss: 1.5732
Epoch [2/30], Batch [2600/6000], Loss: 0.6351
Epoch [2/30], Batch [2700/6000], Loss: 4.4594
Epoch [2/30], Batch [2800/6000], Loss: 0.6363
Epoch [2/30], Batch [2900/6000], Loss: 0.3586
Epoch [2/30], Batch [3000/6000], Loss: 0.5737
Epoch [2/30], Batch [3100/6000], Loss: 3.1398
Epoch [2/30], Batch [3200/6000], Loss: 1.7865
Epoch [2/30], Batch [3300/6000], Loss: 0.9012
Epoch [2/30], Batch [3400/6000], Loss: 1.2979
Epoch [2/30], Batch [3500/6000], Loss: 3.2951
Epoch [2/30], Batch [3600/6000], Loss: 2.9546
Epoch [2/30], Batch [3700/6000], Loss: 0.9458
Epoch [2/30], Batch [3800/6000], Loss: 0.3566
Epoch [2/30], Batch [3900/6000], Loss: 0.2411
Epoch [2/30], Batch [4000/6000], Loss: 0.6700
Epoch [2/30], Batch [4100/6000], Loss: 0.3636
Epoch [2/30], Batch [4200/6000], Loss: 0.2259
Epoch [2/30], Batch [4300/6000], Loss: 0.9375
Epoch [2/30], Batch [4400/6000], Loss: 0.9185
Epoch [2/30], Batch [4500/6000], Loss: 0.8539
Epoch [2/30], Batch [4600/6000], Loss: 0.4319
Epoch [2/30], Batch [4700/6000], Loss: 1.1777
Epoch [2/30], Batch [4800/6000], Loss: 0.2361
Epoch [2/30], Batch [4900/6000], Loss: 1.3116
Epoch [2/30], Batch [5000/6000], Loss: 1.0424
Epoch [2/30], Batch [5100/6000], Loss: 1.2088
Epoch [2/30], Batch [5200/6000], Loss: 1.7786
Epoch [2/30], Batch [5300/6000], Loss: 0.3066
Epoch [2/30], Batch [5400/6000], Loss: 4.1701
Epoch [2/30], Batch [5500/6000], Loss: 0.6188
Epoch [2/30], Batch [5600/6000], Loss: 0.3570
Epoch [2/30], Batch [5700/6000], Loss: 0.6788
Epoch [2/30], Batch [5800/6000], Loss: 1.0420
Epoch [2/30], Batch [5900/6000], Loss: 1.3937
Epoch [2/30], Loss: 1.0824
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 1.6655
Epoch [3/30], Batch [100/6000], Loss: 1.5465
Epoch [3/30], Batch [200/6000], Loss: 1.3431
Epoch [3/30], Batch [300/6000], Loss: 1.0376
Epoch [3/30], Batch [400/6000], Loss: 1.4128
Epoch [3/30], Batch [500/6000], Loss: 1.9242
Epoch [3/30], Batch [600/6000], Loss: 0.3127
Epoch [3/30], Batch [700/6000], Loss: 1.4648
Epoch [3/30], Batch [800/6000], Loss: 1.1870
Epoch [3/30], Batch [900/6000], Loss: 2.5412
Epoch [3/30], Batch [1000/6000], Loss: 0.9972
Epoch [3/30], Batch [1100/6000], Loss: 0.3152
Epoch [3/30], Batch [1200/6000], Loss: 0.2930
Epoch [3/30], Batch [1300/6000], Loss: 0.3595
Epoch [3/30], Batch [1400/6000], Loss: 0.3580
Epoch [3/30], Batch [1500/6000], Loss: 0.3807
Epoch [3/30], Batch [1600/6000], Loss: 1.4875
Epoch [3/30], Batch [1700/6000], Loss: 0.9564
Epoch [3/30], Batch [1800/6000], Loss: 0.7551
Epoch [3/30], Batch [1900/6000], Loss: 0.7180
Epoch [3/30], Batch [2000/6000], Loss: 0.8706
Epoch [3/30], Batch [2100/6000], Loss: 0.5157
Epoch [3/30], Batch [2200/6000], Loss: 1.4164
Epoch [3/30], Batch [2300/6000], Loss: 0.2275
Epoch [3/30], Batch [2400/6000], Loss: 0.2687
Epoch [3/30], Batch [2500/6000], Loss: 0.2920
Epoch [3/30], Batch [2600/6000], Loss: 0.6790
Epoch [3/30], Batch [2700/6000], Loss: 0.7303
Epoch [3/30], Batch [2800/6000], Loss: 0.7208
Epoch [3/30], Batch [2900/6000], Loss: 1.8158
Epoch [3/30], Batch [3000/6000], Loss: 0.4437
Epoch [3/30], Batch [3100/6000], Loss: 0.2436
Epoch [3/30], Batch [3200/6000], Loss: 1.6512
Epoch [3/30], Batch [3300/6000], Loss: 0.9847
Epoch [3/30], Batch [3400/6000], Loss: 0.2544
Epoch [3/30], Batch [3500/6000], Loss: 0.3208
Epoch [3/30], Batch [3600/6000], Loss: 2.3693
Epoch [3/30], Batch [3700/6000], Loss: 1.4532
Epoch [3/30], Batch [3800/6000], Loss: 1.4568
Epoch [3/30], Batch [3900/6000], Loss: 0.4025
Epoch [3/30], Batch [4000/6000], Loss: 0.2888
Epoch [3/30], Batch [4100/6000], Loss: 1.7278
Epoch [3/30], Batch [4200/6000], Loss: 0.7229
Epoch [3/30], Batch [4300/6000], Loss: 0.4361
Epoch [3/30], Batch [4400/6000], Loss: 0.3380
Epoch [3/30], Batch [4500/6000], Loss: 0.3777
Epoch [3/30], Batch [4600/6000], Loss: 0.7405
Epoch [3/30], Batch [4700/6000], Loss: 0.2577
Epoch [3/30], Batch [4800/6000], Loss: 0.2111
Epoch [3/30], Batch [4900/6000], Loss: 0.4543
Epoch [3/30], Batch [5000/6000], Loss: 0.9341
Epoch [3/30], Batch [5100/6000], Loss: 0.2501
Epoch [3/30], Batch [5200/6000], Loss: 0.2297
Epoch [3/30], Batch [5300/6000], Loss: 0.2473
Epoch [3/30], Batch [5400/6000], Loss: 0.2106
Epoch [3/30], Batch [5500/6000], Loss: 0.4954
Epoch [3/30], Batch [5600/6000], Loss: 0.8733
Epoch [3/30], Batch [5700/6000], Loss: 0.2438
Epoch [3/30], Batch [5800/6000], Loss: 0.9103
Epoch [3/30], Batch [5900/6000], Loss: 0.2009
Epoch [3/30], Loss: 0.8775
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.2380
Epoch [4/30], Batch [100/6000], Loss: 0.2632
Epoch [4/30], Batch [200/6000], Loss: 0.3320
Epoch [4/30], Batch [300/6000], Loss: 0.7581
Epoch [4/30], Batch [400/6000], Loss: 1.0382
Epoch [4/30], Batch [500/6000], Loss: 1.5531
Epoch [4/30], Batch [600/6000], Loss: 0.2690
Epoch [4/30], Batch [700/6000], Loss: 0.9826
Epoch [4/30], Batch [800/6000], Loss: 1.6420
Epoch [4/30], Batch [900/6000], Loss: 0.3413
Epoch [4/30], Batch [1000/6000], Loss: 0.7054
Epoch [4/30], Batch [1100/6000], Loss: 0.5869
Epoch [4/30], Batch [1200/6000], Loss: 0.2924
Epoch [4/30], Batch [1300/6000], Loss: 0.2107
Epoch [4/30], Batch [1400/6000], Loss: 1.9219
Epoch [4/30], Batch [1500/6000], Loss: 0.2027
Epoch [4/30], Batch [1600/6000], Loss: 0.5795
Epoch [4/30], Batch [1700/6000], Loss: 0.4611
Epoch [4/30], Batch [1800/6000], Loss: 0.7045
Epoch [4/30], Batch [1900/6000], Loss: 0.2639
Epoch [4/30], Batch [2000/6000], Loss: 0.2648
Epoch [4/30], Batch [2100/6000], Loss: 1.6784
Epoch [4/30], Batch [2200/6000], Loss: 0.5457
Epoch [4/30], Batch [2300/6000], Loss: 0.4796
Epoch [4/30], Batch [2400/6000], Loss: 0.6260
Epoch [4/30], Batch [2500/6000], Loss: 0.5522
Epoch [4/30], Batch [2600/6000], Loss: 0.2586
Epoch [4/30], Batch [2700/6000], Loss: 1.3289
Epoch [4/30], Batch [2800/6000], Loss: 0.8178
Epoch [4/30], Batch [2900/6000], Loss: 0.6338
Epoch [4/30], Batch [3000/6000], Loss: 0.3840
Epoch [4/30], Batch [3100/6000], Loss: 0.3246
Epoch [4/30], Batch [3200/6000], Loss: 0.3329
Epoch [4/30], Batch [3300/6000], Loss: 1.0496
Epoch [4/30], Batch [3400/6000], Loss: 0.4847
Epoch [4/30], Batch [3500/6000], Loss: 1.4949
Epoch [4/30], Batch [3600/6000], Loss: 0.3829
Epoch [4/30], Batch [3700/6000], Loss: 0.3209
Epoch [4/30], Batch [3800/6000], Loss: 1.6488
Epoch [4/30], Batch [3900/6000], Loss: 1.5111
Epoch [4/30], Batch [4000/6000], Loss: 0.9085
Epoch [4/30], Batch [4100/6000], Loss: 0.3006
Epoch [4/30], Batch [4200/6000], Loss: 0.3681
Epoch [4/30], Batch [4300/6000], Loss: 0.7427
Epoch [4/30], Batch [4400/6000], Loss: 1.0395
Epoch [4/30], Batch [4500/6000], Loss: 2.3061
Epoch [4/30], Batch [4600/6000], Loss: 0.2730
Epoch [4/30], Batch [4700/6000], Loss: 0.2950
Epoch [4/30], Batch [4800/6000], Loss: 0.2779
Epoch [4/30], Batch [4900/6000], Loss: 0.4821
Epoch [4/30], Batch [5000/6000], Loss: 1.0041
Epoch [4/30], Batch [5100/6000], Loss: 0.3674
Epoch [4/30], Batch [5200/6000], Loss: 0.2181
Epoch [4/30], Batch [5300/6000], Loss: 0.2104
Epoch [4/30], Batch [5400/6000], Loss: 2.1708
Epoch [4/30], Batch [5500/6000], Loss: 1.6123
Epoch [4/30], Batch [5600/6000], Loss: 0.2479
Epoch [4/30], Batch [5700/6000], Loss: 0.2134
Epoch [4/30], Batch [5800/6000], Loss: 0.2066
Epoch [4/30], Batch [5900/6000], Loss: 0.5870
Epoch [4/30], Loss: 0.7309
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.3309
Epoch [5/30], Batch [100/6000], Loss: 1.1349
Epoch [5/30], Batch [200/6000], Loss: 1.0295
Epoch [5/30], Batch [300/6000], Loss: 0.8243
Epoch [5/30], Batch [400/6000], Loss: 0.2021
Epoch [5/30], Batch [500/6000], Loss: 0.2872
Epoch [5/30], Batch [600/6000], Loss: 0.3236
Epoch [5/30], Batch [700/6000], Loss: 0.2113
Epoch [5/30], Batch [800/6000], Loss: 0.3354
Epoch [5/30], Batch [900/6000], Loss: 0.1714
Epoch [5/30], Batch [1000/6000], Loss: 0.4905
Epoch [5/30], Batch [1100/6000], Loss: 0.3485
Epoch [5/30], Batch [1200/6000], Loss: 0.3298
Epoch [5/30], Batch [1300/6000], Loss: 0.7761
Epoch [5/30], Batch [1400/6000], Loss: 0.1929
Epoch [5/30], Batch [1500/6000], Loss: 0.2059
Epoch [5/30], Batch [1600/6000], Loss: 0.1767
Epoch [5/30], Batch [1700/6000], Loss: 0.4658
Epoch [5/30], Batch [1800/6000], Loss: 0.6683
Epoch [5/30], Batch [1900/6000], Loss: 0.2584
Epoch [5/30], Batch [2000/6000], Loss: 0.4647
Epoch [5/30], Batch [2100/6000], Loss: 0.2490
Epoch [5/30], Batch [2200/6000], Loss: 0.1957
Epoch [5/30], Batch [2300/6000], Loss: 0.7822
Epoch [5/30], Batch [2400/6000], Loss: 0.2538
Epoch [5/30], Batch [2500/6000], Loss: 0.3284
Epoch [5/30], Batch [2600/6000], Loss: 0.4577
Epoch [5/30], Batch [2700/6000], Loss: 0.2689
Epoch [5/30], Batch [2800/6000], Loss: 0.9254
Epoch [5/30], Batch [2900/6000], Loss: 0.3091
Epoch [5/30], Batch [3000/6000], Loss: 0.1993
Epoch [5/30], Batch [3100/6000], Loss: 0.4725
Epoch [5/30], Batch [3200/6000], Loss: 0.2261
Epoch [5/30], Batch [3300/6000], Loss: 0.7131
Epoch [5/30], Batch [3400/6000], Loss: 2.7636
Epoch [5/30], Batch [3500/6000], Loss: 0.2156
Epoch [5/30], Batch [3600/6000], Loss: 0.7267
Epoch [5/30], Batch [3700/6000], Loss: 0.7295
Epoch [5/30], Batch [3800/6000], Loss: 0.3650
Epoch [5/30], Batch [3900/6000], Loss: 0.2049
Epoch [5/30], Batch [4000/6000], Loss: 0.4974
Epoch [5/30], Batch [4100/6000], Loss: 0.1924
Epoch [5/30], Batch [4200/6000], Loss: 0.3941
Epoch [5/30], Batch [4300/6000], Loss: 0.5200
Epoch [5/30], Batch [4400/6000], Loss: 0.1915
Epoch [5/30], Batch [4500/6000], Loss: 0.1898
Epoch [5/30], Batch [4600/6000], Loss: 1.8024
Epoch [5/30], Batch [4700/6000], Loss: 0.7842
Epoch [5/30], Batch [4800/6000], Loss: 0.2849
Epoch [5/30], Batch [4900/6000], Loss: 0.9839
Epoch [5/30], Batch [5000/6000], Loss: 0.2443
Epoch [5/30], Batch [5100/6000], Loss: 0.1538
Epoch [5/30], Batch [5200/6000], Loss: 3.0056
Epoch [5/30], Batch [5300/6000], Loss: 1.7881
Epoch [5/30], Batch [5400/6000], Loss: 1.9397
Epoch [5/30], Batch [5500/6000], Loss: 0.2090
Epoch [5/30], Batch [5600/6000], Loss: 0.1894
Epoch [5/30], Batch [5700/6000], Loss: 0.6657
Epoch [5/30], Batch [5800/6000], Loss: 0.2982
Epoch [5/30], Batch [5900/6000], Loss: 0.1680
Epoch [5/30], Loss: 0.6296
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.3540
Epoch [6/30], Batch [100/6000], Loss: 0.1721
Epoch [6/30], Batch [200/6000], Loss: 0.1745
Epoch [6/30], Batch [300/6000], Loss: 0.2206
Epoch [6/30], Batch [400/6000], Loss: 0.2750
Epoch [6/30], Batch [500/6000], Loss: 1.2524
Epoch [6/30], Batch [600/6000], Loss: 1.1837
Epoch [6/30], Batch [700/6000], Loss: 0.3973
Epoch [6/30], Batch [800/6000], Loss: 1.3988
Epoch [6/30], Batch [900/6000], Loss: 1.5195
Epoch [6/30], Batch [1000/6000], Loss: 0.1859
Epoch [6/30], Batch [1100/6000], Loss: 0.2296
Epoch [6/30], Batch [1200/6000], Loss: 0.1658
Epoch [6/30], Batch [1300/6000], Loss: 0.4807
Epoch [6/30], Batch [1400/6000], Loss: 0.2750
Epoch [6/30], Batch [1500/6000], Loss: 1.5109
Epoch [6/30], Batch [1600/6000], Loss: 0.9392
Epoch [6/30], Batch [1700/6000], Loss: 0.7100
Epoch [6/30], Batch [1800/6000], Loss: 1.6360
Epoch [6/30], Batch [1900/6000], Loss: 0.1874
Epoch [6/30], Batch [2000/6000], Loss: 0.6386
Epoch [6/30], Batch [2100/6000], Loss: 0.2081
Epoch [6/30], Batch [2200/6000], Loss: 0.3753
Epoch [6/30], Batch [2300/6000], Loss: 1.1190
Epoch [6/30], Batch [2400/6000], Loss: 1.1764
Epoch [6/30], Batch [2500/6000], Loss: 0.1931
Epoch [6/30], Batch [2600/6000], Loss: 0.4889
Epoch [6/30], Batch [2700/6000], Loss: 0.1994
Epoch [6/30], Batch [2800/6000], Loss: 0.2119
Epoch [6/30], Batch [2900/6000], Loss: 0.2601
Epoch [6/30], Batch [3000/6000], Loss: 0.1809
Epoch [6/30], Batch [3100/6000], Loss: 0.4102
Epoch [6/30], Batch [3200/6000], Loss: 0.1793
Epoch [6/30], Batch [3300/6000], Loss: 1.5674
Epoch [6/30], Batch [3400/6000], Loss: 1.0033
Epoch [6/30], Batch [3500/6000], Loss: 0.3753
Epoch [6/30], Batch [3600/6000], Loss: 0.2046
Epoch [6/30], Batch [3700/6000], Loss: 0.2186
Epoch [6/30], Batch [3800/6000], Loss: 0.3499
Epoch [6/30], Batch [3900/6000], Loss: 0.1972
Epoch [6/30], Batch [4000/6000], Loss: 0.1817
Epoch [6/30], Batch [4100/6000], Loss: 0.8954
Epoch [6/30], Batch [4200/6000], Loss: 0.6626
Epoch [6/30], Batch [4300/6000], Loss: 0.2915
Epoch [6/30], Batch [4400/6000], Loss: 4.1642
Epoch [6/30], Batch [4500/6000], Loss: 0.1890
Epoch [6/30], Batch [4600/6000], Loss: 2.4530
Epoch [6/30], Batch [4700/6000], Loss: 0.8703
Epoch [6/30], Batch [4800/6000], Loss: 1.0290
Epoch [6/30], Batch [4900/6000], Loss: 1.9385
Epoch [6/30], Batch [5000/6000], Loss: 0.4106
Epoch [6/30], Batch [5100/6000], Loss: 1.7994
Epoch [6/30], Batch [5200/6000], Loss: 0.1701
Epoch [6/30], Batch [5300/6000], Loss: 0.1739
Epoch [6/30], Batch [5400/6000], Loss: 0.2948
Epoch [6/30], Batch [5500/6000], Loss: 0.7594
Epoch [6/30], Batch [5600/6000], Loss: 1.7728
Epoch [6/30], Batch [5700/6000], Loss: 0.5269
Epoch [6/30], Batch [5800/6000], Loss: 0.2521
Epoch [6/30], Batch [5900/6000], Loss: 0.2655
Epoch [6/30], Loss: 0.5651
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 2.5986
Epoch [7/30], Batch [100/6000], Loss: 0.2339
Epoch [7/30], Batch [200/6000], Loss: 0.4150
Epoch [7/30], Batch [300/6000], Loss: 0.1696
Epoch [7/30], Batch [400/6000], Loss: 0.4750
Epoch [7/30], Batch [500/6000], Loss: 0.3920
Epoch [7/30], Batch [600/6000], Loss: 0.9136
Epoch [7/30], Batch [700/6000], Loss: 1.0589
Epoch [7/30], Batch [800/6000], Loss: 0.2047
Epoch [7/30], Batch [900/6000], Loss: 0.3174
Epoch [7/30], Batch [1000/6000], Loss: 0.2256
Epoch [7/30], Batch [1100/6000], Loss: 0.3944
Epoch [7/30], Batch [1200/6000], Loss: 0.2014
Epoch [7/30], Batch [1300/6000], Loss: 0.1755
Epoch [7/30], Batch [1400/6000], Loss: 0.2088
Epoch [7/30], Batch [1500/6000], Loss: 0.1967
Epoch [7/30], Batch [1600/6000], Loss: 0.7099
Epoch [7/30], Batch [1700/6000], Loss: 0.1505
Epoch [7/30], Batch [1800/6000], Loss: 0.7329
Epoch [7/30], Batch [1900/6000], Loss: 0.7274
Epoch [7/30], Batch [2000/6000], Loss: 0.3804
Epoch [7/30], Batch [2100/6000], Loss: 0.2578
Epoch [7/30], Batch [2200/6000], Loss: 0.1728
Epoch [7/30], Batch [2300/6000], Loss: 0.5677
Epoch [7/30], Batch [2400/6000], Loss: 0.2168
Epoch [7/30], Batch [2500/6000], Loss: 1.7214
Epoch [7/30], Batch [2600/6000], Loss: 0.4479
Epoch [7/30], Batch [2700/6000], Loss: 2.6733
Epoch [7/30], Batch [2800/6000], Loss: 0.1743
Epoch [7/30], Batch [2900/6000], Loss: 0.3688
Epoch [7/30], Batch [3000/6000], Loss: 0.2979
Epoch [7/30], Batch [3100/6000], Loss: 0.3298
Epoch [7/30], Batch [3200/6000], Loss: 0.1745
Epoch [7/30], Batch [3300/6000], Loss: 1.5300
Epoch [7/30], Batch [3400/6000], Loss: 1.7937
Epoch [7/30], Batch [3500/6000], Loss: 1.0356
Epoch [7/30], Batch [3600/6000], Loss: 0.1660
Epoch [7/30], Batch [3700/6000], Loss: 0.2157
Epoch [7/30], Batch [3800/6000], Loss: 0.2870
Epoch [7/30], Batch [3900/6000], Loss: 0.1955
Epoch [7/30], Batch [4000/6000], Loss: 1.1389
Epoch [7/30], Batch [4100/6000], Loss: 0.2131
Epoch [7/30], Batch [4200/6000], Loss: 0.2408
Epoch [7/30], Batch [4300/6000], Loss: 0.1615
Epoch [7/30], Batch [4400/6000], Loss: 0.2619
Epoch [7/30], Batch [4500/6000], Loss: 0.1570
Epoch [7/30], Batch [4600/6000], Loss: 0.3516
Epoch [7/30], Batch [4700/6000], Loss: 0.1808
Epoch [7/30], Batch [4800/6000], Loss: 0.1968
Epoch [7/30], Batch [4900/6000], Loss: 0.1730
Epoch [7/30], Batch [5000/6000], Loss: 0.2029
Epoch [7/30], Batch [5100/6000], Loss: 1.3175
Epoch [7/30], Batch [5200/6000], Loss: 0.3333
Epoch [7/30], Batch [5300/6000], Loss: 0.2552
Epoch [7/30], Batch [5400/6000], Loss: 0.3985
Epoch [7/30], Batch [5500/6000], Loss: 0.5302
Epoch [7/30], Batch [5600/6000], Loss: 0.1949
Epoch [7/30], Batch [5700/6000], Loss: 0.2323
Epoch [7/30], Batch [5800/6000], Loss: 0.5529
Epoch [7/30], Batch [5900/6000], Loss: 0.2197
Epoch [7/30], Loss: 0.5144
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.2112
Epoch [8/30], Batch [100/6000], Loss: 0.4106
Epoch [8/30], Batch [200/6000], Loss: 0.2339
Epoch [8/30], Batch [300/6000], Loss: 0.1628
Epoch [8/30], Batch [400/6000], Loss: 0.1530
Epoch [8/30], Batch [500/6000], Loss: 0.8364
Epoch [8/30], Batch [600/6000], Loss: 1.6191
Epoch [8/30], Batch [700/6000], Loss: 0.3428
Epoch [8/30], Batch [800/6000], Loss: 0.1875
Epoch [8/30], Batch [900/6000], Loss: 0.1832
Epoch [8/30], Batch [1000/6000], Loss: 0.1532
Epoch [8/30], Batch [1100/6000], Loss: 1.9568
Epoch [8/30], Batch [1200/6000], Loss: 2.3178
Epoch [8/30], Batch [1300/6000], Loss: 0.1831
Epoch [8/30], Batch [1400/6000], Loss: 0.8799
Epoch [8/30], Batch [1500/6000], Loss: 0.3966
Epoch [8/30], Batch [1600/6000], Loss: 0.2333
Epoch [8/30], Batch [1700/6000], Loss: 0.4846
Epoch [8/30], Batch [1800/6000], Loss: 0.1623
Epoch [8/30], Batch [1900/6000], Loss: 0.2032
Epoch [8/30], Batch [2000/6000], Loss: 0.4146
Epoch [8/30], Batch [2100/6000], Loss: 0.2128
Epoch [8/30], Batch [2200/6000], Loss: 0.5170
Epoch [8/30], Batch [2300/6000], Loss: 0.4327
Epoch [8/30], Batch [2400/6000], Loss: 0.9847
Epoch [8/30], Batch [2500/6000], Loss: 0.9506
Epoch [8/30], Batch [2600/6000], Loss: 0.2411
Epoch [8/30], Batch [2700/6000], Loss: 0.1897
Epoch [8/30], Batch [2800/6000], Loss: 0.3153
Epoch [8/30], Batch [2900/6000], Loss: 1.1022
Epoch [8/30], Batch [3000/6000], Loss: 0.9583
Epoch [8/30], Batch [3100/6000], Loss: 0.7228
Epoch [8/30], Batch [3200/6000], Loss: 0.1981
Epoch [8/30], Batch [3300/6000], Loss: 1.1503
Epoch [8/30], Batch [3400/6000], Loss: 0.6323
Epoch [8/30], Batch [3500/6000], Loss: 1.8307
Epoch [8/30], Batch [3600/6000], Loss: 2.2984
Epoch [8/30], Batch [3700/6000], Loss: 0.1991
Epoch [8/30], Batch [3800/6000], Loss: 0.1871
Epoch [8/30], Batch [3900/6000], Loss: 0.1669
Epoch [8/30], Batch [4000/6000], Loss: 0.1752
Epoch [8/30], Batch [4100/6000], Loss: 0.4530
Epoch [8/30], Batch [4200/6000], Loss: 0.1209
Epoch [8/30], Batch [4300/6000], Loss: 0.1647
Epoch [8/30], Batch [4400/6000], Loss: 0.1638
Epoch [8/30], Batch [4500/6000], Loss: 4.6370
Epoch [8/30], Batch [4600/6000], Loss: 0.6057
Epoch [8/30], Batch [4700/6000], Loss: 0.1548
Epoch [8/30], Batch [4800/6000], Loss: 1.3126
Epoch [8/30], Batch [4900/6000], Loss: 0.1554
Epoch [8/30], Batch [5000/6000], Loss: 0.1741
Epoch [8/30], Batch [5100/6000], Loss: 0.2036
Epoch [8/30], Batch [5200/6000], Loss: 0.2452
Epoch [8/30], Batch [5300/6000], Loss: 0.8863
Epoch [8/30], Batch [5400/6000], Loss: 0.2069
Epoch [8/30], Batch [5500/6000], Loss: 0.2067
Epoch [8/30], Batch [5600/6000], Loss: 0.2076
Epoch [8/30], Batch [5700/6000], Loss: 0.1784
Epoch [8/30], Batch [5800/6000], Loss: 0.1605
Epoch [8/30], Batch [5900/6000], Loss: 0.5307
Epoch [8/30], Loss: 0.4700
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.1565
Epoch [9/30], Batch [100/6000], Loss: 0.1810
Epoch [9/30], Batch [200/6000], Loss: 0.3383
Epoch [9/30], Batch [300/6000], Loss: 0.7910
Epoch [9/30], Batch [400/6000], Loss: 0.4877
Epoch [9/30], Batch [500/6000], Loss: 0.2699
Epoch [9/30], Batch [600/6000], Loss: 0.1765
Epoch [9/30], Batch [700/6000], Loss: 0.5506
Epoch [9/30], Batch [800/6000], Loss: 0.1491
Epoch [9/30], Batch [900/6000], Loss: 0.2066
Epoch [9/30], Batch [1000/6000], Loss: 0.3004
Epoch [9/30], Batch [1100/6000], Loss: 0.1728
Epoch [9/30], Batch [1200/6000], Loss: 0.1988
Epoch [9/30], Batch [1300/6000], Loss: 0.3406
Epoch [9/30], Batch [1400/6000], Loss: 0.1606
Epoch [9/30], Batch [1500/6000], Loss: 0.5581
Epoch [9/30], Batch [1600/6000], Loss: 0.1803
Epoch [9/30], Batch [1700/6000], Loss: 0.1340
Epoch [9/30], Batch [1800/6000], Loss: 0.1617
Epoch [9/30], Batch [1900/6000], Loss: 0.1737
Epoch [9/30], Batch [2000/6000], Loss: 0.1580
Epoch [9/30], Batch [2100/6000], Loss: 0.1475
Epoch [9/30], Batch [2200/6000], Loss: 0.2657
Epoch [9/30], Batch [2300/6000], Loss: 1.2426
Epoch [9/30], Batch [2400/6000], Loss: 1.0028
Epoch [9/30], Batch [2500/6000], Loss: 0.1933
Epoch [9/30], Batch [2600/6000], Loss: 3.2392
Epoch [9/30], Batch [2700/6000], Loss: 0.2719
Epoch [9/30], Batch [2800/6000], Loss: 0.2261
Epoch [9/30], Batch [2900/6000], Loss: 0.1536
Epoch [9/30], Batch [3000/6000], Loss: 0.6321
Epoch [9/30], Batch [3100/6000], Loss: 0.1585
Epoch [9/30], Batch [3200/6000], Loss: 0.1846
Epoch [9/30], Batch [3300/6000], Loss: 0.1662
Epoch [9/30], Batch [3400/6000], Loss: 1.1665
Epoch [9/30], Batch [3500/6000], Loss: 0.1574
Epoch [9/30], Batch [3600/6000], Loss: 0.1805
Epoch [9/30], Batch [3700/6000], Loss: 0.1583
Epoch [9/30], Batch [3800/6000], Loss: 0.1446
Epoch [9/30], Batch [3900/6000], Loss: 0.2249
Epoch [9/30], Batch [4000/6000], Loss: 1.2872
Epoch [9/30], Batch [4100/6000], Loss: 2.3051
Epoch [9/30], Batch [4200/6000], Loss: 0.2417
Epoch [9/30], Batch [4300/6000], Loss: 0.2147
Epoch [9/30], Batch [4400/6000], Loss: 0.2217
Epoch [9/30], Batch [4500/6000], Loss: 0.2721
Epoch [9/30], Batch [4600/6000], Loss: 0.1735
Epoch [9/30], Batch [4700/6000], Loss: 0.4701
Epoch [9/30], Batch [4800/6000], Loss: 0.1652
Epoch [9/30], Batch [4900/6000], Loss: 0.1888
Epoch [9/30], Batch [5000/6000], Loss: 0.3422
Epoch [9/30], Batch [5100/6000], Loss: 0.8980
Epoch [9/30], Batch [5200/6000], Loss: 0.1496
Epoch [9/30], Batch [5300/6000], Loss: 0.2927
Epoch [9/30], Batch [5400/6000], Loss: 0.5337
Epoch [9/30], Batch [5500/6000], Loss: 0.4257
Epoch [9/30], Batch [5600/6000], Loss: 0.3423
Epoch [9/30], Batch [5700/6000], Loss: 0.1361
Epoch [9/30], Batch [5800/6000], Loss: 2.6854
Epoch [9/30], Batch [5900/6000], Loss: 0.1495
Epoch [9/30], Loss: 0.4298
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.2116
Epoch [10/30], Batch [100/6000], Loss: 0.1614
Epoch [10/30], Batch [200/6000], Loss: 0.1974
Epoch [10/30], Batch [300/6000], Loss: 0.1226
Epoch [10/30], Batch [400/6000], Loss: 0.3011
Epoch [10/30], Batch [500/6000], Loss: 1.7793
Epoch [10/30], Batch [600/6000], Loss: 0.1294
Epoch [10/30], Batch [700/6000], Loss: 0.1542
Epoch [10/30], Batch [800/6000], Loss: 0.2004
Epoch [10/30], Batch [900/6000], Loss: 0.1471
Epoch [10/30], Batch [1000/6000], Loss: 0.4163
Epoch [10/30], Batch [1100/6000], Loss: 0.2029
Epoch [10/30], Batch [1200/6000], Loss: 0.2025
Epoch [10/30], Batch [1300/6000], Loss: 0.1529
Epoch [10/30], Batch [1400/6000], Loss: 2.0821
Epoch [10/30], Batch [1500/6000], Loss: 0.3686
Epoch [10/30], Batch [1600/6000], Loss: 0.1945
Epoch [10/30], Batch [1700/6000], Loss: 0.2948
Epoch [10/30], Batch [1800/6000], Loss: 0.1635
Epoch [10/30], Batch [1900/6000], Loss: 0.1566
Epoch [10/30], Batch [2000/6000], Loss: 0.2388
Epoch [10/30], Batch [2100/6000], Loss: 0.1454
Epoch [10/30], Batch [2200/6000], Loss: 0.1381
Epoch [10/30], Batch [2300/6000], Loss: 1.1182
Epoch [10/30], Batch [2400/6000], Loss: 0.1991
Epoch [10/30], Batch [2500/6000], Loss: 0.1549
Epoch [10/30], Batch [2600/6000], Loss: 0.7431
Epoch [10/30], Batch [2700/6000], Loss: 0.2058
Epoch [10/30], Batch [2800/6000], Loss: 0.1943
Epoch [10/30], Batch [2900/6000], Loss: 0.2471
Epoch [10/30], Batch [3000/6000], Loss: 0.2071
Epoch [10/30], Batch [3100/6000], Loss: 0.1258
Epoch [10/30], Batch [3200/6000], Loss: 0.2210
Epoch [10/30], Batch [3300/6000], Loss: 0.4821
Epoch [10/30], Batch [3400/6000], Loss: 0.1804
Epoch [10/30], Batch [3500/6000], Loss: 0.1494
Epoch [10/30], Batch [3600/6000], Loss: 0.1864
Epoch [10/30], Batch [3700/6000], Loss: 0.1454
Epoch [10/30], Batch [3800/6000], Loss: 1.2093
Epoch [10/30], Batch [3900/6000], Loss: 0.1876
Epoch [10/30], Batch [4000/6000], Loss: 0.4494
Epoch [10/30], Batch [4100/6000], Loss: 0.1636
Epoch [10/30], Batch [4200/6000], Loss: 0.1399
Epoch [10/30], Batch [4300/6000], Loss: 0.1608
Epoch [10/30], Batch [4400/6000], Loss: 0.1412
Epoch [10/30], Batch [4500/6000], Loss: 0.1562
Epoch [10/30], Batch [4600/6000], Loss: 0.5022
Epoch [10/30], Batch [4700/6000], Loss: 0.2490
Epoch [10/30], Batch [4800/6000], Loss: 1.4589
Epoch [10/30], Batch [4900/6000], Loss: 1.2429
Epoch [10/30], Batch [5000/6000], Loss: 0.1282
Epoch [10/30], Batch [5100/6000], Loss: 0.1625
Epoch [10/30], Batch [5200/6000], Loss: 0.1608
Epoch [10/30], Batch [5300/6000], Loss: 0.1990
Epoch [10/30], Batch [5400/6000], Loss: 0.2050
Epoch [10/30], Batch [5500/6000], Loss: 0.6395
Epoch [10/30], Batch [5600/6000], Loss: 1.3144
Epoch [10/30], Batch [5700/6000], Loss: 0.1686
Epoch [10/30], Batch [5800/6000], Loss: 0.1472
Epoch [10/30], Batch [5900/6000], Loss: 0.1749
Epoch [10/30], Loss: 0.3985
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.8546
Epoch [11/30], Batch [100/6000], Loss: 0.1633
Epoch [11/30], Batch [200/6000], Loss: 0.1407
Epoch [11/30], Batch [300/6000], Loss: 0.1280
Epoch [11/30], Batch [400/6000], Loss: 0.2256
Epoch [11/30], Batch [500/6000], Loss: 0.1605
Epoch [11/30], Batch [600/6000], Loss: 0.1368
Epoch [11/30], Batch [700/6000], Loss: 0.1675
Epoch [11/30], Batch [800/6000], Loss: 0.1792
Epoch [11/30], Batch [900/6000], Loss: 0.1679
Epoch [11/30], Batch [1000/6000], Loss: 0.2278
Epoch [11/30], Batch [1100/6000], Loss: 0.2723
Epoch [11/30], Batch [1200/6000], Loss: 1.0784
Epoch [11/30], Batch [1300/6000], Loss: 0.1741
Epoch [11/30], Batch [1400/6000], Loss: 0.6405
Epoch [11/30], Batch [1500/6000], Loss: 0.1432
Epoch [11/30], Batch [1600/6000], Loss: 0.1912
Epoch [11/30], Batch [1700/6000], Loss: 0.2188
Epoch [11/30], Batch [1800/6000], Loss: 1.2063
Epoch [11/30], Batch [1900/6000], Loss: 0.3905
Epoch [11/30], Batch [2000/6000], Loss: 0.1531
Epoch [11/30], Batch [2100/6000], Loss: 0.1568
Epoch [11/30], Batch [2200/6000], Loss: 2.0926
Epoch [11/30], Batch [2300/6000], Loss: 0.3522
Epoch [11/30], Batch [2400/6000], Loss: 0.1230
Epoch [11/30], Batch [2500/6000], Loss: 0.1717
Epoch [11/30], Batch [2600/6000], Loss: 0.2331
Epoch [11/30], Batch [2700/6000], Loss: 0.2139
Epoch [11/30], Batch [2800/6000], Loss: 0.1811
Epoch [11/30], Batch [2900/6000], Loss: 0.1867
Epoch [11/30], Batch [3000/6000], Loss: 0.1378
Epoch [11/30], Batch [3100/6000], Loss: 0.1383
Epoch [11/30], Batch [3200/6000], Loss: 0.1785
Epoch [11/30], Batch [3300/6000], Loss: 1.0382
Epoch [11/30], Batch [3400/6000], Loss: 0.1962
Epoch [11/30], Batch [3500/6000], Loss: 0.1936
Epoch [11/30], Batch [3600/6000], Loss: 0.1264
Epoch [11/30], Batch [3700/6000], Loss: 0.5362
Epoch [11/30], Batch [3800/6000], Loss: 0.1403
Epoch [11/30], Batch [3900/6000], Loss: 0.2121
Epoch [11/30], Batch [4000/6000], Loss: 0.7114
Epoch [11/30], Batch [4100/6000], Loss: 0.3672
Epoch [11/30], Batch [4200/6000], Loss: 0.7783
Epoch [11/30], Batch [4300/6000], Loss: 0.2279
Epoch [11/30], Batch [4400/6000], Loss: 0.2110
Epoch [11/30], Batch [4500/6000], Loss: 0.1502
Epoch [11/30], Batch [4600/6000], Loss: 0.2979
Epoch [11/30], Batch [4700/6000], Loss: 0.1410
Epoch [11/30], Batch [4800/6000], Loss: 0.1719
Epoch [11/30], Batch [4900/6000], Loss: 0.2028
Epoch [11/30], Batch [5000/6000], Loss: 0.1616
Epoch [11/30], Batch [5100/6000], Loss: 0.1416
Epoch [11/30], Batch [5200/6000], Loss: 0.2028
Epoch [11/30], Batch [5300/6000], Loss: 0.8217
Epoch [11/30], Batch [5400/6000], Loss: 0.1470
Epoch [11/30], Batch [5500/6000], Loss: 0.5756
Epoch [11/30], Batch [5600/6000], Loss: 0.2217
Epoch [11/30], Batch [5700/6000], Loss: 2.1750
Epoch [11/30], Batch [5800/6000], Loss: 0.2461
Epoch [11/30], Batch [5900/6000], Loss: 0.1822
Epoch [11/30], Loss: 0.3714
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.1261
Epoch [12/30], Batch [100/6000], Loss: 2.7734
Epoch [12/30], Batch [200/6000], Loss: 0.1847
Epoch [12/30], Batch [300/6000], Loss: 0.2131
Epoch [12/30], Batch [400/6000], Loss: 0.1549
Epoch [12/30], Batch [500/6000], Loss: 0.1744
Epoch [12/30], Batch [600/6000], Loss: 0.9594
Epoch [12/30], Batch [700/6000], Loss: 0.8521
Epoch [12/30], Batch [800/6000], Loss: 0.1704
Epoch [12/30], Batch [900/6000], Loss: 0.3980
Epoch [12/30], Batch [1000/6000], Loss: 0.8105
Epoch [12/30], Batch [1100/6000], Loss: 0.2070
Epoch [12/30], Batch [1200/6000], Loss: 1.9152
Epoch [12/30], Batch [1300/6000], Loss: 0.1555
Epoch [12/30], Batch [1400/6000], Loss: 0.7988
Epoch [12/30], Batch [1500/6000], Loss: 0.2858
Epoch [12/30], Batch [1600/6000], Loss: 2.0743
Epoch [12/30], Batch [1700/6000], Loss: 0.1375
Epoch [12/30], Batch [1800/6000], Loss: 0.1910
Epoch [12/30], Batch [1900/6000], Loss: 0.3097
Epoch [12/30], Batch [2000/6000], Loss: 0.1050
Epoch [12/30], Batch [2100/6000], Loss: 0.1351
Epoch [12/30], Batch [2200/6000], Loss: 0.1100
Epoch [12/30], Batch [2300/6000], Loss: 0.1535
Epoch [12/30], Batch [2400/6000], Loss: 0.2059
Epoch [12/30], Batch [2500/6000], Loss: 0.1911
Epoch [12/30], Batch [2600/6000], Loss: 0.7814
Epoch [12/30], Batch [2700/6000], Loss: 0.2446
Epoch [12/30], Batch [2800/6000], Loss: 0.1391
Epoch [12/30], Batch [2900/6000], Loss: 0.1617
Epoch [12/30], Batch [3000/6000], Loss: 0.1399
Epoch [12/30], Batch [3100/6000], Loss: 0.2507
Epoch [12/30], Batch [3200/6000], Loss: 0.2371
Epoch [12/30], Batch [3300/6000], Loss: 0.1515
Epoch [12/30], Batch [3400/6000], Loss: 0.1366
Epoch [12/30], Batch [3500/6000], Loss: 0.2085
Epoch [12/30], Batch [3600/6000], Loss: 0.4866
Epoch [12/30], Batch [3700/6000], Loss: 0.5987
Epoch [12/30], Batch [3800/6000], Loss: 1.1121
Epoch [12/30], Batch [3900/6000], Loss: 1.5383
Epoch [12/30], Batch [4000/6000], Loss: 0.1176
Epoch [12/30], Batch [4100/6000], Loss: 0.2002
Epoch [12/30], Batch [4200/6000], Loss: 0.1464
Epoch [12/30], Batch [4300/6000], Loss: 2.5123
Epoch [12/30], Batch [4400/6000], Loss: 0.1515
Epoch [12/30], Batch [4500/6000], Loss: 0.5887
Epoch [12/30], Batch [4600/6000], Loss: 0.1655
Epoch [12/30], Batch [4700/6000], Loss: 1.4929
Epoch [12/30], Batch [4800/6000], Loss: 0.1914
Epoch [12/30], Batch [4900/6000], Loss: 0.3483
Epoch [12/30], Batch [5000/6000], Loss: 0.1221
Epoch [12/30], Batch [5100/6000], Loss: 0.7441
Epoch [12/30], Batch [5200/6000], Loss: 0.1349
Epoch [12/30], Batch [5300/6000], Loss: 2.2041
Epoch [12/30], Batch [5400/6000], Loss: 0.2655
Epoch [12/30], Batch [5500/6000], Loss: 0.3151
Epoch [12/30], Batch [5600/6000], Loss: 0.1426
Epoch [12/30], Batch [5700/6000], Loss: 0.1979
Epoch [12/30], Batch [5800/6000], Loss: 0.1402
Epoch [12/30], Batch [5900/6000], Loss: 0.1989
Epoch [12/30], Loss: 0.3447
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.1328
Epoch [13/30], Batch [100/6000], Loss: 0.1996
Epoch [13/30], Batch [200/6000], Loss: 0.3077
Epoch [13/30], Batch [300/6000], Loss: 0.1429
Epoch [13/30], Batch [400/6000], Loss: 0.1301
Epoch [13/30], Batch [500/6000], Loss: 0.1486
Epoch [13/30], Batch [600/6000], Loss: 0.1330
Epoch [13/30], Batch [700/6000], Loss: 0.1621
Epoch [13/30], Batch [800/6000], Loss: 0.1436
Epoch [13/30], Batch [900/6000], Loss: 0.1690
Epoch [13/30], Batch [1000/6000], Loss: 0.1495
Epoch [13/30], Batch [1100/6000], Loss: 0.1365
Epoch [13/30], Batch [1200/6000], Loss: 0.1636
Epoch [13/30], Batch [1300/6000], Loss: 0.1535
Epoch [13/30], Batch [1400/6000], Loss: 0.2275
Epoch [13/30], Batch [1500/6000], Loss: 0.2499
Epoch [13/30], Batch [1600/6000], Loss: 0.3518
Epoch [13/30], Batch [1700/6000], Loss: 0.1446
Epoch [13/30], Batch [1800/6000], Loss: 0.1607
Epoch [13/30], Batch [1900/6000], Loss: 0.2064
Epoch [13/30], Batch [2000/6000], Loss: 0.1812
Epoch [13/30], Batch [2100/6000], Loss: 0.1236
Epoch [13/30], Batch [2200/6000], Loss: 0.1465
Epoch [13/30], Batch [2300/6000], Loss: 0.1197
Epoch [13/30], Batch [2400/6000], Loss: 0.2233
Epoch [13/30], Batch [2500/6000], Loss: 0.2895
Epoch [13/30], Batch [2600/6000], Loss: 0.3524
Epoch [13/30], Batch [2700/6000], Loss: 0.1844
Epoch [13/30], Batch [2800/6000], Loss: 0.2126
Epoch [13/30], Batch [2900/6000], Loss: 0.2341
Epoch [13/30], Batch [3000/6000], Loss: 0.1508
Epoch [13/30], Batch [3100/6000], Loss: 0.1349
Epoch [13/30], Batch [3200/6000], Loss: 0.1407
Epoch [13/30], Batch [3300/6000], Loss: 0.2183
Epoch [13/30], Batch [3400/6000], Loss: 0.1285
Epoch [13/30], Batch [3500/6000], Loss: 0.1740
Epoch [13/30], Batch [3600/6000], Loss: 0.2177
Epoch [13/30], Batch [3700/6000], Loss: 0.2410
Epoch [13/30], Batch [3800/6000], Loss: 0.1069
Epoch [13/30], Batch [3900/6000], Loss: 0.1809
Epoch [13/30], Batch [4000/6000], Loss: 0.1506
Epoch [13/30], Batch [4100/6000], Loss: 0.1311
Epoch [13/30], Batch [4200/6000], Loss: 0.6365
Epoch [13/30], Batch [4300/6000], Loss: 0.1676
Epoch [13/30], Batch [4400/6000], Loss: 0.3411
Epoch [13/30], Batch [4500/6000], Loss: 0.1907
Epoch [13/30], Batch [4600/6000], Loss: 0.1621
Epoch [13/30], Batch [4700/6000], Loss: 0.1066
Epoch [13/30], Batch [4800/6000], Loss: 0.1157
Epoch [13/30], Batch [4900/6000], Loss: 0.1514
Epoch [13/30], Batch [5000/6000], Loss: 0.1520
Epoch [13/30], Batch [5100/6000], Loss: 0.2082
Epoch [13/30], Batch [5200/6000], Loss: 0.8294
Epoch [13/30], Batch [5300/6000], Loss: 0.1507
Epoch [13/30], Batch [5400/6000], Loss: 0.1238
Epoch [13/30], Batch [5500/6000], Loss: 0.6241
Epoch [13/30], Batch [5600/6000], Loss: 0.1191
Epoch [13/30], Batch [5700/6000], Loss: 0.1440
Epoch [13/30], Batch [5800/6000], Loss: 0.1280
Epoch [13/30], Batch [5900/6000], Loss: 0.1753
Epoch [13/30], Loss: 0.3245
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.1325
Epoch [14/30], Batch [100/6000], Loss: 0.1325
Epoch [14/30], Batch [200/6000], Loss: 0.5697
Epoch [14/30], Batch [300/6000], Loss: 0.2878
Epoch [14/30], Batch [400/6000], Loss: 0.7983
Epoch [14/30], Batch [500/6000], Loss: 0.1077
Epoch [14/30], Batch [600/6000], Loss: 0.2364
Epoch [14/30], Batch [700/6000], Loss: 0.1185
Epoch [14/30], Batch [800/6000], Loss: 0.1588
Epoch [14/30], Batch [900/6000], Loss: 0.1664
Epoch [14/30], Batch [1000/6000], Loss: 0.7718
Epoch [14/30], Batch [1100/6000], Loss: 0.1397
Epoch [14/30], Batch [1200/6000], Loss: 0.1734
Epoch [14/30], Batch [1300/6000], Loss: 0.1403
Epoch [14/30], Batch [1400/6000], Loss: 0.1654
Epoch [14/30], Batch [1500/6000], Loss: 0.1360
Epoch [14/30], Batch [1600/6000], Loss: 0.3287
Epoch [14/30], Batch [1700/6000], Loss: 0.1904
Epoch [14/30], Batch [1800/6000], Loss: 0.2058
Epoch [14/30], Batch [1900/6000], Loss: 2.0664
Epoch [14/30], Batch [2000/6000], Loss: 0.2049
Epoch [14/30], Batch [2100/6000], Loss: 1.2696
Epoch [14/30], Batch [2200/6000], Loss: 0.7790
Epoch [14/30], Batch [2300/6000], Loss: 0.1373
Epoch [14/30], Batch [2400/6000], Loss: 0.2390
Epoch [14/30], Batch [2500/6000], Loss: 0.1313
Epoch [14/30], Batch [2600/6000], Loss: 0.1138
Epoch [14/30], Batch [2700/6000], Loss: 0.1642
Epoch [14/30], Batch [2800/6000], Loss: 0.1373
Epoch [14/30], Batch [2900/6000], Loss: 0.8341
Epoch [14/30], Batch [3000/6000], Loss: 0.1597
Epoch [14/30], Batch [3100/6000], Loss: 0.2513
Epoch [14/30], Batch [3200/6000], Loss: 0.1290
Epoch [14/30], Batch [3300/6000], Loss: 0.2847
Epoch [14/30], Batch [3400/6000], Loss: 0.1594
Epoch [14/30], Batch [3500/6000], Loss: 0.5238
Epoch [14/30], Batch [3600/6000], Loss: 0.1511
Epoch [14/30], Batch [3700/6000], Loss: 0.1522
Epoch [14/30], Batch [3800/6000], Loss: 0.6550
Epoch [14/30], Batch [3900/6000], Loss: 0.1840
Epoch [14/30], Batch [4000/6000], Loss: 0.1816
Epoch [14/30], Batch [4100/6000], Loss: 0.2223
Epoch [14/30], Batch [4200/6000], Loss: 0.1626
Epoch [14/30], Batch [4300/6000], Loss: 0.1298
Epoch [14/30], Batch [4400/6000], Loss: 0.1900
Epoch [14/30], Batch [4500/6000], Loss: 0.2986
Epoch [14/30], Batch [4600/6000], Loss: 0.1381
Epoch [14/30], Batch [4700/6000], Loss: 0.2502
Epoch [14/30], Batch [4800/6000], Loss: 0.1321
Epoch [14/30], Batch [4900/6000], Loss: 0.1342
Epoch [14/30], Batch [5000/6000], Loss: 0.1363
Epoch [14/30], Batch [5100/6000], Loss: 0.1681
Epoch [14/30], Batch [5200/6000], Loss: 0.1115
Epoch [14/30], Batch [5300/6000], Loss: 0.1197
Epoch [14/30], Batch [5400/6000], Loss: 0.1573
Epoch [14/30], Batch [5500/6000], Loss: 0.1295
Epoch [14/30], Batch [5600/6000], Loss: 0.1357
Epoch [14/30], Batch [5700/6000], Loss: 0.1544
Epoch [14/30], Batch [5800/6000], Loss: 0.1814
Epoch [14/30], Batch [5900/6000], Loss: 0.1321
Epoch [14/30], Loss: 0.3059
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.1302
Epoch [15/30], Batch [100/6000], Loss: 0.1178
Epoch [15/30], Batch [200/6000], Loss: 0.1582
Epoch [15/30], Batch [300/6000], Loss: 0.1400
Epoch [15/30], Batch [400/6000], Loss: 0.1902
Epoch [15/30], Batch [500/6000], Loss: 2.1906
Epoch [15/30], Batch [600/6000], Loss: 0.2414
Epoch [15/30], Batch [700/6000], Loss: 0.1940
Epoch [15/30], Batch [800/6000], Loss: 0.1591
Epoch [15/30], Batch [900/6000], Loss: 1.5567
Epoch [15/30], Batch [1000/6000], Loss: 0.2839
Epoch [15/30], Batch [1100/6000], Loss: 0.1621
Epoch [15/30], Batch [1200/6000], Loss: 0.2006
Epoch [15/30], Batch [1300/6000], Loss: 0.1375
Epoch [15/30], Batch [1400/6000], Loss: 0.1519
Epoch [15/30], Batch [1500/6000], Loss: 0.1562
Epoch [15/30], Batch [1600/6000], Loss: 0.1487
Epoch [15/30], Batch [1700/6000], Loss: 0.2133
Epoch [15/30], Batch [1800/6000], Loss: 0.1922
Epoch [15/30], Batch [1900/6000], Loss: 0.4369
Epoch [15/30], Batch [2000/6000], Loss: 0.1560
Epoch [15/30], Batch [2100/6000], Loss: 0.8036
Epoch [15/30], Batch [2200/6000], Loss: 0.1930
Epoch [15/30], Batch [2300/6000], Loss: 0.5441
Epoch [15/30], Batch [2400/6000], Loss: 0.1916
Epoch [15/30], Batch [2500/6000], Loss: 0.6517
Epoch [15/30], Batch [2600/6000], Loss: 0.1174
Epoch [15/30], Batch [2700/6000], Loss: 0.1340
Epoch [15/30], Batch [2800/6000], Loss: 0.1901
Epoch [15/30], Batch [2900/6000], Loss: 0.2103
Epoch [15/30], Batch [3000/6000], Loss: 0.1541
Epoch [15/30], Batch [3100/6000], Loss: 0.1191
Epoch [15/30], Batch [3200/6000], Loss: 0.5194
Epoch [15/30], Batch [3300/6000], Loss: 0.1168
Epoch [15/30], Batch [3400/6000], Loss: 0.5508
Epoch [15/30], Batch [3500/6000], Loss: 0.5635
Epoch [15/30], Batch [3600/6000], Loss: 0.1226
Epoch [15/30], Batch [3700/6000], Loss: 0.1209
Epoch [15/30], Batch [3800/6000], Loss: 0.1047
Epoch [15/30], Batch [3900/6000], Loss: 0.2253
Epoch [15/30], Batch [4000/6000], Loss: 0.1071
Epoch [15/30], Batch [4100/6000], Loss: 0.1664
Epoch [15/30], Batch [4200/6000], Loss: 0.1426
Epoch [15/30], Batch [4300/6000], Loss: 0.1552
Epoch [15/30], Batch [4400/6000], Loss: 0.1321
Epoch [15/30], Batch [4500/6000], Loss: 0.1036
Epoch [15/30], Batch [4600/6000], Loss: 0.1298
Epoch [15/30], Batch [4700/6000], Loss: 0.1464
Epoch [15/30], Batch [4800/6000], Loss: 0.1609
Epoch [15/30], Batch [4900/6000], Loss: 0.1220
Epoch [15/30], Batch [5000/6000], Loss: 0.1414
Epoch [15/30], Batch [5100/6000], Loss: 0.1509
Epoch [15/30], Batch [5200/6000], Loss: 0.1513
Epoch [15/30], Batch [5300/6000], Loss: 0.4634
Epoch [15/30], Batch [5400/6000], Loss: 0.1137
Epoch [15/30], Batch [5500/6000], Loss: 1.0939
Epoch [15/30], Batch [5600/6000], Loss: 0.1183
Epoch [15/30], Batch [5700/6000], Loss: 0.1573
Epoch [15/30], Batch [5800/6000], Loss: 0.1613
Epoch [15/30], Batch [5900/6000], Loss: 2.1841
Epoch [15/30], Loss: 0.2840
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.1681
Epoch [16/30], Batch [100/6000], Loss: 0.3175
Epoch [16/30], Batch [200/6000], Loss: 0.7187
Epoch [16/30], Batch [300/6000], Loss: 0.1380
Epoch [16/30], Batch [400/6000], Loss: 0.6718
Epoch [16/30], Batch [500/6000], Loss: 0.1279
Epoch [16/30], Batch [600/6000], Loss: 0.1420
Epoch [16/30], Batch [700/6000], Loss: 0.0869
Epoch [16/30], Batch [800/6000], Loss: 0.1383
Epoch [16/30], Batch [900/6000], Loss: 2.6750
Epoch [16/30], Batch [1000/6000], Loss: 0.1351
Epoch [16/30], Batch [1100/6000], Loss: 1.9127
Epoch [16/30], Batch [1200/6000], Loss: 0.1160
Epoch [16/30], Batch [1300/6000], Loss: 0.1477
Epoch [16/30], Batch [1400/6000], Loss: 0.1099
Epoch [16/30], Batch [1500/6000], Loss: 0.5737
Epoch [16/30], Batch [1600/6000], Loss: 0.1823
Epoch [16/30], Batch [1700/6000], Loss: 0.1348
Epoch [16/30], Batch [1800/6000], Loss: 0.3151
Epoch [16/30], Batch [1900/6000], Loss: 0.3921
Epoch [16/30], Batch [2000/6000], Loss: 0.2021
Epoch [16/30], Batch [2100/6000], Loss: 0.0980
Epoch [16/30], Batch [2200/6000], Loss: 0.1369
Epoch [16/30], Batch [2300/6000], Loss: 0.1465
Epoch [16/30], Batch [2400/6000], Loss: 0.1460
Epoch [16/30], Batch [2500/6000], Loss: 0.1371
Epoch [16/30], Batch [2600/6000], Loss: 0.1735
Epoch [16/30], Batch [2700/6000], Loss: 0.1328
Epoch [16/30], Batch [2800/6000], Loss: 0.0938
Epoch [16/30], Batch [2900/6000], Loss: 0.1922
Epoch [16/30], Batch [3000/6000], Loss: 0.1369
Epoch [16/30], Batch [3100/6000], Loss: 0.1281
Epoch [16/30], Batch [3200/6000], Loss: 0.5403
Epoch [16/30], Batch [3300/6000], Loss: 0.1337
Epoch [16/30], Batch [3400/6000], Loss: 0.2043
Epoch [16/30], Batch [3500/6000], Loss: 0.4532
Epoch [16/30], Batch [3600/6000], Loss: 0.1302
Epoch [16/30], Batch [3700/6000], Loss: 0.1183
Epoch [16/30], Batch [3800/6000], Loss: 0.1222
Epoch [16/30], Batch [3900/6000], Loss: 0.3247
Epoch [16/30], Batch [4000/6000], Loss: 0.5778
Epoch [16/30], Batch [4100/6000], Loss: 0.1969
Epoch [16/30], Batch [4200/6000], Loss: 0.1567
Epoch [16/30], Batch [4300/6000], Loss: 0.3488
Epoch [16/30], Batch [4400/6000], Loss: 0.1162
Epoch [16/30], Batch [4500/6000], Loss: 0.0977
Epoch [16/30], Batch [4600/6000], Loss: 0.1279
Epoch [16/30], Batch [4700/6000], Loss: 0.1401
Epoch [16/30], Batch [4800/6000], Loss: 0.1115
Epoch [16/30], Batch [4900/6000], Loss: 0.1578
Epoch [16/30], Batch [5000/6000], Loss: 0.1539
Epoch [16/30], Batch [5100/6000], Loss: 0.1438
Epoch [16/30], Batch [5200/6000], Loss: 0.3465
Epoch [16/30], Batch [5300/6000], Loss: 0.8432
Epoch [16/30], Batch [5400/6000], Loss: 0.1530
Epoch [16/30], Batch [5500/6000], Loss: 1.5771
Epoch [16/30], Batch [5600/6000], Loss: 0.2200
Epoch [16/30], Batch [5700/6000], Loss: 0.2404
Epoch [16/30], Batch [5800/6000], Loss: 0.7677
Epoch [16/30], Batch [5900/6000], Loss: 0.1749
Epoch [16/30], Loss: 0.2728
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.1583
Epoch [17/30], Batch [100/6000], Loss: 0.1631
Epoch [17/30], Batch [200/6000], Loss: 0.1346
Epoch [17/30], Batch [300/6000], Loss: 0.8066
Epoch [17/30], Batch [400/6000], Loss: 0.1137
Epoch [17/30], Batch [500/6000], Loss: 0.3606
Epoch [17/30], Batch [600/6000], Loss: 0.1355
Epoch [17/30], Batch [700/6000], Loss: 0.4362
Epoch [17/30], Batch [800/6000], Loss: 0.2160
Epoch [17/30], Batch [900/6000], Loss: 0.1643
Epoch [17/30], Batch [1000/6000], Loss: 0.1635
Epoch [17/30], Batch [1100/6000], Loss: 0.1915
Epoch [17/30], Batch [1200/6000], Loss: 0.2207
Epoch [17/30], Batch [1300/6000], Loss: 0.1652
Epoch [17/30], Batch [1400/6000], Loss: 0.1704
Epoch [17/30], Batch [1500/6000], Loss: 0.1569
Epoch [17/30], Batch [1600/6000], Loss: 0.2307
Epoch [17/30], Batch [1700/6000], Loss: 0.1357
Epoch [17/30], Batch [1800/6000], Loss: 0.1509
Epoch [17/30], Batch [1900/6000], Loss: 0.1015
Epoch [17/30], Batch [2000/6000], Loss: 0.1799
Epoch [17/30], Batch [2100/6000], Loss: 0.1522
Epoch [17/30], Batch [2200/6000], Loss: 0.1656
Epoch [17/30], Batch [2300/6000], Loss: 0.1415
Epoch [17/30], Batch [2400/6000], Loss: 0.0982
Epoch [17/30], Batch [2500/6000], Loss: 0.1321
Epoch [17/30], Batch [2600/6000], Loss: 0.4777
Epoch [17/30], Batch [2700/6000], Loss: 0.1854
Epoch [17/30], Batch [2800/6000], Loss: 0.1256
Epoch [17/30], Batch [2900/6000], Loss: 0.1437
Epoch [17/30], Batch [3000/6000], Loss: 0.2042
Epoch [17/30], Batch [3100/6000], Loss: 0.5644
Epoch [17/30], Batch [3200/6000], Loss: 0.1234
Epoch [17/30], Batch [3300/6000], Loss: 0.1466
Epoch [17/30], Batch [3400/6000], Loss: 0.1908
Epoch [17/30], Batch [3500/6000], Loss: 0.1220
Epoch [17/30], Batch [3600/6000], Loss: 0.1343
Epoch [17/30], Batch [3700/6000], Loss: 0.1512
Epoch [17/30], Batch [3800/6000], Loss: 0.1100
Epoch [17/30], Batch [3900/6000], Loss: 0.1737
Epoch [17/30], Batch [4000/6000], Loss: 1.6333
Epoch [17/30], Batch [4100/6000], Loss: 0.1069
Epoch [17/30], Batch [4200/6000], Loss: 0.2229
Epoch [17/30], Batch [4300/6000], Loss: 0.1843
Epoch [17/30], Batch [4400/6000], Loss: 0.1737
Epoch [17/30], Batch [4500/6000], Loss: 0.7868
Epoch [17/30], Batch [4600/6000], Loss: 0.1035
Epoch [17/30], Batch [4700/6000], Loss: 1.0869
Epoch [17/30], Batch [4800/6000], Loss: 1.0100
Epoch [17/30], Batch [4900/6000], Loss: 0.3878
Epoch [17/30], Batch [5000/6000], Loss: 0.1508
Epoch [17/30], Batch [5100/6000], Loss: 0.3039
Epoch [17/30], Batch [5200/6000], Loss: 0.1588
Epoch [17/30], Batch [5300/6000], Loss: 0.1245
Epoch [17/30], Batch [5400/6000], Loss: 0.1424
Epoch [17/30], Batch [5500/6000], Loss: 0.1801
Epoch [17/30], Batch [5600/6000], Loss: 0.3916
Epoch [17/30], Batch [5700/6000], Loss: 0.0929
Epoch [17/30], Batch [5800/6000], Loss: 0.1079
Epoch [17/30], Batch [5900/6000], Loss: 0.1016
Epoch [17/30], Loss: 0.2572
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.1333
Epoch [18/30], Batch [100/6000], Loss: 0.1226
Epoch [18/30], Batch [200/6000], Loss: 0.1698
Epoch [18/30], Batch [300/6000], Loss: 0.1164
Epoch [18/30], Batch [400/6000], Loss: 0.1295
Epoch [18/30], Batch [500/6000], Loss: 0.1693
Epoch [18/30], Batch [600/6000], Loss: 0.1229
Epoch [18/30], Batch [700/6000], Loss: 0.1321
Epoch [18/30], Batch [800/6000], Loss: 0.1605
Epoch [18/30], Batch [900/6000], Loss: 0.1159
Epoch [18/30], Batch [1000/6000], Loss: 0.1444
Epoch [18/30], Batch [1100/6000], Loss: 0.1601
Epoch [18/30], Batch [1200/6000], Loss: 0.1277
Epoch [18/30], Batch [1300/6000], Loss: 0.1318
Epoch [18/30], Batch [1400/6000], Loss: 0.1176
Epoch [18/30], Batch [1500/6000], Loss: 0.1169
Epoch [18/30], Batch [1600/6000], Loss: 0.1420
Epoch [18/30], Batch [1700/6000], Loss: 0.1023
Epoch [18/30], Batch [1800/6000], Loss: 0.1550
Epoch [18/30], Batch [1900/6000], Loss: 0.1225
Epoch [18/30], Batch [2000/6000], Loss: 0.1219
Epoch [18/30], Batch [2100/6000], Loss: 0.2596
Epoch [18/30], Batch [2200/6000], Loss: 0.1140
Epoch [18/30], Batch [2300/6000], Loss: 0.1575
Epoch [18/30], Batch [2400/6000], Loss: 0.1212
Epoch [18/30], Batch [2500/6000], Loss: 0.1106
Epoch [18/30], Batch [2600/6000], Loss: 0.1011
Epoch [18/30], Batch [2700/6000], Loss: 0.1395
Epoch [18/30], Batch [2800/6000], Loss: 0.2486
Epoch [18/30], Batch [2900/6000], Loss: 0.1027
Epoch [18/30], Batch [3000/6000], Loss: 0.1853
Epoch [18/30], Batch [3100/6000], Loss: 0.1832
Epoch [18/30], Batch [3200/6000], Loss: 0.1592
Epoch [18/30], Batch [3300/6000], Loss: 0.1339
Epoch [18/30], Batch [3400/6000], Loss: 0.8230
Epoch [18/30], Batch [3500/6000], Loss: 0.1483
Epoch [18/30], Batch [3600/6000], Loss: 0.1294
Epoch [18/30], Batch [3700/6000], Loss: 0.0927
Epoch [18/30], Batch [3800/6000], Loss: 0.1256
Epoch [18/30], Batch [3900/6000], Loss: 0.1446
Epoch [18/30], Batch [4000/6000], Loss: 0.1232
Epoch [18/30], Batch [4100/6000], Loss: 0.1437
Epoch [18/30], Batch [4200/6000], Loss: 0.1358
Epoch [18/30], Batch [4300/6000], Loss: 0.0921
Epoch [18/30], Batch [4400/6000], Loss: 0.1190
Epoch [18/30], Batch [4500/6000], Loss: 0.1459
Epoch [18/30], Batch [4600/6000], Loss: 0.1455
Epoch [18/30], Batch [4700/6000], Loss: 0.1460
Epoch [18/30], Batch [4800/6000], Loss: 0.1384
Epoch [18/30], Batch [4900/6000], Loss: 0.1264
Epoch [18/30], Batch [5000/6000], Loss: 0.1355
Epoch [18/30], Batch [5100/6000], Loss: 0.1415
Epoch [18/30], Batch [5200/6000], Loss: 0.1372
Epoch [18/30], Batch [5300/6000], Loss: 0.2167
Epoch [18/30], Batch [5400/6000], Loss: 0.1294
Epoch [18/30], Batch [5500/6000], Loss: 0.1326
Epoch [18/30], Batch [5600/6000], Loss: 0.3317
Epoch [18/30], Batch [5700/6000], Loss: 0.5668
Epoch [18/30], Batch [5800/6000], Loss: 0.5749
Epoch [18/30], Batch [5900/6000], Loss: 0.1782
Epoch [18/30], Loss: 0.2489
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.1609
Epoch [19/30], Batch [100/6000], Loss: 0.1311
Epoch [19/30], Batch [200/6000], Loss: 0.1089
Epoch [19/30], Batch [300/6000], Loss: 0.1376
Epoch [19/30], Batch [400/6000], Loss: 0.1378
Epoch [19/30], Batch [500/6000], Loss: 0.1238
Epoch [19/30], Batch [600/6000], Loss: 0.1307
Epoch [19/30], Batch [700/6000], Loss: 0.8209
Epoch [19/30], Batch [800/6000], Loss: 0.1125
Epoch [19/30], Batch [900/6000], Loss: 0.1357
Epoch [19/30], Batch [1000/6000], Loss: 0.1590
Epoch [19/30], Batch [1100/6000], Loss: 0.1453
Epoch [19/30], Batch [1200/6000], Loss: 0.1343
Epoch [19/30], Batch [1300/6000], Loss: 0.1415
Epoch [19/30], Batch [1400/6000], Loss: 0.2599
Epoch [19/30], Batch [1500/6000], Loss: 0.1278
Epoch [19/30], Batch [1600/6000], Loss: 0.0962
Epoch [19/30], Batch [1700/6000], Loss: 0.1108
Epoch [19/30], Batch [1800/6000], Loss: 0.1436
Epoch [19/30], Batch [1900/6000], Loss: 2.5679
Epoch [19/30], Batch [2000/6000], Loss: 0.0972
Epoch [19/30], Batch [2100/6000], Loss: 0.0717
Epoch [19/30], Batch [2200/6000], Loss: 0.1225
Epoch [19/30], Batch [2300/6000], Loss: 3.5144
Epoch [19/30], Batch [2400/6000], Loss: 0.1147
Epoch [19/30], Batch [2500/6000], Loss: 0.4624
Epoch [19/30], Batch [2600/6000], Loss: 0.4081
Epoch [19/30], Batch [2700/6000], Loss: 0.1347
Epoch [19/30], Batch [2800/6000], Loss: 0.1120
Epoch [19/30], Batch [2900/6000], Loss: 0.1243
Epoch [19/30], Batch [3000/6000], Loss: 0.1293
Epoch [19/30], Batch [3100/6000], Loss: 0.1075
Epoch [19/30], Batch [3200/6000], Loss: 0.0743
Epoch [19/30], Batch [3300/6000], Loss: 0.1671
Epoch [19/30], Batch [3400/6000], Loss: 0.1217
Epoch [19/30], Batch [3500/6000], Loss: 0.0964
Epoch [19/30], Batch [3600/6000], Loss: 0.2262
Epoch [19/30], Batch [3700/6000], Loss: 0.1310
Epoch [19/30], Batch [3800/6000], Loss: 0.1029
Epoch [19/30], Batch [3900/6000], Loss: 0.0964
Epoch [19/30], Batch [4000/6000], Loss: 0.1374
Epoch [19/30], Batch [4100/6000], Loss: 0.1310
Epoch [19/30], Batch [4200/6000], Loss: 0.1522
Epoch [19/30], Batch [4300/6000], Loss: 0.1724
Epoch [19/30], Batch [4400/6000], Loss: 0.8137
Epoch [19/30], Batch [4500/6000], Loss: 0.1223
Epoch [19/30], Batch [4600/6000], Loss: 0.1196
Epoch [19/30], Batch [4700/6000], Loss: 0.1795
Epoch [19/30], Batch [4800/6000], Loss: 0.1805
Epoch [19/30], Batch [4900/6000], Loss: 0.1261
Epoch [19/30], Batch [5000/6000], Loss: 0.1184
Epoch [19/30], Batch [5100/6000], Loss: 1.2558
Epoch [19/30], Batch [5200/6000], Loss: 0.1591
Epoch [19/30], Batch [5300/6000], Loss: 1.4710
Epoch [19/30], Batch [5400/6000], Loss: 0.1419
Epoch [19/30], Batch [5500/6000], Loss: 0.1279
Epoch [19/30], Batch [5600/6000], Loss: 0.0849
Epoch [19/30], Batch [5700/6000], Loss: 0.1367
Epoch [19/30], Batch [5800/6000], Loss: 0.1667
Epoch [19/30], Batch [5900/6000], Loss: 0.1177
Epoch [19/30], Loss: 0.2320
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.1335
Epoch [20/30], Batch [100/6000], Loss: 0.1386
Epoch [20/30], Batch [200/6000], Loss: 0.2225
Epoch [20/30], Batch [300/6000], Loss: 0.1201
Epoch [20/30], Batch [400/6000], Loss: 0.1046
Epoch [20/30], Batch [500/6000], Loss: 0.1536
Epoch [20/30], Batch [600/6000], Loss: 0.1151
Epoch [20/30], Batch [700/6000], Loss: 0.1105
Epoch [20/30], Batch [800/6000], Loss: 0.1319
Epoch [20/30], Batch [900/6000], Loss: 0.1511
Epoch [20/30], Batch [1000/6000], Loss: 0.1696
Epoch [20/30], Batch [1100/6000], Loss: 0.1309
Epoch [20/30], Batch [1200/6000], Loss: 0.1383
Epoch [20/30], Batch [1300/6000], Loss: 0.3001
Epoch [20/30], Batch [1400/6000], Loss: 0.1041
Epoch [20/30], Batch [1500/6000], Loss: 0.1339
Epoch [20/30], Batch [1600/6000], Loss: 0.1276
Epoch [20/30], Batch [1700/6000], Loss: 0.1288
Epoch [20/30], Batch [1800/6000], Loss: 1.0713
Epoch [20/30], Batch [1900/6000], Loss: 0.1035
Epoch [20/30], Batch [2000/6000], Loss: 0.1530
Epoch [20/30], Batch [2100/6000], Loss: 0.1659
Epoch [20/30], Batch [2200/6000], Loss: 0.1244
Epoch [20/30], Batch [2300/6000], Loss: 0.6657
Epoch [20/30], Batch [2400/6000], Loss: 0.5723
Epoch [20/30], Batch [2500/6000], Loss: 0.1147
Epoch [20/30], Batch [2600/6000], Loss: 0.1261
Epoch [20/30], Batch [2700/6000], Loss: 0.1058
Epoch [20/30], Batch [2800/6000], Loss: 0.1111
Epoch [20/30], Batch [2900/6000], Loss: 0.1235
Epoch [20/30], Batch [3000/6000], Loss: 0.1802
Epoch [20/30], Batch [3100/6000], Loss: 0.1152
Epoch [20/30], Batch [3200/6000], Loss: 0.1452
Epoch [20/30], Batch [3300/6000], Loss: 0.1260
Epoch [20/30], Batch [3400/6000], Loss: 0.1167
Epoch [20/30], Batch [3500/6000], Loss: 0.1319
Epoch [20/30], Batch [3600/6000], Loss: 0.1200
Epoch [20/30], Batch [3700/6000], Loss: 0.1237
Epoch [20/30], Batch [3800/6000], Loss: 0.1200
Epoch [20/30], Batch [3900/6000], Loss: 0.1440
Epoch [20/30], Batch [4000/6000], Loss: 0.1391
Epoch [20/30], Batch [4100/6000], Loss: 0.1143
Epoch [20/30], Batch [4200/6000], Loss: 0.1115
Epoch [20/30], Batch [4300/6000], Loss: 0.1300
Epoch [20/30], Batch [4400/6000], Loss: 0.1090
Epoch [20/30], Batch [4500/6000], Loss: 0.1641
Epoch [20/30], Batch [4600/6000], Loss: 0.1615
Epoch [20/30], Batch [4700/6000], Loss: 0.4184
Epoch [20/30], Batch [4800/6000], Loss: 0.1188
Epoch [20/30], Batch [4900/6000], Loss: 0.1279
Epoch [20/30], Batch [5000/6000], Loss: 0.1099
Epoch [20/30], Batch [5100/6000], Loss: 0.1224
Epoch [20/30], Batch [5200/6000], Loss: 0.9255
Epoch [20/30], Batch [5300/6000], Loss: 0.1304
Epoch [20/30], Batch [5400/6000], Loss: 0.2265
Epoch [20/30], Batch [5500/6000], Loss: 0.1207
Epoch [20/30], Batch [5600/6000], Loss: 0.4735
Epoch [20/30], Batch [5700/6000], Loss: 0.0964
Epoch [20/30], Batch [5800/6000], Loss: 0.1250
Epoch [20/30], Batch [5900/6000], Loss: 0.1337
Epoch [20/30], Loss: 0.2258
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.1497
Epoch [21/30], Batch [100/6000], Loss: 0.1717
Epoch [21/30], Batch [200/6000], Loss: 0.1673
Epoch [21/30], Batch [300/6000], Loss: 0.1133
Epoch [21/30], Batch [400/6000], Loss: 0.1266
Epoch [21/30], Batch [500/6000], Loss: 0.1257
Epoch [21/30], Batch [600/6000], Loss: 0.1879
Epoch [21/30], Batch [700/6000], Loss: 0.0999
Epoch [21/30], Batch [800/6000], Loss: 1.3574
Epoch [21/30], Batch [900/6000], Loss: 0.1179
Epoch [21/30], Batch [1000/6000], Loss: 0.1817
Epoch [21/30], Batch [1100/6000], Loss: 0.1360
Epoch [21/30], Batch [1200/6000], Loss: 0.1381
Epoch [21/30], Batch [1300/6000], Loss: 0.1385
Epoch [21/30], Batch [1400/6000], Loss: 0.2073
Epoch [21/30], Batch [1500/6000], Loss: 1.0775
Epoch [21/30], Batch [1600/6000], Loss: 0.1247
Epoch [21/30], Batch [1700/6000], Loss: 0.5977
Epoch [21/30], Batch [1800/6000], Loss: 0.1288
Epoch [21/30], Batch [1900/6000], Loss: 0.1062
Epoch [21/30], Batch [2000/6000], Loss: 0.1236
Epoch [21/30], Batch [2100/6000], Loss: 0.1222
Epoch [21/30], Batch [2200/6000], Loss: 0.1114
Epoch [21/30], Batch [2300/6000], Loss: 0.1213
Epoch [21/30], Batch [2400/6000], Loss: 0.1361
Epoch [21/30], Batch [2500/6000], Loss: 0.1244
Epoch [21/30], Batch [2600/6000], Loss: 0.2056
Epoch [21/30], Batch [2700/6000], Loss: 0.1489
Epoch [21/30], Batch [2800/6000], Loss: 0.1394
Epoch [21/30], Batch [2900/6000], Loss: 0.4779
Epoch [21/30], Batch [3000/6000], Loss: 0.7151
Epoch [21/30], Batch [3100/6000], Loss: 0.1345
Epoch [21/30], Batch [3200/6000], Loss: 0.1309
Epoch [21/30], Batch [3300/6000], Loss: 0.8683
Epoch [21/30], Batch [3400/6000], Loss: 0.0919
Epoch [21/30], Batch [3500/6000], Loss: 0.1244
Epoch [21/30], Batch [3600/6000], Loss: 0.1156
Epoch [21/30], Batch [3700/6000], Loss: 0.0854
Epoch [21/30], Batch [3800/6000], Loss: 0.1063
Epoch [21/30], Batch [3900/6000], Loss: 0.0923
Epoch [21/30], Batch [4000/6000], Loss: 0.1020
Epoch [21/30], Batch [4100/6000], Loss: 0.2054
Epoch [21/30], Batch [4200/6000], Loss: 0.0897
Epoch [21/30], Batch [4300/6000], Loss: 0.1179
Epoch [21/30], Batch [4400/6000], Loss: 0.1293
Epoch [21/30], Batch [4500/6000], Loss: 0.1069
Epoch [21/30], Batch [4600/6000], Loss: 0.1233
Epoch [21/30], Batch [4700/6000], Loss: 0.2088
Epoch [21/30], Batch [4800/6000], Loss: 0.1332
Epoch [21/30], Batch [4900/6000], Loss: 0.1182
Epoch [21/30], Batch [5000/6000], Loss: 0.1216
Epoch [21/30], Batch [5100/6000], Loss: 0.1166
Epoch [21/30], Batch [5200/6000], Loss: 0.3757
Epoch [21/30], Batch [5300/6000], Loss: 0.1250
Epoch [21/30], Batch [5400/6000], Loss: 0.1025
Epoch [21/30], Batch [5500/6000], Loss: 0.2805
Epoch [21/30], Batch [5600/6000], Loss: 0.0878
Epoch [21/30], Batch [5700/6000], Loss: 0.1371
Epoch [21/30], Batch [5800/6000], Loss: 0.1335
Epoch [21/30], Batch [5900/6000], Loss: 0.1323
Epoch [21/30], Loss: 0.2186
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.1432
Epoch [22/30], Batch [100/6000], Loss: 0.3453
Epoch [22/30], Batch [200/6000], Loss: 0.1213
Epoch [22/30], Batch [300/6000], Loss: 0.0987
Epoch [22/30], Batch [400/6000], Loss: 0.1392
Epoch [22/30], Batch [500/6000], Loss: 0.1121
Epoch [22/30], Batch [600/6000], Loss: 0.1559
Epoch [22/30], Batch [700/6000], Loss: 0.1202
Epoch [22/30], Batch [800/6000], Loss: 0.1552
Epoch [22/30], Batch [900/6000], Loss: 0.1038
Epoch [22/30], Batch [1000/6000], Loss: 0.0857
Epoch [22/30], Batch [1100/6000], Loss: 0.1144
Epoch [22/30], Batch [1200/6000], Loss: 0.0965
Epoch [22/30], Batch [1300/6000], Loss: 0.6117
Epoch [22/30], Batch [1400/6000], Loss: 0.1214
Epoch [22/30], Batch [1500/6000], Loss: 0.1096
Epoch [22/30], Batch [1600/6000], Loss: 0.1254
Epoch [22/30], Batch [1700/6000], Loss: 0.5218
Epoch [22/30], Batch [1800/6000], Loss: 0.1213
Epoch [22/30], Batch [1900/6000], Loss: 0.1502
Epoch [22/30], Batch [2000/6000], Loss: 0.1612
Epoch [22/30], Batch [2100/6000], Loss: 1.1527
Epoch [22/30], Batch [2200/6000], Loss: 0.1025
Epoch [22/30], Batch [2300/6000], Loss: 0.1306
Epoch [22/30], Batch [2400/6000], Loss: 1.5280
Epoch [22/30], Batch [2500/6000], Loss: 0.1118
Epoch [22/30], Batch [2600/6000], Loss: 0.1095
Epoch [22/30], Batch [2700/6000], Loss: 0.1175
Epoch [22/30], Batch [2800/6000], Loss: 0.1025
Epoch [22/30], Batch [2900/6000], Loss: 0.1189
Epoch [22/30], Batch [3000/6000], Loss: 0.2121
Epoch [22/30], Batch [3100/6000], Loss: 0.1349
Epoch [22/30], Batch [3200/6000], Loss: 0.7026
Epoch [22/30], Batch [3300/6000], Loss: 0.4561
Epoch [22/30], Batch [3400/6000], Loss: 0.1064
Epoch [22/30], Batch [3500/6000], Loss: 0.1296
Epoch [22/30], Batch [3600/6000], Loss: 0.4888
Epoch [22/30], Batch [3700/6000], Loss: 0.1270
Epoch [22/30], Batch [3800/6000], Loss: 0.1509
Epoch [22/30], Batch [3900/6000], Loss: 0.1111
Epoch [22/30], Batch [4000/6000], Loss: 0.1111
Epoch [22/30], Batch [4100/6000], Loss: 0.1823
Epoch [22/30], Batch [4200/6000], Loss: 0.1784
Epoch [22/30], Batch [4300/6000], Loss: 0.0971
Epoch [22/30], Batch [4400/6000], Loss: 0.1258
Epoch [22/30], Batch [4500/6000], Loss: 0.3130
Epoch [22/30], Batch [4600/6000], Loss: 0.1242
Epoch [22/30], Batch [4700/6000], Loss: 0.6214
Epoch [22/30], Batch [4800/6000], Loss: 0.1191
Epoch [22/30], Batch [4900/6000], Loss: 0.1355
Epoch [22/30], Batch [5000/6000], Loss: 0.1053
Epoch [22/30], Batch [5100/6000], Loss: 0.3041
Epoch [22/30], Batch [5200/6000], Loss: 0.0767
Epoch [22/30], Batch [5300/6000], Loss: 0.1054
Epoch [22/30], Batch [5400/6000], Loss: 1.0942
Epoch [22/30], Batch [5500/6000], Loss: 0.5341
Epoch [22/30], Batch [5600/6000], Loss: 0.1397
Epoch [22/30], Batch [5700/6000], Loss: 0.1065
Epoch [22/30], Batch [5800/6000], Loss: 0.1464
Epoch [22/30], Batch [5900/6000], Loss: 0.1394
Epoch [22/30], Loss: 0.2078
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.1088
Epoch [23/30], Batch [100/6000], Loss: 0.2023
Epoch [23/30], Batch [200/6000], Loss: 0.1348
Epoch [23/30], Batch [300/6000], Loss: 0.1254
Epoch [23/30], Batch [400/6000], Loss: 0.0939
Epoch [23/30], Batch [500/6000], Loss: 1.2813
Epoch [23/30], Batch [600/6000], Loss: 0.1125
Epoch [23/30], Batch [700/6000], Loss: 0.1403
Epoch [23/30], Batch [800/6000], Loss: 0.1235
Epoch [23/30], Batch [900/6000], Loss: 0.0992
Epoch [23/30], Batch [1000/6000], Loss: 0.0993
Epoch [23/30], Batch [1100/6000], Loss: 0.1065
Epoch [23/30], Batch [1200/6000], Loss: 0.1158
Epoch [23/30], Batch [1300/6000], Loss: 0.1394
Epoch [23/30], Batch [1400/6000], Loss: 0.2137
Epoch [23/30], Batch [1500/6000], Loss: 0.1415
Epoch [23/30], Batch [1600/6000], Loss: 0.1218
Epoch [23/30], Batch [1700/6000], Loss: 0.1307
Epoch [23/30], Batch [1800/6000], Loss: 0.1350
Epoch [23/30], Batch [1900/6000], Loss: 0.1082
Epoch [23/30], Batch [2000/6000], Loss: 0.1177
Epoch [23/30], Batch [2100/6000], Loss: 0.1147
Epoch [23/30], Batch [2200/6000], Loss: 0.1051
Epoch [23/30], Batch [2300/6000], Loss: 0.1245
Epoch [23/30], Batch [2400/6000], Loss: 0.1121
Epoch [23/30], Batch [2500/6000], Loss: 0.1154
Epoch [23/30], Batch [2600/6000], Loss: 0.1025
Epoch [23/30], Batch [2700/6000], Loss: 0.0925
Epoch [23/30], Batch [2800/6000], Loss: 0.1290
Epoch [23/30], Batch [2900/6000], Loss: 0.1957
Epoch [23/30], Batch [3000/6000], Loss: 0.1023
Epoch [23/30], Batch [3100/6000], Loss: 0.1370
Epoch [23/30], Batch [3200/6000], Loss: 0.1377
Epoch [23/30], Batch [3300/6000], Loss: 0.1006
Epoch [23/30], Batch [3400/6000], Loss: 0.1459
Epoch [23/30], Batch [3500/6000], Loss: 0.1064
Epoch [23/30], Batch [3600/6000], Loss: 0.1305
Epoch [23/30], Batch [3700/6000], Loss: 0.1527
Epoch [23/30], Batch [3800/6000], Loss: 0.0986
Epoch [23/30], Batch [3900/6000], Loss: 0.1301
Epoch [23/30], Batch [4000/6000], Loss: 0.0990
Epoch [23/30], Batch [4100/6000], Loss: 0.1583
Epoch [23/30], Batch [4200/6000], Loss: 0.1057
Epoch [23/30], Batch [4300/6000], Loss: 0.1384
Epoch [23/30], Batch [4400/6000], Loss: 0.1116
Epoch [23/30], Batch [4500/6000], Loss: 0.1501
Epoch [23/30], Batch [4600/6000], Loss: 0.1232
Epoch [23/30], Batch [4700/6000], Loss: 0.1332
Epoch [23/30], Batch [4800/6000], Loss: 0.1343
Epoch [23/30], Batch [4900/6000], Loss: 0.1513
Epoch [23/30], Batch [5000/6000], Loss: 0.1129
Epoch [23/30], Batch [5100/6000], Loss: 2.0333
Epoch [23/30], Batch [5200/6000], Loss: 0.1432
Epoch [23/30], Batch [5300/6000], Loss: 0.1044
Epoch [23/30], Batch [5400/6000], Loss: 0.1181
Epoch [23/30], Batch [5500/6000], Loss: 0.2294
Epoch [23/30], Batch [5600/6000], Loss: 0.1203
Epoch [23/30], Batch [5700/6000], Loss: 0.1320
Epoch [23/30], Batch [5800/6000], Loss: 0.1297
Epoch [23/30], Batch [5900/6000], Loss: 0.2119
Epoch [23/30], Loss: 0.2013
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.1080
Epoch [24/30], Batch [100/6000], Loss: 0.2358
Epoch [24/30], Batch [200/6000], Loss: 0.1298
Epoch [24/30], Batch [300/6000], Loss: 0.1167
Epoch [24/30], Batch [400/6000], Loss: 0.0915
Epoch [24/30], Batch [500/6000], Loss: 0.5489
Epoch [24/30], Batch [600/6000], Loss: 0.0887
Epoch [24/30], Batch [700/6000], Loss: 0.1103
Epoch [24/30], Batch [800/6000], Loss: 0.1275
Epoch [24/30], Batch [900/6000], Loss: 0.1393
Epoch [24/30], Batch [1000/6000], Loss: 0.1108
Epoch [24/30], Batch [1100/6000], Loss: 0.1096
Epoch [24/30], Batch [1200/6000], Loss: 0.1346
Epoch [24/30], Batch [1300/6000], Loss: 0.2914
Epoch [24/30], Batch [1400/6000], Loss: 0.1415
Epoch [24/30], Batch [1500/6000], Loss: 0.8871
Epoch [24/30], Batch [1600/6000], Loss: 0.1392
Epoch [24/30], Batch [1700/6000], Loss: 0.1205
Epoch [24/30], Batch [1800/6000], Loss: 0.1239
Epoch [24/30], Batch [1900/6000], Loss: 0.1244
Epoch [24/30], Batch [2000/6000], Loss: 0.0955
Epoch [24/30], Batch [2100/6000], Loss: 0.1510
Epoch [24/30], Batch [2200/6000], Loss: 0.1114
Epoch [24/30], Batch [2300/6000], Loss: 0.1084
Epoch [24/30], Batch [2400/6000], Loss: 0.1080
Epoch [24/30], Batch [2500/6000], Loss: 0.1497
Epoch [24/30], Batch [2600/6000], Loss: 0.1220
Epoch [24/30], Batch [2700/6000], Loss: 0.1329
Epoch [24/30], Batch [2800/6000], Loss: 0.1229
Epoch [24/30], Batch [2900/6000], Loss: 0.1271
Epoch [24/30], Batch [3000/6000], Loss: 0.1512
Epoch [24/30], Batch [3100/6000], Loss: 0.1270
Epoch [24/30], Batch [3200/6000], Loss: 0.4444
Epoch [24/30], Batch [3300/6000], Loss: 0.1233
Epoch [24/30], Batch [3400/6000], Loss: 0.1349
Epoch [24/30], Batch [3500/6000], Loss: 0.1312
Epoch [24/30], Batch [3600/6000], Loss: 0.2094
Epoch [24/30], Batch [3700/6000], Loss: 0.1226
Epoch [24/30], Batch [3800/6000], Loss: 0.1287
Epoch [24/30], Batch [3900/6000], Loss: 0.1401
Epoch [24/30], Batch [4000/6000], Loss: 0.1076
Epoch [24/30], Batch [4100/6000], Loss: 0.1072
Epoch [24/30], Batch [4200/6000], Loss: 0.3584
Epoch [24/30], Batch [4300/6000], Loss: 0.1683
Epoch [24/30], Batch [4400/6000], Loss: 1.2426
Epoch [24/30], Batch [4500/6000], Loss: 0.1230
Epoch [24/30], Batch [4600/6000], Loss: 0.1196
Epoch [24/30], Batch [4700/6000], Loss: 0.1034
Epoch [24/30], Batch [4800/6000], Loss: 0.1239
Epoch [24/30], Batch [4900/6000], Loss: 0.1203
Epoch [24/30], Batch [5000/6000], Loss: 0.1279
Epoch [24/30], Batch [5100/6000], Loss: 0.1160
Epoch [24/30], Batch [5200/6000], Loss: 0.1133
Epoch [24/30], Batch [5300/6000], Loss: 0.2273
Epoch [24/30], Batch [5400/6000], Loss: 0.1149
Epoch [24/30], Batch [5500/6000], Loss: 0.1166
Epoch [24/30], Batch [5600/6000], Loss: 0.1181
Epoch [24/30], Batch [5700/6000], Loss: 0.0996
Epoch [24/30], Batch [5800/6000], Loss: 0.2736
Epoch [24/30], Batch [5900/6000], Loss: 0.1302
Epoch [24/30], Loss: 0.1945
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.2400
Epoch [25/30], Batch [100/6000], Loss: 0.1060
Epoch [25/30], Batch [200/6000], Loss: 0.1424
Epoch [25/30], Batch [300/6000], Loss: 0.0998
Epoch [25/30], Batch [400/6000], Loss: 0.1623
Epoch [25/30], Batch [500/6000], Loss: 0.2270
Epoch [25/30], Batch [600/6000], Loss: 0.1239
Epoch [25/30], Batch [700/6000], Loss: 0.1440
Epoch [25/30], Batch [800/6000], Loss: 0.1331
Epoch [25/30], Batch [900/6000], Loss: 0.1088
Epoch [25/30], Batch [1000/6000], Loss: 0.1270
Epoch [25/30], Batch [1100/6000], Loss: 0.1560
Epoch [25/30], Batch [1200/6000], Loss: 0.1643
Epoch [25/30], Batch [1300/6000], Loss: 0.1183
Epoch [25/30], Batch [1400/6000], Loss: 0.8350
Epoch [25/30], Batch [1500/6000], Loss: 0.1317
Epoch [25/30], Batch [1600/6000], Loss: 0.1570
Epoch [25/30], Batch [1700/6000], Loss: 0.1044
Epoch [25/30], Batch [1800/6000], Loss: 0.1275
Epoch [25/30], Batch [1900/6000], Loss: 0.1018
Epoch [25/30], Batch [2000/6000], Loss: 0.1267
Epoch [25/30], Batch [2100/6000], Loss: 0.1055
Epoch [25/30], Batch [2200/6000], Loss: 0.1418
Epoch [25/30], Batch [2300/6000], Loss: 0.1501
Epoch [25/30], Batch [2400/6000], Loss: 0.1617
Epoch [25/30], Batch [2500/6000], Loss: 0.1427
Epoch [25/30], Batch [2600/6000], Loss: 0.1041
Epoch [25/30], Batch [2700/6000], Loss: 0.1108
Epoch [25/30], Batch [2800/6000], Loss: 0.1210
Epoch [25/30], Batch [2900/6000], Loss: 0.1257
Epoch [25/30], Batch [3000/6000], Loss: 0.3741
Epoch [25/30], Batch [3100/6000], Loss: 0.0869
Epoch [25/30], Batch [3200/6000], Loss: 0.1424
Epoch [25/30], Batch [3300/6000], Loss: 0.1060
Epoch [25/30], Batch [3400/6000], Loss: 0.1544
Epoch [25/30], Batch [3500/6000], Loss: 0.0919
Epoch [25/30], Batch [3600/6000], Loss: 0.1149
Epoch [25/30], Batch [3700/6000], Loss: 0.1775
Epoch [25/30], Batch [3800/6000], Loss: 0.1035
Epoch [25/30], Batch [3900/6000], Loss: 0.1149
Epoch [25/30], Batch [4000/6000], Loss: 0.1037
Epoch [25/30], Batch [4100/6000], Loss: 0.0962
Epoch [25/30], Batch [4200/6000], Loss: 0.1356
Epoch [25/30], Batch [4300/6000], Loss: 0.1302
Epoch [25/30], Batch [4400/6000], Loss: 0.1069
Epoch [25/30], Batch [4500/6000], Loss: 0.1199
Epoch [25/30], Batch [4600/6000], Loss: 0.1063
Epoch [25/30], Batch [4700/6000], Loss: 0.0853
Epoch [25/30], Batch [4800/6000], Loss: 0.1195
Epoch [25/30], Batch [4900/6000], Loss: 0.1230
Epoch [25/30], Batch [5000/6000], Loss: 0.1399
Epoch [25/30], Batch [5100/6000], Loss: 0.1104
Epoch [25/30], Batch [5200/6000], Loss: 0.1154
Epoch [25/30], Batch [5300/6000], Loss: 0.3691
Epoch [25/30], Batch [5400/6000], Loss: 0.4631
Epoch [25/30], Batch [5500/6000], Loss: 0.1191
Epoch [25/30], Batch [5600/6000], Loss: 0.1455
Epoch [25/30], Batch [5700/6000], Loss: 0.1108
Epoch [25/30], Batch [5800/6000], Loss: 0.5565
Epoch [25/30], Batch [5900/6000], Loss: 0.1228
Epoch [25/30], Loss: 0.1844
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.1231
Epoch [26/30], Batch [100/6000], Loss: 0.1065
Epoch [26/30], Batch [200/6000], Loss: 0.1013
Epoch [26/30], Batch [300/6000], Loss: 0.1189
Epoch [26/30], Batch [400/6000], Loss: 0.1069
Epoch [26/30], Batch [500/6000], Loss: 0.1429
Epoch [26/30], Batch [600/6000], Loss: 0.1060
Epoch [26/30], Batch [700/6000], Loss: 0.0877
Epoch [26/30], Batch [800/6000], Loss: 0.1451
Epoch [26/30], Batch [900/6000], Loss: 0.1232
Epoch [26/30], Batch [1000/6000], Loss: 0.2457
Epoch [26/30], Batch [1100/6000], Loss: 0.1293
Epoch [26/30], Batch [1200/6000], Loss: 0.1426
Epoch [26/30], Batch [1300/6000], Loss: 0.0973
Epoch [26/30], Batch [1400/6000], Loss: 0.2614
Epoch [26/30], Batch [1500/6000], Loss: 0.1239
Epoch [26/30], Batch [1600/6000], Loss: 0.1051
Epoch [26/30], Batch [1700/6000], Loss: 0.1076
Epoch [26/30], Batch [1800/6000], Loss: 0.1297
Epoch [26/30], Batch [1900/6000], Loss: 0.1411
Epoch [26/30], Batch [2000/6000], Loss: 0.1073
Epoch [26/30], Batch [2100/6000], Loss: 0.1007
Epoch [26/30], Batch [2200/6000], Loss: 0.1075
Epoch [26/30], Batch [2300/6000], Loss: 0.1888
Epoch [26/30], Batch [2400/6000], Loss: 0.1073
Epoch [26/30], Batch [2500/6000], Loss: 0.1157
Epoch [26/30], Batch [2600/6000], Loss: 0.0767
Epoch [26/30], Batch [2700/6000], Loss: 0.1126
Epoch [26/30], Batch [2800/6000], Loss: 0.0885
Epoch [26/30], Batch [2900/6000], Loss: 0.1239
Epoch [26/30], Batch [3000/6000], Loss: 0.1143
Epoch [26/30], Batch [3100/6000], Loss: 0.1056
Epoch [26/30], Batch [3200/6000], Loss: 0.1469
Epoch [26/30], Batch [3300/6000], Loss: 0.1295
Epoch [26/30], Batch [3400/6000], Loss: 0.2109
Epoch [26/30], Batch [3500/6000], Loss: 0.1479
Epoch [26/30], Batch [3600/6000], Loss: 0.6307
Epoch [26/30], Batch [3700/6000], Loss: 0.1402
Epoch [26/30], Batch [3800/6000], Loss: 0.1152
Epoch [26/30], Batch [3900/6000], Loss: 0.1341
Epoch [26/30], Batch [4000/6000], Loss: 0.0859
Epoch [26/30], Batch [4100/6000], Loss: 0.0839
Epoch [26/30], Batch [4200/6000], Loss: 0.1110
Epoch [26/30], Batch [4300/6000], Loss: 0.1022
Epoch [26/30], Batch [4400/6000], Loss: 0.1624
Epoch [26/30], Batch [4500/6000], Loss: 0.1190
Epoch [26/30], Batch [4600/6000], Loss: 0.1264
Epoch [26/30], Batch [4700/6000], Loss: 0.1321
Epoch [26/30], Batch [4800/6000], Loss: 0.1156
Epoch [26/30], Batch [4900/6000], Loss: 0.0991
Epoch [26/30], Batch [5000/6000], Loss: 0.1226
Epoch [26/30], Batch [5100/6000], Loss: 0.1115
Epoch [26/30], Batch [5200/6000], Loss: 0.1014
Epoch [26/30], Batch [5300/6000], Loss: 0.1118
Epoch [26/30], Batch [5400/6000], Loss: 0.4624
Epoch [26/30], Batch [5500/6000], Loss: 0.1164
Epoch [26/30], Batch [5600/6000], Loss: 0.2846
Epoch [26/30], Batch [5700/6000], Loss: 0.2800
Epoch [26/30], Batch [5800/6000], Loss: 0.1299
Epoch [26/30], Batch [5900/6000], Loss: 0.1629
Epoch [26/30], Loss: 0.1844
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0950
Epoch [27/30], Batch [100/6000], Loss: 0.0864
Epoch [27/30], Batch [200/6000], Loss: 0.0723
Epoch [27/30], Batch [300/6000], Loss: 0.1238
Epoch [27/30], Batch [400/6000], Loss: 0.0959
Epoch [27/30], Batch [500/6000], Loss: 0.1027
Epoch [27/30], Batch [600/6000], Loss: 0.0994
Epoch [27/30], Batch [700/6000], Loss: 0.1504
Epoch [27/30], Batch [800/6000], Loss: 0.1085
Epoch [27/30], Batch [900/6000], Loss: 0.0822
Epoch [27/30], Batch [1000/6000], Loss: 0.1285
Epoch [27/30], Batch [1100/6000], Loss: 0.1191
Epoch [27/30], Batch [1200/6000], Loss: 0.1025
Epoch [27/30], Batch [1300/6000], Loss: 0.1009
Epoch [27/30], Batch [1400/6000], Loss: 0.1179
Epoch [27/30], Batch [1500/6000], Loss: 0.0979
Epoch [27/30], Batch [1600/6000], Loss: 0.1065
Epoch [27/30], Batch [1700/6000], Loss: 0.1085
Epoch [27/30], Batch [1800/6000], Loss: 0.0968
Epoch [27/30], Batch [1900/6000], Loss: 0.1371
Epoch [27/30], Batch [2000/6000], Loss: 0.9721
Epoch [27/30], Batch [2100/6000], Loss: 0.0998
Epoch [27/30], Batch [2200/6000], Loss: 0.3071
Epoch [27/30], Batch [2300/6000], Loss: 0.1073
Epoch [27/30], Batch [2400/6000], Loss: 0.1245
Epoch [27/30], Batch [2500/6000], Loss: 0.1040
Epoch [27/30], Batch [2600/6000], Loss: 0.1035
Epoch [27/30], Batch [2700/6000], Loss: 0.1420
Epoch [27/30], Batch [2800/6000], Loss: 0.1076
Epoch [27/30], Batch [2900/6000], Loss: 0.1311
Epoch [27/30], Batch [3000/6000], Loss: 0.3497
Epoch [27/30], Batch [3100/6000], Loss: 0.1149
Epoch [27/30], Batch [3200/6000], Loss: 0.1277
Epoch [27/30], Batch [3300/6000], Loss: 0.1323
Epoch [27/30], Batch [3400/6000], Loss: 0.4705
Epoch [27/30], Batch [3500/6000], Loss: 0.1050
Epoch [27/30], Batch [3600/6000], Loss: 0.1208
Epoch [27/30], Batch [3700/6000], Loss: 0.1207
Epoch [27/30], Batch [3800/6000], Loss: 0.1765
Epoch [27/30], Batch [3900/6000], Loss: 0.0981
Epoch [27/30], Batch [4000/6000], Loss: 0.1153
Epoch [27/30], Batch [4100/6000], Loss: 0.1156
Epoch [27/30], Batch [4200/6000], Loss: 0.0952
Epoch [27/30], Batch [4300/6000], Loss: 0.1901
Epoch [27/30], Batch [4400/6000], Loss: 0.1047
Epoch [27/30], Batch [4500/6000], Loss: 0.1479
Epoch [27/30], Batch [4600/6000], Loss: 0.0960
Epoch [27/30], Batch [4700/6000], Loss: 0.0989
Epoch [27/30], Batch [4800/6000], Loss: 0.1032
Epoch [27/30], Batch [4900/6000], Loss: 0.1058
Epoch [27/30], Batch [5000/6000], Loss: 0.6416
Epoch [27/30], Batch [5100/6000], Loss: 0.1146
Epoch [27/30], Batch [5200/6000], Loss: 0.1071
Epoch [27/30], Batch [5300/6000], Loss: 0.0981
Epoch [27/30], Batch [5400/6000], Loss: 0.1324
Epoch [27/30], Batch [5500/6000], Loss: 0.1358
Epoch [27/30], Batch [5600/6000], Loss: 0.1231
Epoch [27/30], Batch [5700/6000], Loss: 0.0935
Epoch [27/30], Batch [5800/6000], Loss: 0.1576
Epoch [27/30], Batch [5900/6000], Loss: 0.0966
Epoch [27/30], Loss: 0.1715
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.1092
Epoch [28/30], Batch [100/6000], Loss: 0.1026
Epoch [28/30], Batch [200/6000], Loss: 0.1050
Epoch [28/30], Batch [300/6000], Loss: 0.1094
Epoch [28/30], Batch [400/6000], Loss: 0.0839
Epoch [28/30], Batch [500/6000], Loss: 0.1131
Epoch [28/30], Batch [600/6000], Loss: 0.6748
Epoch [28/30], Batch [700/6000], Loss: 0.0950
Epoch [28/30], Batch [800/6000], Loss: 0.1231
Epoch [28/30], Batch [900/6000], Loss: 0.2152
Epoch [28/30], Batch [1000/6000], Loss: 0.1129
Epoch [28/30], Batch [1100/6000], Loss: 0.2348
Epoch [28/30], Batch [1200/6000], Loss: 0.1205
Epoch [28/30], Batch [1300/6000], Loss: 0.1303
Epoch [28/30], Batch [1400/6000], Loss: 0.0991
Epoch [28/30], Batch [1500/6000], Loss: 0.1131
Epoch [28/30], Batch [1600/6000], Loss: 0.0846
Epoch [28/30], Batch [1700/6000], Loss: 0.0993
Epoch [28/30], Batch [1800/6000], Loss: 0.1027
Epoch [28/30], Batch [1900/6000], Loss: 0.1025
Epoch [28/30], Batch [2000/6000], Loss: 0.1172
Epoch [28/30], Batch [2100/6000], Loss: 0.1025
Epoch [28/30], Batch [2200/6000], Loss: 0.1600
Epoch [28/30], Batch [2300/6000], Loss: 0.1042
Epoch [28/30], Batch [2400/6000], Loss: 0.1030
Epoch [28/30], Batch [2500/6000], Loss: 0.1385
Epoch [28/30], Batch [2600/6000], Loss: 0.1174
Epoch [28/30], Batch [2700/6000], Loss: 0.1075
Epoch [28/30], Batch [2800/6000], Loss: 0.1144
Epoch [28/30], Batch [2900/6000], Loss: 0.3486
Epoch [28/30], Batch [3000/6000], Loss: 0.2221
Epoch [28/30], Batch [3100/6000], Loss: 0.1181
Epoch [28/30], Batch [3200/6000], Loss: 0.1437
Epoch [28/30], Batch [3300/6000], Loss: 0.1456
Epoch [28/30], Batch [3400/6000], Loss: 0.1763
Epoch [28/30], Batch [3500/6000], Loss: 0.1698
Epoch [28/30], Batch [3600/6000], Loss: 0.1123
Epoch [28/30], Batch [3700/6000], Loss: 0.1466
Epoch [28/30], Batch [3800/6000], Loss: 0.1308
Epoch [28/30], Batch [3900/6000], Loss: 0.0983
Epoch [28/30], Batch [4000/6000], Loss: 0.1959
Epoch [28/30], Batch [4100/6000], Loss: 0.1219
Epoch [28/30], Batch [4200/6000], Loss: 0.1087
Epoch [28/30], Batch [4300/6000], Loss: 0.1234
Epoch [28/30], Batch [4400/6000], Loss: 0.0950
Epoch [28/30], Batch [4500/6000], Loss: 0.1029
Epoch [28/30], Batch [4600/6000], Loss: 0.5245
Epoch [28/30], Batch [4700/6000], Loss: 0.1273
Epoch [28/30], Batch [4800/6000], Loss: 0.1065
Epoch [28/30], Batch [4900/6000], Loss: 0.1313
Epoch [28/30], Batch [5000/6000], Loss: 0.6130
Epoch [28/30], Batch [5100/6000], Loss: 0.1072
Epoch [28/30], Batch [5200/6000], Loss: 0.0904
Epoch [28/30], Batch [5300/6000], Loss: 0.1296
Epoch [28/30], Batch [5400/6000], Loss: 0.1461
Epoch [28/30], Batch [5500/6000], Loss: 0.1175
Epoch [28/30], Batch [5600/6000], Loss: 0.0895
Epoch [28/30], Batch [5700/6000], Loss: 0.1170
Epoch [28/30], Batch [5800/6000], Loss: 0.1272
Epoch [28/30], Batch [5900/6000], Loss: 0.1082
Epoch [28/30], Loss: 0.1748
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.1700
Epoch [29/30], Batch [100/6000], Loss: 0.1562
Epoch [29/30], Batch [200/6000], Loss: 0.1431
Epoch [29/30], Batch [300/6000], Loss: 0.9054
Epoch [29/30], Batch [400/6000], Loss: 0.1252
Epoch [29/30], Batch [500/6000], Loss: 0.0944
Epoch [29/30], Batch [600/6000], Loss: 0.1234
Epoch [29/30], Batch [700/6000], Loss: 0.1084
Epoch [29/30], Batch [800/6000], Loss: 0.0923
Epoch [29/30], Batch [900/6000], Loss: 0.1192
Epoch [29/30], Batch [1000/6000], Loss: 0.1009
Epoch [29/30], Batch [1100/6000], Loss: 0.6367
Epoch [29/30], Batch [1200/6000], Loss: 0.1225
Epoch [29/30], Batch [1300/6000], Loss: 0.1195
Epoch [29/30], Batch [1400/6000], Loss: 0.0824
Epoch [29/30], Batch [1500/6000], Loss: 0.1191
Epoch [29/30], Batch [1600/6000], Loss: 2.5911
Epoch [29/30], Batch [1700/6000], Loss: 0.1249
Epoch [29/30], Batch [1800/6000], Loss: 0.1299
Epoch [29/30], Batch [1900/6000], Loss: 0.1360
Epoch [29/30], Batch [2000/6000], Loss: 0.1173
Epoch [29/30], Batch [2100/6000], Loss: 0.0748
Epoch [29/30], Batch [2200/6000], Loss: 0.0997
Epoch [29/30], Batch [2300/6000], Loss: 0.1192
Epoch [29/30], Batch [2400/6000], Loss: 0.1015
Epoch [29/30], Batch [2500/6000], Loss: 0.1108
Epoch [29/30], Batch [2600/6000], Loss: 0.1063
Epoch [29/30], Batch [2700/6000], Loss: 0.1192
Epoch [29/30], Batch [2800/6000], Loss: 0.1181
Epoch [29/30], Batch [2900/6000], Loss: 0.1466
Epoch [29/30], Batch [3000/6000], Loss: 0.1263
Epoch [29/30], Batch [3100/6000], Loss: 0.2743
Epoch [29/30], Batch [3200/6000], Loss: 0.0871
Epoch [29/30], Batch [3300/6000], Loss: 0.2063
Epoch [29/30], Batch [3400/6000], Loss: 0.1459
Epoch [29/30], Batch [3500/6000], Loss: 0.1155
Epoch [29/30], Batch [3600/6000], Loss: 0.1048
Epoch [29/30], Batch [3700/6000], Loss: 0.1055
Epoch [29/30], Batch [3800/6000], Loss: 0.2670
Epoch [29/30], Batch [3900/6000], Loss: 0.0971
Epoch [29/30], Batch [4000/6000], Loss: 0.1482
Epoch [29/30], Batch [4100/6000], Loss: 0.0948
Epoch [29/30], Batch [4200/6000], Loss: 0.3665
Epoch [29/30], Batch [4300/6000], Loss: 0.1068
Epoch [29/30], Batch [4400/6000], Loss: 0.1070
Epoch [29/30], Batch [4500/6000], Loss: 0.1160
Epoch [29/30], Batch [4600/6000], Loss: 0.1051
Epoch [29/30], Batch [4700/6000], Loss: 0.0956
Epoch [29/30], Batch [4800/6000], Loss: 0.1087
Epoch [29/30], Batch [4900/6000], Loss: 0.0933
Epoch [29/30], Batch [5000/6000], Loss: 0.1223
Epoch [29/30], Batch [5100/6000], Loss: 0.0987
Epoch [29/30], Batch [5200/6000], Loss: 0.1171
Epoch [29/30], Batch [5300/6000], Loss: 0.8738
Epoch [29/30], Batch [5400/6000], Loss: 0.1281
Epoch [29/30], Batch [5500/6000], Loss: 0.1566
Epoch [29/30], Batch [5600/6000], Loss: 0.1555
Epoch [29/30], Batch [5700/6000], Loss: 0.2404
Epoch [29/30], Batch [5800/6000], Loss: 0.1295
Epoch [29/30], Batch [5900/6000], Loss: 0.1036
Epoch [29/30], Loss: 0.1680
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.1894
Epoch [30/30], Batch [100/6000], Loss: 0.0937
Epoch [30/30], Batch [200/6000], Loss: 0.1174
Epoch [30/30], Batch [300/6000], Loss: 0.1032
Epoch [30/30], Batch [400/6000], Loss: 0.1222
Epoch [30/30], Batch [500/6000], Loss: 0.2850
Epoch [30/30], Batch [600/6000], Loss: 0.1083
Epoch [30/30], Batch [700/6000], Loss: 0.1223
Epoch [30/30], Batch [800/6000], Loss: 0.0976
Epoch [30/30], Batch [900/6000], Loss: 0.5158
Epoch [30/30], Batch [1000/6000], Loss: 0.1372
Epoch [30/30], Batch [1100/6000], Loss: 0.1158
Epoch [30/30], Batch [1200/6000], Loss: 0.0973
Epoch [30/30], Batch [1300/6000], Loss: 0.0916
Epoch [30/30], Batch [1400/6000], Loss: 0.1172
Epoch [30/30], Batch [1500/6000], Loss: 0.1136
Epoch [30/30], Batch [1600/6000], Loss: 0.2394
Epoch [30/30], Batch [1700/6000], Loss: 0.0779
Epoch [30/30], Batch [1800/6000], Loss: 0.0898
Epoch [30/30], Batch [1900/6000], Loss: 0.0979
Epoch [30/30], Batch [2000/6000], Loss: 0.1138
Epoch [30/30], Batch [2100/6000], Loss: 0.2737
Epoch [30/30], Batch [2200/6000], Loss: 0.1147
Epoch [30/30], Batch [2300/6000], Loss: 0.1382
Epoch [30/30], Batch [2400/6000], Loss: 0.5018
Epoch [30/30], Batch [2500/6000], Loss: 0.1239
Epoch [30/30], Batch [2600/6000], Loss: 0.0830
Epoch [30/30], Batch [2700/6000], Loss: 0.0982
Epoch [30/30], Batch [2800/6000], Loss: 0.3220
Epoch [30/30], Batch [2900/6000], Loss: 0.0965
Epoch [30/30], Batch [3000/6000], Loss: 0.1179
Epoch [30/30], Batch [3100/6000], Loss: 0.1756
Epoch [30/30], Batch [3200/6000], Loss: 0.0906
Epoch [30/30], Batch [3300/6000], Loss: 0.0811
Epoch [30/30], Batch [3400/6000], Loss: 0.1219
Epoch [30/30], Batch [3500/6000], Loss: 0.1005
Epoch [30/30], Batch [3600/6000], Loss: 0.1338
Epoch [30/30], Batch [3700/6000], Loss: 0.1430
Epoch [30/30], Batch [3800/6000], Loss: 0.1189
Epoch [30/30], Batch [3900/6000], Loss: 0.1155
Epoch [30/30], Batch [4000/6000], Loss: 0.0858
Epoch [30/30], Batch [4100/6000], Loss: 0.1046
Epoch [30/30], Batch [4200/6000], Loss: 0.1076
Epoch [30/30], Batch [4300/6000], Loss: 0.3573
Epoch [30/30], Batch [4400/6000], Loss: 0.0934
Epoch [30/30], Batch [4500/6000], Loss: 0.0987
Epoch [30/30], Batch [4600/6000], Loss: 0.1008
Epoch [30/30], Batch [4700/6000], Loss: 0.1147
Epoch [30/30], Batch [4800/6000], Loss: 0.1467
Epoch [30/30], Batch [4900/6000], Loss: 0.0946
Epoch [30/30], Batch [5000/6000], Loss: 0.0829
Epoch [30/30], Batch [5100/6000], Loss: 0.2183
Epoch [30/30], Batch [5200/6000], Loss: 0.1099
Epoch [30/30], Batch [5300/6000], Loss: 0.1017
Epoch [30/30], Batch [5400/6000], Loss: 0.1333
Epoch [30/30], Batch [5500/6000], Loss: 0.1566
Epoch [30/30], Batch [5600/6000], Loss: 0.1122
Epoch [30/30], Batch [5700/6000], Loss: 0.0965
Epoch [30/30], Batch [5800/6000], Loss: 0.1012
Epoch [30/30], Batch [5900/6000], Loss: 0.1002
Epoch [30/30], Loss: 0.1617
Visualization saved to figures/visualization_0.png
Test Loss: 0.1223, Accuracy: 97.99%
Visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 0.3614
  Image Loss: 0.0180
  Total Loss: 3.6323
  Image grad max: 1.0685864686965942
  Output probs: [[0.    0.997 0.    0.    0.002 0.    0.    0.    0.002 0.   ]]
Adversarial Training Loop 2/300:
  Label Loss: 0.2549
  Image Loss: 0.0182
  Total Loss: 2.5673
  Image grad max: 1.005288004875183
  Output probs: [[0.    0.968 0.    0.    0.011 0.    0.    0.    0.021 0.   ]]
Adversarial Training Loop 3/300:
  Label Loss: 0.1586
  Image Loss: 0.0184
  Total Loss: 1.6041
  Image grad max: 0.9077533483505249
  Output probs: [[0.    0.755 0.    0.    0.058 0.    0.    0.    0.184 0.002]]
Adversarial Training Loop 4/300:
  Label Loss: 0.0875
  Image Loss: 0.0187
  Total Loss: 0.8932
  Image grad max: 0.5105546116828918
  Output probs: [[0.    0.375 0.    0.    0.137 0.    0.001 0.    0.478 0.009]]
Adversarial Training Loop 5/300:
  Label Loss: 0.0793
  Image Loss: 0.0190
  Total Loss: 0.8123
  Image grad max: 0.8184055685997009
  Output probs: [[0.    0.248 0.    0.    0.243 0.    0.001 0.    0.49  0.017]]
Adversarial Training Loop 6/300:
  Label Loss: 0.0711
  Image Loss: 0.0194
  Total Loss: 0.7302
  Image grad max: 0.8718207478523254
  Output probs: [[0.    0.217 0.001 0.    0.434 0.    0.001 0.    0.325 0.022]]
Adversarial Training Loop 7/300:
  Label Loss: 0.0489
  Image Loss: 0.0197
  Total Loss: 0.5082
  Image grad max: 0.6444047689437866
  Output probs: [[0.    0.183 0.001 0.    0.651 0.    0.001 0.    0.145 0.019]]
Adversarial Training Loop 8/300:
  Label Loss: 0.0370
  Image Loss: 0.0200
  Total Loss: 0.3898
  Image grad max: 0.5791383385658264
  Output probs: [[0.    0.153 0.    0.    0.777 0.    0.    0.    0.056 0.013]]
Adversarial Training Loop 9/300:
  Label Loss: 0.0372
  Image Loss: 0.0203
  Total Loss: 0.3922
  Image grad max: 0.6276996731758118
  Output probs: [[0.    0.147 0.    0.    0.819 0.    0.    0.    0.024 0.008]]
Adversarial Training Loop 10/300:
  Label Loss: 0.0365
  Image Loss: 0.0205
  Total Loss: 0.3850
  Image grad max: 0.636978030204773
  Output probs: [[0.    0.169 0.    0.    0.811 0.    0.    0.    0.013 0.005]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0299
  Image Loss: 0.0207
  Total Loss: 0.3200
  Image grad max: 0.5910874009132385
  Output probs: [[0.    0.226 0.    0.    0.761 0.    0.    0.    0.009 0.004]]
Adversarial Training Loop 12/300:
  Label Loss: 0.0187
  Image Loss: 0.0208
  Total Loss: 0.2079
  Image grad max: 0.4872247278690338
  Output probs: [[0.    0.327 0.    0.    0.663 0.    0.    0.    0.006 0.003]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0071
  Image Loss: 0.0209
  Total Loss: 0.0919
  Image grad max: 0.3076710104942322
  Output probs: [[0.    0.467 0.    0.    0.525 0.    0.    0.001 0.004 0.002]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0009
  Image Loss: 0.0209
  Total Loss: 0.0301
  Image grad max: 0.05587967857718468
  Output probs: [[0.    0.605 0.    0.    0.389 0.    0.    0.001 0.003 0.001]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0030
  Image Loss: 0.0210
  Total Loss: 0.0505
  Image grad max: 0.19380171597003937
  Output probs: [[0.    0.699 0.    0.    0.297 0.    0.    0.001 0.002 0.001]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0093
  Image Loss: 0.0211
  Total Loss: 0.1137
  Image grad max: 0.3604348599910736
  Output probs: [[0.    0.739 0.    0.    0.257 0.    0.    0.001 0.002 0.001]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0136
  Image Loss: 0.0212
  Total Loss: 0.1576
  Image grad max: 0.4312572479248047
  Output probs: [[0.    0.736 0.    0.    0.261 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0132
  Image Loss: 0.0213
  Total Loss: 0.1531
  Image grad max: 0.42213788628578186
  Output probs: [[0.    0.696 0.    0.    0.301 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0088
  Image Loss: 0.0215
  Total Loss: 0.1096
  Image grad max: 0.3475579619407654
  Output probs: [[0.    0.626 0.    0.    0.371 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0037
  Image Loss: 0.0217
  Total Loss: 0.0584
  Image grad max: 0.22218750417232513
  Output probs: [[0.    0.538 0.    0.    0.459 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0006
  Image Loss: 0.0219
  Total Loss: 0.0279
  Image grad max: 0.06806592643260956
  Output probs: [[0.    0.452 0.    0.    0.545 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0007
  Image Loss: 0.0220
  Total Loss: 0.0291
  Image grad max: 0.07897306978702545
  Output probs: [[0.    0.386 0.    0.    0.611 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0029
  Image Loss: 0.0222
  Total Loss: 0.0509
  Image grad max: 0.18885555863380432
  Output probs: [[0.    0.348 0.    0.    0.649 0.    0.    0.    0.001 0.001]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0050
  Image Loss: 0.0223
  Total Loss: 0.0728
  Image grad max: 0.24972248077392578
  Output probs: [[0.    0.338 0.    0.    0.66  0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0058
  Image Loss: 0.0224
  Total Loss: 0.0801
  Image grad max: 0.26395609974861145
  Output probs: [[0.    0.352 0.    0.    0.645 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0048
  Image Loss: 0.0225
  Total Loss: 0.0702
  Image grad max: 0.23727919161319733
  Output probs: [[0.    0.39  0.    0.    0.607 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0027
  Image Loss: 0.0225
  Total Loss: 0.0497
  Image grad max: 0.17448602616786957
  Output probs: [[0.    0.444 0.    0.    0.553 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0009
  Image Loss: 0.0225
  Total Loss: 0.0315
  Image grad max: 0.08676029741764069
  Output probs: [[0.    0.503 0.001 0.    0.494 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0003
  Image Loss: 0.0225
  Total Loss: 0.0257
  Image grad max: 0.009101654402911663
  Output probs: [[0.    0.555 0.001 0.    0.442 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0010
  Image Loss: 0.0225
  Total Loss: 0.0322
  Image grad max: 0.08773953467607498
  Output probs: [[0.    0.59  0.001 0.    0.406 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0021
  Image Loss: 0.0225
  Total Loss: 0.0432
  Image grad max: 0.14188076555728912
  Output probs: [[0.    0.606 0.001 0.    0.391 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0027
  Image Loss: 0.0226
  Total Loss: 0.0497
  Image grad max: 0.1648377776145935
  Output probs: [[0.    0.602 0.001 0.    0.395 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0025
  Image Loss: 0.0226
  Total Loss: 0.0480
  Image grad max: 0.15744809806346893
  Output probs: [[0.    0.581 0.001 0.    0.416 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0017
  Image Loss: 0.0227
  Total Loss: 0.0400
  Image grad max: 0.1247393861413002
  Output probs: [[0.    0.547 0.001 0.    0.449 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0008
  Image Loss: 0.0227
  Total Loss: 0.0311
  Image grad max: 0.07382773607969284
  Output probs: [[0.    0.508 0.001 0.    0.488 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0004
  Image Loss: 0.0228
  Total Loss: 0.0264
  Image grad max: 0.014844481833279133
  Output probs: [[0.    0.472 0.001 0.    0.525 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0005
  Image Loss: 0.0228
  Total Loss: 0.0277
  Image grad max: 0.0397864431142807
  Output probs: [[0.    0.444 0.001 0.    0.553 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0009
  Image Loss: 0.0229
  Total Loss: 0.0322
  Image grad max: 0.08102511614561081
  Output probs: [[0.    0.429 0.001 0.    0.568 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0013
  Image Loss: 0.0229
  Total Loss: 0.0360
  Image grad max: 0.10276840627193451
  Output probs: [[0.    0.428 0.001 0.    0.569 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0013
  Image Loss: 0.0229
  Total Loss: 0.0364
  Image grad max: 0.10437104851007462
  Output probs: [[0.    0.439 0.001 0.    0.557 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0010
  Image Loss: 0.0229
  Total Loss: 0.0334
  Image grad max: 0.08710795640945435
  Output probs: [[0.    0.461 0.001 0.    0.536 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0006
  Image Loss: 0.0229
  Total Loss: 0.0291
  Image grad max: 0.05500977113842964
  Output probs: [[0.    0.488 0.001 0.    0.509 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0004
  Image Loss: 0.0229
  Total Loss: 0.0266
  Image grad max: 0.015584439039230347
  Output probs: [[0.    0.514 0.001 0.    0.483 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0004
  Image Loss: 0.0229
  Total Loss: 0.0269
  Image grad max: 0.02261442504823208
  Output probs: [[0.    0.534 0.001 0.    0.463 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0006
  Image Loss: 0.0229
  Total Loss: 0.0289
  Image grad max: 0.05239110067486763
  Output probs: [[0.    0.545 0.001 0.    0.451 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0008
  Image Loss: 0.0228
  Total Loss: 0.0308
  Image grad max: 0.0689498707652092
  Output probs: [[0.    0.546 0.001 0.    0.451 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0008
  Image Loss: 0.0228
  Total Loss: 0.0309
  Image grad max: 0.07038737088441849
  Output probs: [[0.    0.537 0.001 0.    0.459 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0006
  Image Loss: 0.0228
  Total Loss: 0.0293
  Image grad max: 0.05753571540117264
  Output probs: [[0.    0.521 0.001 0.    0.476 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0004
  Image Loss: 0.0229
  Total Loss: 0.0272
  Image grad max: 0.033613353967666626
  Output probs: [[0.    0.501 0.001 0.    0.495 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0261
  Image grad max: 0.008693918585777283
  Output probs: [[0.    0.483 0.001 0.    0.514 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0004
  Image Loss: 0.0229
  Total Loss: 0.0264
  Image grad max: 0.022542331367731094
  Output probs: [[0.    0.47  0.001 0.    0.527 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0005
  Image Loss: 0.0229
  Total Loss: 0.0275
  Image grad max: 0.042342737317085266
  Output probs: [[0.    0.464 0.001 0.    0.533 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0005
  Image Loss: 0.0229
  Total Loss: 0.0281
  Image grad max: 0.05120715871453285
  Output probs: [[0.    0.467 0.001 0.    0.531 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0005
  Image Loss: 0.0228
  Total Loss: 0.0277
  Image grad max: 0.047977011650800705
  Output probs: [[0.    0.476 0.001 0.    0.521 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0004
  Image Loss: 0.0228
  Total Loss: 0.0266
  Image grad max: 0.033828701823949814
  Output probs: [[0.    0.49  0.001 0.    0.507 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0003
  Image Loss: 0.0228
  Total Loss: 0.0256
  Image grad max: 0.012941758148372173
  Output probs: [[0.    0.504 0.001 0.    0.493 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0003
  Image Loss: 0.0228
  Total Loss: 0.0255
  Image grad max: 0.008858142420649529
  Output probs: [[0.    0.516 0.001 0.    0.481 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0003
  Image Loss: 0.0227
  Total Loss: 0.0259
  Image grad max: 0.026662731543183327
  Output probs: [[0.    0.522 0.    0.    0.475 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0004
  Image Loss: 0.0227
  Total Loss: 0.0263
  Image grad max: 0.036476850509643555
  Output probs: [[0.    0.522 0.    0.    0.475 0.    0.    0.001 0.001 0.001]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0004
  Image Loss: 0.0227
  Total Loss: 0.0262
  Image grad max: 0.036501795053482056
  Output probs: [[0.    0.516 0.    0.    0.481 0.    0.    0.001 0.001 0.   ]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0003
  Image Loss: 0.0227
  Total Loss: 0.0256
  Image grad max: 0.027278969064354897
  Output probs: [[0.    0.506 0.    0.    0.492 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0250
  Image grad max: 0.011682001873850822
  Output probs: [[0.    0.495 0.    0.    0.502 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0248
  Image grad max: 0.0061071040108799934
  Output probs: [[0.    0.487 0.    0.    0.511 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0250
  Image grad max: 0.019403332844376564
  Output probs: [[0.    0.482 0.    0.    0.516 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0003
  Image Loss: 0.0226
  Total Loss: 0.0252
  Image grad max: 0.02685336209833622
  Output probs: [[0.    0.482 0.    0.    0.516 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0251
  Image grad max: 0.026284977793693542
  Output probs: [[0.    0.487 0.    0.    0.511 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0247
  Image grad max: 0.018342390656471252
  Output probs: [[0.    0.495 0.    0.    0.503 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0244
  Image grad max: 0.006245242431759834
  Output probs: [[0.    0.504 0.    0.    0.495 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0243
  Image grad max: 0.007586667779833078
  Output probs: [[0.    0.51  0.    0.    0.488 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0244
  Image grad max: 0.017730215564370155
  Output probs: [[0.    0.512 0.    0.    0.486 0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0244
  Image grad max: 0.021853119134902954
  Output probs: [[0.    0.511 0.    0.    0.488 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0243
  Image grad max: 0.01914944127202034
  Output probs: [[0.    0.506 0.    0.    0.493 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0240
  Image grad max: 0.010940266773104668
  Output probs: [[0.    0.499 0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0001
  Image Loss: 0.0224
  Total Loss: 0.0239
  Image grad max: 0.0034979763440787792
  Output probs: [[0.    0.494 0.    0.    0.505 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0239
  Image grad max: 0.009316648356616497
  Output probs: [[0.    0.49  0.    0.    0.508 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0002
  Image Loss: 0.0223
  Total Loss: 0.0239
  Image grad max: 0.014933797530829906
  Output probs: [[0.    0.49  0.    0.    0.508 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0002
  Image Loss: 0.0223
  Total Loss: 0.0238
  Image grad max: 0.015105310827493668
  Output probs: [[0.    0.493 0.    0.    0.505 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0001
  Image Loss: 0.0223
  Total Loss: 0.0237
  Image grad max: 0.01011089701205492
  Output probs: [[0.    0.498 0.    0.    0.501 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0001
  Image Loss: 0.0223
  Total Loss: 0.0235
  Image grad max: 0.003203335218131542
  Output probs: [[0.    0.503 0.    0.    0.496 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0001
  Image Loss: 0.0222
  Total Loss: 0.0235
  Image grad max: 0.006269550416618586
  Output probs: [[0.    0.506 0.    0.    0.493 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0001
  Image Loss: 0.0222
  Total Loss: 0.0235
  Image grad max: 0.01184787042438984
  Output probs: [[0.    0.507 0.    0.    0.492 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0001
  Image Loss: 0.0222
  Total Loss: 0.0235
  Image grad max: 0.012968909926712513
  Output probs: [[0.    0.505 0.    0.    0.494 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0001
  Image Loss: 0.0221
  Total Loss: 0.0234
  Image grad max: 0.009664378128945827
  Output probs: [[0.    0.501 0.    0.    0.498 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0001
  Image Loss: 0.0221
  Total Loss: 0.0232
  Image grad max: 0.003361479612067342
  Output probs: [[0.    0.497 0.    0.    0.502 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0001
  Image Loss: 0.0221
  Total Loss: 0.0232
  Image grad max: 0.004476646892726421
  Output probs: [[0.    0.495 0.    0.    0.504 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0001
  Image Loss: 0.0221
  Total Loss: 0.0232
  Image grad max: 0.008388061076402664
  Output probs: [[0.    0.494 0.    0.    0.505 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0001
  Image Loss: 0.0221
  Total Loss: 0.0231
  Image grad max: 0.009277028031647205
  Output probs: [[0.    0.496 0.    0.    0.503 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0001
  Image Loss: 0.0220
  Total Loss: 0.0231
  Image grad max: 0.007057118229568005
  Output probs: [[0.    0.499 0.    0.    0.5   0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0001
  Image Loss: 0.0220
  Total Loss: 0.0230
  Image grad max: 0.002814983483403921
  Output probs: [[0.    0.502 0.    0.    0.497 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0001
  Image Loss: 0.0220
  Total Loss: 0.0229
  Image grad max: 0.004080400802195072
  Output probs: [[0.    0.504 0.    0.    0.495 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0001
  Image Loss: 0.0219
  Total Loss: 0.0229
  Image grad max: 0.007670728489756584
  Output probs: [[0.    0.504 0.    0.    0.495 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0001
  Image Loss: 0.0219
  Total Loss: 0.0229
  Image grad max: 0.008152601309120655
  Output probs: [[0.    0.503 0.    0.    0.497 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0001
  Image Loss: 0.0219
  Total Loss: 0.0228
  Image grad max: 0.005561218596994877
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0001
  Image Loss: 0.0219
  Total Loss: 0.0227
  Image grad max: 0.0023225180339068174
  Output probs: [[0.    0.498 0.    0.    0.501 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0001
  Image Loss: 0.0218
  Total Loss: 0.0227
  Image grad max: 0.00428681168705225
  Output probs: [[0.    0.497 0.    0.    0.503 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0001
  Image Loss: 0.0218
  Total Loss: 0.0227
  Image grad max: 0.006200128234922886
  Output probs: [[0.    0.497 0.    0.    0.502 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0001
  Image Loss: 0.0218
  Total Loss: 0.0226
  Image grad max: 0.005891456268727779
  Output probs: [[0.    0.498 0.    0.    0.501 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0001
  Image Loss: 0.0218
  Total Loss: 0.0226
  Image grad max: 0.003686711424961686
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0001
  Image Loss: 0.0217
  Total Loss: 0.0225
  Image grad max: 0.002238455694168806
  Output probs: [[0.    0.502 0.    0.    0.497 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0001
  Image Loss: 0.0217
  Total Loss: 0.0225
  Image grad max: 0.004393309820443392
  Output probs: [[0.    0.503 0.    0.    0.497 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0001
  Image Loss: 0.0217
  Total Loss: 0.0225
  Image grad max: 0.00547909876331687
  Output probs: [[0.    0.502 0.    0.    0.497 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0001
  Image Loss: 0.0217
  Total Loss: 0.0224
  Image grad max: 0.0043262518011033535
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0001
  Image Loss: 0.0216
  Total Loss: 0.0224
  Image grad max: 0.002278115600347519
  Output probs: [[0.    0.499 0.    0.    0.5   0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0001
  Image Loss: 0.0216
  Total Loss: 0.0223
  Image grad max: 0.002890815259888768
  Output probs: [[0.    0.498 0.    0.    0.501 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0001
  Image Loss: 0.0216
  Total Loss: 0.0223
  Image grad max: 0.0043711611069738865
  Output probs: [[0.    0.498 0.    0.    0.501 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0001
  Image Loss: 0.0216
  Total Loss: 0.0222
  Image grad max: 0.0043499949388206005
  Output probs: [[0.    0.499 0.    0.    0.5   0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0001
  Image Loss: 0.0215
  Total Loss: 0.0222
  Image grad max: 0.002950070658698678
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0001
  Image Loss: 0.0215
  Total Loss: 0.0222
  Image grad max: 0.0020556561648845673
  Output probs: [[0.    0.501 0.    0.    0.498 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0001
  Image Loss: 0.0215
  Total Loss: 0.0221
  Image grad max: 0.0031270987819880247
  Output probs: [[0.    0.502 0.    0.    0.498 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0001
  Image Loss: 0.0214
  Total Loss: 0.0221
  Image grad max: 0.003746090456843376
  Output probs: [[0.    0.501 0.    0.    0.498 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0001
  Image Loss: 0.0214
  Total Loss: 0.0221
  Image grad max: 0.0028010928072035313
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0001
  Image Loss: 0.0214
  Total Loss: 0.0220
  Image grad max: 0.0019294386729598045
  Output probs: [[0.    0.499 0.    0.    0.5   0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0001
  Image Loss: 0.0214
  Total Loss: 0.0220
  Image grad max: 0.0026979115791618824
  Output probs: [[0.    0.499 0.    0.    0.501 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0001
  Image Loss: 0.0213
  Total Loss: 0.0219
  Image grad max: 0.0034467310179024935
  Output probs: [[0.    0.499 0.    0.    0.501 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0001
  Image Loss: 0.0213
  Total Loss: 0.0219
  Image grad max: 0.0031016706489026546
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0001
  Image Loss: 0.0213
  Total Loss: 0.0219
  Image grad max: 0.0019064554944634438
  Output probs: [[0.    0.501 0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0001
  Image Loss: 0.0213
  Total Loss: 0.0218
  Image grad max: 0.0021734172478318214
  Output probs: [[0.    0.501 0.    0.    0.498 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0001
  Image Loss: 0.0212
  Total Loss: 0.0218
  Image grad max: 0.002692441688850522
  Output probs: [[0.    0.501 0.    0.    0.498 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0001
  Image Loss: 0.0212
  Total Loss: 0.0218
  Image grad max: 0.002560773165896535
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0001
  Image Loss: 0.0212
  Total Loss: 0.0217
  Image grad max: 0.002071203663945198
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0001
  Image Loss: 0.0212
  Total Loss: 0.0217
  Image grad max: 0.001905145589262247
  Output probs: [[0.    0.499 0.    0.    0.5   0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0217
  Image grad max: 0.002698405645787716
  Output probs: [[0.    0.499 0.    0.    0.5   0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0216
  Image grad max: 0.002725294791162014
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0001
  Image Loss: 0.0211
  Total Loss: 0.0216
  Image grad max: 0.002036208752542734
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0216
  Image grad max: 0.0019040738698095083
  Output probs: [[0.    0.501 0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0215
  Image grad max: 0.0022016919683665037
  Output probs: [[0.    0.501 0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0001
  Image Loss: 0.0210
  Total Loss: 0.0215
  Image grad max: 0.0022249515168368816
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0000
  Image Loss: 0.0210
  Total Loss: 0.0215
  Image grad max: 0.001982694724574685
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0000
  Image Loss: 0.0209
  Total Loss: 0.0214
  Image grad max: 0.0016558005008846521
  Output probs: [[0.    0.499 0.    0.    0.5   0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0000
  Image Loss: 0.0209
  Total Loss: 0.0214
  Image grad max: 0.0022540949285030365
  Output probs: [[0.    0.499 0.    0.    0.5   0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0000
  Image Loss: 0.0209
  Total Loss: 0.0214
  Image grad max: 0.0023333351127803326
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0000
  Image Loss: 0.0209
  Total Loss: 0.0213
  Image grad max: 0.0018829714972525835
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0000
  Image Loss: 0.0208
  Total Loss: 0.0213
  Image grad max: 0.001807651249691844
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0000
  Image Loss: 0.0208
  Total Loss: 0.0213
  Image grad max: 0.002014315687119961
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0000
  Image Loss: 0.0208
  Total Loss: 0.0212
  Image grad max: 0.002027715090662241
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0000
  Image Loss: 0.0207
  Total Loss: 0.0212
  Image grad max: 0.0018544532358646393
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0000
  Image Loss: 0.0207
  Total Loss: 0.0212
  Image grad max: 0.0016411484684795141
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0000
  Image Loss: 0.0207
  Total Loss: 0.0211
  Image grad max: 0.002022553002461791
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0000
  Image Loss: 0.0207
  Total Loss: 0.0211
  Image grad max: 0.0020111468620598316
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0000
  Image Loss: 0.0206
  Total Loss: 0.0211
  Image grad max: 0.0016493771690875292
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0000
  Image Loss: 0.0206
  Total Loss: 0.0210
  Image grad max: 0.0017889776499941945
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0000
  Image Loss: 0.0206
  Total Loss: 0.0210
  Image grad max: 0.0019016204169020057
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0000
  Image Loss: 0.0206
  Total Loss: 0.0210
  Image grad max: 0.0018685783725231886
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0000
  Image Loss: 0.0205
  Total Loss: 0.0209
  Image grad max: 0.00172074930742383
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0000
  Image Loss: 0.0205
  Total Loss: 0.0209
  Image grad max: 0.00170517573133111
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0000
  Image Loss: 0.0205
  Total Loss: 0.0209
  Image grad max: 0.001893077977001667
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0000
  Image Loss: 0.0204
  Total Loss: 0.0208
  Image grad max: 0.0017777872271835804
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0000
  Image Loss: 0.0204
  Total Loss: 0.0208
  Image grad max: 0.0016521075740456581
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0000
  Image Loss: 0.0204
  Total Loss: 0.0208
  Image grad max: 0.0017764021176844835
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0000
  Image Loss: 0.0204
  Total Loss: 0.0207
  Image grad max: 0.0018108889926224947
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0000
  Image Loss: 0.0203
  Total Loss: 0.0207
  Image grad max: 0.001738818595185876
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0000
  Image Loss: 0.0203
  Total Loss: 0.0207
  Image grad max: 0.001615635002963245
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0000
  Image Loss: 0.0203
  Total Loss: 0.0207
  Image grad max: 0.0017326751258224249
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0000
  Image Loss: 0.0203
  Total Loss: 0.0206
  Image grad max: 0.0017434540204703808
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0000
  Image Loss: 0.0202
  Total Loss: 0.0206
  Image grad max: 0.0015903448220342398
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0000
  Image Loss: 0.0202
  Total Loss: 0.0206
  Image grad max: 0.0016866439254954457
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0000
  Image Loss: 0.0202
  Total Loss: 0.0205
  Image grad max: 0.0017385247629135847
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0000
  Image Loss: 0.0201
  Total Loss: 0.0205
  Image grad max: 0.0017111669294536114
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0000
  Image Loss: 0.0201
  Total Loss: 0.0205
  Image grad max: 0.0016289397608488798
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0000
  Image Loss: 0.0201
  Total Loss: 0.0204
  Image grad max: 0.0016267334576696157
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0000
  Image Loss: 0.0201
  Total Loss: 0.0204
  Image grad max: 0.001679288106970489
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0000
  Image Loss: 0.0200
  Total Loss: 0.0204
  Image grad max: 0.0015719052171334624
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0000
  Image Loss: 0.0200
  Total Loss: 0.0203
  Image grad max: 0.0016500174533575773
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0000
  Image Loss: 0.0200
  Total Loss: 0.0203
  Image grad max: 0.0016946494579315186
  Output probs: [[0.    0.5   0.    0.    0.499 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0000
  Image Loss: 0.0199
  Total Loss: 0.0203
  Image grad max: 0.0016760858707129955
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0000
  Image Loss: 0.0199
  Total Loss: 0.0203
  Image grad max: 0.0016111400909721851
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0000
  Image Loss: 0.0199
  Total Loss: 0.0202
  Image grad max: 0.001594344386830926
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0000
  Image Loss: 0.0199
  Total Loss: 0.0202
  Image grad max: 0.0016238511307165027
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0000
  Image Loss: 0.0198
  Total Loss: 0.0202
  Image grad max: 0.0015678128693252802
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0000
  Image Loss: 0.0198
  Total Loss: 0.0201
  Image grad max: 0.0016213395865634084
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0000
  Image Loss: 0.0198
  Total Loss: 0.0201
  Image grad max: 0.0016524593811482191
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0000
  Image Loss: 0.0198
  Total Loss: 0.0201
  Image grad max: 0.0016382904723286629
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0000
  Image Loss: 0.0197
  Total Loss: 0.0200
  Image grad max: 0.0015891353832557797
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0000
  Image Loss: 0.0197
  Total Loss: 0.0200
  Image grad max: 0.0015503830509260297
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0000
  Image Loss: 0.0197
  Total Loss: 0.0200
  Image grad max: 0.0015645622042939067
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0000
  Image Loss: 0.0196
  Total Loss: 0.0200
  Image grad max: 0.0015659817727282643
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0000
  Image Loss: 0.0196
  Total Loss: 0.0199
  Image grad max: 0.0015983437187969685
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0000
  Image Loss: 0.0196
  Total Loss: 0.0199
  Image grad max: 0.001614417415112257
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0000
  Image Loss: 0.0196
  Total Loss: 0.0199
  Image grad max: 0.0015999169554561377
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0000
  Image Loss: 0.0195
  Total Loss: 0.0198
  Image grad max: 0.0015684443060308695
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0000
  Image Loss: 0.0195
  Total Loss: 0.0198
  Image grad max: 0.001542604761198163
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0000
  Image Loss: 0.0195
  Total Loss: 0.0198
  Image grad max: 0.0015437703114002943
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0000
  Image Loss: 0.0194
  Total Loss: 0.0197
  Image grad max: 0.0015703181270509958
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0000
  Image Loss: 0.0194
  Total Loss: 0.0197
  Image grad max: 0.0015908923232927918
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0000
  Image Loss: 0.0194
  Total Loss: 0.0197
  Image grad max: 0.0015910724177956581
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0000
  Image Loss: 0.0194
  Total Loss: 0.0197
  Image grad max: 0.0015680647920817137
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0000
  Image Loss: 0.0193
  Total Loss: 0.0196
  Image grad max: 0.0015415068482980132
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0000
  Image Loss: 0.0193
  Total Loss: 0.0196
  Image grad max: 0.0015315255150198936
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0000
  Image Loss: 0.0193
  Total Loss: 0.0196
  Image grad max: 0.0015417583053931594
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0000
  Image Loss: 0.0193
  Total Loss: 0.0195
  Image grad max: 0.001562788151204586
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0000
  Image Loss: 0.0192
  Total Loss: 0.0195
  Image grad max: 0.0015741046518087387
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0000
  Image Loss: 0.0192
  Total Loss: 0.0195
  Image grad max: 0.0015655846800655127
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0000
  Image Loss: 0.0192
  Total Loss: 0.0194
  Image grad max: 0.0015461622970178723
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0000
  Image Loss: 0.0191
  Total Loss: 0.0194
  Image grad max: 0.00153118628077209
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0000
  Image Loss: 0.0191
  Total Loss: 0.0194
  Image grad max: 0.0015288704307749867
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0000
  Image Loss: 0.0191
  Total Loss: 0.0194
  Image grad max: 0.0015396769158542156
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0000
  Image Loss: 0.0191
  Total Loss: 0.0193
  Image grad max: 0.0015500974841415882
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0000
  Image Loss: 0.0190
  Total Loss: 0.0193
  Image grad max: 0.0015513254329562187
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0000
  Image Loss: 0.0190
  Total Loss: 0.0193
  Image grad max: 0.0015474660322070122
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0000
  Image Loss: 0.0190
  Total Loss: 0.0192
  Image grad max: 0.001534618204459548
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0000
  Image Loss: 0.0189
  Total Loss: 0.0192
  Image grad max: 0.0015236795879900455
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0000
  Image Loss: 0.0189
  Total Loss: 0.0192
  Image grad max: 0.0015475534601137042
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0000
  Image Loss: 0.0189
  Total Loss: 0.0191
  Image grad max: 0.0015568053349852562
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0000
  Image Loss: 0.0189
  Total Loss: 0.0191
  Image grad max: 0.0015473030507564545
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0000
  Image Loss: 0.0188
  Total Loss: 0.0191
  Image grad max: 0.0015252492157742381
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0000
  Image Loss: 0.0188
  Total Loss: 0.0191
  Image grad max: 0.0015102715697139502
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0000
  Image Loss: 0.0188
  Total Loss: 0.0190
  Image grad max: 0.0015081876190379262
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0000
  Image Loss: 0.0188
  Total Loss: 0.0190
  Image grad max: 0.0015223771333694458
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0000
  Image Loss: 0.0187
  Total Loss: 0.0190
  Image grad max: 0.001538518350571394
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0000
  Image Loss: 0.0187
  Total Loss: 0.0189
  Image grad max: 0.0015399138210341334
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0000
  Image Loss: 0.0187
  Total Loss: 0.0189
  Image grad max: 0.0015239259228110313
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0000
  Image Loss: 0.0186
  Total Loss: 0.0189
  Image grad max: 0.0015064700273796916
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0000
  Image Loss: 0.0186
  Total Loss: 0.0189
  Image grad max: 0.0015052136732265353
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0000
  Image Loss: 0.0186
  Total Loss: 0.0188
  Image grad max: 0.001507752574980259
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0000
  Image Loss: 0.0186
  Total Loss: 0.0188
  Image grad max: 0.0015213240403681993
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0000
  Image Loss: 0.0185
  Total Loss: 0.0188
  Image grad max: 0.001525003113783896
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0000
  Image Loss: 0.0185
  Total Loss: 0.0187
  Image grad max: 0.001516633783467114
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0000
  Image Loss: 0.0185
  Total Loss: 0.0187
  Image grad max: 0.0015027953777462244
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0000
  Image Loss: 0.0185
  Total Loss: 0.0187
  Image grad max: 0.0014956396771594882
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0000
  Image Loss: 0.0184
  Total Loss: 0.0187
  Image grad max: 0.0014975587837398052
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0000
  Image Loss: 0.0184
  Total Loss: 0.0186
  Image grad max: 0.0015068789944052696
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0000
  Image Loss: 0.0184
  Total Loss: 0.0186
  Image grad max: 0.0015115179121494293
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0000
  Image Loss: 0.0183
  Total Loss: 0.0186
  Image grad max: 0.0015065293991938233
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0000
  Image Loss: 0.0183
  Total Loss: 0.0185
  Image grad max: 0.0014957112725824118
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0000
  Image Loss: 0.0183
  Total Loss: 0.0185
  Image grad max: 0.0014885475393384695
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0000
  Image Loss: 0.0183
  Total Loss: 0.0185
  Image grad max: 0.001489338232204318
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0000
  Image Loss: 0.0182
  Total Loss: 0.0185
  Image grad max: 0.001495033735409379
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0000
  Image Loss: 0.0182
  Total Loss: 0.0184
  Image grad max: 0.0014995814999565482
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0000
  Image Loss: 0.0182
  Total Loss: 0.0184
  Image grad max: 0.0014990632189437747
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0000
  Image Loss: 0.0181
  Total Loss: 0.0184
  Image grad max: 0.001491110771894455
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0000
  Image Loss: 0.0181
  Total Loss: 0.0183
  Image grad max: 0.0014826161786913872
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0000
  Image Loss: 0.0181
  Total Loss: 0.0183
  Image grad max: 0.0014820219948887825
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0000
  Image Loss: 0.0181
  Total Loss: 0.0183
  Image grad max: 0.0014839168870821595
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0000
  Image Loss: 0.0180
  Total Loss: 0.0183
  Image grad max: 0.001488261390477419
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0000
  Image Loss: 0.0180
  Total Loss: 0.0182
  Image grad max: 0.0014875680208206177
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0000
  Image Loss: 0.0180
  Total Loss: 0.0182
  Image grad max: 0.0014815920731052756
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0000
  Image Loss: 0.0180
  Total Loss: 0.0182
  Image grad max: 0.0014763810904696584
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0000
  Image Loss: 0.0179
  Total Loss: 0.0181
  Image grad max: 0.00147567770909518
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0000
  Image Loss: 0.0179
  Total Loss: 0.0181
  Image grad max: 0.0014776326715946198
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0000
  Image Loss: 0.0179
  Total Loss: 0.0181
  Image grad max: 0.0014763071667402983
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0000
  Image Loss: 0.0178
  Total Loss: 0.0181
  Image grad max: 0.0014785192906856537
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0000
  Image Loss: 0.0178
  Total Loss: 0.0180
  Image grad max: 0.0014772833092138171
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0000
  Image Loss: 0.0178
  Total Loss: 0.0180
  Image grad max: 0.0014708953676745296
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0000
  Image Loss: 0.0178
  Total Loss: 0.0180
  Image grad max: 0.001472430769354105
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0000
  Image Loss: 0.0177
  Total Loss: 0.0179
  Image grad max: 0.0014688072260469198
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0000
  Image Loss: 0.0177
  Total Loss: 0.0179
  Image grad max: 0.0014736223965883255
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0000
  Image Loss: 0.0177
  Total Loss: 0.0179
  Image grad max: 0.001476295292377472
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0000
  Image Loss: 0.0177
  Total Loss: 0.0179
  Image grad max: 0.001473265583626926
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0000
  Image Loss: 0.0176
  Total Loss: 0.0178
  Image grad max: 0.0014689877862110734
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0000
  Image Loss: 0.0176
  Total Loss: 0.0178
  Image grad max: 0.001470305840484798
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0000
  Image Loss: 0.0176
  Total Loss: 0.0178
  Image grad max: 0.0014726868830621243
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0000
  Image Loss: 0.0175
  Total Loss: 0.0177
  Image grad max: 0.0014728948008269072
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0000
  Image Loss: 0.0175
  Total Loss: 0.0177
  Image grad max: 0.0014731022529304028
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0000
  Image Loss: 0.0175
  Total Loss: 0.0177
  Image grad max: 0.0014702570624649525
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0000
  Image Loss: 0.0175
  Total Loss: 0.0177
  Image grad max: 0.0014682728797197342
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0000
  Image Loss: 0.0174
  Total Loss: 0.0176
  Image grad max: 0.001469791168347001
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0000
  Image Loss: 0.0174
  Total Loss: 0.0176
  Image grad max: 0.001471737865358591
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0000
  Image Loss: 0.0174
  Total Loss: 0.0176
  Image grad max: 0.0014719434548169374
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0000
  Image Loss: 0.0174
  Total Loss: 0.0176
  Image grad max: 0.001469493843615055
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0000
  Image Loss: 0.0173
  Total Loss: 0.0175
  Image grad max: 0.0014685995411127806
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0000
  Image Loss: 0.0173
  Total Loss: 0.0175
  Image grad max: 0.0014749111142009497
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0000
  Image Loss: 0.0173
  Total Loss: 0.0175
  Image grad max: 0.0014801265206187963
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0000
  Image Loss: 0.0173
  Total Loss: 0.0174
  Image grad max: 0.001473285723477602
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0000
  Image Loss: 0.0172
  Total Loss: 0.0174
  Image grad max: 0.001465704757720232
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0000
  Image Loss: 0.0172
  Total Loss: 0.0174
  Image grad max: 0.0014693255070596933
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0000
  Image Loss: 0.0172
  Total Loss: 0.0174
  Image grad max: 0.0014686801005154848
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0173
  Image grad max: 0.0014798593474552035
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0173
  Image grad max: 0.001477150130085647
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0173
  Image grad max: 0.001464502653107047
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0173
  Image grad max: 0.0014671706594526768
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0172
  Image grad max: 0.0014624018222093582
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0172
  Image grad max: 0.0014695613645017147
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0172
  Image grad max: 0.001462534535676241
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0171
  Image grad max: 0.0014612539671361446
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0171
  Image grad max: 0.0014690518146380782
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0171
  Image grad max: 0.0014729254180565476
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0171
  Image grad max: 0.0014602195005863905
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0170
  Image grad max: 0.0014782677171751857
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0170
  Image grad max: 0.0014650396769866347
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0170
  Image grad max: 0.0014689869713038206
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0170
  Image grad max: 0.0014735376462340355
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0169
  Image grad max: 0.0014580183196812868
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.0014861382078379393
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.0014727860689163208
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0168
  Image grad max: 0.0014693249249830842
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.001475876197218895
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.0014634899562224746
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.0014733315911144018
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0167
  Image grad max: 0.001475548604503274
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.0014604206662625074
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.0014694202691316605
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.0014641627203673124
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0166
  Image grad max: 0.0014581461437046528
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0014711969997733831
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0014549632323905826
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0014629507204517722
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0165
  Image grad max: 0.001462623244151473
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.001452525146305561
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.0014682312030345201
  Output probs: [[0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0. ]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.0014595554675906897
Visualization saved to adversarial_figures/adversarial_training.png
Visualization saved to adversarial_figures/adversarial_testing.png
