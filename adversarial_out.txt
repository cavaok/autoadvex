Epoch [1/30], Batch [0/6000], Loss: 2.5351
Epoch [1/30], Batch [100/6000], Loss: 2.3760
Epoch [1/30], Batch [200/6000], Loss: 1.9605
Epoch [1/30], Batch [300/6000], Loss: 1.9202
Epoch [1/30], Batch [400/6000], Loss: 1.7713
Epoch [1/30], Batch [500/6000], Loss: 1.7668
Epoch [1/30], Batch [600/6000], Loss: 1.6472
Epoch [1/30], Batch [700/6000], Loss: 1.7236
Epoch [1/30], Batch [800/6000], Loss: 1.7000
Epoch [1/30], Batch [900/6000], Loss: 1.6536
Epoch [1/30], Batch [1000/6000], Loss: 1.7980
Epoch [1/30], Batch [1100/6000], Loss: 1.7991
Epoch [1/30], Batch [1200/6000], Loss: 1.6636
Epoch [1/30], Batch [1300/6000], Loss: 1.7652
Epoch [1/30], Batch [1400/6000], Loss: 1.6763
Epoch [1/30], Batch [1500/6000], Loss: 1.8334
Epoch [1/30], Batch [1600/6000], Loss: 1.7223
Epoch [1/30], Batch [1700/6000], Loss: 1.7279
Epoch [1/30], Batch [1800/6000], Loss: 1.6882
Epoch [1/30], Batch [1900/6000], Loss: 1.5968
Epoch [1/30], Batch [2000/6000], Loss: 1.5660
Epoch [1/30], Batch [2100/6000], Loss: 1.6936
Epoch [1/30], Batch [2200/6000], Loss: 1.5793
Epoch [1/30], Batch [2300/6000], Loss: 1.5534
Epoch [1/30], Batch [2400/6000], Loss: 1.5799
Epoch [1/30], Batch [2500/6000], Loss: 1.6093
Epoch [1/30], Batch [2600/6000], Loss: 1.7615
Epoch [1/30], Batch [2700/6000], Loss: 1.6017
Epoch [1/30], Batch [2800/6000], Loss: 1.5631
Epoch [1/30], Batch [2900/6000], Loss: 1.6800
Epoch [1/30], Batch [3000/6000], Loss: 1.6012
Epoch [1/30], Batch [3100/6000], Loss: 1.5450
Epoch [1/30], Batch [3200/6000], Loss: 1.7680
Epoch [1/30], Batch [3300/6000], Loss: 1.5458
Epoch [1/30], Batch [3400/6000], Loss: 1.5856
Epoch [1/30], Batch [3500/6000], Loss: 1.6870
Epoch [1/30], Batch [3600/6000], Loss: 1.5409
Epoch [1/30], Batch [3700/6000], Loss: 1.5672
Epoch [1/30], Batch [3800/6000], Loss: 1.6657
Epoch [1/30], Batch [3900/6000], Loss: 1.6559
Epoch [1/30], Batch [4000/6000], Loss: 1.6476
Epoch [1/30], Batch [4100/6000], Loss: 1.5362
Epoch [1/30], Batch [4200/6000], Loss: 1.5167
Epoch [1/30], Batch [4300/6000], Loss: 1.5716
Epoch [1/30], Batch [4400/6000], Loss: 1.6375
Epoch [1/30], Batch [4500/6000], Loss: 1.6638
Epoch [1/30], Batch [4600/6000], Loss: 1.5419
Epoch [1/30], Batch [4700/6000], Loss: 1.5266
Epoch [1/30], Batch [4800/6000], Loss: 1.5667
Epoch [1/30], Batch [4900/6000], Loss: 1.5955
Epoch [1/30], Batch [5000/6000], Loss: 1.7826
Epoch [1/30], Batch [5100/6000], Loss: 1.5889
Epoch [1/30], Batch [5200/6000], Loss: 1.5326
Epoch [1/30], Batch [5300/6000], Loss: 1.5904
Epoch [1/30], Batch [5400/6000], Loss: 1.5686
Epoch [1/30], Batch [5500/6000], Loss: 1.7374
Epoch [1/30], Batch [5600/6000], Loss: 1.5572
Epoch [1/30], Batch [5700/6000], Loss: 1.6151
Epoch [1/30], Batch [5800/6000], Loss: 1.6565
Epoch [1/30], Batch [5900/6000], Loss: 1.5414
Epoch [1/30], Loss: 1.6753
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 1.5530
Epoch [2/30], Batch [100/6000], Loss: 1.5215
Epoch [2/30], Batch [200/6000], Loss: 1.5162
Epoch [2/30], Batch [300/6000], Loss: 1.6207
Epoch [2/30], Batch [400/6000], Loss: 1.6213
Epoch [2/30], Batch [500/6000], Loss: 1.5153
Epoch [2/30], Batch [600/6000], Loss: 1.5892
Epoch [2/30], Batch [700/6000], Loss: 1.6225
Epoch [2/30], Batch [800/6000], Loss: 1.5306
Epoch [2/30], Batch [900/6000], Loss: 1.5346
Epoch [2/30], Batch [1000/6000], Loss: 1.5643
Epoch [2/30], Batch [1100/6000], Loss: 1.6183
Epoch [2/30], Batch [1200/6000], Loss: 1.7144
Epoch [2/30], Batch [1300/6000], Loss: 1.5684
Epoch [2/30], Batch [1400/6000], Loss: 1.5590
Epoch [2/30], Batch [1500/6000], Loss: 1.5261
Epoch [2/30], Batch [1600/6000], Loss: 1.5390
Epoch [2/30], Batch [1700/6000], Loss: 1.5349
Epoch [2/30], Batch [1800/6000], Loss: 1.6319
Epoch [2/30], Batch [1900/6000], Loss: 1.6298
Epoch [2/30], Batch [2000/6000], Loss: 1.6096
Epoch [2/30], Batch [2100/6000], Loss: 1.5306
Epoch [2/30], Batch [2200/6000], Loss: 1.6083
Epoch [2/30], Batch [2300/6000], Loss: 1.5340
Epoch [2/30], Batch [2400/6000], Loss: 1.6280
Epoch [2/30], Batch [2500/6000], Loss: 1.5315
Epoch [2/30], Batch [2600/6000], Loss: 1.6110
Epoch [2/30], Batch [2700/6000], Loss: 1.5498
Epoch [2/30], Batch [2800/6000], Loss: 1.6279
Epoch [2/30], Batch [2900/6000], Loss: 1.5206
Epoch [2/30], Batch [3000/6000], Loss: 1.6618
Epoch [2/30], Batch [3100/6000], Loss: 1.5354
Epoch [2/30], Batch [3200/6000], Loss: 1.5142
Epoch [2/30], Batch [3300/6000], Loss: 1.5393
Epoch [2/30], Batch [3400/6000], Loss: 1.5287
Epoch [2/30], Batch [3500/6000], Loss: 1.5251
Epoch [2/30], Batch [3600/6000], Loss: 1.6058
Epoch [2/30], Batch [3700/6000], Loss: 1.5256
Epoch [2/30], Batch [3800/6000], Loss: 1.6072
Epoch [2/30], Batch [3900/6000], Loss: 1.5547
Epoch [2/30], Batch [4000/6000], Loss: 1.5111
Epoch [2/30], Batch [4100/6000], Loss: 1.6960
Epoch [2/30], Batch [4200/6000], Loss: 1.6943
Epoch [2/30], Batch [4300/6000], Loss: 1.5153
Epoch [2/30], Batch [4400/6000], Loss: 1.5071
Epoch [2/30], Batch [4500/6000], Loss: 1.5718
Epoch [2/30], Batch [4600/6000], Loss: 1.5133
Epoch [2/30], Batch [4700/6000], Loss: 1.5349
Epoch [2/30], Batch [4800/6000], Loss: 1.5242
Epoch [2/30], Batch [4900/6000], Loss: 1.4993
Epoch [2/30], Batch [5000/6000], Loss: 1.5015
Epoch [2/30], Batch [5100/6000], Loss: 1.5795
Epoch [2/30], Batch [5200/6000], Loss: 1.6057
Epoch [2/30], Batch [5300/6000], Loss: 1.5134
Epoch [2/30], Batch [5400/6000], Loss: 1.4989
Epoch [2/30], Batch [5500/6000], Loss: 1.4936
Epoch [2/30], Batch [5600/6000], Loss: 1.5034
Epoch [2/30], Batch [5700/6000], Loss: 1.5037
Epoch [2/30], Batch [5800/6000], Loss: 1.5034
Epoch [2/30], Batch [5900/6000], Loss: 1.6902
Epoch [2/30], Loss: 1.5631
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 1.5165
Epoch [3/30], Batch [100/6000], Loss: 1.5380
Epoch [3/30], Batch [200/6000], Loss: 1.5138
Epoch [3/30], Batch [300/6000], Loss: 1.4945
Epoch [3/30], Batch [400/6000], Loss: 1.6155
Epoch [3/30], Batch [500/6000], Loss: 1.6005
Epoch [3/30], Batch [600/6000], Loss: 1.6454
Epoch [3/30], Batch [700/6000], Loss: 1.4939
Epoch [3/30], Batch [800/6000], Loss: 1.4984
Epoch [3/30], Batch [900/6000], Loss: 1.4993
Epoch [3/30], Batch [1000/6000], Loss: 1.7026
Epoch [3/30], Batch [1100/6000], Loss: 1.7122
Epoch [3/30], Batch [1200/6000], Loss: 1.6083
Epoch [3/30], Batch [1300/6000], Loss: 1.5302
Epoch [3/30], Batch [1400/6000], Loss: 1.5139
Epoch [3/30], Batch [1500/6000], Loss: 1.5328
Epoch [3/30], Batch [1600/6000], Loss: 1.5099
Epoch [3/30], Batch [1700/6000], Loss: 1.5139
Epoch [3/30], Batch [1800/6000], Loss: 1.5120
Epoch [3/30], Batch [1900/6000], Loss: 1.5226
Epoch [3/30], Batch [2000/6000], Loss: 1.5184
Epoch [3/30], Batch [2100/6000], Loss: 1.5603
Epoch [3/30], Batch [2200/6000], Loss: 1.5141
Epoch [3/30], Batch [2300/6000], Loss: 1.5161
Epoch [3/30], Batch [2400/6000], Loss: 1.5102
Epoch [3/30], Batch [2500/6000], Loss: 1.5508
Epoch [3/30], Batch [2600/6000], Loss: 1.5021
Epoch [3/30], Batch [2700/6000], Loss: 1.6132
Epoch [3/30], Batch [2800/6000], Loss: 1.5143
Epoch [3/30], Batch [2900/6000], Loss: 1.6450
Epoch [3/30], Batch [3000/6000], Loss: 1.6086
Epoch [3/30], Batch [3100/6000], Loss: 1.5145
Epoch [3/30], Batch [3200/6000], Loss: 1.7189
Epoch [3/30], Batch [3300/6000], Loss: 1.4950
Epoch [3/30], Batch [3400/6000], Loss: 1.5380
Epoch [3/30], Batch [3500/6000], Loss: 1.5094
Epoch [3/30], Batch [3600/6000], Loss: 1.5097
Epoch [3/30], Batch [3700/6000], Loss: 1.6200
Epoch [3/30], Batch [3800/6000], Loss: 1.5049
Epoch [3/30], Batch [3900/6000], Loss: 1.5003
Epoch [3/30], Batch [4000/6000], Loss: 1.5633
Epoch [3/30], Batch [4100/6000], Loss: 1.5496
Epoch [3/30], Batch [4200/6000], Loss: 1.4954
Epoch [3/30], Batch [4300/6000], Loss: 1.5278
Epoch [3/30], Batch [4400/6000], Loss: 1.4924
Epoch [3/30], Batch [4500/6000], Loss: 1.5099
Epoch [3/30], Batch [4600/6000], Loss: 1.5594
Epoch [3/30], Batch [4700/6000], Loss: 1.4909
Epoch [3/30], Batch [4800/6000], Loss: 1.5303
Epoch [3/30], Batch [4900/6000], Loss: 1.4879
Epoch [3/30], Batch [5000/6000], Loss: 1.5236
Epoch [3/30], Batch [5100/6000], Loss: 1.5155
Epoch [3/30], Batch [5200/6000], Loss: 1.5520
Epoch [3/30], Batch [5300/6000], Loss: 1.5211
Epoch [3/30], Batch [5400/6000], Loss: 1.5998
Epoch [3/30], Batch [5500/6000], Loss: 1.5765
Epoch [3/30], Batch [5600/6000], Loss: 1.5049
Epoch [3/30], Batch [5700/6000], Loss: 1.5411
Epoch [3/30], Batch [5800/6000], Loss: 1.4968
Epoch [3/30], Batch [5900/6000], Loss: 1.4908
Epoch [3/30], Loss: 1.5394
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 1.5643
Epoch [4/30], Batch [100/6000], Loss: 1.5093
Epoch [4/30], Batch [200/6000], Loss: 1.5553
Epoch [4/30], Batch [300/6000], Loss: 1.5103
Epoch [4/30], Batch [400/6000], Loss: 1.4970
Epoch [4/30], Batch [500/6000], Loss: 1.6886
Epoch [4/30], Batch [600/6000], Loss: 1.4998
Epoch [4/30], Batch [700/6000], Loss: 1.4936
Epoch [4/30], Batch [800/6000], Loss: 1.5274
Epoch [4/30], Batch [900/6000], Loss: 1.5036
Epoch [4/30], Batch [1000/6000], Loss: 1.5087
Epoch [4/30], Batch [1100/6000], Loss: 1.5026
Epoch [4/30], Batch [1200/6000], Loss: 1.6096
Epoch [4/30], Batch [1300/6000], Loss: 1.5028
Epoch [4/30], Batch [1400/6000], Loss: 1.5068
Epoch [4/30], Batch [1500/6000], Loss: 1.5061
Epoch [4/30], Batch [1600/6000], Loss: 1.5063
Epoch [4/30], Batch [1700/6000], Loss: 1.5628
Epoch [4/30], Batch [1800/6000], Loss: 1.4982
Epoch [4/30], Batch [1900/6000], Loss: 1.4990
Epoch [4/30], Batch [2000/6000], Loss: 1.5799
Epoch [4/30], Batch [2100/6000], Loss: 1.6072
Epoch [4/30], Batch [2200/6000], Loss: 1.5171
Epoch [4/30], Batch [2300/6000], Loss: 1.5188
Epoch [4/30], Batch [2400/6000], Loss: 1.4931
Epoch [4/30], Batch [2500/6000], Loss: 1.4928
Epoch [4/30], Batch [2600/6000], Loss: 1.5020
Epoch [4/30], Batch [2700/6000], Loss: 1.5547
Epoch [4/30], Batch [2800/6000], Loss: 1.4980
Epoch [4/30], Batch [2900/6000], Loss: 1.6191
Epoch [4/30], Batch [3000/6000], Loss: 1.5111
Epoch [4/30], Batch [3100/6000], Loss: 1.5090
Epoch [4/30], Batch [3200/6000], Loss: 1.4958
Epoch [4/30], Batch [3300/6000], Loss: 1.5390
Epoch [4/30], Batch [3400/6000], Loss: 1.4954
Epoch [4/30], Batch [3500/6000], Loss: 1.5127
Epoch [4/30], Batch [3600/6000], Loss: 1.4967
Epoch [4/30], Batch [3700/6000], Loss: 1.5262
Epoch [4/30], Batch [3800/6000], Loss: 1.5172
Epoch [4/30], Batch [3900/6000], Loss: 1.5092
Epoch [4/30], Batch [4000/6000], Loss: 1.5093
Epoch [4/30], Batch [4100/6000], Loss: 1.4978
Epoch [4/30], Batch [4200/6000], Loss: 1.5016
Epoch [4/30], Batch [4300/6000], Loss: 1.5060
Epoch [4/30], Batch [4400/6000], Loss: 1.5224
Epoch [4/30], Batch [4500/6000], Loss: 1.5002
Epoch [4/30], Batch [4600/6000], Loss: 1.5056
Epoch [4/30], Batch [4700/6000], Loss: 1.5128
Epoch [4/30], Batch [4800/6000], Loss: 1.4955
Epoch [4/30], Batch [4900/6000], Loss: 1.6173
Epoch [4/30], Batch [5000/6000], Loss: 1.5050
Epoch [4/30], Batch [5100/6000], Loss: 1.4934
Epoch [4/30], Batch [5200/6000], Loss: 1.5162
Epoch [4/30], Batch [5300/6000], Loss: 1.5069
Epoch [4/30], Batch [5400/6000], Loss: 1.5056
Epoch [4/30], Batch [5500/6000], Loss: 1.5991
Epoch [4/30], Batch [5600/6000], Loss: 1.5084
Epoch [4/30], Batch [5700/6000], Loss: 1.5080
Epoch [4/30], Batch [5800/6000], Loss: 1.4936
Epoch [4/30], Batch [5900/6000], Loss: 1.4956
Epoch [4/30], Loss: 1.5268
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 1.5097
Epoch [5/30], Batch [100/6000], Loss: 1.5883
Epoch [5/30], Batch [200/6000], Loss: 1.5332
Epoch [5/30], Batch [300/6000], Loss: 1.5003
Epoch [5/30], Batch [400/6000], Loss: 1.6217
Epoch [5/30], Batch [500/6000], Loss: 1.5088
Epoch [5/30], Batch [600/6000], Loss: 1.5024
Epoch [5/30], Batch [700/6000], Loss: 1.5780
Epoch [5/30], Batch [800/6000], Loss: 1.4917
Epoch [5/30], Batch [900/6000], Loss: 1.5026
Epoch [5/30], Batch [1000/6000], Loss: 1.5101
Epoch [5/30], Batch [1100/6000], Loss: 1.4934
Epoch [5/30], Batch [1200/6000], Loss: 1.6998
Epoch [5/30], Batch [1300/6000], Loss: 1.5038
Epoch [5/30], Batch [1400/6000], Loss: 1.5022
Epoch [5/30], Batch [1500/6000], Loss: 1.5001
Epoch [5/30], Batch [1600/6000], Loss: 1.4860
Epoch [5/30], Batch [1700/6000], Loss: 1.5048
Epoch [5/30], Batch [1800/6000], Loss: 1.8070
Epoch [5/30], Batch [1900/6000], Loss: 1.5977
Epoch [5/30], Batch [2000/6000], Loss: 1.7506
Epoch [5/30], Batch [2100/6000], Loss: 1.6107
Epoch [5/30], Batch [2200/6000], Loss: 1.5082
Epoch [5/30], Batch [2300/6000], Loss: 1.5167
Epoch [5/30], Batch [2400/6000], Loss: 1.4978
Epoch [5/30], Batch [2500/6000], Loss: 1.4971
Epoch [5/30], Batch [2600/6000], Loss: 1.6108
Epoch [5/30], Batch [2700/6000], Loss: 1.4935
Epoch [5/30], Batch [2800/6000], Loss: 1.6051
Epoch [5/30], Batch [2900/6000], Loss: 1.5022
Epoch [5/30], Batch [3000/6000], Loss: 1.6893
Epoch [5/30], Batch [3100/6000], Loss: 1.4937
Epoch [5/30], Batch [3200/6000], Loss: 1.5055
Epoch [5/30], Batch [3300/6000], Loss: 1.6235
Epoch [5/30], Batch [3400/6000], Loss: 1.4956
Epoch [5/30], Batch [3500/6000], Loss: 1.4915
Epoch [5/30], Batch [3600/6000], Loss: 1.5040
Epoch [5/30], Batch [3700/6000], Loss: 1.5069
Epoch [5/30], Batch [3800/6000], Loss: 1.5517
Epoch [5/30], Batch [3900/6000], Loss: 1.4875
Epoch [5/30], Batch [4000/6000], Loss: 1.4945
Epoch [5/30], Batch [4100/6000], Loss: 1.5901
Epoch [5/30], Batch [4200/6000], Loss: 1.4925
Epoch [5/30], Batch [4300/6000], Loss: 1.5144
Epoch [5/30], Batch [4400/6000], Loss: 1.4992
Epoch [5/30], Batch [4500/6000], Loss: 1.5032
Epoch [5/30], Batch [4600/6000], Loss: 1.4958
Epoch [5/30], Batch [4700/6000], Loss: 1.4930
Epoch [5/30], Batch [4800/6000], Loss: 1.5066
Epoch [5/30], Batch [4900/6000], Loss: 1.5947
Epoch [5/30], Batch [5000/6000], Loss: 1.4949
Epoch [5/30], Batch [5100/6000], Loss: 1.5014
Epoch [5/30], Batch [5200/6000], Loss: 1.4980
Epoch [5/30], Batch [5300/6000], Loss: 1.5017
Epoch [5/30], Batch [5400/6000], Loss: 1.5718
Epoch [5/30], Batch [5500/6000], Loss: 1.4954
Epoch [5/30], Batch [5600/6000], Loss: 1.5238
Epoch [5/30], Batch [5700/6000], Loss: 1.4917
Epoch [5/30], Batch [5800/6000], Loss: 1.6048
Epoch [5/30], Batch [5900/6000], Loss: 1.5072
Epoch [5/30], Loss: 1.5192
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 1.4941
Epoch [6/30], Batch [100/6000], Loss: 1.5107
Epoch [6/30], Batch [200/6000], Loss: 1.4954
Epoch [6/30], Batch [300/6000], Loss: 1.5096
Epoch [6/30], Batch [400/6000], Loss: 1.4969
Epoch [6/30], Batch [500/6000], Loss: 1.4929
Epoch [6/30], Batch [600/6000], Loss: 1.5078
Epoch [6/30], Batch [700/6000], Loss: 1.4998
Epoch [6/30], Batch [800/6000], Loss: 1.4920
Epoch [6/30], Batch [900/6000], Loss: 1.5049
Epoch [6/30], Batch [1000/6000], Loss: 1.4949
Epoch [6/30], Batch [1100/6000], Loss: 1.5030
Epoch [6/30], Batch [1200/6000], Loss: 1.4886
Epoch [6/30], Batch [1300/6000], Loss: 1.4939
Epoch [6/30], Batch [1400/6000], Loss: 1.5692
Epoch [6/30], Batch [1500/6000], Loss: 1.4980
Epoch [6/30], Batch [1600/6000], Loss: 1.4935
Epoch [6/30], Batch [1700/6000], Loss: 1.6098
Epoch [6/30], Batch [1800/6000], Loss: 1.4981
Epoch [6/30], Batch [1900/6000], Loss: 1.4899
Epoch [6/30], Batch [2000/6000], Loss: 1.4943
Epoch [6/30], Batch [2100/6000], Loss: 1.5563
Epoch [6/30], Batch [2200/6000], Loss: 1.5184
Epoch [6/30], Batch [2300/6000], Loss: 1.4904
Epoch [6/30], Batch [2400/6000], Loss: 1.4899
Epoch [6/30], Batch [2500/6000], Loss: 1.4878
Epoch [6/30], Batch [2600/6000], Loss: 1.4995
Epoch [6/30], Batch [2700/6000], Loss: 1.4905
Epoch [6/30], Batch [2800/6000], Loss: 1.4919
Epoch [6/30], Batch [2900/6000], Loss: 1.4939
Epoch [6/30], Batch [3000/6000], Loss: 1.4942
Epoch [6/30], Batch [3100/6000], Loss: 1.4936
Epoch [6/30], Batch [3200/6000], Loss: 1.5956
Epoch [6/30], Batch [3300/6000], Loss: 1.5046
Epoch [6/30], Batch [3400/6000], Loss: 1.5170
Epoch [6/30], Batch [3500/6000], Loss: 1.4853
Epoch [6/30], Batch [3600/6000], Loss: 1.4992
Epoch [6/30], Batch [3700/6000], Loss: 1.4955
Epoch [6/30], Batch [3800/6000], Loss: 1.4971
Epoch [6/30], Batch [3900/6000], Loss: 1.4962
Epoch [6/30], Batch [4000/6000], Loss: 1.5090
Epoch [6/30], Batch [4100/6000], Loss: 1.5101
Epoch [6/30], Batch [4200/6000], Loss: 1.4918
Epoch [6/30], Batch [4300/6000], Loss: 1.5999
Epoch [6/30], Batch [4400/6000], Loss: 1.4888
Epoch [6/30], Batch [4500/6000], Loss: 1.5023
Epoch [6/30], Batch [4600/6000], Loss: 1.5929
Epoch [6/30], Batch [4700/6000], Loss: 1.4958
Epoch [6/30], Batch [4800/6000], Loss: 1.4891
Epoch [6/30], Batch [4900/6000], Loss: 1.4893
Epoch [6/30], Batch [5000/6000], Loss: 1.5006
Epoch [6/30], Batch [5100/6000], Loss: 1.4958
Epoch [6/30], Batch [5200/6000], Loss: 1.5176
Epoch [6/30], Batch [5300/6000], Loss: 1.4896
Epoch [6/30], Batch [5400/6000], Loss: 1.5960
Epoch [6/30], Batch [5500/6000], Loss: 1.5030
Epoch [6/30], Batch [5600/6000], Loss: 1.4924
Epoch [6/30], Batch [5700/6000], Loss: 1.4839
Epoch [6/30], Batch [5800/6000], Loss: 1.5872
Epoch [6/30], Batch [5900/6000], Loss: 1.6016
Epoch [6/30], Loss: 1.5138
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 1.6002
Epoch [7/30], Batch [100/6000], Loss: 1.5031
Epoch [7/30], Batch [200/6000], Loss: 1.5066
Epoch [7/30], Batch [300/6000], Loss: 1.4978
Epoch [7/30], Batch [400/6000], Loss: 1.4815
Epoch [7/30], Batch [500/6000], Loss: 1.5718
Epoch [7/30], Batch [600/6000], Loss: 1.4958
Epoch [7/30], Batch [700/6000], Loss: 1.4956
Epoch [7/30], Batch [800/6000], Loss: 1.4889
Epoch [7/30], Batch [900/6000], Loss: 1.5067
Epoch [7/30], Batch [1000/6000], Loss: 1.4861
Epoch [7/30], Batch [1100/6000], Loss: 1.5306
Epoch [7/30], Batch [1200/6000], Loss: 1.4898
Epoch [7/30], Batch [1300/6000], Loss: 1.4904
Epoch [7/30], Batch [1400/6000], Loss: 1.4887
Epoch [7/30], Batch [1500/6000], Loss: 1.5010
Epoch [7/30], Batch [1600/6000], Loss: 1.5130
Epoch [7/30], Batch [1700/6000], Loss: 1.4981
Epoch [7/30], Batch [1800/6000], Loss: 1.4906
Epoch [7/30], Batch [1900/6000], Loss: 1.5078
Epoch [7/30], Batch [2000/6000], Loss: 1.5982
Epoch [7/30], Batch [2100/6000], Loss: 1.5040
Epoch [7/30], Batch [2200/6000], Loss: 1.6045
Epoch [7/30], Batch [2300/6000], Loss: 1.5045
Epoch [7/30], Batch [2400/6000], Loss: 1.4908
Epoch [7/30], Batch [2500/6000], Loss: 1.5302
Epoch [7/30], Batch [2600/6000], Loss: 1.5039
Epoch [7/30], Batch [2700/6000], Loss: 1.4847
Epoch [7/30], Batch [2800/6000], Loss: 1.7011
Epoch [7/30], Batch [2900/6000], Loss: 1.4918
Epoch [7/30], Batch [3000/6000], Loss: 1.4871
Epoch [7/30], Batch [3100/6000], Loss: 1.5895
Epoch [7/30], Batch [3200/6000], Loss: 1.5024
Epoch [7/30], Batch [3300/6000], Loss: 1.4972
Epoch [7/30], Batch [3400/6000], Loss: 1.4969
Epoch [7/30], Batch [3500/6000], Loss: 1.4890
Epoch [7/30], Batch [3600/6000], Loss: 1.4862
Epoch [7/30], Batch [3700/6000], Loss: 1.4939
Epoch [7/30], Batch [3800/6000], Loss: 1.4879
Epoch [7/30], Batch [3900/6000], Loss: 1.4902
Epoch [7/30], Batch [4000/6000], Loss: 1.4909
Epoch [7/30], Batch [4100/6000], Loss: 1.4965
Epoch [7/30], Batch [4200/6000], Loss: 1.4911
Epoch [7/30], Batch [4300/6000], Loss: 1.6059
Epoch [7/30], Batch [4400/6000], Loss: 1.4897
Epoch [7/30], Batch [4500/6000], Loss: 1.5029
Epoch [7/30], Batch [4600/6000], Loss: 1.4756
Epoch [7/30], Batch [4700/6000], Loss: 1.4901
Epoch [7/30], Batch [4800/6000], Loss: 1.4921
Epoch [7/30], Batch [4900/6000], Loss: 1.5007
Epoch [7/30], Batch [5000/6000], Loss: 1.4946
Epoch [7/30], Batch [5100/6000], Loss: 1.5008
Epoch [7/30], Batch [5200/6000], Loss: 1.5161
Epoch [7/30], Batch [5300/6000], Loss: 1.4994
Epoch [7/30], Batch [5400/6000], Loss: 1.6415
Epoch [7/30], Batch [5500/6000], Loss: 1.5009
Epoch [7/30], Batch [5600/6000], Loss: 1.4877
Epoch [7/30], Batch [5700/6000], Loss: 1.4880
Epoch [7/30], Batch [5800/6000], Loss: 1.4871
Epoch [7/30], Batch [5900/6000], Loss: 1.4996
Epoch [7/30], Loss: 1.5093
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 1.5012
Epoch [8/30], Batch [100/6000], Loss: 1.4865
Epoch [8/30], Batch [200/6000], Loss: 1.6039
Epoch [8/30], Batch [300/6000], Loss: 1.4936
Epoch [8/30], Batch [400/6000], Loss: 1.5155
Epoch [8/30], Batch [500/6000], Loss: 1.5909
Epoch [8/30], Batch [600/6000], Loss: 1.5046
Epoch [8/30], Batch [700/6000], Loss: 1.4851
Epoch [8/30], Batch [800/6000], Loss: 1.4910
Epoch [8/30], Batch [900/6000], Loss: 1.4937
Epoch [8/30], Batch [1000/6000], Loss: 1.6041
Epoch [8/30], Batch [1100/6000], Loss: 1.5046
Epoch [8/30], Batch [1200/6000], Loss: 1.4939
Epoch [8/30], Batch [1300/6000], Loss: 1.5909
Epoch [8/30], Batch [1400/6000], Loss: 1.5038
Epoch [8/30], Batch [1500/6000], Loss: 1.4894
Epoch [8/30], Batch [1600/6000], Loss: 1.5042
Epoch [8/30], Batch [1700/6000], Loss: 1.4931
Epoch [8/30], Batch [1800/6000], Loss: 1.5908
Epoch [8/30], Batch [1900/6000], Loss: 1.5016
Epoch [8/30], Batch [2000/6000], Loss: 1.6132
Epoch [8/30], Batch [2100/6000], Loss: 1.4952
Epoch [8/30], Batch [2200/6000], Loss: 1.5064
Epoch [8/30], Batch [2300/6000], Loss: 1.5688
Epoch [8/30], Batch [2400/6000], Loss: 1.4861
Epoch [8/30], Batch [2500/6000], Loss: 1.4877
Epoch [8/30], Batch [2600/6000], Loss: 1.4920
Epoch [8/30], Batch [2700/6000], Loss: 1.4987
Epoch [8/30], Batch [2800/6000], Loss: 1.4960
Epoch [8/30], Batch [2900/6000], Loss: 1.5180
Epoch [8/30], Batch [3000/6000], Loss: 1.4887
Epoch [8/30], Batch [3100/6000], Loss: 1.6009
Epoch [8/30], Batch [3200/6000], Loss: 1.4848
Epoch [8/30], Batch [3300/6000], Loss: 1.5614
Epoch [8/30], Batch [3400/6000], Loss: 1.4940
Epoch [8/30], Batch [3500/6000], Loss: 1.4862
Epoch [8/30], Batch [3600/6000], Loss: 1.4829
Epoch [8/30], Batch [3700/6000], Loss: 1.4840
Epoch [8/30], Batch [3800/6000], Loss: 1.4937
Epoch [8/30], Batch [3900/6000], Loss: 1.4816
Epoch [8/30], Batch [4000/6000], Loss: 1.4921
Epoch [8/30], Batch [4100/6000], Loss: 1.4980
Epoch [8/30], Batch [4200/6000], Loss: 1.6022
Epoch [8/30], Batch [4300/6000], Loss: 1.4902
Epoch [8/30], Batch [4400/6000], Loss: 1.4834
Epoch [8/30], Batch [4500/6000], Loss: 1.5012
Epoch [8/30], Batch [4600/6000], Loss: 1.4880
Epoch [8/30], Batch [4700/6000], Loss: 1.5141
Epoch [8/30], Batch [4800/6000], Loss: 1.5899
Epoch [8/30], Batch [4900/6000], Loss: 1.6005
Epoch [8/30], Batch [5000/6000], Loss: 1.4887
Epoch [8/30], Batch [5100/6000], Loss: 1.5924
Epoch [8/30], Batch [5200/6000], Loss: 1.4927
Epoch [8/30], Batch [5300/6000], Loss: 1.4895
Epoch [8/30], Batch [5400/6000], Loss: 1.4915
Epoch [8/30], Batch [5500/6000], Loss: 1.5848
Epoch [8/30], Batch [5600/6000], Loss: 1.4997
Epoch [8/30], Batch [5700/6000], Loss: 1.4900
Epoch [8/30], Batch [5800/6000], Loss: 1.4935
Epoch [8/30], Batch [5900/6000], Loss: 1.5154
Epoch [8/30], Loss: 1.5059
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 1.5999
Epoch [9/30], Batch [100/6000], Loss: 1.4940
Epoch [9/30], Batch [200/6000], Loss: 1.6119
Epoch [9/30], Batch [300/6000], Loss: 1.4917
Epoch [9/30], Batch [400/6000], Loss: 1.4871
Epoch [9/30], Batch [500/6000], Loss: 1.4952
Epoch [9/30], Batch [600/6000], Loss: 1.4902
Epoch [9/30], Batch [700/6000], Loss: 1.4902
Epoch [9/30], Batch [800/6000], Loss: 1.4947
Epoch [9/30], Batch [900/6000], Loss: 1.4937
Epoch [9/30], Batch [1000/6000], Loss: 1.4921
Epoch [9/30], Batch [1100/6000], Loss: 1.4993
Epoch [9/30], Batch [1200/6000], Loss: 1.5164
Epoch [9/30], Batch [1300/6000], Loss: 1.4926
Epoch [9/30], Batch [1400/6000], Loss: 1.4809
Epoch [9/30], Batch [1500/6000], Loss: 1.4954
Epoch [9/30], Batch [1600/6000], Loss: 1.5063
Epoch [9/30], Batch [1700/6000], Loss: 1.4869
Epoch [9/30], Batch [1800/6000], Loss: 1.4846
Epoch [9/30], Batch [1900/6000], Loss: 1.4870
Epoch [9/30], Batch [2000/6000], Loss: 1.5956
Epoch [9/30], Batch [2100/6000], Loss: 1.4961
Epoch [9/30], Batch [2200/6000], Loss: 1.4919
Epoch [9/30], Batch [2300/6000], Loss: 1.4865
Epoch [9/30], Batch [2400/6000], Loss: 1.4803
Epoch [9/30], Batch [2500/6000], Loss: 1.4891
Epoch [9/30], Batch [2600/6000], Loss: 1.4845
Epoch [9/30], Batch [2700/6000], Loss: 1.4886
Epoch [9/30], Batch [2800/6000], Loss: 1.4844
Epoch [9/30], Batch [2900/6000], Loss: 1.4907
Epoch [9/30], Batch [3000/6000], Loss: 1.5034
Epoch [9/30], Batch [3100/6000], Loss: 1.4905
Epoch [9/30], Batch [3200/6000], Loss: 1.4944
Epoch [9/30], Batch [3300/6000], Loss: 1.4934
Epoch [9/30], Batch [3400/6000], Loss: 1.5065
Epoch [9/30], Batch [3500/6000], Loss: 1.4895
Epoch [9/30], Batch [3600/6000], Loss: 1.5517
Epoch [9/30], Batch [3700/6000], Loss: 1.4864
Epoch [9/30], Batch [3800/6000], Loss: 1.4838
Epoch [9/30], Batch [3900/6000], Loss: 1.4865
Epoch [9/30], Batch [4000/6000], Loss: 1.4909
Epoch [9/30], Batch [4100/6000], Loss: 1.5297
Epoch [9/30], Batch [4200/6000], Loss: 1.4905
Epoch [9/30], Batch [4300/6000], Loss: 1.4882
Epoch [9/30], Batch [4400/6000], Loss: 1.4838
Epoch [9/30], Batch [4500/6000], Loss: 1.5011
Epoch [9/30], Batch [4600/6000], Loss: 1.4906
Epoch [9/30], Batch [4700/6000], Loss: 1.4920
Epoch [9/30], Batch [4800/6000], Loss: 1.5056
Epoch [9/30], Batch [4900/6000], Loss: 1.5011
Epoch [9/30], Batch [5000/6000], Loss: 1.5860
Epoch [9/30], Batch [5100/6000], Loss: 1.5026
Epoch [9/30], Batch [5200/6000], Loss: 1.4941
Epoch [9/30], Batch [5300/6000], Loss: 1.4895
Epoch [9/30], Batch [5400/6000], Loss: 1.4855
Epoch [9/30], Batch [5500/6000], Loss: 1.4881
Epoch [9/30], Batch [5600/6000], Loss: 1.5001
Epoch [9/30], Batch [5700/6000], Loss: 1.4900
Epoch [9/30], Batch [5800/6000], Loss: 1.5091
Epoch [9/30], Batch [5900/6000], Loss: 1.5009
Epoch [9/30], Loss: 1.5026
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 1.5000
Epoch [10/30], Batch [100/6000], Loss: 1.4910
Epoch [10/30], Batch [200/6000], Loss: 1.4853
Epoch [10/30], Batch [300/6000], Loss: 1.4985
Epoch [10/30], Batch [400/6000], Loss: 1.4858
Epoch [10/30], Batch [500/6000], Loss: 1.4818
Epoch [10/30], Batch [600/6000], Loss: 1.4839
Epoch [10/30], Batch [700/6000], Loss: 1.4861
Epoch [10/30], Batch [800/6000], Loss: 1.4970
Epoch [10/30], Batch [900/6000], Loss: 1.4843
Epoch [10/30], Batch [1000/6000], Loss: 1.5916
Epoch [10/30], Batch [1100/6000], Loss: 1.5749
Epoch [10/30], Batch [1200/6000], Loss: 1.4842
Epoch [10/30], Batch [1300/6000], Loss: 1.4910
Epoch [10/30], Batch [1400/6000], Loss: 1.4785
Epoch [10/30], Batch [1500/6000], Loss: 1.4900
Epoch [10/30], Batch [1600/6000], Loss: 1.4837
Epoch [10/30], Batch [1700/6000], Loss: 1.4995
Epoch [10/30], Batch [1800/6000], Loss: 1.5049
Epoch [10/30], Batch [1900/6000], Loss: 1.4868
Epoch [10/30], Batch [2000/6000], Loss: 1.4917
Epoch [10/30], Batch [2100/6000], Loss: 1.4898
Epoch [10/30], Batch [2200/6000], Loss: 1.4865
Epoch [10/30], Batch [2300/6000], Loss: 1.4834
Epoch [10/30], Batch [2400/6000], Loss: 1.5844
Epoch [10/30], Batch [2500/6000], Loss: 1.4891
Epoch [10/30], Batch [2600/6000], Loss: 1.4877
Epoch [10/30], Batch [2700/6000], Loss: 1.4899
Epoch [10/30], Batch [2800/6000], Loss: 1.5192
Epoch [10/30], Batch [2900/6000], Loss: 1.4868
Epoch [10/30], Batch [3000/6000], Loss: 1.4908
Epoch [10/30], Batch [3100/6000], Loss: 1.4886
Epoch [10/30], Batch [3200/6000], Loss: 1.5836
Epoch [10/30], Batch [3300/6000], Loss: 1.4825
Epoch [10/30], Batch [3400/6000], Loss: 1.4824
Epoch [10/30], Batch [3500/6000], Loss: 1.4874
Epoch [10/30], Batch [3600/6000], Loss: 1.5082
Epoch [10/30], Batch [3700/6000], Loss: 1.5063
Epoch [10/30], Batch [3800/6000], Loss: 1.4848
Epoch [10/30], Batch [3900/6000], Loss: 1.4869
Epoch [10/30], Batch [4000/6000], Loss: 1.4888
Epoch [10/30], Batch [4100/6000], Loss: 1.4866
Epoch [10/30], Batch [4200/6000], Loss: 1.4852
Epoch [10/30], Batch [4300/6000], Loss: 1.4947
Epoch [10/30], Batch [4400/6000], Loss: 1.5444
Epoch [10/30], Batch [4500/6000], Loss: 1.4853
Epoch [10/30], Batch [4600/6000], Loss: 1.4832
Epoch [10/30], Batch [4700/6000], Loss: 1.4931
Epoch [10/30], Batch [4800/6000], Loss: 1.4855
Epoch [10/30], Batch [4900/6000], Loss: 1.4761
Epoch [10/30], Batch [5000/6000], Loss: 1.4841
Epoch [10/30], Batch [5100/6000], Loss: 1.5086
Epoch [10/30], Batch [5200/6000], Loss: 1.4837
Epoch [10/30], Batch [5300/6000], Loss: 1.4806
Epoch [10/30], Batch [5400/6000], Loss: 1.4852
Epoch [10/30], Batch [5500/6000], Loss: 1.4882
Epoch [10/30], Batch [5600/6000], Loss: 1.4863
Epoch [10/30], Batch [5700/6000], Loss: 1.4860
Epoch [10/30], Batch [5800/6000], Loss: 1.4896
Epoch [10/30], Batch [5900/6000], Loss: 1.4937
Epoch [10/30], Loss: 1.5001
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 1.4841
Epoch [11/30], Batch [100/6000], Loss: 1.4855
Epoch [11/30], Batch [200/6000], Loss: 1.4859
Epoch [11/30], Batch [300/6000], Loss: 1.4779
Epoch [11/30], Batch [400/6000], Loss: 1.5202
Epoch [11/30], Batch [500/6000], Loss: 1.4847
Epoch [11/30], Batch [600/6000], Loss: 1.4870
Epoch [11/30], Batch [700/6000], Loss: 1.4928
Epoch [11/30], Batch [800/6000], Loss: 1.4858
Epoch [11/30], Batch [900/6000], Loss: 1.5008
Epoch [11/30], Batch [1000/6000], Loss: 1.4855
Epoch [11/30], Batch [1100/6000], Loss: 1.4865
Epoch [11/30], Batch [1200/6000], Loss: 1.4842
Epoch [11/30], Batch [1300/6000], Loss: 1.4772
Epoch [11/30], Batch [1400/6000], Loss: 1.4882
Epoch [11/30], Batch [1500/6000], Loss: 1.4888
Epoch [11/30], Batch [1600/6000], Loss: 1.4852
Epoch [11/30], Batch [1700/6000], Loss: 1.5456
Epoch [11/30], Batch [1800/6000], Loss: 1.4887
Epoch [11/30], Batch [1900/6000], Loss: 1.4793
Epoch [11/30], Batch [2000/6000], Loss: 1.4847
Epoch [11/30], Batch [2100/6000], Loss: 1.4880
Epoch [11/30], Batch [2200/6000], Loss: 1.5143
Epoch [11/30], Batch [2300/6000], Loss: 1.4870
Epoch [11/30], Batch [2400/6000], Loss: 1.5762
Epoch [11/30], Batch [2500/6000], Loss: 1.4891
Epoch [11/30], Batch [2600/6000], Loss: 1.5833
Epoch [11/30], Batch [2700/6000], Loss: 1.4842
Epoch [11/30], Batch [2800/6000], Loss: 1.4882
Epoch [11/30], Batch [2900/6000], Loss: 1.4894
Epoch [11/30], Batch [3000/6000], Loss: 1.4942
Epoch [11/30], Batch [3100/6000], Loss: 1.4874
Epoch [11/30], Batch [3200/6000], Loss: 1.4772
Epoch [11/30], Batch [3300/6000], Loss: 1.4855
Epoch [11/30], Batch [3400/6000], Loss: 1.5921
Epoch [11/30], Batch [3500/6000], Loss: 1.5339
Epoch [11/30], Batch [3600/6000], Loss: 1.4876
Epoch [11/30], Batch [3700/6000], Loss: 1.4859
Epoch [11/30], Batch [3800/6000], Loss: 1.4895
Epoch [11/30], Batch [3900/6000], Loss: 1.4835
Epoch [11/30], Batch [4000/6000], Loss: 1.4856
Epoch [11/30], Batch [4100/6000], Loss: 1.4864
Epoch [11/30], Batch [4200/6000], Loss: 1.4873
Epoch [11/30], Batch [4300/6000], Loss: 1.4879
Epoch [11/30], Batch [4400/6000], Loss: 1.4890
Epoch [11/30], Batch [4500/6000], Loss: 1.4834
Epoch [11/30], Batch [4600/6000], Loss: 1.4939
Epoch [11/30], Batch [4700/6000], Loss: 1.4876
Epoch [11/30], Batch [4800/6000], Loss: 1.4889
Epoch [11/30], Batch [4900/6000], Loss: 1.4897
Epoch [11/30], Batch [5000/6000], Loss: 1.4911
Epoch [11/30], Batch [5100/6000], Loss: 1.4836
Epoch [11/30], Batch [5200/6000], Loss: 1.5783
Epoch [11/30], Batch [5300/6000], Loss: 1.4773
Epoch [11/30], Batch [5400/6000], Loss: 1.5846
Epoch [11/30], Batch [5500/6000], Loss: 1.4806
Epoch [11/30], Batch [5600/6000], Loss: 1.4819
Epoch [11/30], Batch [5700/6000], Loss: 1.4894
Epoch [11/30], Batch [5800/6000], Loss: 1.4807
Epoch [11/30], Batch [5900/6000], Loss: 1.4869
Epoch [11/30], Loss: 1.4975
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 1.4829
Epoch [12/30], Batch [100/6000], Loss: 1.4783
Epoch [12/30], Batch [200/6000], Loss: 1.4847
Epoch [12/30], Batch [300/6000], Loss: 1.4892
Epoch [12/30], Batch [400/6000], Loss: 1.4890
Epoch [12/30], Batch [500/6000], Loss: 1.4814
Epoch [12/30], Batch [600/6000], Loss: 1.4846
Epoch [12/30], Batch [700/6000], Loss: 1.4801
Epoch [12/30], Batch [800/6000], Loss: 1.4887
Epoch [12/30], Batch [900/6000], Loss: 1.4857
Epoch [12/30], Batch [1000/6000], Loss: 1.4902
Epoch [12/30], Batch [1100/6000], Loss: 1.4807
Epoch [12/30], Batch [1200/6000], Loss: 1.5833
Epoch [12/30], Batch [1300/6000], Loss: 1.6053
Epoch [12/30], Batch [1400/6000], Loss: 1.4931
Epoch [12/30], Batch [1500/6000], Loss: 1.4789
Epoch [12/30], Batch [1600/6000], Loss: 1.4857
Epoch [12/30], Batch [1700/6000], Loss: 1.4817
Epoch [12/30], Batch [1800/6000], Loss: 1.4827
Epoch [12/30], Batch [1900/6000], Loss: 1.4852
Epoch [12/30], Batch [2000/6000], Loss: 1.4830
Epoch [12/30], Batch [2100/6000], Loss: 1.4918
Epoch [12/30], Batch [2200/6000], Loss: 1.4945
Epoch [12/30], Batch [2300/6000], Loss: 1.4841
Epoch [12/30], Batch [2400/6000], Loss: 1.4876
Epoch [12/30], Batch [2500/6000], Loss: 1.4862
Epoch [12/30], Batch [2600/6000], Loss: 1.4808
Epoch [12/30], Batch [2700/6000], Loss: 1.4874
Epoch [12/30], Batch [2800/6000], Loss: 1.4854
Epoch [12/30], Batch [2900/6000], Loss: 1.4808
Epoch [12/30], Batch [3000/6000], Loss: 1.4866
Epoch [12/30], Batch [3100/6000], Loss: 1.4978
Epoch [12/30], Batch [3200/6000], Loss: 1.4845
Epoch [12/30], Batch [3300/6000], Loss: 1.4841
Epoch [12/30], Batch [3400/6000], Loss: 1.4937
Epoch [12/30], Batch [3500/6000], Loss: 1.5850
Epoch [12/30], Batch [3600/6000], Loss: 1.4801
Epoch [12/30], Batch [3700/6000], Loss: 1.5015
Epoch [12/30], Batch [3800/6000], Loss: 1.6102
Epoch [12/30], Batch [3900/6000], Loss: 1.4854
Epoch [12/30], Batch [4000/6000], Loss: 1.4829
Epoch [12/30], Batch [4100/6000], Loss: 1.4805
Epoch [12/30], Batch [4200/6000], Loss: 1.5880
Epoch [12/30], Batch [4300/6000], Loss: 1.5704
Epoch [12/30], Batch [4400/6000], Loss: 1.4811
Epoch [12/30], Batch [4500/6000], Loss: 1.4877
Epoch [12/30], Batch [4600/6000], Loss: 1.4764
Epoch [12/30], Batch [4700/6000], Loss: 1.4856
Epoch [12/30], Batch [4800/6000], Loss: 1.4896
Epoch [12/30], Batch [4900/6000], Loss: 1.4771
Epoch [12/30], Batch [5000/6000], Loss: 1.6820
Epoch [12/30], Batch [5100/6000], Loss: 1.4845
Epoch [12/30], Batch [5200/6000], Loss: 1.4991
Epoch [12/30], Batch [5300/6000], Loss: 1.5344
Epoch [12/30], Batch [5400/6000], Loss: 1.4791
Epoch [12/30], Batch [5500/6000], Loss: 1.4791
Epoch [12/30], Batch [5600/6000], Loss: 1.4850
Epoch [12/30], Batch [5700/6000], Loss: 1.4784
Epoch [12/30], Batch [5800/6000], Loss: 1.4941
Epoch [12/30], Batch [5900/6000], Loss: 1.5915
Epoch [12/30], Loss: 1.4952
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 1.4841
Epoch [13/30], Batch [100/6000], Loss: 1.4826
Epoch [13/30], Batch [200/6000], Loss: 1.4928
Epoch [13/30], Batch [300/6000], Loss: 1.4800
Epoch [13/30], Batch [400/6000], Loss: 1.4970
Epoch [13/30], Batch [500/6000], Loss: 1.4841
Epoch [13/30], Batch [600/6000], Loss: 1.5864
Epoch [13/30], Batch [700/6000], Loss: 1.4963
Epoch [13/30], Batch [800/6000], Loss: 1.5842
Epoch [13/30], Batch [900/6000], Loss: 1.5902
Epoch [13/30], Batch [1000/6000], Loss: 1.4870
Epoch [13/30], Batch [1100/6000], Loss: 1.4849
Epoch [13/30], Batch [1200/6000], Loss: 1.4841
Epoch [13/30], Batch [1300/6000], Loss: 1.4993
Epoch [13/30], Batch [1400/6000], Loss: 1.4838
Epoch [13/30], Batch [1500/6000], Loss: 1.5861
Epoch [13/30], Batch [1600/6000], Loss: 1.4864
Epoch [13/30], Batch [1700/6000], Loss: 1.4892
Epoch [13/30], Batch [1800/6000], Loss: 1.4888
Epoch [13/30], Batch [1900/6000], Loss: 1.4856
Epoch [13/30], Batch [2000/6000], Loss: 1.4816
Epoch [13/30], Batch [2100/6000], Loss: 1.4803
Epoch [13/30], Batch [2200/6000], Loss: 1.4820
Epoch [13/30], Batch [2300/6000], Loss: 1.4798
Epoch [13/30], Batch [2400/6000], Loss: 1.4890
Epoch [13/30], Batch [2500/6000], Loss: 1.4801
Epoch [13/30], Batch [2600/6000], Loss: 1.4823
Epoch [13/30], Batch [2700/6000], Loss: 1.4869
Epoch [13/30], Batch [2800/6000], Loss: 1.4969
Epoch [13/30], Batch [2900/6000], Loss: 1.4794
Epoch [13/30], Batch [3000/6000], Loss: 1.4880
Epoch [13/30], Batch [3100/6000], Loss: 1.4895
Epoch [13/30], Batch [3200/6000], Loss: 1.4806
Epoch [13/30], Batch [3300/6000], Loss: 1.4810
Epoch [13/30], Batch [3400/6000], Loss: 1.4747
Epoch [13/30], Batch [3500/6000], Loss: 1.4803
Epoch [13/30], Batch [3600/6000], Loss: 1.4872
Epoch [13/30], Batch [3700/6000], Loss: 1.5783
Epoch [13/30], Batch [3800/6000], Loss: 1.4828
Epoch [13/30], Batch [3900/6000], Loss: 1.4839
Epoch [13/30], Batch [4000/6000], Loss: 1.5838
Epoch [13/30], Batch [4100/6000], Loss: 1.4807
Epoch [13/30], Batch [4200/6000], Loss: 1.4819
Epoch [13/30], Batch [4300/6000], Loss: 1.5785
Epoch [13/30], Batch [4400/6000], Loss: 1.4867
Epoch [13/30], Batch [4500/6000], Loss: 1.4822
Epoch [13/30], Batch [4600/6000], Loss: 1.4944
Epoch [13/30], Batch [4700/6000], Loss: 1.4793
Epoch [13/30], Batch [4800/6000], Loss: 1.4877
Epoch [13/30], Batch [4900/6000], Loss: 1.4824
Epoch [13/30], Batch [5000/6000], Loss: 1.4779
Epoch [13/30], Batch [5100/6000], Loss: 1.5758
Epoch [13/30], Batch [5200/6000], Loss: 1.5828
Epoch [13/30], Batch [5300/6000], Loss: 1.4875
Epoch [13/30], Batch [5400/6000], Loss: 1.5858
Epoch [13/30], Batch [5500/6000], Loss: 1.4901
Epoch [13/30], Batch [5600/6000], Loss: 1.4818
Epoch [13/30], Batch [5700/6000], Loss: 1.4788
Epoch [13/30], Batch [5800/6000], Loss: 1.4847
Epoch [13/30], Batch [5900/6000], Loss: 1.4872
Epoch [13/30], Loss: 1.4935
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 1.4901
Epoch [14/30], Batch [100/6000], Loss: 1.4869
Epoch [14/30], Batch [200/6000], Loss: 1.4828
Epoch [14/30], Batch [300/6000], Loss: 1.4818
Epoch [14/30], Batch [400/6000], Loss: 1.4845
Epoch [14/30], Batch [500/6000], Loss: 1.5014
Epoch [14/30], Batch [600/6000], Loss: 1.4864
Epoch [14/30], Batch [700/6000], Loss: 1.4876
Epoch [14/30], Batch [800/6000], Loss: 1.4813
Epoch [14/30], Batch [900/6000], Loss: 1.5015
Epoch [14/30], Batch [1000/6000], Loss: 1.4928
Epoch [14/30], Batch [1100/6000], Loss: 1.4777
Epoch [14/30], Batch [1200/6000], Loss: 1.4878
Epoch [14/30], Batch [1300/6000], Loss: 1.4974
Epoch [14/30], Batch [1400/6000], Loss: 1.4850
Epoch [14/30], Batch [1500/6000], Loss: 1.4826
Epoch [14/30], Batch [1600/6000], Loss: 1.4820
Epoch [14/30], Batch [1700/6000], Loss: 1.4802
Epoch [14/30], Batch [1800/6000], Loss: 1.4797
Epoch [14/30], Batch [1900/6000], Loss: 1.4998
Epoch [14/30], Batch [2000/6000], Loss: 1.4821
Epoch [14/30], Batch [2100/6000], Loss: 1.4798
Epoch [14/30], Batch [2200/6000], Loss: 1.4894
Epoch [14/30], Batch [2300/6000], Loss: 1.4815
Epoch [14/30], Batch [2400/6000], Loss: 1.4807
Epoch [14/30], Batch [2500/6000], Loss: 1.4793
Epoch [14/30], Batch [2600/6000], Loss: 1.4792
Epoch [14/30], Batch [2700/6000], Loss: 1.4850
Epoch [14/30], Batch [2800/6000], Loss: 1.4813
Epoch [14/30], Batch [2900/6000], Loss: 1.4893
Epoch [14/30], Batch [3000/6000], Loss: 1.4803
Epoch [14/30], Batch [3100/6000], Loss: 1.4900
Epoch [14/30], Batch [3200/6000], Loss: 1.4951
Epoch [14/30], Batch [3300/6000], Loss: 1.4816
Epoch [14/30], Batch [3400/6000], Loss: 1.4732
Epoch [14/30], Batch [3500/6000], Loss: 1.4797
Epoch [14/30], Batch [3600/6000], Loss: 1.7767
Epoch [14/30], Batch [3700/6000], Loss: 1.4797
Epoch [14/30], Batch [3800/6000], Loss: 1.4778
Epoch [14/30], Batch [3900/6000], Loss: 1.4855
Epoch [14/30], Batch [4000/6000], Loss: 1.4800
Epoch [14/30], Batch [4100/6000], Loss: 1.4975
Epoch [14/30], Batch [4200/6000], Loss: 1.5048
Epoch [14/30], Batch [4300/6000], Loss: 1.4823
Epoch [14/30], Batch [4400/6000], Loss: 1.4825
Epoch [14/30], Batch [4500/6000], Loss: 1.4817
Epoch [14/30], Batch [4600/6000], Loss: 1.4803
Epoch [14/30], Batch [4700/6000], Loss: 1.4776
Epoch [14/30], Batch [4800/6000], Loss: 1.4871
Epoch [14/30], Batch [4900/6000], Loss: 1.4808
Epoch [14/30], Batch [5000/6000], Loss: 1.4873
Epoch [14/30], Batch [5100/6000], Loss: 1.4955
Epoch [14/30], Batch [5200/6000], Loss: 1.4839
Epoch [14/30], Batch [5300/6000], Loss: 1.4800
Epoch [14/30], Batch [5400/6000], Loss: 1.4840
Epoch [14/30], Batch [5500/6000], Loss: 1.4788
Epoch [14/30], Batch [5600/6000], Loss: 1.4848
Epoch [14/30], Batch [5700/6000], Loss: 1.4829
Epoch [14/30], Batch [5800/6000], Loss: 1.4855
Epoch [14/30], Batch [5900/6000], Loss: 1.4948
Epoch [14/30], Loss: 1.4915
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 1.4826
Epoch [15/30], Batch [100/6000], Loss: 1.4807
Epoch [15/30], Batch [200/6000], Loss: 1.5012
Epoch [15/30], Batch [300/6000], Loss: 1.4842
Epoch [15/30], Batch [400/6000], Loss: 1.4853
Epoch [15/30], Batch [500/6000], Loss: 1.4796
Epoch [15/30], Batch [600/6000], Loss: 1.4795
Epoch [15/30], Batch [700/6000], Loss: 1.4802
Epoch [15/30], Batch [800/6000], Loss: 1.4912
Epoch [15/30], Batch [900/6000], Loss: 1.4806
Epoch [15/30], Batch [1000/6000], Loss: 1.4833
Epoch [15/30], Batch [1100/6000], Loss: 1.4800
Epoch [15/30], Batch [1200/6000], Loss: 1.4788
Epoch [15/30], Batch [1300/6000], Loss: 1.4754
Epoch [15/30], Batch [1400/6000], Loss: 1.5845
Epoch [15/30], Batch [1500/6000], Loss: 1.4809
Epoch [15/30], Batch [1600/6000], Loss: 1.4837
Epoch [15/30], Batch [1700/6000], Loss: 1.4928
Epoch [15/30], Batch [1800/6000], Loss: 1.4799
Epoch [15/30], Batch [1900/6000], Loss: 1.4784
Epoch [15/30], Batch [2000/6000], Loss: 1.4823
Epoch [15/30], Batch [2100/6000], Loss: 1.5002
Epoch [15/30], Batch [2200/6000], Loss: 1.4782
Epoch [15/30], Batch [2300/6000], Loss: 1.4860
Epoch [15/30], Batch [2400/6000], Loss: 1.4761
Epoch [15/30], Batch [2500/6000], Loss: 1.4846
Epoch [15/30], Batch [2600/6000], Loss: 1.4828
Epoch [15/30], Batch [2700/6000], Loss: 1.4809
Epoch [15/30], Batch [2800/6000], Loss: 1.4843
Epoch [15/30], Batch [2900/6000], Loss: 1.4803
Epoch [15/30], Batch [3000/6000], Loss: 1.4955
Epoch [15/30], Batch [3100/6000], Loss: 1.4846
Epoch [15/30], Batch [3200/6000], Loss: 1.4835
Epoch [15/30], Batch [3300/6000], Loss: 1.4785
Epoch [15/30], Batch [3400/6000], Loss: 1.4865
Epoch [15/30], Batch [3500/6000], Loss: 1.4905
Epoch [15/30], Batch [3600/6000], Loss: 1.4811
Epoch [15/30], Batch [3700/6000], Loss: 1.4879
Epoch [15/30], Batch [3800/6000], Loss: 1.4824
Epoch [15/30], Batch [3900/6000], Loss: 1.4797
Epoch [15/30], Batch [4000/6000], Loss: 1.4814
Epoch [15/30], Batch [4100/6000], Loss: 1.4863
Epoch [15/30], Batch [4200/6000], Loss: 1.4815
Epoch [15/30], Batch [4300/6000], Loss: 1.4791
Epoch [15/30], Batch [4400/6000], Loss: 1.4802
Epoch [15/30], Batch [4500/6000], Loss: 1.4798
Epoch [15/30], Batch [4600/6000], Loss: 1.4790
Epoch [15/30], Batch [4700/6000], Loss: 1.4807
Epoch [15/30], Batch [4800/6000], Loss: 1.4831
Epoch [15/30], Batch [4900/6000], Loss: 1.4777
Epoch [15/30], Batch [5000/6000], Loss: 1.4845
Epoch [15/30], Batch [5100/6000], Loss: 1.4797
Epoch [15/30], Batch [5200/6000], Loss: 1.4797
Epoch [15/30], Batch [5300/6000], Loss: 1.4859
Epoch [15/30], Batch [5400/6000], Loss: 1.4758
Epoch [15/30], Batch [5500/6000], Loss: 1.4822
Epoch [15/30], Batch [5600/6000], Loss: 1.4771
Epoch [15/30], Batch [5700/6000], Loss: 1.4827
Epoch [15/30], Batch [5800/6000], Loss: 1.4804
Epoch [15/30], Batch [5900/6000], Loss: 1.5818
Epoch [15/30], Loss: 1.4907
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 1.4800
Epoch [16/30], Batch [100/6000], Loss: 1.5817
Epoch [16/30], Batch [200/6000], Loss: 1.4825
Epoch [16/30], Batch [300/6000], Loss: 1.4859
Epoch [16/30], Batch [400/6000], Loss: 1.4762
Epoch [16/30], Batch [500/6000], Loss: 1.4776
Epoch [16/30], Batch [600/6000], Loss: 1.4778
Epoch [16/30], Batch [700/6000], Loss: 1.4909
Epoch [16/30], Batch [800/6000], Loss: 1.4819
Epoch [16/30], Batch [900/6000], Loss: 1.5870
Epoch [16/30], Batch [1000/6000], Loss: 1.4815
Epoch [16/30], Batch [1100/6000], Loss: 1.4834
Epoch [16/30], Batch [1200/6000], Loss: 1.4830
Epoch [16/30], Batch [1300/6000], Loss: 1.4759
Epoch [16/30], Batch [1400/6000], Loss: 1.4843
Epoch [16/30], Batch [1500/6000], Loss: 1.4800
Epoch [16/30], Batch [1600/6000], Loss: 1.4726
Epoch [16/30], Batch [1700/6000], Loss: 1.4793
Epoch [16/30], Batch [1800/6000], Loss: 1.4798
Epoch [16/30], Batch [1900/6000], Loss: 1.4840
Epoch [16/30], Batch [2000/6000], Loss: 1.4911
Epoch [16/30], Batch [2100/6000], Loss: 1.4819
Epoch [16/30], Batch [2200/6000], Loss: 1.4774
Epoch [16/30], Batch [2300/6000], Loss: 1.4799
Epoch [16/30], Batch [2400/6000], Loss: 1.4802
Epoch [16/30], Batch [2500/6000], Loss: 1.5630
Epoch [16/30], Batch [2600/6000], Loss: 1.4822
Epoch [16/30], Batch [2700/6000], Loss: 1.4971
Epoch [16/30], Batch [2800/6000], Loss: 1.4771
Epoch [16/30], Batch [2900/6000], Loss: 1.4792
Epoch [16/30], Batch [3000/6000], Loss: 1.4814
Epoch [16/30], Batch [3100/6000], Loss: 1.4825
Epoch [16/30], Batch [3200/6000], Loss: 1.4761
Epoch [16/30], Batch [3300/6000], Loss: 1.4985
Epoch [16/30], Batch [3400/6000], Loss: 1.5858
Epoch [16/30], Batch [3500/6000], Loss: 1.4907
Epoch [16/30], Batch [3600/6000], Loss: 1.4807
Epoch [16/30], Batch [3700/6000], Loss: 1.4794
Epoch [16/30], Batch [3800/6000], Loss: 1.4792
Epoch [16/30], Batch [3900/6000], Loss: 1.4865
Epoch [16/30], Batch [4000/6000], Loss: 1.4771
Epoch [16/30], Batch [4100/6000], Loss: 1.4935
Epoch [16/30], Batch [4200/6000], Loss: 1.4847
Epoch [16/30], Batch [4300/6000], Loss: 1.4775
Epoch [16/30], Batch [4400/6000], Loss: 1.4898
Epoch [16/30], Batch [4500/6000], Loss: 1.4743
Epoch [16/30], Batch [4600/6000], Loss: 1.4834
Epoch [16/30], Batch [4700/6000], Loss: 1.4811
Epoch [16/30], Batch [4800/6000], Loss: 1.4973
Epoch [16/30], Batch [4900/6000], Loss: 1.4787
Epoch [16/30], Batch [5000/6000], Loss: 1.5300
Epoch [16/30], Batch [5100/6000], Loss: 1.4850
Epoch [16/30], Batch [5200/6000], Loss: 1.4817
Epoch [16/30], Batch [5300/6000], Loss: 1.4834
Epoch [16/30], Batch [5400/6000], Loss: 1.4811
Epoch [16/30], Batch [5500/6000], Loss: 1.4899
Epoch [16/30], Batch [5600/6000], Loss: 1.4814
Epoch [16/30], Batch [5700/6000], Loss: 1.4803
Epoch [16/30], Batch [5800/6000], Loss: 1.4910
Epoch [16/30], Batch [5900/6000], Loss: 1.4953
Epoch [16/30], Loss: 1.4892
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 1.4761
Epoch [17/30], Batch [100/6000], Loss: 1.4767
Epoch [17/30], Batch [200/6000], Loss: 1.4976
Epoch [17/30], Batch [300/6000], Loss: 1.4785
Epoch [17/30], Batch [400/6000], Loss: 1.4985
Epoch [17/30], Batch [500/6000], Loss: 1.4805
Epoch [17/30], Batch [600/6000], Loss: 1.4790
Epoch [17/30], Batch [700/6000], Loss: 1.4922
Epoch [17/30], Batch [800/6000], Loss: 1.4770
Epoch [17/30], Batch [900/6000], Loss: 1.4812
Epoch [17/30], Batch [1000/6000], Loss: 1.4836
Epoch [17/30], Batch [1100/6000], Loss: 1.4868
Epoch [17/30], Batch [1200/6000], Loss: 1.5030
Epoch [17/30], Batch [1300/6000], Loss: 1.4835
Epoch [17/30], Batch [1400/6000], Loss: 1.4801
Epoch [17/30], Batch [1500/6000], Loss: 1.4808
Epoch [17/30], Batch [1600/6000], Loss: 1.4789
Epoch [17/30], Batch [1700/6000], Loss: 1.4788
Epoch [17/30], Batch [1800/6000], Loss: 1.4830
Epoch [17/30], Batch [1900/6000], Loss: 1.4765
Epoch [17/30], Batch [2000/6000], Loss: 1.4797
Epoch [17/30], Batch [2100/6000], Loss: 1.4928
Epoch [17/30], Batch [2200/6000], Loss: 1.4816
Epoch [17/30], Batch [2300/6000], Loss: 1.4749
Epoch [17/30], Batch [2400/6000], Loss: 1.4841
Epoch [17/30], Batch [2500/6000], Loss: 1.4820
Epoch [17/30], Batch [2600/6000], Loss: 1.4763
Epoch [17/30], Batch [2700/6000], Loss: 1.4735
Epoch [17/30], Batch [2800/6000], Loss: 1.4773
Epoch [17/30], Batch [2900/6000], Loss: 1.4789
Epoch [17/30], Batch [3000/6000], Loss: 1.4763
Epoch [17/30], Batch [3100/6000], Loss: 1.4913
Epoch [17/30], Batch [3200/6000], Loss: 1.4767
Epoch [17/30], Batch [3300/6000], Loss: 1.4803
Epoch [17/30], Batch [3400/6000], Loss: 1.4792
Epoch [17/30], Batch [3500/6000], Loss: 1.4865
Epoch [17/30], Batch [3600/6000], Loss: 1.4879
Epoch [17/30], Batch [3700/6000], Loss: 1.4810
Epoch [17/30], Batch [3800/6000], Loss: 1.4831
Epoch [17/30], Batch [3900/6000], Loss: 1.4753
Epoch [17/30], Batch [4000/6000], Loss: 1.4788
Epoch [17/30], Batch [4100/6000], Loss: 1.4748
Epoch [17/30], Batch [4200/6000], Loss: 1.4788
Epoch [17/30], Batch [4300/6000], Loss: 1.4844
Epoch [17/30], Batch [4400/6000], Loss: 1.4871
Epoch [17/30], Batch [4500/6000], Loss: 1.4826
Epoch [17/30], Batch [4600/6000], Loss: 1.4798
Epoch [17/30], Batch [4700/6000], Loss: 1.4788
Epoch [17/30], Batch [4800/6000], Loss: 1.4797
Epoch [17/30], Batch [4900/6000], Loss: 1.4857
Epoch [17/30], Batch [5000/6000], Loss: 1.4797
Epoch [17/30], Batch [5100/6000], Loss: 1.4796
Epoch [17/30], Batch [5200/6000], Loss: 1.4747
Epoch [17/30], Batch [5300/6000], Loss: 1.4835
Epoch [17/30], Batch [5400/6000], Loss: 1.4861
Epoch [17/30], Batch [5500/6000], Loss: 1.4772
Epoch [17/30], Batch [5600/6000], Loss: 1.4997
Epoch [17/30], Batch [5700/6000], Loss: 1.4821
Epoch [17/30], Batch [5800/6000], Loss: 1.4799
Epoch [17/30], Batch [5900/6000], Loss: 1.4808
Epoch [17/30], Loss: 1.4880
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 1.4849
Epoch [18/30], Batch [100/6000], Loss: 1.5786
Epoch [18/30], Batch [200/6000], Loss: 1.4969
Epoch [18/30], Batch [300/6000], Loss: 1.5797
Epoch [18/30], Batch [400/6000], Loss: 1.4769
Epoch [18/30], Batch [500/6000], Loss: 1.4943
Epoch [18/30], Batch [600/6000], Loss: 1.4796
Epoch [18/30], Batch [700/6000], Loss: 1.4943
Epoch [18/30], Batch [800/6000], Loss: 1.4803
Epoch [18/30], Batch [900/6000], Loss: 1.4835
Epoch [18/30], Batch [1000/6000], Loss: 1.4806
Epoch [18/30], Batch [1100/6000], Loss: 1.4906
Epoch [18/30], Batch [1200/6000], Loss: 1.4839
Epoch [18/30], Batch [1300/6000], Loss: 1.4791
Epoch [18/30], Batch [1400/6000], Loss: 1.4802
Epoch [18/30], Batch [1500/6000], Loss: 1.5677
Epoch [18/30], Batch [1600/6000], Loss: 1.4758
Epoch [18/30], Batch [1700/6000], Loss: 1.4775
Epoch [18/30], Batch [1800/6000], Loss: 1.4820
Epoch [18/30], Batch [1900/6000], Loss: 1.4782
Epoch [18/30], Batch [2000/6000], Loss: 1.4768
Epoch [18/30], Batch [2100/6000], Loss: 1.4942
Epoch [18/30], Batch [2200/6000], Loss: 1.4764
Epoch [18/30], Batch [2300/6000], Loss: 1.4767
Epoch [18/30], Batch [2400/6000], Loss: 1.4797
Epoch [18/30], Batch [2500/6000], Loss: 1.4793
Epoch [18/30], Batch [2600/6000], Loss: 1.4883
Epoch [18/30], Batch [2700/6000], Loss: 1.4786
Epoch [18/30], Batch [2800/6000], Loss: 1.5740
Epoch [18/30], Batch [2900/6000], Loss: 1.4751
Epoch [18/30], Batch [3000/6000], Loss: 1.4788
Epoch [18/30], Batch [3100/6000], Loss: 1.4768
Epoch [18/30], Batch [3200/6000], Loss: 1.5865
Epoch [18/30], Batch [3300/6000], Loss: 1.4802
Epoch [18/30], Batch [3400/6000], Loss: 1.4780
Epoch [18/30], Batch [3500/6000], Loss: 1.4760
Epoch [18/30], Batch [3600/6000], Loss: 1.4824
Epoch [18/30], Batch [3700/6000], Loss: 1.4759
Epoch [18/30], Batch [3800/6000], Loss: 1.4750
Epoch [18/30], Batch [3900/6000], Loss: 1.4960
Epoch [18/30], Batch [4000/6000], Loss: 1.4917
Epoch [18/30], Batch [4100/6000], Loss: 1.5663
Epoch [18/30], Batch [4200/6000], Loss: 1.4786
Epoch [18/30], Batch [4300/6000], Loss: 1.5785
Epoch [18/30], Batch [4400/6000], Loss: 1.5761
Epoch [18/30], Batch [4500/6000], Loss: 1.4740
Epoch [18/30], Batch [4600/6000], Loss: 1.4760
Epoch [18/30], Batch [4700/6000], Loss: 1.4771
Epoch [18/30], Batch [4800/6000], Loss: 1.4799
Epoch [18/30], Batch [4900/6000], Loss: 1.4934
Epoch [18/30], Batch [5000/6000], Loss: 1.4784
Epoch [18/30], Batch [5100/6000], Loss: 1.4862
Epoch [18/30], Batch [5200/6000], Loss: 1.4758
Epoch [18/30], Batch [5300/6000], Loss: 1.4802
Epoch [18/30], Batch [5400/6000], Loss: 1.4884
Epoch [18/30], Batch [5500/6000], Loss: 1.4827
Epoch [18/30], Batch [5600/6000], Loss: 1.4947
Epoch [18/30], Batch [5700/6000], Loss: 1.5856
Epoch [18/30], Batch [5800/6000], Loss: 1.4797
Epoch [18/30], Batch [5900/6000], Loss: 1.4764
Epoch [18/30], Loss: 1.4875
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 1.4945
Epoch [19/30], Batch [100/6000], Loss: 1.4782
Epoch [19/30], Batch [200/6000], Loss: 1.4829
Epoch [19/30], Batch [300/6000], Loss: 1.4819
Epoch [19/30], Batch [400/6000], Loss: 1.4816
Epoch [19/30], Batch [500/6000], Loss: 1.4918
Epoch [19/30], Batch [600/6000], Loss: 1.4789
Epoch [19/30], Batch [700/6000], Loss: 1.4789
Epoch [19/30], Batch [800/6000], Loss: 1.4769
Epoch [19/30], Batch [900/6000], Loss: 1.4783
Epoch [19/30], Batch [1000/6000], Loss: 1.4748
Epoch [19/30], Batch [1100/6000], Loss: 1.4775
Epoch [19/30], Batch [1200/6000], Loss: 1.4900
Epoch [19/30], Batch [1300/6000], Loss: 1.4787
Epoch [19/30], Batch [1400/6000], Loss: 1.4816
Epoch [19/30], Batch [1500/6000], Loss: 1.4788
Epoch [19/30], Batch [1600/6000], Loss: 1.4894
Epoch [19/30], Batch [1700/6000], Loss: 1.4792
Epoch [19/30], Batch [1800/6000], Loss: 1.4924
Epoch [19/30], Batch [1900/6000], Loss: 1.4923
Epoch [19/30], Batch [2000/6000], Loss: 1.4791
Epoch [19/30], Batch [2100/6000], Loss: 1.4811
Epoch [19/30], Batch [2200/6000], Loss: 1.4810
Epoch [19/30], Batch [2300/6000], Loss: 1.4817
Epoch [19/30], Batch [2400/6000], Loss: 1.4804
Epoch [19/30], Batch [2500/6000], Loss: 1.4779
Epoch [19/30], Batch [2600/6000], Loss: 1.4806
Epoch [19/30], Batch [2700/6000], Loss: 1.4929
Epoch [19/30], Batch [2800/6000], Loss: 1.4736
Epoch [19/30], Batch [2900/6000], Loss: 1.4789
Epoch [19/30], Batch [3000/6000], Loss: 1.4812
Epoch [19/30], Batch [3100/6000], Loss: 1.4735
Epoch [19/30], Batch [3200/6000], Loss: 1.4831
Epoch [19/30], Batch [3300/6000], Loss: 1.4763
Epoch [19/30], Batch [3400/6000], Loss: 1.4766
Epoch [19/30], Batch [3500/6000], Loss: 1.4846
Epoch [19/30], Batch [3600/6000], Loss: 1.5775
Epoch [19/30], Batch [3700/6000], Loss: 1.4951
Epoch [19/30], Batch [3800/6000], Loss: 1.4796
Epoch [19/30], Batch [3900/6000], Loss: 1.4751
Epoch [19/30], Batch [4000/6000], Loss: 1.4820
Epoch [19/30], Batch [4100/6000], Loss: 1.4800
Epoch [19/30], Batch [4200/6000], Loss: 1.4793
Epoch [19/30], Batch [4300/6000], Loss: 1.4953
Epoch [19/30], Batch [4400/6000], Loss: 1.4769
Epoch [19/30], Batch [4500/6000], Loss: 1.4836
Epoch [19/30], Batch [4600/6000], Loss: 1.4767
Epoch [19/30], Batch [4700/6000], Loss: 1.4985
Epoch [19/30], Batch [4800/6000], Loss: 1.4768
Epoch [19/30], Batch [4900/6000], Loss: 1.4859
Epoch [19/30], Batch [5000/6000], Loss: 1.4767
Epoch [19/30], Batch [5100/6000], Loss: 1.4752
Epoch [19/30], Batch [5200/6000], Loss: 1.4773
Epoch [19/30], Batch [5300/6000], Loss: 1.5239
Epoch [19/30], Batch [5400/6000], Loss: 1.4741
Epoch [19/30], Batch [5500/6000], Loss: 1.4737
Epoch [19/30], Batch [5600/6000], Loss: 1.4763
Epoch [19/30], Batch [5700/6000], Loss: 1.4753
Epoch [19/30], Batch [5800/6000], Loss: 1.4780
Epoch [19/30], Batch [5900/6000], Loss: 1.4775
Epoch [19/30], Loss: 1.4862
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 1.4725
Epoch [20/30], Batch [100/6000], Loss: 1.4780
Epoch [20/30], Batch [200/6000], Loss: 1.4923
Epoch [20/30], Batch [300/6000], Loss: 1.4765
Epoch [20/30], Batch [400/6000], Loss: 1.4790
Epoch [20/30], Batch [500/6000], Loss: 1.4778
Epoch [20/30], Batch [600/6000], Loss: 1.5149
Epoch [20/30], Batch [700/6000], Loss: 1.4782
Epoch [20/30], Batch [800/6000], Loss: 1.4806
Epoch [20/30], Batch [900/6000], Loss: 1.4807
Epoch [20/30], Batch [1000/6000], Loss: 1.4739
Epoch [20/30], Batch [1100/6000], Loss: 1.4726
Epoch [20/30], Batch [1200/6000], Loss: 1.4799
Epoch [20/30], Batch [1300/6000], Loss: 1.4726
Epoch [20/30], Batch [1400/6000], Loss: 1.4783
Epoch [20/30], Batch [1500/6000], Loss: 1.4773
Epoch [20/30], Batch [1600/6000], Loss: 1.4763
Epoch [20/30], Batch [1700/6000], Loss: 1.4779
Epoch [20/30], Batch [1800/6000], Loss: 1.4833
Epoch [20/30], Batch [1900/6000], Loss: 1.5771
Epoch [20/30], Batch [2000/6000], Loss: 1.4751
Epoch [20/30], Batch [2100/6000], Loss: 1.4793
Epoch [20/30], Batch [2200/6000], Loss: 1.4775
Epoch [20/30], Batch [2300/6000], Loss: 1.4793
Epoch [20/30], Batch [2400/6000], Loss: 1.4815
Epoch [20/30], Batch [2500/6000], Loss: 1.4816
Epoch [20/30], Batch [2600/6000], Loss: 1.4956
Epoch [20/30], Batch [2700/6000], Loss: 1.4734
Epoch [20/30], Batch [2800/6000], Loss: 1.4745
Epoch [20/30], Batch [2900/6000], Loss: 1.4802
Epoch [20/30], Batch [3000/6000], Loss: 1.4768
Epoch [20/30], Batch [3100/6000], Loss: 1.4740
Epoch [20/30], Batch [3200/6000], Loss: 1.4780
Epoch [20/30], Batch [3300/6000], Loss: 1.4770
Epoch [20/30], Batch [3400/6000], Loss: 1.4753
Epoch [20/30], Batch [3500/6000], Loss: 1.4786
Epoch [20/30], Batch [3600/6000], Loss: 1.4742
Epoch [20/30], Batch [3700/6000], Loss: 1.4764
Epoch [20/30], Batch [3800/6000], Loss: 1.4721
Epoch [20/30], Batch [3900/6000], Loss: 1.4768
Epoch [20/30], Batch [4000/6000], Loss: 1.4765
Epoch [20/30], Batch [4100/6000], Loss: 1.4763
Epoch [20/30], Batch [4200/6000], Loss: 1.4758
Epoch [20/30], Batch [4300/6000], Loss: 1.4808
Epoch [20/30], Batch [4400/6000], Loss: 1.4994
Epoch [20/30], Batch [4500/6000], Loss: 1.4770
Epoch [20/30], Batch [4600/6000], Loss: 1.4761
Epoch [20/30], Batch [4700/6000], Loss: 1.4753
Epoch [20/30], Batch [4800/6000], Loss: 1.4772
Epoch [20/30], Batch [4900/6000], Loss: 1.4764
Epoch [20/30], Batch [5000/6000], Loss: 1.4890
Epoch [20/30], Batch [5100/6000], Loss: 1.5828
Epoch [20/30], Batch [5200/6000], Loss: 1.4847
Epoch [20/30], Batch [5300/6000], Loss: 1.4784
Epoch [20/30], Batch [5400/6000], Loss: 1.4786
Epoch [20/30], Batch [5500/6000], Loss: 1.5284
Epoch [20/30], Batch [5600/6000], Loss: 1.4796
Epoch [20/30], Batch [5700/6000], Loss: 1.4808
Epoch [20/30], Batch [5800/6000], Loss: 1.4759
Epoch [20/30], Batch [5900/6000], Loss: 1.4791
Epoch [20/30], Loss: 1.4853
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 1.4808
Epoch [21/30], Batch [100/6000], Loss: 1.4789
Epoch [21/30], Batch [200/6000], Loss: 1.4804
Epoch [21/30], Batch [300/6000], Loss: 1.4726
Epoch [21/30], Batch [400/6000], Loss: 1.4761
Epoch [21/30], Batch [500/6000], Loss: 1.4775
Epoch [21/30], Batch [600/6000], Loss: 1.4764
Epoch [21/30], Batch [700/6000], Loss: 1.4762
Epoch [21/30], Batch [800/6000], Loss: 1.4768
Epoch [21/30], Batch [900/6000], Loss: 1.4808
Epoch [21/30], Batch [1000/6000], Loss: 1.4758
Epoch [21/30], Batch [1100/6000], Loss: 1.4815
Epoch [21/30], Batch [1200/6000], Loss: 1.5772
Epoch [21/30], Batch [1300/6000], Loss: 1.4769
Epoch [21/30], Batch [1400/6000], Loss: 1.4772
Epoch [21/30], Batch [1500/6000], Loss: 1.5871
Epoch [21/30], Batch [1600/6000], Loss: 1.4785
Epoch [21/30], Batch [1700/6000], Loss: 1.4746
Epoch [21/30], Batch [1800/6000], Loss: 1.4734
Epoch [21/30], Batch [1900/6000], Loss: 1.4820
Epoch [21/30], Batch [2000/6000], Loss: 1.4765
Epoch [21/30], Batch [2100/6000], Loss: 1.4783
Epoch [21/30], Batch [2200/6000], Loss: 1.4828
Epoch [21/30], Batch [2300/6000], Loss: 1.4755
Epoch [21/30], Batch [2400/6000], Loss: 1.4822
Epoch [21/30], Batch [2500/6000], Loss: 1.4776
Epoch [21/30], Batch [2600/6000], Loss: 1.4742
Epoch [21/30], Batch [2700/6000], Loss: 1.4770
Epoch [21/30], Batch [2800/6000], Loss: 1.4735
Epoch [21/30], Batch [2900/6000], Loss: 1.4779
Epoch [21/30], Batch [3000/6000], Loss: 1.4858
Epoch [21/30], Batch [3100/6000], Loss: 1.4790
Epoch [21/30], Batch [3200/6000], Loss: 1.4797
Epoch [21/30], Batch [3300/6000], Loss: 1.4816
Epoch [21/30], Batch [3400/6000], Loss: 1.4766
Epoch [21/30], Batch [3500/6000], Loss: 1.4766
Epoch [21/30], Batch [3600/6000], Loss: 1.5763
Epoch [21/30], Batch [3700/6000], Loss: 1.4770
Epoch [21/30], Batch [3800/6000], Loss: 1.4732
Epoch [21/30], Batch [3900/6000], Loss: 1.4717
Epoch [21/30], Batch [4000/6000], Loss: 1.4779
Epoch [21/30], Batch [4100/6000], Loss: 1.4744
Epoch [21/30], Batch [4200/6000], Loss: 1.4789
Epoch [21/30], Batch [4300/6000], Loss: 1.4802
Epoch [21/30], Batch [4400/6000], Loss: 1.4791
Epoch [21/30], Batch [4500/6000], Loss: 1.4799
Epoch [21/30], Batch [4600/6000], Loss: 1.4949
Epoch [21/30], Batch [4700/6000], Loss: 1.4783
Epoch [21/30], Batch [4800/6000], Loss: 1.4968
Epoch [21/30], Batch [4900/6000], Loss: 1.4751
Epoch [21/30], Batch [5000/6000], Loss: 1.4791
Epoch [21/30], Batch [5100/6000], Loss: 1.4801
Epoch [21/30], Batch [5200/6000], Loss: 1.4784
Epoch [21/30], Batch [5300/6000], Loss: 1.4850
Epoch [21/30], Batch [5400/6000], Loss: 1.4776
Epoch [21/30], Batch [5500/6000], Loss: 1.4757
Epoch [21/30], Batch [5600/6000], Loss: 1.4758
Epoch [21/30], Batch [5700/6000], Loss: 1.5820
Epoch [21/30], Batch [5800/6000], Loss: 1.4779
Epoch [21/30], Batch [5900/6000], Loss: 1.4795
Epoch [21/30], Loss: 1.4847
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 1.4807
Epoch [22/30], Batch [100/6000], Loss: 1.4772
Epoch [22/30], Batch [200/6000], Loss: 1.4748
Epoch [22/30], Batch [300/6000], Loss: 1.4733
Epoch [22/30], Batch [400/6000], Loss: 1.4812
Epoch [22/30], Batch [500/6000], Loss: 1.4756
Epoch [22/30], Batch [600/6000], Loss: 1.4752
Epoch [22/30], Batch [700/6000], Loss: 1.4757
Epoch [22/30], Batch [800/6000], Loss: 1.4867
Epoch [22/30], Batch [900/6000], Loss: 1.4786
Epoch [22/30], Batch [1000/6000], Loss: 1.5819
Epoch [22/30], Batch [1100/6000], Loss: 1.4828
Epoch [22/30], Batch [1200/6000], Loss: 1.4727
Epoch [22/30], Batch [1300/6000], Loss: 1.4796
Epoch [22/30], Batch [1400/6000], Loss: 1.4823
Epoch [22/30], Batch [1500/6000], Loss: 1.4726
Epoch [22/30], Batch [1600/6000], Loss: 1.4795
Epoch [22/30], Batch [1700/6000], Loss: 1.4755
Epoch [22/30], Batch [1800/6000], Loss: 1.4759
Epoch [22/30], Batch [1900/6000], Loss: 1.4758
Epoch [22/30], Batch [2000/6000], Loss: 1.4731
Epoch [22/30], Batch [2100/6000], Loss: 1.4709
Epoch [22/30], Batch [2200/6000], Loss: 1.4906
Epoch [22/30], Batch [2300/6000], Loss: 1.4790
Epoch [22/30], Batch [2400/6000], Loss: 1.4818
Epoch [22/30], Batch [2500/6000], Loss: 1.4702
Epoch [22/30], Batch [2600/6000], Loss: 1.4756
Epoch [22/30], Batch [2700/6000], Loss: 1.4802
Epoch [22/30], Batch [2800/6000], Loss: 1.4767
Epoch [22/30], Batch [2900/6000], Loss: 1.4759
Epoch [22/30], Batch [3000/6000], Loss: 1.4752
Epoch [22/30], Batch [3100/6000], Loss: 1.4790
Epoch [22/30], Batch [3200/6000], Loss: 1.4786
Epoch [22/30], Batch [3300/6000], Loss: 1.4717
Epoch [22/30], Batch [3400/6000], Loss: 1.4779
Epoch [22/30], Batch [3500/6000], Loss: 1.4817
Epoch [22/30], Batch [3600/6000], Loss: 1.4768
Epoch [22/30], Batch [3700/6000], Loss: 1.4826
Epoch [22/30], Batch [3800/6000], Loss: 1.4749
Epoch [22/30], Batch [3900/6000], Loss: 1.5740
Epoch [22/30], Batch [4000/6000], Loss: 1.4870
Epoch [22/30], Batch [4100/6000], Loss: 1.4741
Epoch [22/30], Batch [4200/6000], Loss: 1.4757
Epoch [22/30], Batch [4300/6000], Loss: 1.4731
Epoch [22/30], Batch [4400/6000], Loss: 1.4807
Epoch [22/30], Batch [4500/6000], Loss: 1.4772
Epoch [22/30], Batch [4600/6000], Loss: 1.4812
Epoch [22/30], Batch [4700/6000], Loss: 1.4752
Epoch [22/30], Batch [4800/6000], Loss: 1.4751
Epoch [22/30], Batch [4900/6000], Loss: 1.5046
Epoch [22/30], Batch [5000/6000], Loss: 1.4789
Epoch [22/30], Batch [5100/6000], Loss: 1.4751
Epoch [22/30], Batch [5200/6000], Loss: 1.4763
Epoch [22/30], Batch [5300/6000], Loss: 1.4742
Epoch [22/30], Batch [5400/6000], Loss: 1.4820
Epoch [22/30], Batch [5500/6000], Loss: 1.4768
Epoch [22/30], Batch [5600/6000], Loss: 1.4775
Epoch [22/30], Batch [5700/6000], Loss: 1.4736
Epoch [22/30], Batch [5800/6000], Loss: 1.4778
Epoch [22/30], Batch [5900/6000], Loss: 1.4771
Epoch [22/30], Loss: 1.4835
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 1.4781
Epoch [23/30], Batch [100/6000], Loss: 1.4800
Epoch [23/30], Batch [200/6000], Loss: 1.4728
Epoch [23/30], Batch [300/6000], Loss: 1.4752
Epoch [23/30], Batch [400/6000], Loss: 1.4783
Epoch [23/30], Batch [500/6000], Loss: 1.4754
Epoch [23/30], Batch [600/6000], Loss: 1.4803
Epoch [23/30], Batch [700/6000], Loss: 1.4742
Epoch [23/30], Batch [800/6000], Loss: 1.4774
Epoch [23/30], Batch [900/6000], Loss: 1.4747
Epoch [23/30], Batch [1000/6000], Loss: 1.4747
Epoch [23/30], Batch [1100/6000], Loss: 1.4782
Epoch [23/30], Batch [1200/6000], Loss: 1.5801
Epoch [23/30], Batch [1300/6000], Loss: 1.4767
Epoch [23/30], Batch [1400/6000], Loss: 1.4758
Epoch [23/30], Batch [1500/6000], Loss: 1.4875
Epoch [23/30], Batch [1600/6000], Loss: 1.4702
Epoch [23/30], Batch [1700/6000], Loss: 1.4950
Epoch [23/30], Batch [1800/6000], Loss: 1.4789
Epoch [23/30], Batch [1900/6000], Loss: 1.4791
Epoch [23/30], Batch [2000/6000], Loss: 1.4813
Epoch [23/30], Batch [2100/6000], Loss: 1.4791
Epoch [23/30], Batch [2200/6000], Loss: 1.4770
Epoch [23/30], Batch [2300/6000], Loss: 1.4748
Epoch [23/30], Batch [2400/6000], Loss: 1.4744
Epoch [23/30], Batch [2500/6000], Loss: 1.4744
Epoch [23/30], Batch [2600/6000], Loss: 1.4767
Epoch [23/30], Batch [2700/6000], Loss: 1.4703
Epoch [23/30], Batch [2800/6000], Loss: 1.4765
Epoch [23/30], Batch [2900/6000], Loss: 1.4786
Epoch [23/30], Batch [3000/6000], Loss: 1.4760
Epoch [23/30], Batch [3100/6000], Loss: 1.4769
Epoch [23/30], Batch [3200/6000], Loss: 1.4741
Epoch [23/30], Batch [3300/6000], Loss: 1.4773
Epoch [23/30], Batch [3400/6000], Loss: 1.4790
Epoch [23/30], Batch [3500/6000], Loss: 1.4733
Epoch [23/30], Batch [3600/6000], Loss: 1.4797
Epoch [23/30], Batch [3700/6000], Loss: 1.4743
Epoch [23/30], Batch [3800/6000], Loss: 1.4757
Epoch [23/30], Batch [3900/6000], Loss: 1.4811
Epoch [23/30], Batch [4000/6000], Loss: 1.4949
Epoch [23/30], Batch [4100/6000], Loss: 1.4718
Epoch [23/30], Batch [4200/6000], Loss: 1.4746
Epoch [23/30], Batch [4300/6000], Loss: 1.4901
Epoch [23/30], Batch [4400/6000], Loss: 1.5747
Epoch [23/30], Batch [4500/6000], Loss: 1.4922
Epoch [23/30], Batch [4600/6000], Loss: 1.4731
Epoch [23/30], Batch [4700/6000], Loss: 1.4725
Epoch [23/30], Batch [4800/6000], Loss: 1.4809
Epoch [23/30], Batch [4900/6000], Loss: 1.4755
Epoch [23/30], Batch [5000/6000], Loss: 1.4770
Epoch [23/30], Batch [5100/6000], Loss: 1.4745
Epoch [23/30], Batch [5200/6000], Loss: 1.4778
Epoch [23/30], Batch [5300/6000], Loss: 1.5724
Epoch [23/30], Batch [5400/6000], Loss: 1.4762
Epoch [23/30], Batch [5500/6000], Loss: 1.4744
Epoch [23/30], Batch [5600/6000], Loss: 1.4787
Epoch [23/30], Batch [5700/6000], Loss: 1.4764
Epoch [23/30], Batch [5800/6000], Loss: 1.4789
Epoch [23/30], Batch [5900/6000], Loss: 1.4751
Epoch [23/30], Loss: 1.4830
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 1.4724
Epoch [24/30], Batch [100/6000], Loss: 1.4748
Epoch [24/30], Batch [200/6000], Loss: 1.4753
Epoch [24/30], Batch [300/6000], Loss: 1.4832
Epoch [24/30], Batch [400/6000], Loss: 1.4767
Epoch [24/30], Batch [500/6000], Loss: 1.4789
Epoch [24/30], Batch [600/6000], Loss: 1.4790
Epoch [24/30], Batch [700/6000], Loss: 1.4772
Epoch [24/30], Batch [800/6000], Loss: 1.4789
Epoch [24/30], Batch [900/6000], Loss: 1.4734
Epoch [24/30], Batch [1000/6000], Loss: 1.4760
Epoch [24/30], Batch [1100/6000], Loss: 1.4717
Epoch [24/30], Batch [1200/6000], Loss: 1.4773
Epoch [24/30], Batch [1300/6000], Loss: 1.4746
Epoch [24/30], Batch [1400/6000], Loss: 1.4890
Epoch [24/30], Batch [1500/6000], Loss: 1.4754
Epoch [24/30], Batch [1600/6000], Loss: 1.5762
Epoch [24/30], Batch [1700/6000], Loss: 1.4828
Epoch [24/30], Batch [1800/6000], Loss: 1.4728
Epoch [24/30], Batch [1900/6000], Loss: 1.4776
Epoch [24/30], Batch [2000/6000], Loss: 1.4710
Epoch [24/30], Batch [2100/6000], Loss: 1.4769
Epoch [24/30], Batch [2200/6000], Loss: 1.4957
Epoch [24/30], Batch [2300/6000], Loss: 1.4733
Epoch [24/30], Batch [2400/6000], Loss: 1.4753
Epoch [24/30], Batch [2500/6000], Loss: 1.4936
Epoch [24/30], Batch [2600/6000], Loss: 1.4757
Epoch [24/30], Batch [2700/6000], Loss: 1.4727
Epoch [24/30], Batch [2800/6000], Loss: 1.4774
Epoch [24/30], Batch [2900/6000], Loss: 1.5430
Epoch [24/30], Batch [3000/6000], Loss: 1.4784
Epoch [24/30], Batch [3100/6000], Loss: 1.4752
Epoch [24/30], Batch [3200/6000], Loss: 1.4762
Epoch [24/30], Batch [3300/6000], Loss: 1.4783
Epoch [24/30], Batch [3400/6000], Loss: 1.4766
Epoch [24/30], Batch [3500/6000], Loss: 1.4792
Epoch [24/30], Batch [3600/6000], Loss: 1.4909
Epoch [24/30], Batch [3700/6000], Loss: 1.4713
Epoch [24/30], Batch [3800/6000], Loss: 1.4858
Epoch [24/30], Batch [3900/6000], Loss: 1.5029
Epoch [24/30], Batch [4000/6000], Loss: 1.4765
Epoch [24/30], Batch [4100/6000], Loss: 1.4783
Epoch [24/30], Batch [4200/6000], Loss: 1.5844
Epoch [24/30], Batch [4300/6000], Loss: 1.5159
Epoch [24/30], Batch [4400/6000], Loss: 1.4804
Epoch [24/30], Batch [4500/6000], Loss: 1.4787
Epoch [24/30], Batch [4600/6000], Loss: 1.4740
Epoch [24/30], Batch [4700/6000], Loss: 1.4857
Epoch [24/30], Batch [4800/6000], Loss: 1.4766
Epoch [24/30], Batch [4900/6000], Loss: 1.4703
Epoch [24/30], Batch [5000/6000], Loss: 1.4721
Epoch [24/30], Batch [5100/6000], Loss: 1.4742
Epoch [24/30], Batch [5200/6000], Loss: 1.4749
Epoch [24/30], Batch [5300/6000], Loss: 1.4734
Epoch [24/30], Batch [5400/6000], Loss: 1.5916
Epoch [24/30], Batch [5500/6000], Loss: 1.4797
Epoch [24/30], Batch [5600/6000], Loss: 1.4758
Epoch [24/30], Batch [5700/6000], Loss: 1.4770
Epoch [24/30], Batch [5800/6000], Loss: 1.4779
Epoch [24/30], Batch [5900/6000], Loss: 1.4717
Epoch [24/30], Loss: 1.4820
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 1.4745
Epoch [25/30], Batch [100/6000], Loss: 1.4779
Epoch [25/30], Batch [200/6000], Loss: 1.4740
Epoch [25/30], Batch [300/6000], Loss: 1.4713
Epoch [25/30], Batch [400/6000], Loss: 1.4762
Epoch [25/30], Batch [500/6000], Loss: 1.4725
Epoch [25/30], Batch [600/6000], Loss: 1.4779
Epoch [25/30], Batch [700/6000], Loss: 1.4736
Epoch [25/30], Batch [800/6000], Loss: 1.4732
Epoch [25/30], Batch [900/6000], Loss: 1.4746
Epoch [25/30], Batch [1000/6000], Loss: 1.4735
Epoch [25/30], Batch [1100/6000], Loss: 1.4803
Epoch [25/30], Batch [1200/6000], Loss: 1.4776
Epoch [25/30], Batch [1300/6000], Loss: 1.4754
Epoch [25/30], Batch [1400/6000], Loss: 1.4759
Epoch [25/30], Batch [1500/6000], Loss: 1.4765
Epoch [25/30], Batch [1600/6000], Loss: 1.4778
Epoch [25/30], Batch [1700/6000], Loss: 1.4793
Epoch [25/30], Batch [1800/6000], Loss: 1.4772
Epoch [25/30], Batch [1900/6000], Loss: 1.4764
Epoch [25/30], Batch [2000/6000], Loss: 1.4760
Epoch [25/30], Batch [2100/6000], Loss: 1.4841
Epoch [25/30], Batch [2200/6000], Loss: 1.4757
Epoch [25/30], Batch [2300/6000], Loss: 1.4755
Epoch [25/30], Batch [2400/6000], Loss: 1.4764
Epoch [25/30], Batch [2500/6000], Loss: 1.4752
Epoch [25/30], Batch [2600/6000], Loss: 1.4753
Epoch [25/30], Batch [2700/6000], Loss: 1.4705
Epoch [25/30], Batch [2800/6000], Loss: 1.4796
Epoch [25/30], Batch [2900/6000], Loss: 1.4731
Epoch [25/30], Batch [3000/6000], Loss: 1.4812
Epoch [25/30], Batch [3100/6000], Loss: 1.4772
Epoch [25/30], Batch [3200/6000], Loss: 1.4719
Epoch [25/30], Batch [3300/6000], Loss: 1.4862
Epoch [25/30], Batch [3400/6000], Loss: 1.4758
Epoch [25/30], Batch [3500/6000], Loss: 1.4745
Epoch [25/30], Batch [3600/6000], Loss: 1.4789
Epoch [25/30], Batch [3700/6000], Loss: 1.4895
Epoch [25/30], Batch [3800/6000], Loss: 1.4752
Epoch [25/30], Batch [3900/6000], Loss: 1.4764
Epoch [25/30], Batch [4000/6000], Loss: 1.4970
Epoch [25/30], Batch [4100/6000], Loss: 1.4761
Epoch [25/30], Batch [4200/6000], Loss: 1.4785
Epoch [25/30], Batch [4300/6000], Loss: 1.4772
Epoch [25/30], Batch [4400/6000], Loss: 1.4798
Epoch [25/30], Batch [4500/6000], Loss: 1.4757
Epoch [25/30], Batch [4600/6000], Loss: 1.4770
Epoch [25/30], Batch [4700/6000], Loss: 1.4743
Epoch [25/30], Batch [4800/6000], Loss: 1.4749
Epoch [25/30], Batch [4900/6000], Loss: 1.4773
Epoch [25/30], Batch [5000/6000], Loss: 1.4730
Epoch [25/30], Batch [5100/6000], Loss: 1.4745
Epoch [25/30], Batch [5200/6000], Loss: 1.4777
Epoch [25/30], Batch [5300/6000], Loss: 1.4715
Epoch [25/30], Batch [5400/6000], Loss: 1.4779
Epoch [25/30], Batch [5500/6000], Loss: 1.4768
Epoch [25/30], Batch [5600/6000], Loss: 1.4756
Epoch [25/30], Batch [5700/6000], Loss: 1.4741
Epoch [25/30], Batch [5800/6000], Loss: 1.4744
Epoch [25/30], Batch [5900/6000], Loss: 1.4774
Epoch [25/30], Loss: 1.4816
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 1.5809
Epoch [26/30], Batch [100/6000], Loss: 1.4734
Epoch [26/30], Batch [200/6000], Loss: 1.4750
Epoch [26/30], Batch [300/6000], Loss: 1.4719
Epoch [26/30], Batch [400/6000], Loss: 1.4735
Epoch [26/30], Batch [500/6000], Loss: 1.4874
Epoch [26/30], Batch [600/6000], Loss: 1.4732
Epoch [26/30], Batch [700/6000], Loss: 1.4749
Epoch [26/30], Batch [800/6000], Loss: 1.4757
Epoch [26/30], Batch [900/6000], Loss: 1.4751
Epoch [26/30], Batch [1000/6000], Loss: 1.4771
Epoch [26/30], Batch [1100/6000], Loss: 1.4733
Epoch [26/30], Batch [1200/6000], Loss: 1.4796
Epoch [26/30], Batch [1300/6000], Loss: 1.4753
Epoch [26/30], Batch [1400/6000], Loss: 1.4767
Epoch [26/30], Batch [1500/6000], Loss: 1.4799
Epoch [26/30], Batch [1600/6000], Loss: 1.4779
Epoch [26/30], Batch [1700/6000], Loss: 1.4753
Epoch [26/30], Batch [1800/6000], Loss: 1.4769
Epoch [26/30], Batch [1900/6000], Loss: 1.5579
Epoch [26/30], Batch [2000/6000], Loss: 1.4757
Epoch [26/30], Batch [2100/6000], Loss: 1.4750
Epoch [26/30], Batch [2200/6000], Loss: 1.4744
Epoch [26/30], Batch [2300/6000], Loss: 1.4759
Epoch [26/30], Batch [2400/6000], Loss: 1.5725
Epoch [26/30], Batch [2500/6000], Loss: 1.4752
Epoch [26/30], Batch [2600/6000], Loss: 1.4730
Epoch [26/30], Batch [2700/6000], Loss: 1.4756
Epoch [26/30], Batch [2800/6000], Loss: 1.4743
Epoch [26/30], Batch [2900/6000], Loss: 1.5762
Epoch [26/30], Batch [3000/6000], Loss: 1.4804
Epoch [26/30], Batch [3100/6000], Loss: 1.4765
Epoch [26/30], Batch [3200/6000], Loss: 1.4769
Epoch [26/30], Batch [3300/6000], Loss: 1.4730
Epoch [26/30], Batch [3400/6000], Loss: 1.4758
Epoch [26/30], Batch [3500/6000], Loss: 1.4749
Epoch [26/30], Batch [3600/6000], Loss: 1.4766
Epoch [26/30], Batch [3700/6000], Loss: 1.4754
Epoch [26/30], Batch [3800/6000], Loss: 1.4743
Epoch [26/30], Batch [3900/6000], Loss: 1.4730
Epoch [26/30], Batch [4000/6000], Loss: 1.4742
Epoch [26/30], Batch [4100/6000], Loss: 1.4843
Epoch [26/30], Batch [4200/6000], Loss: 1.4760
Epoch [26/30], Batch [4300/6000], Loss: 1.4782
Epoch [26/30], Batch [4400/6000], Loss: 1.4725
Epoch [26/30], Batch [4500/6000], Loss: 1.4804
Epoch [26/30], Batch [4600/6000], Loss: 1.4729
Epoch [26/30], Batch [4700/6000], Loss: 1.4737
Epoch [26/30], Batch [4800/6000], Loss: 1.4731
Epoch [26/30], Batch [4900/6000], Loss: 1.4709
Epoch [26/30], Batch [5000/6000], Loss: 1.4775
Epoch [26/30], Batch [5100/6000], Loss: 1.4717
Epoch [26/30], Batch [5200/6000], Loss: 1.4787
Epoch [26/30], Batch [5300/6000], Loss: 1.4753
Epoch [26/30], Batch [5400/6000], Loss: 1.4723
Epoch [26/30], Batch [5500/6000], Loss: 1.4803
Epoch [26/30], Batch [5600/6000], Loss: 1.4740
Epoch [26/30], Batch [5700/6000], Loss: 1.4766
Epoch [26/30], Batch [5800/6000], Loss: 1.4725
Epoch [26/30], Batch [5900/6000], Loss: 1.5770
Epoch [26/30], Loss: 1.4811
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 1.4923
Epoch [27/30], Batch [100/6000], Loss: 1.4734
Epoch [27/30], Batch [200/6000], Loss: 1.4731
Epoch [27/30], Batch [300/6000], Loss: 1.4762
Epoch [27/30], Batch [400/6000], Loss: 1.4790
Epoch [27/30], Batch [500/6000], Loss: 1.4770
Epoch [27/30], Batch [600/6000], Loss: 1.4752
Epoch [27/30], Batch [700/6000], Loss: 1.4938
Epoch [27/30], Batch [800/6000], Loss: 1.4760
Epoch [27/30], Batch [900/6000], Loss: 1.4787
Epoch [27/30], Batch [1000/6000], Loss: 1.4795
Epoch [27/30], Batch [1100/6000], Loss: 1.4813
Epoch [27/30], Batch [1200/6000], Loss: 1.4752
Epoch [27/30], Batch [1300/6000], Loss: 1.4763
Epoch [27/30], Batch [1400/6000], Loss: 1.4785
Epoch [27/30], Batch [1500/6000], Loss: 1.4761
Epoch [27/30], Batch [1600/6000], Loss: 1.4739
Epoch [27/30], Batch [1700/6000], Loss: 1.4770
Epoch [27/30], Batch [1800/6000], Loss: 1.4770
Epoch [27/30], Batch [1900/6000], Loss: 1.4735
Epoch [27/30], Batch [2000/6000], Loss: 1.4715
Epoch [27/30], Batch [2100/6000], Loss: 1.4758
Epoch [27/30], Batch [2200/6000], Loss: 1.4731
Epoch [27/30], Batch [2300/6000], Loss: 1.4730
Epoch [27/30], Batch [2400/6000], Loss: 1.4747
Epoch [27/30], Batch [2500/6000], Loss: 1.4753
Epoch [27/30], Batch [2600/6000], Loss: 1.4764
Epoch [27/30], Batch [2700/6000], Loss: 1.4779
Epoch [27/30], Batch [2800/6000], Loss: 1.4733
Epoch [27/30], Batch [2900/6000], Loss: 1.4755
Epoch [27/30], Batch [3000/6000], Loss: 1.4764
Epoch [27/30], Batch [3100/6000], Loss: 1.4883
Epoch [27/30], Batch [3200/6000], Loss: 1.4905
Epoch [27/30], Batch [3300/6000], Loss: 1.4758
Epoch [27/30], Batch [3400/6000], Loss: 1.4761
Epoch [27/30], Batch [3500/6000], Loss: 1.4779
Epoch [27/30], Batch [3600/6000], Loss: 1.4745
Epoch [27/30], Batch [3700/6000], Loss: 1.4714
Epoch [27/30], Batch [3800/6000], Loss: 1.4709
Epoch [27/30], Batch [3900/6000], Loss: 1.4744
Epoch [27/30], Batch [4000/6000], Loss: 1.4759
Epoch [27/30], Batch [4100/6000], Loss: 1.4768
Epoch [27/30], Batch [4200/6000], Loss: 1.4720
Epoch [27/30], Batch [4300/6000], Loss: 1.4748
Epoch [27/30], Batch [4400/6000], Loss: 1.4815
Epoch [27/30], Batch [4500/6000], Loss: 1.4743
Epoch [27/30], Batch [4600/6000], Loss: 1.4753
Epoch [27/30], Batch [4700/6000], Loss: 1.4772
Epoch [27/30], Batch [4800/6000], Loss: 1.4762
Epoch [27/30], Batch [4900/6000], Loss: 1.4748
Epoch [27/30], Batch [5000/6000], Loss: 1.4765
Epoch [27/30], Batch [5100/6000], Loss: 1.4852
Epoch [27/30], Batch [5200/6000], Loss: 1.4722
Epoch [27/30], Batch [5300/6000], Loss: 1.4737
Epoch [27/30], Batch [5400/6000], Loss: 1.4773
Epoch [27/30], Batch [5500/6000], Loss: 1.4908
Epoch [27/30], Batch [5600/6000], Loss: 1.4725
Epoch [27/30], Batch [5700/6000], Loss: 1.4736
Epoch [27/30], Batch [5800/6000], Loss: 1.5794
Epoch [27/30], Batch [5900/6000], Loss: 1.4773
Epoch [27/30], Loss: 1.4804
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 1.4770
Epoch [28/30], Batch [100/6000], Loss: 1.4739
Epoch [28/30], Batch [200/6000], Loss: 1.5199
Epoch [28/30], Batch [300/6000], Loss: 1.4762
Epoch [28/30], Batch [400/6000], Loss: 1.4780
Epoch [28/30], Batch [500/6000], Loss: 1.4751
Epoch [28/30], Batch [600/6000], Loss: 1.4870
Epoch [28/30], Batch [700/6000], Loss: 1.4719
Epoch [28/30], Batch [800/6000], Loss: 1.4959
Epoch [28/30], Batch [900/6000], Loss: 1.4705
Epoch [28/30], Batch [1000/6000], Loss: 1.4761
Epoch [28/30], Batch [1100/6000], Loss: 1.4785
Epoch [28/30], Batch [1200/6000], Loss: 1.4790
Epoch [28/30], Batch [1300/6000], Loss: 1.4770
Epoch [28/30], Batch [1400/6000], Loss: 1.4756
Epoch [28/30], Batch [1500/6000], Loss: 1.4731
Epoch [28/30], Batch [1600/6000], Loss: 1.4702
Epoch [28/30], Batch [1700/6000], Loss: 1.4780
Epoch [28/30], Batch [1800/6000], Loss: 1.4737
Epoch [28/30], Batch [1900/6000], Loss: 1.4774
Epoch [28/30], Batch [2000/6000], Loss: 1.4788
Epoch [28/30], Batch [2100/6000], Loss: 1.4759
Epoch [28/30], Batch [2200/6000], Loss: 1.4813
Epoch [28/30], Batch [2300/6000], Loss: 1.4879
Epoch [28/30], Batch [2400/6000], Loss: 1.5002
Epoch [28/30], Batch [2500/6000], Loss: 1.4754
Epoch [28/30], Batch [2600/6000], Loss: 1.4732
Epoch [28/30], Batch [2700/6000], Loss: 1.4765
Epoch [28/30], Batch [2800/6000], Loss: 1.4765
Epoch [28/30], Batch [2900/6000], Loss: 1.4787
Epoch [28/30], Batch [3000/6000], Loss: 1.4719
Epoch [28/30], Batch [3100/6000], Loss: 1.4796
Epoch [28/30], Batch [3200/6000], Loss: 1.5735
Epoch [28/30], Batch [3300/6000], Loss: 1.4708
Epoch [28/30], Batch [3400/6000], Loss: 1.4745
Epoch [28/30], Batch [3500/6000], Loss: 1.4739
Epoch [28/30], Batch [3600/6000], Loss: 1.4749
Epoch [28/30], Batch [3700/6000], Loss: 1.4786
Epoch [28/30], Batch [3800/6000], Loss: 1.4779
Epoch [28/30], Batch [3900/6000], Loss: 1.4725
Epoch [28/30], Batch [4000/6000], Loss: 1.4736
Epoch [28/30], Batch [4100/6000], Loss: 1.4707
Epoch [28/30], Batch [4200/6000], Loss: 1.4723
Epoch [28/30], Batch [4300/6000], Loss: 1.4763
Epoch [28/30], Batch [4400/6000], Loss: 1.4713
Epoch [28/30], Batch [4500/6000], Loss: 1.4740
Epoch [28/30], Batch [4600/6000], Loss: 1.4752
Epoch [28/30], Batch [4700/6000], Loss: 1.4768
Epoch [28/30], Batch [4800/6000], Loss: 1.4737
Epoch [28/30], Batch [4900/6000], Loss: 1.4763
Epoch [28/30], Batch [5000/6000], Loss: 1.4721
Epoch [28/30], Batch [5100/6000], Loss: 1.5757
Epoch [28/30], Batch [5200/6000], Loss: 1.4750
Epoch [28/30], Batch [5300/6000], Loss: 1.4708
Epoch [28/30], Batch [5400/6000], Loss: 1.4768
Epoch [28/30], Batch [5500/6000], Loss: 1.4758
Epoch [28/30], Batch [5600/6000], Loss: 1.4742
Epoch [28/30], Batch [5700/6000], Loss: 1.4714
Epoch [28/30], Batch [5800/6000], Loss: 1.4767
Epoch [28/30], Batch [5900/6000], Loss: 1.4730
Epoch [28/30], Loss: 1.4802
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 1.4740
Epoch [29/30], Batch [100/6000], Loss: 1.4699
Epoch [29/30], Batch [200/6000], Loss: 1.4758
Epoch [29/30], Batch [300/6000], Loss: 1.4732
Epoch [29/30], Batch [400/6000], Loss: 1.4737
Epoch [29/30], Batch [500/6000], Loss: 1.4745
Epoch [29/30], Batch [600/6000], Loss: 1.4753
Epoch [29/30], Batch [700/6000], Loss: 1.4722
Epoch [29/30], Batch [800/6000], Loss: 1.4737
Epoch [29/30], Batch [900/6000], Loss: 1.4729
Epoch [29/30], Batch [1000/6000], Loss: 1.4729
Epoch [29/30], Batch [1100/6000], Loss: 1.4730
Epoch [29/30], Batch [1200/6000], Loss: 1.4742
Epoch [29/30], Batch [1300/6000], Loss: 1.4760
Epoch [29/30], Batch [1400/6000], Loss: 1.4716
Epoch [29/30], Batch [1500/6000], Loss: 1.4774
Epoch [29/30], Batch [1600/6000], Loss: 1.5007
Epoch [29/30], Batch [1700/6000], Loss: 1.4914
Epoch [29/30], Batch [1800/6000], Loss: 1.4726
Epoch [29/30], Batch [1900/6000], Loss: 1.4751
Epoch [29/30], Batch [2000/6000], Loss: 1.4731
Epoch [29/30], Batch [2100/6000], Loss: 1.4736
Epoch [29/30], Batch [2200/6000], Loss: 1.4716
Epoch [29/30], Batch [2300/6000], Loss: 1.4791
Epoch [29/30], Batch [2400/6000], Loss: 1.4760
Epoch [29/30], Batch [2500/6000], Loss: 1.4747
Epoch [29/30], Batch [2600/6000], Loss: 1.4783
Epoch [29/30], Batch [2700/6000], Loss: 1.4744
Epoch [29/30], Batch [2800/6000], Loss: 1.4748
Epoch [29/30], Batch [2900/6000], Loss: 1.4764
Epoch [29/30], Batch [3000/6000], Loss: 1.4744
Epoch [29/30], Batch [3100/6000], Loss: 1.4749
Epoch [29/30], Batch [3200/6000], Loss: 1.4762
Epoch [29/30], Batch [3300/6000], Loss: 1.4727
Epoch [29/30], Batch [3400/6000], Loss: 1.4761
Epoch [29/30], Batch [3500/6000], Loss: 1.5728
Epoch [29/30], Batch [3600/6000], Loss: 1.4741
Epoch [29/30], Batch [3700/6000], Loss: 1.4753
Epoch [29/30], Batch [3800/6000], Loss: 1.4730
Epoch [29/30], Batch [3900/6000], Loss: 1.4756
Epoch [29/30], Batch [4000/6000], Loss: 1.4728
Epoch [29/30], Batch [4100/6000], Loss: 1.4742
Epoch [29/30], Batch [4200/6000], Loss: 1.4748
Epoch [29/30], Batch [4300/6000], Loss: 1.5774
Epoch [29/30], Batch [4400/6000], Loss: 1.4734
Epoch [29/30], Batch [4500/6000], Loss: 1.4750
Epoch [29/30], Batch [4600/6000], Loss: 1.5818
Epoch [29/30], Batch [4700/6000], Loss: 1.4726
Epoch [29/30], Batch [4800/6000], Loss: 1.4821
Epoch [29/30], Batch [4900/6000], Loss: 1.5256
Epoch [29/30], Batch [5000/6000], Loss: 1.4742
Epoch [29/30], Batch [5100/6000], Loss: 1.4732
Epoch [29/30], Batch [5200/6000], Loss: 1.4774
Epoch [29/30], Batch [5300/6000], Loss: 1.4748
Epoch [29/30], Batch [5400/6000], Loss: 1.4744
Epoch [29/30], Batch [5500/6000], Loss: 1.4710
Epoch [29/30], Batch [5600/6000], Loss: 1.4725
Epoch [29/30], Batch [5700/6000], Loss: 1.4735
Epoch [29/30], Batch [5800/6000], Loss: 1.4747
Epoch [29/30], Batch [5900/6000], Loss: 1.4756
Epoch [29/30], Loss: 1.4795
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 1.4734
Epoch [30/30], Batch [100/6000], Loss: 1.4760
Epoch [30/30], Batch [200/6000], Loss: 1.4702
Epoch [30/30], Batch [300/6000], Loss: 1.4749
Epoch [30/30], Batch [400/6000], Loss: 1.4744
Epoch [30/30], Batch [500/6000], Loss: 1.4740
Epoch [30/30], Batch [600/6000], Loss: 1.4742
Epoch [30/30], Batch [700/6000], Loss: 1.4736
Epoch [30/30], Batch [800/6000], Loss: 1.5740
Epoch [30/30], Batch [900/6000], Loss: 1.4748
Epoch [30/30], Batch [1000/6000], Loss: 1.4732
Epoch [30/30], Batch [1100/6000], Loss: 1.4743
Epoch [30/30], Batch [1200/6000], Loss: 1.4756
Epoch [30/30], Batch [1300/6000], Loss: 1.4710
Epoch [30/30], Batch [1400/6000], Loss: 1.4736
Epoch [30/30], Batch [1500/6000], Loss: 1.4736
Epoch [30/30], Batch [1600/6000], Loss: 1.4746
Epoch [30/30], Batch [1700/6000], Loss: 1.4768
Epoch [30/30], Batch [1800/6000], Loss: 1.4857
Epoch [30/30], Batch [1900/6000], Loss: 1.4740
Epoch [30/30], Batch [2000/6000], Loss: 1.5730
Epoch [30/30], Batch [2100/6000], Loss: 1.4744
Epoch [30/30], Batch [2200/6000], Loss: 1.4779
Epoch [30/30], Batch [2300/6000], Loss: 1.4726
Epoch [30/30], Batch [2400/6000], Loss: 1.4721
Epoch [30/30], Batch [2500/6000], Loss: 1.4717
Epoch [30/30], Batch [2600/6000], Loss: 1.4759
Epoch [30/30], Batch [2700/6000], Loss: 1.4733
Epoch [30/30], Batch [2800/6000], Loss: 1.4743
Epoch [30/30], Batch [2900/6000], Loss: 1.4711
Epoch [30/30], Batch [3000/6000], Loss: 1.4884
Epoch [30/30], Batch [3100/6000], Loss: 1.4710
Epoch [30/30], Batch [3200/6000], Loss: 1.4732
Epoch [30/30], Batch [3300/6000], Loss: 1.4713
Epoch [30/30], Batch [3400/6000], Loss: 1.4754
Epoch [30/30], Batch [3500/6000], Loss: 1.4784
Epoch [30/30], Batch [3600/6000], Loss: 1.4718
Epoch [30/30], Batch [3700/6000], Loss: 1.4727
Epoch [30/30], Batch [3800/6000], Loss: 1.4749
Epoch [30/30], Batch [3900/6000], Loss: 1.4734
Epoch [30/30], Batch [4000/6000], Loss: 1.4743
Epoch [30/30], Batch [4100/6000], Loss: 1.4744
Epoch [30/30], Batch [4200/6000], Loss: 1.4752
Epoch [30/30], Batch [4300/6000], Loss: 1.4699
Epoch [30/30], Batch [4400/6000], Loss: 1.4805
Epoch [30/30], Batch [4500/6000], Loss: 1.4727
Epoch [30/30], Batch [4600/6000], Loss: 1.4727
Epoch [30/30], Batch [4700/6000], Loss: 1.4705
Epoch [30/30], Batch [4800/6000], Loss: 1.4757
Epoch [30/30], Batch [4900/6000], Loss: 1.4720
Epoch [30/30], Batch [5000/6000], Loss: 1.4861
Epoch [30/30], Batch [5100/6000], Loss: 1.4745
Epoch [30/30], Batch [5200/6000], Loss: 1.4722
Epoch [30/30], Batch [5300/6000], Loss: 1.4747
Epoch [30/30], Batch [5400/6000], Loss: 1.4754
Epoch [30/30], Batch [5500/6000], Loss: 1.4716
Epoch [30/30], Batch [5600/6000], Loss: 1.4778
Epoch [30/30], Batch [5700/6000], Loss: 1.4711
Epoch [30/30], Batch [5800/6000], Loss: 1.4769
Epoch [30/30], Batch [5900/6000], Loss: 1.5733
Epoch [30/30], Loss: 1.4791
Visualization saved to figures/visualization_0.png
Test Loss: 1.4930, Accuracy: 97.81%
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 1/300:
  Label Loss: 0.0302
  Image Loss: 0.0098
  Total Loss: 30.2143
  Image grad max: 0.0019396285060793161
  Output probs: [[0.194 0.075 0.204 0.075 0.075 0.075 0.075 0.075 0.075 0.075]]
Adversarial Training Loop 2/300:
  Label Loss: 0.0345
  Image Loss: 0.2792
  Total Loss: 34.8110
  Image grad max: 1.4096291065216064
  Output probs: [[0.074 0.202 0.202 0.074 0.074 0.074 0.074 0.074 0.074 0.074]]
Adversarial Training Loop 3/300:
  Label Loss: 0.0349
  Image Loss: 0.3462
  Total Loss: 35.2906
  Image grad max: 0.005293041467666626
  Output probs: [[0.085 0.232 0.086 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 4/300:
  Label Loss: 0.0448
  Image Loss: 0.4777
  Total Loss: 45.2946
  Image grad max: 0.2338612973690033
  Output probs: [[0.074 0.202 0.202 0.074 0.074 0.074 0.074 0.074 0.074 0.074]]
Adversarial Training Loop 5/300:
  Label Loss: 0.0349
  Image Loss: 0.4636
  Total Loss: 35.4073
  Image grad max: 0.002567410934716463
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 6/300:
  Label Loss: 0.0302
  Image Loss: 0.4612
  Total Loss: 30.6657
  Image grad max: 0.0026024056132882833
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 7/300:
  Label Loss: 0.0302
  Image Loss: 0.4607
  Total Loss: 30.6652
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 8/300:
  Label Loss: 0.0302
  Image Loss: 0.4606
  Total Loss: 30.6651
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 9/300:
  Label Loss: 0.0302
  Image Loss: 0.4605
  Total Loss: 30.6650
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 10/300:
  Label Loss: 0.0302
  Image Loss: 0.4599
  Total Loss: 30.6644
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0302
  Image Loss: 0.4582
  Total Loss: 30.6627
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 12/300:
  Label Loss: 0.0302
  Image Loss: 0.4557
  Total Loss: 30.6602
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0302
  Image Loss: 0.4529
  Total Loss: 30.6574
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0302
  Image Loss: 0.4501
  Total Loss: 30.6546
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0302
  Image Loss: 0.4472
  Total Loss: 30.6517
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0302
  Image Loss: 0.4440
  Total Loss: 30.6485
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0302
  Image Loss: 0.4406
  Total Loss: 30.6450
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0302
  Image Loss: 0.4368
  Total Loss: 30.6413
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0302
  Image Loss: 0.4325
  Total Loss: 30.6370
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0302
  Image Loss: 0.4276
  Total Loss: 30.6321
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0302
  Image Loss: 0.4220
  Total Loss: 30.6264
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0302
  Image Loss: 0.4157
  Total Loss: 30.6202
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0302
  Image Loss: 0.4089
  Total Loss: 30.6134
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0302
  Image Loss: 0.4015
  Total Loss: 30.6059
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0302
  Image Loss: 0.3932
  Total Loss: 30.5977
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0302
  Image Loss: 0.3842
  Total Loss: 30.5887
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0302
  Image Loss: 0.3742
  Total Loss: 30.5787
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0302
  Image Loss: 0.3635
  Total Loss: 30.5680
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0302
  Image Loss: 0.3520
  Total Loss: 30.5565
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0302
  Image Loss: 0.3399
  Total Loss: 30.5444
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0302
  Image Loss: 0.3273
  Total Loss: 30.5318
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0302
  Image Loss: 0.3142
  Total Loss: 30.5187
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0302
  Image Loss: 0.3008
  Total Loss: 30.5053
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0302
  Image Loss: 0.2871
  Total Loss: 30.4916
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0302
  Image Loss: 0.2732
  Total Loss: 30.4777
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0302
  Image Loss: 0.2591
  Total Loss: 30.4636
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0302
  Image Loss: 0.2451
  Total Loss: 30.4496
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0302
  Image Loss: 0.2312
  Total Loss: 30.4357
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0302
  Image Loss: 0.2176
  Total Loss: 30.4221
  Image grad max: 0.0025493281427770853
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0302
  Image Loss: 0.2044
  Total Loss: 30.4089
  Image grad max: 0.0025439122691750526
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0302
  Image Loss: 0.1916
  Total Loss: 30.3961
  Image grad max: 0.0025350721552968025
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0302
  Image Loss: 0.1793
  Total Loss: 30.3838
  Image grad max: 0.0025230864994227886
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0302
  Image Loss: 0.1676
  Total Loss: 30.3721
  Image grad max: 0.0025082153733819723
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0302
  Image Loss: 0.1564
  Total Loss: 30.3609
  Image grad max: 0.0024907030165195465
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0302
  Image Loss: 0.1459
  Total Loss: 30.3503
  Image grad max: 0.002470782259479165
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0302
  Image Loss: 0.1359
  Total Loss: 30.3404
  Image grad max: 0.002448702696710825
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0302
  Image Loss: 0.1265
  Total Loss: 30.3310
  Image grad max: 0.0024248489644378424
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0302
  Image Loss: 0.1176
  Total Loss: 30.3221
  Image grad max: 0.002400362165644765
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0302
  Image Loss: 0.1093
  Total Loss: 30.3138
  Image grad max: 0.0023776753805577755
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0302
  Image Loss: 0.1014
  Total Loss: 30.3059
  Image grad max: 0.0023538151290267706
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0302
  Image Loss: 0.0940
  Total Loss: 30.2985
  Image grad max: 0.002329894108697772
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0302
  Image Loss: 0.0871
  Total Loss: 30.2916
  Image grad max: 0.002306983107700944
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0302
  Image Loss: 0.0807
  Total Loss: 30.2852
  Image grad max: 0.0022826867643743753
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0302
  Image Loss: 0.0746
  Total Loss: 30.2791
  Image grad max: 0.00225633243098855
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0302
  Image Loss: 0.0690
  Total Loss: 30.2735
  Image grad max: 0.002229328267276287
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0302
  Image Loss: 0.0637
  Total Loss: 30.2682
  Image grad max: 0.0021928499918431044
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0302
  Image Loss: 0.0588
  Total Loss: 30.2633
  Image grad max: 0.002158562419936061
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0302
  Image Loss: 0.0542
  Total Loss: 30.2587
  Image grad max: 0.002122153528034687
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0302
  Image Loss: 0.0499
  Total Loss: 30.2544
  Image grad max: 0.0020873441826552153
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0302
  Image Loss: 0.0460
  Total Loss: 30.2505
  Image grad max: 0.0020542407874017954
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0302
  Image Loss: 0.0423
  Total Loss: 30.2468
  Image grad max: 0.0020208784844726324
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0302
  Image Loss: 0.0388
  Total Loss: 30.2433
  Image grad max: 0.0019873965065926313
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0302
  Image Loss: 0.0357
  Total Loss: 30.2402
  Image grad max: 0.0019537657499313354
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0302
  Image Loss: 0.0327
  Total Loss: 30.2372
  Image grad max: 0.0019184885313734412
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0302
  Image Loss: 0.0300
  Total Loss: 30.2345
  Image grad max: 0.0018296504858881235
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0302
  Image Loss: 0.0275
  Total Loss: 30.2361
  Image grad max: 0.05135089159011841
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0302
  Image Loss: 0.0286
  Total Loss: 30.2331
  Image grad max: 0.0018259566277265549
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0302
  Image Loss: 0.0319
  Total Loss: 30.2364
  Image grad max: 0.0020913209300488234
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0302
  Image Loss: 0.0348
  Total Loss: 30.2393
  Image grad max: 0.0020913209300488234
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0302
  Image Loss: 0.0374
  Total Loss: 30.2418
  Image grad max: 0.0022907399106770754
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0302
  Image Loss: 0.0395
  Total Loss: 30.2440
  Image grad max: 0.0025446179788559675
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0302
  Image Loss: 0.0409
  Total Loss: 30.2454
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0302
  Image Loss: 0.0415
  Total Loss: 30.2460
  Image grad max: 0.0025473223067820072
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0302
  Image Loss: 0.0412
  Total Loss: 30.2457
  Image grad max: 0.0024319090880453587
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0302
  Image Loss: 0.0403
  Total Loss: 30.2447
  Image grad max: 0.0022938342299312353
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0302
  Image Loss: 0.0388
  Total Loss: 30.2433
  Image grad max: 0.0021036742255091667
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0302
  Image Loss: 0.0369
  Total Loss: 30.2414
  Image grad max: 0.0021541903261095285
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0302
  Image Loss: 0.0347
  Total Loss: 30.2392
  Image grad max: 0.0021834622602909803
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0302
  Image Loss: 0.0324
  Total Loss: 30.2369
  Image grad max: 0.002192034153267741
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0302
  Image Loss: 0.0300
  Total Loss: 30.2345
  Image grad max: 0.002181662479415536
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0302
  Image Loss: 0.0276
  Total Loss: 30.2321
  Image grad max: 0.0021540981251746416
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0302
  Image Loss: 0.0253
  Total Loss: 30.2298
  Image grad max: 0.0021110759116709232
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0302
  Image Loss: 0.0230
  Total Loss: 30.2275
  Image grad max: 0.002054302953183651
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0302
  Image Loss: 0.0209
  Total Loss: 30.2254
  Image grad max: 0.0019854491110891104
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0302
  Image Loss: 0.0188
  Total Loss: 30.2233
  Image grad max: 0.0019061386119574308
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0302
  Image Loss: 0.0169
  Total Loss: 30.2214
  Image grad max: 0.0018179401522502303
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0302
  Image Loss: 0.0152
  Total Loss: 30.2197
  Image grad max: 0.0017223614268004894
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0302
  Image Loss: 0.0136
  Total Loss: 30.2181
  Image grad max: 0.0016208424931392074
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0302
  Image Loss: 0.0121
  Total Loss: 30.2166
  Image grad max: 0.0015147498343139887
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0302
  Image Loss: 0.0109
  Total Loss: 30.2154
  Image grad max: 0.001405436429195106
  Output probs: [[0.085 0.085 0.232 0.085 0.085 0.085 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0302
  Image Loss: 0.0097
  Total Loss: 30.2142
  Image grad max: 0.001667830510996282
  Output probs: [[0.081 0.081 0.22  0.081 0.081 0.081 0.131 0.081 0.081 0.081]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0267
  Image Loss: 0.0087
  Total Loss: 26.6988
  Image grad max: 31.121450424194336
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0449
  Image Loss: 0.3684
  Total Loss: 45.2361
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0449
  Image Loss: 0.4220
  Total Loss: 45.2897
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0449
  Image Loss: 0.4249
  Total Loss: 45.2927
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0449
  Image Loss: 0.4265
  Total Loss: 45.2942
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0449
  Image Loss: 0.4277
  Total Loss: 45.2955
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0449
  Image Loss: 0.4284
  Total Loss: 45.2961
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0449
  Image Loss: 0.4289
  Total Loss: 45.2967
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0449
  Image Loss: 0.4291
  Total Loss: 45.2968
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0449
  Image Loss: 0.4292
  Total Loss: 45.2969
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0449
  Image Loss: 0.4293
  Total Loss: 45.2970
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0449
  Image Loss: 0.4293
  Total Loss: 45.2971
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0449
  Image Loss: 0.4293
  Total Loss: 45.2971
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0449
  Image Loss: 0.4292
  Total Loss: 45.2970
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0449
  Image Loss: 0.4290
  Total Loss: 45.2968
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0449
  Image Loss: 0.4288
  Total Loss: 45.2965
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0449
  Image Loss: 0.4284
  Total Loss: 45.2961
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0449
  Image Loss: 0.4279
  Total Loss: 45.2956
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0449
  Image Loss: 0.4273
  Total Loss: 45.2950
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0449
  Image Loss: 0.4267
  Total Loss: 45.2944
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0449
  Image Loss: 0.4260
  Total Loss: 45.2937
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0449
  Image Loss: 0.4251
  Total Loss: 45.2929
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0449
  Image Loss: 0.4242
  Total Loss: 45.2920
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0449
  Image Loss: 0.4233
  Total Loss: 45.2910
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0449
  Image Loss: 0.4223
  Total Loss: 45.2900
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0449
  Image Loss: 0.4213
  Total Loss: 45.2890
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0449
  Image Loss: 0.4202
  Total Loss: 45.2880
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0449
  Image Loss: 0.4192
  Total Loss: 45.2869
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0449
  Image Loss: 0.4181
  Total Loss: 45.2859
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0449
  Image Loss: 0.4170
  Total Loss: 45.2848
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0449
  Image Loss: 0.4159
  Total Loss: 45.2836
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0449
  Image Loss: 0.4147
  Total Loss: 45.2825
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0449
  Image Loss: 0.4135
  Total Loss: 45.2813
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0449
  Image Loss: 0.4123
  Total Loss: 45.2800
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0449
  Image Loss: 0.4109
  Total Loss: 45.2787
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0449
  Image Loss: 0.4095
  Total Loss: 45.2773
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0449
  Image Loss: 0.4081
  Total Loss: 45.2758
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0449
  Image Loss: 0.4065
  Total Loss: 45.2742
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0449
  Image Loss: 0.4048
  Total Loss: 45.2726
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0449
  Image Loss: 0.4031
  Total Loss: 45.2708
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0449
  Image Loss: 0.4012
  Total Loss: 45.2690
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0449
  Image Loss: 0.3992
  Total Loss: 45.2670
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0449
  Image Loss: 0.3971
  Total Loss: 45.2648
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0449
  Image Loss: 0.3948
  Total Loss: 45.2626
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0449
  Image Loss: 0.3924
  Total Loss: 45.2601
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0449
  Image Loss: 0.3898
  Total Loss: 45.2575
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0449
  Image Loss: 0.3870
  Total Loss: 45.2548
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0449
  Image Loss: 0.3842
  Total Loss: 45.2519
  Image grad max: 0.0025510205887258053
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0449
  Image Loss: 0.3812
  Total Loss: 45.2490
  Image grad max: 0.0025510205887258053
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0449
  Image Loss: 0.3781
  Total Loss: 45.2458
  Image grad max: 0.0025510210543870926
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0449
  Image Loss: 0.3749
  Total Loss: 45.2426
  Image grad max: 0.00255102152004838
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0449
  Image Loss: 0.3715
  Total Loss: 45.2392
  Image grad max: 0.0025510231498628855
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0449
  Image Loss: 0.3680
  Total Loss: 45.2358
  Image grad max: 0.0025510259438306093
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0449
  Image Loss: 0.3644
  Total Loss: 45.2322
  Image grad max: 0.0025510319974273443
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0449
  Image Loss: 0.3607
  Total Loss: 45.2285
  Image grad max: 0.0025510452687740326
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0449
  Image Loss: 0.3570
  Total Loss: 45.2247
  Image grad max: 0.00255106994882226
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0449
  Image Loss: 0.3531
  Total Loss: 45.2209
  Image grad max: 0.0025511158164590597
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0449
  Image Loss: 0.3492
  Total Loss: 45.2169
  Image grad max: 0.0025511831045150757
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0449
  Image Loss: 0.3452
  Total Loss: 45.2129
  Image grad max: 0.0025513528380542994
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0449
  Image Loss: 0.3411
  Total Loss: 45.2089
  Image grad max: 0.002551700919866562
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0449
  Image Loss: 0.3370
  Total Loss: 45.2048
  Image grad max: 0.0025523300282657146
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0449
  Image Loss: 0.3329
  Total Loss: 45.2007
  Image grad max: 0.0025536513421684504
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0449
  Image Loss: 0.3288
  Total Loss: 45.1965
  Image grad max: 0.0025560036301612854
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0449
  Image Loss: 0.3246
  Total Loss: 45.1923
  Image grad max: 0.0025601338129490614
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0449
  Image Loss: 0.3204
  Total Loss: 45.1881
  Image grad max: 0.0025691636838018894
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0449
  Image Loss: 0.3162
  Total Loss: 45.1839
  Image grad max: 0.0025874648708850145
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0449
  Image Loss: 0.3120
  Total Loss: 45.1798
  Image grad max: 0.002625079592689872
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0449
  Image Loss: 0.3079
  Total Loss: 45.1756
  Image grad max: 0.002710598986595869
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0449
  Image Loss: 0.3037
  Total Loss: 45.1714
  Image grad max: 0.002902725012972951
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0449
  Image Loss: 0.2996
  Total Loss: 45.1672
  Image grad max: 0.0033388908486813307
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0449
  Image Loss: 0.2955
  Total Loss: 45.1630
  Image grad max: 0.004457914270460606
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0449
  Image Loss: 0.2914
  Total Loss: 45.1583
  Image grad max: 0.008239846676588058
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085 0.085]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0449
  Image Loss: 0.2874
  Total Loss: 45.1494
  Image grad max: 0.041276782751083374
  Output probs: [[0.081 0.081 0.081 0.081 0.081 0.221 0.129 0.081 0.081 0.081]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0408
  Image Loss: 0.2831
  Total Loss: 41.1035
  Image grad max: 14.767641067504883
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0302
  Image Loss: 0.3311
  Total Loss: 30.5356
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0302
  Image Loss: 0.3656
  Total Loss: 30.5701
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0302
  Image Loss: 0.3739
  Total Loss: 30.5784
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0302
  Image Loss: 0.3784
  Total Loss: 30.5829
  Image grad max: 0.0025520871859043837
  Output probs: [[0.085 0.085 0.086 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0302
  Image Loss: 0.3817
  Total Loss: 30.5500
  Image grad max: 0.30199912190437317
  Output probs: [[0.074 0.074 0.202 0.074 0.074 0.074 0.202 0.074 0.074 0.074]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0222
  Image Loss: 0.3842
  Total Loss: 22.5619
  Image grad max: 0.1585865020751953
  Output probs: [[0.074 0.074 0.202 0.074 0.074 0.074 0.202 0.074 0.074 0.074]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0222
  Image Loss: 0.3863
  Total Loss: 22.5472
  Image grad max: 0.03787267953157425
  Output probs: [[0.075 0.075 0.205 0.075 0.075 0.075 0.193 0.075 0.075 0.075]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0227
  Image Loss: 0.3881
  Total Loss: 23.0680
  Image grad max: 3.3762011528015137
  Output probs: [[0.085 0.085 0.086 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0302
  Image Loss: 0.3775
  Total Loss: 30.5588
  Image grad max: 0.16247889399528503
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0302
  Image Loss: 0.3744
  Total Loss: 30.5789
  Image grad max: 0.0025510273408144712
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0302
  Image Loss: 0.3738
  Total Loss: 30.5783
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0302
  Image Loss: 0.3731
  Total Loss: 30.5776
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0302
  Image Loss: 0.3732
  Total Loss: 30.5777
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0302
  Image Loss: 0.3735
  Total Loss: 30.5780
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0302
  Image Loss: 0.3737
  Total Loss: 30.5782
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0302
  Image Loss: 0.3736
  Total Loss: 30.5781
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0302
  Image Loss: 0.3736
  Total Loss: 30.5780
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0302
  Image Loss: 0.3735
  Total Loss: 30.5780
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0302
  Image Loss: 0.3735
  Total Loss: 30.5780
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0302
  Image Loss: 0.3735
  Total Loss: 30.5780
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0302
  Image Loss: 0.3734
  Total Loss: 30.5779
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0302
  Image Loss: 0.3732
  Total Loss: 30.5777
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0302
  Image Loss: 0.3731
  Total Loss: 30.5776
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0302
  Image Loss: 0.3729
  Total Loss: 30.5774
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0302
  Image Loss: 0.3728
  Total Loss: 30.5773
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0302
  Image Loss: 0.3726
  Total Loss: 30.5771
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0302
  Image Loss: 0.3725
  Total Loss: 30.5770
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0302
  Image Loss: 0.3723
  Total Loss: 30.5768
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0302
  Image Loss: 0.3721
  Total Loss: 30.5766
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0302
  Image Loss: 0.3719
  Total Loss: 30.5764
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0302
  Image Loss: 0.3716
  Total Loss: 30.5761
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0302
  Image Loss: 0.3712
  Total Loss: 30.5757
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0302
  Image Loss: 0.3708
  Total Loss: 30.5753
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0302
  Image Loss: 0.3703
  Total Loss: 30.5747
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0302
  Image Loss: 0.3696
  Total Loss: 30.5741
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0302
  Image Loss: 0.3689
  Total Loss: 30.5734
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0302
  Image Loss: 0.3680
  Total Loss: 30.5725
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0302
  Image Loss: 0.3670
  Total Loss: 30.5715
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0302
  Image Loss: 0.3659
  Total Loss: 30.5704
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0302
  Image Loss: 0.3646
  Total Loss: 30.5691
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0302
  Image Loss: 0.3632
  Total Loss: 30.5677
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0302
  Image Loss: 0.3617
  Total Loss: 30.5661
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0302
  Image Loss: 0.3600
  Total Loss: 30.5645
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0302
  Image Loss: 0.3582
  Total Loss: 30.5627
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0302
  Image Loss: 0.3563
  Total Loss: 30.5608
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0302
  Image Loss: 0.3542
  Total Loss: 30.5587
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0302
  Image Loss: 0.3521
  Total Loss: 30.5566
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0302
  Image Loss: 0.3497
  Total Loss: 30.5542
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0302
  Image Loss: 0.3473
  Total Loss: 30.5518
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0302
  Image Loss: 0.3447
  Total Loss: 30.5492
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0302
  Image Loss: 0.3420
  Total Loss: 30.5465
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0302
  Image Loss: 0.3391
  Total Loss: 30.5436
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0302
  Image Loss: 0.3361
  Total Loss: 30.5406
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0302
  Image Loss: 0.3330
  Total Loss: 30.5375
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0302
  Image Loss: 0.3298
  Total Loss: 30.5343
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0302
  Image Loss: 0.3265
  Total Loss: 30.5310
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0302
  Image Loss: 0.3232
  Total Loss: 30.5277
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0302
  Image Loss: 0.3197
  Total Loss: 30.5242
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0302
  Image Loss: 0.3162
  Total Loss: 30.5207
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0302
  Image Loss: 0.3126
  Total Loss: 30.5171
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0302
  Image Loss: 0.3090
  Total Loss: 30.5135
  Image grad max: 0.0025510203558951616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0302
  Image Loss: 0.3054
  Total Loss: 30.5099
  Image grad max: 0.0025508932303637266
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0302
  Image Loss: 0.3017
  Total Loss: 30.5062
  Image grad max: 0.002550088334828615
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0302
  Image Loss: 0.2981
  Total Loss: 30.5026
  Image grad max: 0.0025490049738436937
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0302
  Image Loss: 0.2944
  Total Loss: 30.4989
  Image grad max: 0.002547640586271882
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0302
  Image Loss: 0.2907
  Total Loss: 30.4952
  Image grad max: 0.002545984461903572
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0302
  Image Loss: 0.2871
  Total Loss: 30.4915
  Image grad max: 0.0025440638419240713
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0302
  Image Loss: 0.2834
  Total Loss: 30.4879
  Image grad max: 0.002541904104873538
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0302
  Image Loss: 0.2798
  Total Loss: 30.4843
  Image grad max: 0.002539528300985694
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0302
  Image Loss: 0.2762
  Total Loss: 30.4806
  Image grad max: 0.0025369564536958933
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0302
  Image Loss: 0.2726
  Total Loss: 30.4771
  Image grad max: 0.002534207422286272
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0302
  Image Loss: 0.2690
  Total Loss: 30.4735
  Image grad max: 0.0025312979705631733
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0302
  Image Loss: 0.2655
  Total Loss: 30.4700
  Image grad max: 0.0025282432325184345
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0302
  Image Loss: 0.2620
  Total Loss: 30.4665
  Image grad max: 0.002525057177990675
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0302
  Image Loss: 0.2585
  Total Loss: 30.4630
  Image grad max: 0.0025217521470040083
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0302
  Image Loss: 0.2551
  Total Loss: 30.4596
  Image grad max: 0.0025183388497680426
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0302
  Image Loss: 0.2518
  Total Loss: 30.4562
  Image grad max: 0.0025148282293230295
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0302
  Image Loss: 0.2484
  Total Loss: 30.4529
  Image grad max: 0.0025112289004027843
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0302
  Image Loss: 0.2451
  Total Loss: 30.4496
  Image grad max: 0.0025075494777411222
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0302
  Image Loss: 0.2419
  Total Loss: 30.4464
  Image grad max: 0.00250379741191864
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0302
  Image Loss: 0.2387
  Total Loss: 30.4432
  Image grad max: 0.0024999796878546476
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0302
  Image Loss: 0.2355
  Total Loss: 30.4400
  Image grad max: 0.0024961018934845924
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0302
  Image Loss: 0.2324
  Total Loss: 30.4369
  Image grad max: 0.0024921700824052095
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0302
  Image Loss: 0.2293
  Total Loss: 30.4338
  Image grad max: 0.002488188911229372
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0302
  Image Loss: 0.2263
  Total Loss: 30.4308
  Image grad max: 0.0024841635022312403
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0302
  Image Loss: 0.2233
  Total Loss: 30.4278
  Image grad max: 0.002480097347870469
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0302
  Image Loss: 0.2204
  Total Loss: 30.4249
  Image grad max: 0.0024759946390986443
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0302
  Image Loss: 0.2175
  Total Loss: 30.4220
  Image grad max: 0.0024718581698834896
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0302
  Image Loss: 0.2147
  Total Loss: 30.4192
  Image grad max: 0.0024676916655153036
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0302
  Image Loss: 0.2119
  Total Loss: 30.4164
  Image grad max: 0.0024634976871311665
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0302
  Image Loss: 0.2091
  Total Loss: 30.4136
  Image grad max: 0.002459570299834013
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0302
  Image Loss: 0.2064
  Total Loss: 30.4109
  Image grad max: 0.002456072950735688
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0302
  Image Loss: 0.2037
  Total Loss: 30.4082
  Image grad max: 0.0024525641929358244
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0302
  Image Loss: 0.2011
  Total Loss: 30.4056
  Image grad max: 0.002449044492095709
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0302
  Image Loss: 0.1985
  Total Loss: 30.4030
  Image grad max: 0.002445514779537916
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0302
  Image Loss: 0.1959
  Total Loss: 30.4004
  Image grad max: 0.002441975986585021
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0302
  Image Loss: 0.1934
  Total Loss: 30.3979
  Image grad max: 0.0024384285788983107
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0302
  Image Loss: 0.1910
  Total Loss: 30.3954
  Image grad max: 0.002434873254969716
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0302
  Image Loss: 0.1885
  Total Loss: 30.3930
  Image grad max: 0.002431310713291168
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0302
  Image Loss: 0.1861
  Total Loss: 30.3906
  Image grad max: 0.0024277411866933107
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0302
  Image Loss: 0.1838
  Total Loss: 30.3883
  Image grad max: 0.002424165140837431
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0302
  Image Loss: 0.1814
  Total Loss: 30.3859
  Image grad max: 0.00242058327421546
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0302
  Image Loss: 0.1792
  Total Loss: 30.3836
  Image grad max: 0.002416995819658041
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0302
  Image Loss: 0.1769
  Total Loss: 30.3814
  Image grad max: 0.002413403009995818
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0302
  Image Loss: 0.1747
  Total Loss: 30.3792
  Image grad max: 0.002409805078059435
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0302
  Image Loss: 0.1725
  Total Loss: 30.3770
  Image grad max: 0.0024062027223408222
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0302
  Image Loss: 0.1704
  Total Loss: 30.3748
  Image grad max: 0.0024025957100093365
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0302
  Image Loss: 0.1682
  Total Loss: 30.3727
  Image grad max: 0.002398984506726265
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0302
  Image Loss: 0.1662
  Total Loss: 30.3706
  Image grad max: 0.002395369578152895
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0302
  Image Loss: 0.1641
  Total Loss: 30.3686
  Image grad max: 0.002391750691458583
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0302
  Image Loss: 0.1621
  Total Loss: 30.3666
  Image grad max: 0.002388128312304616
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0302
  Image Loss: 0.1601
  Total Loss: 30.3646
  Image grad max: 0.0023845024406909943
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0302
  Image Loss: 0.1581
  Total Loss: 30.3626
  Image grad max: 0.002380873542279005
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0302
  Image Loss: 0.1562
  Total Loss: 30.3607
  Image grad max: 0.0023772413842380047
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0302
  Image Loss: 0.1543
  Total Loss: 30.3588
  Image grad max: 0.002373606199398637
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0302
  Image Loss: 0.1524
  Total Loss: 30.3569
  Image grad max: 0.002369968220591545
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0302
  Image Loss: 0.1506
  Total Loss: 30.3551
  Image grad max: 0.0023663274478167295
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0302
  Image Loss: 0.1487
  Total Loss: 30.3532
  Image grad max: 0.002362684113904834
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0302
  Image Loss: 0.1469
  Total Loss: 30.3514
  Image grad max: 0.0023590384516865015
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0302
  Image Loss: 0.1452
  Total Loss: 30.3497
  Image grad max: 0.002355390228331089
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0302
  Image Loss: 0.1434
  Total Loss: 30.3479
  Image grad max: 0.0023517394438385963
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0302
  Image Loss: 0.1417
  Total Loss: 30.3462
  Image grad max: 0.0023480867967009544
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0302
  Image Loss: 0.1400
  Total Loss: 30.3445
  Image grad max: 0.002344431821256876
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0302
  Image Loss: 0.1383
  Total Loss: 30.3428
  Image grad max: 0.002340774517506361
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0302
  Image Loss: 0.1367
  Total Loss: 30.3412
  Image grad max: 0.002337115351110697
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0302
  Image Loss: 0.1351
  Total Loss: 30.3396
  Image grad max: 0.0023334543220698833
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0302
  Image Loss: 0.1335
  Total Loss: 30.3380
  Image grad max: 0.0023297914303839207
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0302
  Image Loss: 0.1319
  Total Loss: 30.3364
  Image grad max: 0.002326126443222165
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0302
  Image Loss: 0.1303
  Total Loss: 30.3348
  Image grad max: 0.002322459826245904
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0302
  Image Loss: 0.1288
  Total Loss: 30.3333
  Image grad max: 0.0023187913466244936
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0302
  Image Loss: 0.1273
  Total Loss: 30.3318
  Image grad max: 0.0023151212371885777
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0302
  Image Loss: 0.1258
  Total Loss: 30.3303
  Image grad max: 0.0023114497307687998
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0302
  Image Loss: 0.1243
  Total Loss: 30.3288
  Image grad max: 0.0023077763617038727
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0302
  Image Loss: 0.1229
  Total Loss: 30.3274
  Image grad max: 0.0023041015956550837
  Output probs: [[0.085 0.085 0.085 0.085 0.085 0.085 0.232 0.085 0.085 0.085]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0302
  Image Loss: 0.1214
  Total Loss: 30.3259
  Image grad max: 0.002300425199791789
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
