Epoch [1/30], Batch [0/6000], Loss: 2.4240
Epoch [1/30], Batch [100/6000], Loss: 1.0534
Epoch [1/30], Batch [200/6000], Loss: 0.7385
Epoch [1/30], Batch [300/6000], Loss: 0.9571
Epoch [1/30], Batch [400/6000], Loss: 0.5395
Epoch [1/30], Batch [500/6000], Loss: 0.2010
Epoch [1/30], Batch [600/6000], Loss: 0.2143
Epoch [1/30], Batch [700/6000], Loss: 0.1414
Epoch [1/30], Batch [800/6000], Loss: 0.6815
Epoch [1/30], Batch [900/6000], Loss: 0.3016
Epoch [1/30], Batch [1000/6000], Loss: 0.4913
Epoch [1/30], Batch [1100/6000], Loss: 0.7741
Epoch [1/30], Batch [1200/6000], Loss: 0.0887
Epoch [1/30], Batch [1300/6000], Loss: 0.4470
Epoch [1/30], Batch [1400/6000], Loss: 0.1237
Epoch [1/30], Batch [1500/6000], Loss: 0.4163
Epoch [1/30], Batch [1600/6000], Loss: 0.4917
Epoch [1/30], Batch [1700/6000], Loss: 0.1630
Epoch [1/30], Batch [1800/6000], Loss: 0.2763
Epoch [1/30], Batch [1900/6000], Loss: 0.2318
Epoch [1/30], Batch [2000/6000], Loss: 0.3117
Epoch [1/30], Batch [2100/6000], Loss: 0.3299
Epoch [1/30], Batch [2200/6000], Loss: 0.0734
Epoch [1/30], Batch [2300/6000], Loss: 0.0952
Epoch [1/30], Batch [2400/6000], Loss: 0.5776
Epoch [1/30], Batch [2500/6000], Loss: 0.0704
Epoch [1/30], Batch [2600/6000], Loss: 0.5716
Epoch [1/30], Batch [2700/6000], Loss: 0.1918
Epoch [1/30], Batch [2800/6000], Loss: 0.9006
Epoch [1/30], Batch [2900/6000], Loss: 0.2951
Epoch [1/30], Batch [3000/6000], Loss: 0.3870
Epoch [1/30], Batch [3100/6000], Loss: 0.4740
Epoch [1/30], Batch [3200/6000], Loss: 1.3489
Epoch [1/30], Batch [3300/6000], Loss: 1.0063
Epoch [1/30], Batch [3400/6000], Loss: 0.1044
Epoch [1/30], Batch [3500/6000], Loss: 0.1597
Epoch [1/30], Batch [3600/6000], Loss: 0.2075
Epoch [1/30], Batch [3700/6000], Loss: 0.5596
Epoch [1/30], Batch [3800/6000], Loss: 0.0758
Epoch [1/30], Batch [3900/6000], Loss: 0.2293
Epoch [1/30], Batch [4000/6000], Loss: 0.1593
Epoch [1/30], Batch [4100/6000], Loss: 0.0933
Epoch [1/30], Batch [4200/6000], Loss: 0.5847
Epoch [1/30], Batch [4300/6000], Loss: 0.0598
Epoch [1/30], Batch [4400/6000], Loss: 0.3517
Epoch [1/30], Batch [4500/6000], Loss: 0.3566
Epoch [1/30], Batch [4600/6000], Loss: 0.2524
Epoch [1/30], Batch [4700/6000], Loss: 0.1348
Epoch [1/30], Batch [4800/6000], Loss: 0.2751
Epoch [1/30], Batch [4900/6000], Loss: 0.2004
Epoch [1/30], Batch [5000/6000], Loss: 0.1007
Epoch [1/30], Batch [5100/6000], Loss: 0.8238
Epoch [1/30], Batch [5200/6000], Loss: 0.0452
Epoch [1/30], Batch [5300/6000], Loss: 0.0685
Epoch [1/30], Batch [5400/6000], Loss: 0.0596
Epoch [1/30], Batch [5500/6000], Loss: 0.0769
Epoch [1/30], Batch [5600/6000], Loss: 0.4779
Epoch [1/30], Batch [5700/6000], Loss: 0.2278
Epoch [1/30], Batch [5800/6000], Loss: 0.1417
Epoch [1/30], Batch [5900/6000], Loss: 0.0427
Epoch [1/30], Loss: 0.3937
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.0672
Epoch [2/30], Batch [100/6000], Loss: 0.9160
Epoch [2/30], Batch [200/6000], Loss: 0.0760
Epoch [2/30], Batch [300/6000], Loss: 0.0596
Epoch [2/30], Batch [400/6000], Loss: 0.0472
Epoch [2/30], Batch [500/6000], Loss: 0.2732
Epoch [2/30], Batch [600/6000], Loss: 0.1099
Epoch [2/30], Batch [700/6000], Loss: 0.0938
Epoch [2/30], Batch [800/6000], Loss: 0.3651
Epoch [2/30], Batch [900/6000], Loss: 0.5963
Epoch [2/30], Batch [1000/6000], Loss: 0.0845
Epoch [2/30], Batch [1100/6000], Loss: 0.2609
Epoch [2/30], Batch [1200/6000], Loss: 0.1455
Epoch [2/30], Batch [1300/6000], Loss: 0.0638
Epoch [2/30], Batch [1400/6000], Loss: 0.1358
Epoch [2/30], Batch [1500/6000], Loss: 0.0586
Epoch [2/30], Batch [1600/6000], Loss: 0.0609
Epoch [2/30], Batch [1700/6000], Loss: 0.0535
Epoch [2/30], Batch [1800/6000], Loss: 0.3172
Epoch [2/30], Batch [1900/6000], Loss: 0.0563
Epoch [2/30], Batch [2000/6000], Loss: 0.1016
Epoch [2/30], Batch [2100/6000], Loss: 0.1075
Epoch [2/30], Batch [2200/6000], Loss: 0.2498
Epoch [2/30], Batch [2300/6000], Loss: 0.0496
Epoch [2/30], Batch [2400/6000], Loss: 0.1127
Epoch [2/30], Batch [2500/6000], Loss: 0.1340
Epoch [2/30], Batch [2600/6000], Loss: 0.1204
Epoch [2/30], Batch [2700/6000], Loss: 0.5037
Epoch [2/30], Batch [2800/6000], Loss: 0.4123
Epoch [2/30], Batch [2900/6000], Loss: 0.1916
Epoch [2/30], Batch [3000/6000], Loss: 0.0473
Epoch [2/30], Batch [3100/6000], Loss: 0.0918
Epoch [2/30], Batch [3200/6000], Loss: 0.1799
Epoch [2/30], Batch [3300/6000], Loss: 0.0934
Epoch [2/30], Batch [3400/6000], Loss: 0.1189
Epoch [2/30], Batch [3500/6000], Loss: 0.0445
Epoch [2/30], Batch [3600/6000], Loss: 0.6826
Epoch [2/30], Batch [3700/6000], Loss: 0.1084
Epoch [2/30], Batch [3800/6000], Loss: 0.2451
Epoch [2/30], Batch [3900/6000], Loss: 0.4241
Epoch [2/30], Batch [4000/6000], Loss: 0.8361
Epoch [2/30], Batch [4100/6000], Loss: 0.1520
Epoch [2/30], Batch [4200/6000], Loss: 0.0568
Epoch [2/30], Batch [4300/6000], Loss: 0.9235
Epoch [2/30], Batch [4400/6000], Loss: 0.0516
Epoch [2/30], Batch [4500/6000], Loss: 0.6005
Epoch [2/30], Batch [4600/6000], Loss: 0.2456
Epoch [2/30], Batch [4700/6000], Loss: 0.1161
Epoch [2/30], Batch [4800/6000], Loss: 1.6189
Epoch [2/30], Batch [4900/6000], Loss: 0.0893
Epoch [2/30], Batch [5000/6000], Loss: 0.1420
Epoch [2/30], Batch [5100/6000], Loss: 0.3859
Epoch [2/30], Batch [5200/6000], Loss: 0.0554
Epoch [2/30], Batch [5300/6000], Loss: 0.3134
Epoch [2/30], Batch [5400/6000], Loss: 0.0605
Epoch [2/30], Batch [5500/6000], Loss: 0.0547
Epoch [2/30], Batch [5600/6000], Loss: 0.1637
Epoch [2/30], Batch [5700/6000], Loss: 0.0479
Epoch [2/30], Batch [5800/6000], Loss: 0.5585
Epoch [2/30], Batch [5900/6000], Loss: 0.0603
Epoch [2/30], Loss: 0.2217
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.0978
Epoch [3/30], Batch [100/6000], Loss: 0.1203
Epoch [3/30], Batch [200/6000], Loss: 0.0829
Epoch [3/30], Batch [300/6000], Loss: 0.0397
Epoch [3/30], Batch [400/6000], Loss: 0.1197
Epoch [3/30], Batch [500/6000], Loss: 0.0722
Epoch [3/30], Batch [600/6000], Loss: 0.3944
Epoch [3/30], Batch [700/6000], Loss: 0.0615
Epoch [3/30], Batch [800/6000], Loss: 0.0511
Epoch [3/30], Batch [900/6000], Loss: 0.0905
Epoch [3/30], Batch [1000/6000], Loss: 0.1527
Epoch [3/30], Batch [1100/6000], Loss: 0.0359
Epoch [3/30], Batch [1200/6000], Loss: 0.7885
Epoch [3/30], Batch [1300/6000], Loss: 0.4351
Epoch [3/30], Batch [1400/6000], Loss: 0.0781
Epoch [3/30], Batch [1500/6000], Loss: 0.1140
Epoch [3/30], Batch [1600/6000], Loss: 0.0424
Epoch [3/30], Batch [1700/6000], Loss: 0.2536
Epoch [3/30], Batch [1800/6000], Loss: 0.1166
Epoch [3/30], Batch [1900/6000], Loss: 0.0444
Epoch [3/30], Batch [2000/6000], Loss: 0.3327
Epoch [3/30], Batch [2100/6000], Loss: 0.1327
Epoch [3/30], Batch [2200/6000], Loss: 0.0430
Epoch [3/30], Batch [2300/6000], Loss: 0.1512
Epoch [3/30], Batch [2400/6000], Loss: 0.0504
Epoch [3/30], Batch [2500/6000], Loss: 0.2105
Epoch [3/30], Batch [2600/6000], Loss: 0.0981
Epoch [3/30], Batch [2700/6000], Loss: 0.0655
Epoch [3/30], Batch [2800/6000], Loss: 0.0685
Epoch [3/30], Batch [2900/6000], Loss: 0.1567
Epoch [3/30], Batch [3000/6000], Loss: 0.0364
Epoch [3/30], Batch [3100/6000], Loss: 0.4842
Epoch [3/30], Batch [3200/6000], Loss: 0.0611
Epoch [3/30], Batch [3300/6000], Loss: 0.0973
Epoch [3/30], Batch [3400/6000], Loss: 0.0479
Epoch [3/30], Batch [3500/6000], Loss: 0.0515
Epoch [3/30], Batch [3600/6000], Loss: 0.0984
Epoch [3/30], Batch [3700/6000], Loss: 0.0324
Epoch [3/30], Batch [3800/6000], Loss: 0.1271
Epoch [3/30], Batch [3900/6000], Loss: 0.0558
Epoch [3/30], Batch [4000/6000], Loss: 0.2680
Epoch [3/30], Batch [4100/6000], Loss: 0.1152
Epoch [3/30], Batch [4200/6000], Loss: 0.0370
Epoch [3/30], Batch [4300/6000], Loss: 0.1228
Epoch [3/30], Batch [4400/6000], Loss: 0.0511
Epoch [3/30], Batch [4500/6000], Loss: 0.0981
Epoch [3/30], Batch [4600/6000], Loss: 0.0799
Epoch [3/30], Batch [4700/6000], Loss: 0.3225
Epoch [3/30], Batch [4800/6000], Loss: 0.4946
Epoch [3/30], Batch [4900/6000], Loss: 0.0860
Epoch [3/30], Batch [5000/6000], Loss: 0.3094
Epoch [3/30], Batch [5100/6000], Loss: 0.0706
Epoch [3/30], Batch [5200/6000], Loss: 0.1410
Epoch [3/30], Batch [5300/6000], Loss: 0.0304
Epoch [3/30], Batch [5400/6000], Loss: 0.2180
Epoch [3/30], Batch [5500/6000], Loss: 0.0450
Epoch [3/30], Batch [5600/6000], Loss: 0.0380
Epoch [3/30], Batch [5700/6000], Loss: 0.1255
Epoch [3/30], Batch [5800/6000], Loss: 0.4200
Epoch [3/30], Batch [5900/6000], Loss: 0.1201
Epoch [3/30], Loss: 0.1701
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.0560
Epoch [4/30], Batch [100/6000], Loss: 0.2911
Epoch [4/30], Batch [200/6000], Loss: 0.6219
Epoch [4/30], Batch [300/6000], Loss: 0.0292
Epoch [4/30], Batch [400/6000], Loss: 0.0480
Epoch [4/30], Batch [500/6000], Loss: 0.0859
Epoch [4/30], Batch [600/6000], Loss: 0.4977
Epoch [4/30], Batch [700/6000], Loss: 0.3826
Epoch [4/30], Batch [800/6000], Loss: 0.0316
Epoch [4/30], Batch [900/6000], Loss: 0.1135
Epoch [4/30], Batch [1000/6000], Loss: 0.1031
Epoch [4/30], Batch [1100/6000], Loss: 0.0447
Epoch [4/30], Batch [1200/6000], Loss: 0.1976
Epoch [4/30], Batch [1300/6000], Loss: 0.0418
Epoch [4/30], Batch [1400/6000], Loss: 0.4963
Epoch [4/30], Batch [1500/6000], Loss: 0.0306
Epoch [4/30], Batch [1600/6000], Loss: 0.3295
Epoch [4/30], Batch [1700/6000], Loss: 0.0778
Epoch [4/30], Batch [1800/6000], Loss: 0.0552
Epoch [4/30], Batch [1900/6000], Loss: 0.0880
Epoch [4/30], Batch [2000/6000], Loss: 0.0503
Epoch [4/30], Batch [2100/6000], Loss: 0.1237
Epoch [4/30], Batch [2200/6000], Loss: 0.1599
Epoch [4/30], Batch [2300/6000], Loss: 0.6970
Epoch [4/30], Batch [2400/6000], Loss: 0.0999
Epoch [4/30], Batch [2500/6000], Loss: 0.0392
Epoch [4/30], Batch [2600/6000], Loss: 0.1267
Epoch [4/30], Batch [2700/6000], Loss: 0.1271
Epoch [4/30], Batch [2800/6000], Loss: 0.0461
Epoch [4/30], Batch [2900/6000], Loss: 0.0343
Epoch [4/30], Batch [3000/6000], Loss: 0.0576
Epoch [4/30], Batch [3100/6000], Loss: 0.0487
Epoch [4/30], Batch [3200/6000], Loss: 0.2771
Epoch [4/30], Batch [3300/6000], Loss: 0.0955
Epoch [4/30], Batch [3400/6000], Loss: 0.1044
Epoch [4/30], Batch [3500/6000], Loss: 0.1083
Epoch [4/30], Batch [3600/6000], Loss: 0.1275
Epoch [4/30], Batch [3700/6000], Loss: 0.2253
Epoch [4/30], Batch [3800/6000], Loss: 0.0782
Epoch [4/30], Batch [3900/6000], Loss: 0.3093
Epoch [4/30], Batch [4000/6000], Loss: 0.1105
Epoch [4/30], Batch [4100/6000], Loss: 0.0354
Epoch [4/30], Batch [4200/6000], Loss: 0.0419
Epoch [4/30], Batch [4300/6000], Loss: 0.1965
Epoch [4/30], Batch [4400/6000], Loss: 0.0322
Epoch [4/30], Batch [4500/6000], Loss: 0.4127
Epoch [4/30], Batch [4600/6000], Loss: 0.2118
Epoch [4/30], Batch [4700/6000], Loss: 0.0628
Epoch [4/30], Batch [4800/6000], Loss: 0.0436
Epoch [4/30], Batch [4900/6000], Loss: 0.0256
Epoch [4/30], Batch [5000/6000], Loss: 0.0848
Epoch [4/30], Batch [5100/6000], Loss: 0.0354
Epoch [4/30], Batch [5200/6000], Loss: 0.0320
Epoch [4/30], Batch [5300/6000], Loss: 0.0382
Epoch [4/30], Batch [5400/6000], Loss: 0.0349
Epoch [4/30], Batch [5500/6000], Loss: 0.0381
Epoch [4/30], Batch [5600/6000], Loss: 0.0432
Epoch [4/30], Batch [5700/6000], Loss: 0.6051
Epoch [4/30], Batch [5800/6000], Loss: 0.0391
Epoch [4/30], Batch [5900/6000], Loss: 0.0677
Epoch [4/30], Loss: 0.1399
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.0369
Epoch [5/30], Batch [100/6000], Loss: 0.0353
Epoch [5/30], Batch [200/6000], Loss: 0.0463
Epoch [5/30], Batch [300/6000], Loss: 0.1861
Epoch [5/30], Batch [400/6000], Loss: 0.2408
Epoch [5/30], Batch [500/6000], Loss: 0.3439
Epoch [5/30], Batch [600/6000], Loss: 0.0268
Epoch [5/30], Batch [700/6000], Loss: 0.0620
Epoch [5/30], Batch [800/6000], Loss: 0.5546
Epoch [5/30], Batch [900/6000], Loss: 0.0537
Epoch [5/30], Batch [1000/6000], Loss: 0.0430
Epoch [5/30], Batch [1100/6000], Loss: 0.4871
Epoch [5/30], Batch [1200/6000], Loss: 0.0288
Epoch [5/30], Batch [1300/6000], Loss: 0.0317
Epoch [5/30], Batch [1400/6000], Loss: 0.0269
Epoch [5/30], Batch [1500/6000], Loss: 0.0344
Epoch [5/30], Batch [1600/6000], Loss: 0.0571
Epoch [5/30], Batch [1700/6000], Loss: 0.0293
Epoch [5/30], Batch [1800/6000], Loss: 0.2860
Epoch [5/30], Batch [1900/6000], Loss: 0.0349
Epoch [5/30], Batch [2000/6000], Loss: 0.2554
Epoch [5/30], Batch [2100/6000], Loss: 0.0719
Epoch [5/30], Batch [2200/6000], Loss: 0.0531
Epoch [5/30], Batch [2300/6000], Loss: 0.0268
Epoch [5/30], Batch [2400/6000], Loss: 0.0731
Epoch [5/30], Batch [2500/6000], Loss: 0.1241
Epoch [5/30], Batch [2600/6000], Loss: 0.0332
Epoch [5/30], Batch [2700/6000], Loss: 0.0549
Epoch [5/30], Batch [2800/6000], Loss: 0.2481
Epoch [5/30], Batch [2900/6000], Loss: 0.0260
Epoch [5/30], Batch [3000/6000], Loss: 0.0305
Epoch [5/30], Batch [3100/6000], Loss: 0.0985
Epoch [5/30], Batch [3200/6000], Loss: 0.0496
Epoch [5/30], Batch [3300/6000], Loss: 0.0585
Epoch [5/30], Batch [3400/6000], Loss: 0.4736
Epoch [5/30], Batch [3500/6000], Loss: 0.0303
Epoch [5/30], Batch [3600/6000], Loss: 0.0838
Epoch [5/30], Batch [3700/6000], Loss: 0.4988
Epoch [5/30], Batch [3800/6000], Loss: 0.3079
Epoch [5/30], Batch [3900/6000], Loss: 0.0566
Epoch [5/30], Batch [4000/6000], Loss: 0.0658
Epoch [5/30], Batch [4100/6000], Loss: 0.4247
Epoch [5/30], Batch [4200/6000], Loss: 0.0846
Epoch [5/30], Batch [4300/6000], Loss: 0.0259
Epoch [5/30], Batch [4400/6000], Loss: 0.0286
Epoch [5/30], Batch [4500/6000], Loss: 0.0305
Epoch [5/30], Batch [4600/6000], Loss: 0.0446
Epoch [5/30], Batch [4700/6000], Loss: 0.4601
Epoch [5/30], Batch [4800/6000], Loss: 0.1415
Epoch [5/30], Batch [4900/6000], Loss: 0.0308
Epoch [5/30], Batch [5000/6000], Loss: 0.0232
Epoch [5/30], Batch [5100/6000], Loss: 0.0853
Epoch [5/30], Batch [5200/6000], Loss: 0.0427
Epoch [5/30], Batch [5300/6000], Loss: 0.0574
Epoch [5/30], Batch [5400/6000], Loss: 0.1854
Epoch [5/30], Batch [5500/6000], Loss: 0.2673
Epoch [5/30], Batch [5600/6000], Loss: 0.1910
Epoch [5/30], Batch [5700/6000], Loss: 0.0274
Epoch [5/30], Batch [5800/6000], Loss: 0.2493
Epoch [5/30], Batch [5900/6000], Loss: 0.2540
Epoch [5/30], Loss: 0.1198
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.0558
Epoch [6/30], Batch [100/6000], Loss: 0.0389
Epoch [6/30], Batch [200/6000], Loss: 0.7594
Epoch [6/30], Batch [300/6000], Loss: 0.0309
Epoch [6/30], Batch [400/6000], Loss: 0.0291
Epoch [6/30], Batch [500/6000], Loss: 0.0580
Epoch [6/30], Batch [600/6000], Loss: 0.0312
Epoch [6/30], Batch [700/6000], Loss: 0.0354
Epoch [6/30], Batch [800/6000], Loss: 0.1197
Epoch [6/30], Batch [900/6000], Loss: 0.0463
Epoch [6/30], Batch [1000/6000], Loss: 0.0264
Epoch [6/30], Batch [1100/6000], Loss: 0.0586
Epoch [6/30], Batch [1200/6000], Loss: 0.0381
Epoch [6/30], Batch [1300/6000], Loss: 0.0380
Epoch [6/30], Batch [1400/6000], Loss: 0.0358
Epoch [6/30], Batch [1500/6000], Loss: 0.0265
Epoch [6/30], Batch [1600/6000], Loss: 0.0330
Epoch [6/30], Batch [1700/6000], Loss: 0.0529
Epoch [6/30], Batch [1800/6000], Loss: 0.0860
Epoch [6/30], Batch [1900/6000], Loss: 0.0299
Epoch [6/30], Batch [2000/6000], Loss: 0.0460
Epoch [6/30], Batch [2100/6000], Loss: 0.3664
Epoch [6/30], Batch [2200/6000], Loss: 0.0350
Epoch [6/30], Batch [2300/6000], Loss: 0.0533
Epoch [6/30], Batch [2400/6000], Loss: 0.0425
Epoch [6/30], Batch [2500/6000], Loss: 0.3662
Epoch [6/30], Batch [2600/6000], Loss: 0.4769
Epoch [6/30], Batch [2700/6000], Loss: 0.2409
Epoch [6/30], Batch [2800/6000], Loss: 0.0778
Epoch [6/30], Batch [2900/6000], Loss: 0.0441
Epoch [6/30], Batch [3000/6000], Loss: 0.0474
Epoch [6/30], Batch [3100/6000], Loss: 0.0247
Epoch [6/30], Batch [3200/6000], Loss: 0.0316
Epoch [6/30], Batch [3300/6000], Loss: 0.0287
Epoch [6/30], Batch [3400/6000], Loss: 0.0326
Epoch [6/30], Batch [3500/6000], Loss: 0.0241
Epoch [6/30], Batch [3600/6000], Loss: 0.0523
Epoch [6/30], Batch [3700/6000], Loss: 0.0253
Epoch [6/30], Batch [3800/6000], Loss: 0.0370
Epoch [6/30], Batch [3900/6000], Loss: 0.0400
Epoch [6/30], Batch [4000/6000], Loss: 0.0444
Epoch [6/30], Batch [4100/6000], Loss: 0.0261
Epoch [6/30], Batch [4200/6000], Loss: 0.4758
Epoch [6/30], Batch [4300/6000], Loss: 0.0610
Epoch [6/30], Batch [4400/6000], Loss: 0.0493
Epoch [6/30], Batch [4500/6000], Loss: 0.0278
Epoch [6/30], Batch [4600/6000], Loss: 0.0255
Epoch [6/30], Batch [4700/6000], Loss: 0.2110
Epoch [6/30], Batch [4800/6000], Loss: 0.1710
Epoch [6/30], Batch [4900/6000], Loss: 0.0318
Epoch [6/30], Batch [5000/6000], Loss: 0.6154
Epoch [6/30], Batch [5100/6000], Loss: 0.0311
Epoch [6/30], Batch [5200/6000], Loss: 0.0356
Epoch [6/30], Batch [5300/6000], Loss: 0.0356
Epoch [6/30], Batch [5400/6000], Loss: 0.0351
Epoch [6/30], Batch [5500/6000], Loss: 0.0821
Epoch [6/30], Batch [5600/6000], Loss: 0.0295
Epoch [6/30], Batch [5700/6000], Loss: 0.0408
Epoch [6/30], Batch [5800/6000], Loss: 0.0276
Epoch [6/30], Batch [5900/6000], Loss: 0.2211
Epoch [6/30], Loss: 0.1047
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.0338
Epoch [7/30], Batch [100/6000], Loss: 0.3754
Epoch [7/30], Batch [200/6000], Loss: 0.0372
Epoch [7/30], Batch [300/6000], Loss: 0.0211
Epoch [7/30], Batch [400/6000], Loss: 0.0370
Epoch [7/30], Batch [500/6000], Loss: 0.0348
Epoch [7/30], Batch [600/6000], Loss: 0.0257
Epoch [7/30], Batch [700/6000], Loss: 0.0377
Epoch [7/30], Batch [800/6000], Loss: 0.0318
Epoch [7/30], Batch [900/6000], Loss: 0.0295
Epoch [7/30], Batch [1000/6000], Loss: 0.2581
Epoch [7/30], Batch [1100/6000], Loss: 0.0291
Epoch [7/30], Batch [1200/6000], Loss: 0.1884
Epoch [7/30], Batch [1300/6000], Loss: 0.0314
Epoch [7/30], Batch [1400/6000], Loss: 0.1035
Epoch [7/30], Batch [1500/6000], Loss: 0.0228
Epoch [7/30], Batch [1600/6000], Loss: 0.0414
Epoch [7/30], Batch [1700/6000], Loss: 0.1928
Epoch [7/30], Batch [1800/6000], Loss: 0.0471
Epoch [7/30], Batch [1900/6000], Loss: 0.0444
Epoch [7/30], Batch [2000/6000], Loss: 0.1311
Epoch [7/30], Batch [2100/6000], Loss: 0.0248
Epoch [7/30], Batch [2200/6000], Loss: 0.1481
Epoch [7/30], Batch [2300/6000], Loss: 0.0304
Epoch [7/30], Batch [2400/6000], Loss: 0.0307
Epoch [7/30], Batch [2500/6000], Loss: 0.0266
Epoch [7/30], Batch [2600/6000], Loss: 0.1843
Epoch [7/30], Batch [2700/6000], Loss: 0.1078
Epoch [7/30], Batch [2800/6000], Loss: 0.0406
Epoch [7/30], Batch [2900/6000], Loss: 0.0303
Epoch [7/30], Batch [3000/6000], Loss: 0.0277
Epoch [7/30], Batch [3100/6000], Loss: 0.0400
Epoch [7/30], Batch [3200/6000], Loss: 0.0327
Epoch [7/30], Batch [3300/6000], Loss: 0.0271
Epoch [7/30], Batch [3400/6000], Loss: 0.0236
Epoch [7/30], Batch [3500/6000], Loss: 0.0720
Epoch [7/30], Batch [3600/6000], Loss: 0.1886
Epoch [7/30], Batch [3700/6000], Loss: 0.7470
Epoch [7/30], Batch [3800/6000], Loss: 0.0245
Epoch [7/30], Batch [3900/6000], Loss: 0.0309
Epoch [7/30], Batch [4000/6000], Loss: 0.0359
Epoch [7/30], Batch [4100/6000], Loss: 0.0278
Epoch [7/30], Batch [4200/6000], Loss: 0.1548
Epoch [7/30], Batch [4300/6000], Loss: 0.0416
Epoch [7/30], Batch [4400/6000], Loss: 0.0240
Epoch [7/30], Batch [4500/6000], Loss: 0.0270
Epoch [7/30], Batch [4600/6000], Loss: 0.0232
Epoch [7/30], Batch [4700/6000], Loss: 0.0373
Epoch [7/30], Batch [4800/6000], Loss: 0.0299
Epoch [7/30], Batch [4900/6000], Loss: 0.0437
Epoch [7/30], Batch [5000/6000], Loss: 0.0326
Epoch [7/30], Batch [5100/6000], Loss: 0.1487
Epoch [7/30], Batch [5200/6000], Loss: 0.0277
Epoch [7/30], Batch [5300/6000], Loss: 0.0221
Epoch [7/30], Batch [5400/6000], Loss: 0.0274
Epoch [7/30], Batch [5500/6000], Loss: 0.0249
Epoch [7/30], Batch [5600/6000], Loss: 0.1646
Epoch [7/30], Batch [5700/6000], Loss: 0.3394
Epoch [7/30], Batch [5800/6000], Loss: 0.0497
Epoch [7/30], Batch [5900/6000], Loss: 0.4218
Epoch [7/30], Loss: 0.0922
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.0388
Epoch [8/30], Batch [100/6000], Loss: 0.0346
Epoch [8/30], Batch [200/6000], Loss: 0.0229
Epoch [8/30], Batch [300/6000], Loss: 0.3047
Epoch [8/30], Batch [400/6000], Loss: 0.4180
Epoch [8/30], Batch [500/6000], Loss: 0.0235
Epoch [8/30], Batch [600/6000], Loss: 0.0313
Epoch [8/30], Batch [700/6000], Loss: 0.0293
Epoch [8/30], Batch [800/6000], Loss: 0.0329
Epoch [8/30], Batch [900/6000], Loss: 0.0275
Epoch [8/30], Batch [1000/6000], Loss: 0.0350
Epoch [8/30], Batch [1100/6000], Loss: 0.0493
Epoch [8/30], Batch [1200/6000], Loss: 0.0269
Epoch [8/30], Batch [1300/6000], Loss: 1.5820
Epoch [8/30], Batch [1400/6000], Loss: 0.0329
Epoch [8/30], Batch [1500/6000], Loss: 0.0779
Epoch [8/30], Batch [1600/6000], Loss: 0.0264
Epoch [8/30], Batch [1700/6000], Loss: 0.0263
Epoch [8/30], Batch [1800/6000], Loss: 0.0266
Epoch [8/30], Batch [1900/6000], Loss: 0.0240
Epoch [8/30], Batch [2000/6000], Loss: 0.2401
Epoch [8/30], Batch [2100/6000], Loss: 0.0584
Epoch [8/30], Batch [2200/6000], Loss: 0.2832
Epoch [8/30], Batch [2300/6000], Loss: 0.0267
Epoch [8/30], Batch [2400/6000], Loss: 0.0268
Epoch [8/30], Batch [2500/6000], Loss: 0.0244
Epoch [8/30], Batch [2600/6000], Loss: 0.0332
Epoch [8/30], Batch [2700/6000], Loss: 0.0252
Epoch [8/30], Batch [2800/6000], Loss: 0.0535
Epoch [8/30], Batch [2900/6000], Loss: 0.0375
Epoch [8/30], Batch [3000/6000], Loss: 0.3931
Epoch [8/30], Batch [3100/6000], Loss: 0.0257
Epoch [8/30], Batch [3200/6000], Loss: 0.0848
Epoch [8/30], Batch [3300/6000], Loss: 0.0267
Epoch [8/30], Batch [3400/6000], Loss: 0.1148
Epoch [8/30], Batch [3500/6000], Loss: 0.0415
Epoch [8/30], Batch [3600/6000], Loss: 0.1357
Epoch [8/30], Batch [3700/6000], Loss: 0.0243
Epoch [8/30], Batch [3800/6000], Loss: 0.0409
Epoch [8/30], Batch [3900/6000], Loss: 0.0424
Epoch [8/30], Batch [4000/6000], Loss: 0.0307
Epoch [8/30], Batch [4100/6000], Loss: 0.0465
Epoch [8/30], Batch [4200/6000], Loss: 0.0709
Epoch [8/30], Batch [4300/6000], Loss: 0.0233
Epoch [8/30], Batch [4400/6000], Loss: 0.0556
Epoch [8/30], Batch [4500/6000], Loss: 0.3445
Epoch [8/30], Batch [4600/6000], Loss: 0.0278
Epoch [8/30], Batch [4700/6000], Loss: 0.0260
Epoch [8/30], Batch [4800/6000], Loss: 0.0261
Epoch [8/30], Batch [4900/6000], Loss: 0.7790
Epoch [8/30], Batch [5000/6000], Loss: 0.0279
Epoch [8/30], Batch [5100/6000], Loss: 0.0440
Epoch [8/30], Batch [5200/6000], Loss: 0.3672
Epoch [8/30], Batch [5300/6000], Loss: 0.0260
Epoch [8/30], Batch [5400/6000], Loss: 0.0334
Epoch [8/30], Batch [5500/6000], Loss: 0.1025
Epoch [8/30], Batch [5600/6000], Loss: 0.1906
Epoch [8/30], Batch [5700/6000], Loss: 0.0426
Epoch [8/30], Batch [5800/6000], Loss: 0.0243
Epoch [8/30], Batch [5900/6000], Loss: 0.0223
Epoch [8/30], Loss: 0.0836
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.0265
Epoch [9/30], Batch [100/6000], Loss: 0.0338
Epoch [9/30], Batch [200/6000], Loss: 0.0239
Epoch [9/30], Batch [300/6000], Loss: 0.0721
Epoch [9/30], Batch [400/6000], Loss: 0.0348
Epoch [9/30], Batch [500/6000], Loss: 0.0281
Epoch [9/30], Batch [600/6000], Loss: 0.0818
Epoch [9/30], Batch [700/6000], Loss: 0.0309
Epoch [9/30], Batch [800/6000], Loss: 0.0364
Epoch [9/30], Batch [900/6000], Loss: 0.0219
Epoch [9/30], Batch [1000/6000], Loss: 0.0271
Epoch [9/30], Batch [1100/6000], Loss: 0.0297
Epoch [9/30], Batch [1200/6000], Loss: 0.0758
Epoch [9/30], Batch [1300/6000], Loss: 0.2316
Epoch [9/30], Batch [1400/6000], Loss: 0.2434
Epoch [9/30], Batch [1500/6000], Loss: 0.0286
Epoch [9/30], Batch [1600/6000], Loss: 0.0290
Epoch [9/30], Batch [1700/6000], Loss: 0.0315
Epoch [9/30], Batch [1800/6000], Loss: 0.0449
Epoch [9/30], Batch [1900/6000], Loss: 0.0359
Epoch [9/30], Batch [2000/6000], Loss: 0.0250
Epoch [9/30], Batch [2100/6000], Loss: 0.0305
Epoch [9/30], Batch [2200/6000], Loss: 0.0282
Epoch [9/30], Batch [2300/6000], Loss: 0.0274
Epoch [9/30], Batch [2400/6000], Loss: 0.5578
Epoch [9/30], Batch [2500/6000], Loss: 0.0300
Epoch [9/30], Batch [2600/6000], Loss: 0.0383
Epoch [9/30], Batch [2700/6000], Loss: 0.0223
Epoch [9/30], Batch [2800/6000], Loss: 0.3154
Epoch [9/30], Batch [2900/6000], Loss: 0.0206
Epoch [9/30], Batch [3000/6000], Loss: 0.4466
Epoch [9/30], Batch [3100/6000], Loss: 0.0271
Epoch [9/30], Batch [3200/6000], Loss: 0.0297
Epoch [9/30], Batch [3300/6000], Loss: 0.0416
Epoch [9/30], Batch [3400/6000], Loss: 0.0200
Epoch [9/30], Batch [3500/6000], Loss: 0.0518
Epoch [9/30], Batch [3600/6000], Loss: 0.2813
Epoch [9/30], Batch [3700/6000], Loss: 0.3296
Epoch [9/30], Batch [3800/6000], Loss: 0.0787
Epoch [9/30], Batch [3900/6000], Loss: 0.0302
Epoch [9/30], Batch [4000/6000], Loss: 0.0287
Epoch [9/30], Batch [4100/6000], Loss: 0.0608
Epoch [9/30], Batch [4200/6000], Loss: 0.0292
Epoch [9/30], Batch [4300/6000], Loss: 0.2299
Epoch [9/30], Batch [4400/6000], Loss: 0.0222
Epoch [9/30], Batch [4500/6000], Loss: 0.0309
Epoch [9/30], Batch [4600/6000], Loss: 0.1268
Epoch [9/30], Batch [4700/6000], Loss: 0.0526
Epoch [9/30], Batch [4800/6000], Loss: 0.0287
Epoch [9/30], Batch [4900/6000], Loss: 0.0594
Epoch [9/30], Batch [5000/6000], Loss: 0.0248
Epoch [9/30], Batch [5100/6000], Loss: 0.0202
Epoch [9/30], Batch [5200/6000], Loss: 0.3305
Epoch [9/30], Batch [5300/6000], Loss: 0.1092
Epoch [9/30], Batch [5400/6000], Loss: 0.0314
Epoch [9/30], Batch [5500/6000], Loss: 0.0246
Epoch [9/30], Batch [5600/6000], Loss: 0.0283
Epoch [9/30], Batch [5700/6000], Loss: 0.1011
Epoch [9/30], Batch [5800/6000], Loss: 0.0937
Epoch [9/30], Batch [5900/6000], Loss: 0.3990
Epoch [9/30], Loss: 0.0746
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.1289
Epoch [10/30], Batch [100/6000], Loss: 0.0692
Epoch [10/30], Batch [200/6000], Loss: 0.0242
Epoch [10/30], Batch [300/6000], Loss: 0.0242
Epoch [10/30], Batch [400/6000], Loss: 0.0332
Epoch [10/30], Batch [500/6000], Loss: 0.0256
Epoch [10/30], Batch [600/6000], Loss: 0.0233
Epoch [10/30], Batch [700/6000], Loss: 0.2367
Epoch [10/30], Batch [800/6000], Loss: 0.0675
Epoch [10/30], Batch [900/6000], Loss: 0.0295
Epoch [10/30], Batch [1000/6000], Loss: 0.0205
Epoch [10/30], Batch [1100/6000], Loss: 0.0475
Epoch [10/30], Batch [1200/6000], Loss: 0.0263
Epoch [10/30], Batch [1300/6000], Loss: 0.0239
Epoch [10/30], Batch [1400/6000], Loss: 0.0171
Epoch [10/30], Batch [1500/6000], Loss: 0.0236
Epoch [10/30], Batch [1600/6000], Loss: 0.0246
Epoch [10/30], Batch [1700/6000], Loss: 0.0666
Epoch [10/30], Batch [1800/6000], Loss: 0.0308
Epoch [10/30], Batch [1900/6000], Loss: 0.0502
Epoch [10/30], Batch [2000/6000], Loss: 0.0292
Epoch [10/30], Batch [2100/6000], Loss: 0.0863
Epoch [10/30], Batch [2200/6000], Loss: 0.0281
Epoch [10/30], Batch [2300/6000], Loss: 0.0239
Epoch [10/30], Batch [2400/6000], Loss: 0.0520
Epoch [10/30], Batch [2500/6000], Loss: 0.0483
Epoch [10/30], Batch [2600/6000], Loss: 0.0811
Epoch [10/30], Batch [2700/6000], Loss: 0.0449
Epoch [10/30], Batch [2800/6000], Loss: 0.0284
Epoch [10/30], Batch [2900/6000], Loss: 0.0230
Epoch [10/30], Batch [3000/6000], Loss: 0.0247
Epoch [10/30], Batch [3100/6000], Loss: 0.0493
Epoch [10/30], Batch [3200/6000], Loss: 0.0527
Epoch [10/30], Batch [3300/6000], Loss: 0.2372
Epoch [10/30], Batch [3400/6000], Loss: 0.4656
Epoch [10/30], Batch [3500/6000], Loss: 0.0300
Epoch [10/30], Batch [3600/6000], Loss: 0.0199
Epoch [10/30], Batch [3700/6000], Loss: 0.0741
Epoch [10/30], Batch [3800/6000], Loss: 0.7579
Epoch [10/30], Batch [3900/6000], Loss: 0.0226
Epoch [10/30], Batch [4000/6000], Loss: 0.0610
Epoch [10/30], Batch [4100/6000], Loss: 0.0237
Epoch [10/30], Batch [4200/6000], Loss: 0.0219
Epoch [10/30], Batch [4300/6000], Loss: 0.0265
Epoch [10/30], Batch [4400/6000], Loss: 0.0263
Epoch [10/30], Batch [4500/6000], Loss: 0.3542
Epoch [10/30], Batch [4600/6000], Loss: 0.0516
Epoch [10/30], Batch [4700/6000], Loss: 0.0242
Epoch [10/30], Batch [4800/6000], Loss: 0.0273
Epoch [10/30], Batch [4900/6000], Loss: 0.0853
Epoch [10/30], Batch [5000/6000], Loss: 0.0555
Epoch [10/30], Batch [5100/6000], Loss: 0.0256
Epoch [10/30], Batch [5200/6000], Loss: 0.0265
Epoch [10/30], Batch [5300/6000], Loss: 0.0267
Epoch [10/30], Batch [5400/6000], Loss: 0.0313
Epoch [10/30], Batch [5500/6000], Loss: 0.0264
Epoch [10/30], Batch [5600/6000], Loss: 0.0213
Epoch [10/30], Batch [5700/6000], Loss: 0.0247
Epoch [10/30], Batch [5800/6000], Loss: 0.0751
Epoch [10/30], Batch [5900/6000], Loss: 0.3231
Epoch [10/30], Loss: 0.0683
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.0272
Epoch [11/30], Batch [100/6000], Loss: 0.2886
Epoch [11/30], Batch [200/6000], Loss: 0.0252
Epoch [11/30], Batch [300/6000], Loss: 0.0565
Epoch [11/30], Batch [400/6000], Loss: 0.0263
Epoch [11/30], Batch [500/6000], Loss: 0.0254
Epoch [11/30], Batch [600/6000], Loss: 0.0187
Epoch [11/30], Batch [700/6000], Loss: 0.0294
Epoch [11/30], Batch [800/6000], Loss: 0.0884
Epoch [11/30], Batch [900/6000], Loss: 0.8728
Epoch [11/30], Batch [1000/6000], Loss: 0.0201
Epoch [11/30], Batch [1100/6000], Loss: 0.0237
Epoch [11/30], Batch [1200/6000], Loss: 0.6120
Epoch [11/30], Batch [1300/6000], Loss: 0.0312
Epoch [11/30], Batch [1400/6000], Loss: 0.0229
Epoch [11/30], Batch [1500/6000], Loss: 0.0242
Epoch [11/30], Batch [1600/6000], Loss: 0.0231
Epoch [11/30], Batch [1700/6000], Loss: 0.0196
Epoch [11/30], Batch [1800/6000], Loss: 0.0235
Epoch [11/30], Batch [1900/6000], Loss: 0.0275
Epoch [11/30], Batch [2000/6000], Loss: 1.1498
Epoch [11/30], Batch [2100/6000], Loss: 0.0222
Epoch [11/30], Batch [2200/6000], Loss: 0.2493
Epoch [11/30], Batch [2300/6000], Loss: 0.0227
Epoch [11/30], Batch [2400/6000], Loss: 0.0195
Epoch [11/30], Batch [2500/6000], Loss: 0.0192
Epoch [11/30], Batch [2600/6000], Loss: 0.0324
Epoch [11/30], Batch [2700/6000], Loss: 0.0257
Epoch [11/30], Batch [2800/6000], Loss: 0.0418
Epoch [11/30], Batch [2900/6000], Loss: 0.1449
Epoch [11/30], Batch [3000/6000], Loss: 0.0216
Epoch [11/30], Batch [3100/6000], Loss: 0.0398
Epoch [11/30], Batch [3200/6000], Loss: 0.0700
Epoch [11/30], Batch [3300/6000], Loss: 0.0259
Epoch [11/30], Batch [3400/6000], Loss: 0.0546
Epoch [11/30], Batch [3500/6000], Loss: 0.0731
Epoch [11/30], Batch [3600/6000], Loss: 0.7940
Epoch [11/30], Batch [3700/6000], Loss: 0.0257
Epoch [11/30], Batch [3800/6000], Loss: 0.4444
Epoch [11/30], Batch [3900/6000], Loss: 0.0307
Epoch [11/30], Batch [4000/6000], Loss: 0.0261
Epoch [11/30], Batch [4100/6000], Loss: 0.0442
Epoch [11/30], Batch [4200/6000], Loss: 0.0492
Epoch [11/30], Batch [4300/6000], Loss: 0.0250
Epoch [11/30], Batch [4400/6000], Loss: 0.0252
Epoch [11/30], Batch [4500/6000], Loss: 0.0211
Epoch [11/30], Batch [4600/6000], Loss: 0.0201
Epoch [11/30], Batch [4700/6000], Loss: 0.0595
Epoch [11/30], Batch [4800/6000], Loss: 0.0801
Epoch [11/30], Batch [4900/6000], Loss: 0.0238
Epoch [11/30], Batch [5000/6000], Loss: 0.0171
Epoch [11/30], Batch [5100/6000], Loss: 0.0262
Epoch [11/30], Batch [5200/6000], Loss: 0.0239
Epoch [11/30], Batch [5300/6000], Loss: 0.1560
Epoch [11/30], Batch [5400/6000], Loss: 0.0948
Epoch [11/30], Batch [5500/6000], Loss: 0.0291
Epoch [11/30], Batch [5600/6000], Loss: 0.1779
Epoch [11/30], Batch [5700/6000], Loss: 0.0477
Epoch [11/30], Batch [5800/6000], Loss: 0.0768
Epoch [11/30], Batch [5900/6000], Loss: 0.0209
Epoch [11/30], Loss: 0.0637
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.0429
Epoch [12/30], Batch [100/6000], Loss: 0.0227
Epoch [12/30], Batch [200/6000], Loss: 0.0218
Epoch [12/30], Batch [300/6000], Loss: 0.0575
Epoch [12/30], Batch [400/6000], Loss: 0.0553
Epoch [12/30], Batch [500/6000], Loss: 0.0205
Epoch [12/30], Batch [600/6000], Loss: 0.0240
Epoch [12/30], Batch [700/6000], Loss: 0.0280
Epoch [12/30], Batch [800/6000], Loss: 0.0313
Epoch [12/30], Batch [900/6000], Loss: 0.1094
Epoch [12/30], Batch [1000/6000], Loss: 0.0366
Epoch [12/30], Batch [1100/6000], Loss: 0.0248
Epoch [12/30], Batch [1200/6000], Loss: 0.0233
Epoch [12/30], Batch [1300/6000], Loss: 0.0294
Epoch [12/30], Batch [1400/6000], Loss: 0.0259
Epoch [12/30], Batch [1500/6000], Loss: 0.0377
Epoch [12/30], Batch [1600/6000], Loss: 0.0280
Epoch [12/30], Batch [1700/6000], Loss: 0.0289
Epoch [12/30], Batch [1800/6000], Loss: 0.0267
Epoch [12/30], Batch [1900/6000], Loss: 0.1883
Epoch [12/30], Batch [2000/6000], Loss: 0.0221
Epoch [12/30], Batch [2100/6000], Loss: 0.0676
Epoch [12/30], Batch [2200/6000], Loss: 0.0400
Epoch [12/30], Batch [2300/6000], Loss: 0.0752
Epoch [12/30], Batch [2400/6000], Loss: 0.1181
Epoch [12/30], Batch [2500/6000], Loss: 0.0307
Epoch [12/30], Batch [2600/6000], Loss: 0.0200
Epoch [12/30], Batch [2700/6000], Loss: 0.0401
Epoch [12/30], Batch [2800/6000], Loss: 0.0363
Epoch [12/30], Batch [2900/6000], Loss: 0.0218
Epoch [12/30], Batch [3000/6000], Loss: 0.0391
Epoch [12/30], Batch [3100/6000], Loss: 0.0270
Epoch [12/30], Batch [3200/6000], Loss: 0.0207
Epoch [12/30], Batch [3300/6000], Loss: 0.0431
Epoch [12/30], Batch [3400/6000], Loss: 0.0266
Epoch [12/30], Batch [3500/6000], Loss: 0.0194
Epoch [12/30], Batch [3600/6000], Loss: 0.0195
Epoch [12/30], Batch [3700/6000], Loss: 0.0290
Epoch [12/30], Batch [3800/6000], Loss: 0.4802
Epoch [12/30], Batch [3900/6000], Loss: 0.0281
Epoch [12/30], Batch [4000/6000], Loss: 0.0302
Epoch [12/30], Batch [4100/6000], Loss: 0.0290
Epoch [12/30], Batch [4200/6000], Loss: 0.0583
Epoch [12/30], Batch [4300/6000], Loss: 0.5944
Epoch [12/30], Batch [4400/6000], Loss: 0.0201
Epoch [12/30], Batch [4500/6000], Loss: 0.1446
Epoch [12/30], Batch [4600/6000], Loss: 0.0630
Epoch [12/30], Batch [4700/6000], Loss: 0.0272
Epoch [12/30], Batch [4800/6000], Loss: 0.3611
Epoch [12/30], Batch [4900/6000], Loss: 0.0266
Epoch [12/30], Batch [5000/6000], Loss: 0.0227
Epoch [12/30], Batch [5100/6000], Loss: 0.0215
Epoch [12/30], Batch [5200/6000], Loss: 0.0354
Epoch [12/30], Batch [5300/6000], Loss: 0.0436
Epoch [12/30], Batch [5400/6000], Loss: 0.0506
Epoch [12/30], Batch [5500/6000], Loss: 0.0197
Epoch [12/30], Batch [5600/6000], Loss: 0.0188
Epoch [12/30], Batch [5700/6000], Loss: 0.0326
Epoch [12/30], Batch [5800/6000], Loss: 0.0226
Epoch [12/30], Batch [5900/6000], Loss: 0.1072
Epoch [12/30], Loss: 0.0568
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.0334
Epoch [13/30], Batch [100/6000], Loss: 0.0231
Epoch [13/30], Batch [200/6000], Loss: 0.0761
Epoch [13/30], Batch [300/6000], Loss: 0.0205
Epoch [13/30], Batch [400/6000], Loss: 0.0240
Epoch [13/30], Batch [500/6000], Loss: 0.0646
Epoch [13/30], Batch [600/6000], Loss: 0.2546
Epoch [13/30], Batch [700/6000], Loss: 0.1197
Epoch [13/30], Batch [800/6000], Loss: 0.0339
Epoch [13/30], Batch [900/6000], Loss: 0.0214
Epoch [13/30], Batch [1000/6000], Loss: 0.0246
Epoch [13/30], Batch [1100/6000], Loss: 0.0288
Epoch [13/30], Batch [1200/6000], Loss: 0.0221
Epoch [13/30], Batch [1300/6000], Loss: 0.0209
Epoch [13/30], Batch [1400/6000], Loss: 0.0212
Epoch [13/30], Batch [1500/6000], Loss: 0.0779
Epoch [13/30], Batch [1600/6000], Loss: 0.0202
Epoch [13/30], Batch [1700/6000], Loss: 0.0243
Epoch [13/30], Batch [1800/6000], Loss: 0.0190
Epoch [13/30], Batch [1900/6000], Loss: 0.0210
Epoch [13/30], Batch [2000/6000], Loss: 0.0167
Epoch [13/30], Batch [2100/6000], Loss: 0.0231
Epoch [13/30], Batch [2200/6000], Loss: 0.1746
Epoch [13/30], Batch [2300/6000], Loss: 0.0240
Epoch [13/30], Batch [2400/6000], Loss: 0.2208
Epoch [13/30], Batch [2500/6000], Loss: 0.0243
Epoch [13/30], Batch [2600/6000], Loss: 0.0212
Epoch [13/30], Batch [2700/6000], Loss: 0.0393
Epoch [13/30], Batch [2800/6000], Loss: 0.0183
Epoch [13/30], Batch [2900/6000], Loss: 0.0286
Epoch [13/30], Batch [3000/6000], Loss: 0.0314
Epoch [13/30], Batch [3100/6000], Loss: 0.0246
Epoch [13/30], Batch [3200/6000], Loss: 0.0244
Epoch [13/30], Batch [3300/6000], Loss: 0.0219
Epoch [13/30], Batch [3400/6000], Loss: 0.0204
Epoch [13/30], Batch [3500/6000], Loss: 0.0232
Epoch [13/30], Batch [3600/6000], Loss: 0.0196
Epoch [13/30], Batch [3700/6000], Loss: 0.0251
Epoch [13/30], Batch [3800/6000], Loss: 0.0248
Epoch [13/30], Batch [3900/6000], Loss: 0.0255
Epoch [13/30], Batch [4000/6000], Loss: 0.0243
Epoch [13/30], Batch [4100/6000], Loss: 0.0289
Epoch [13/30], Batch [4200/6000], Loss: 0.0357
Epoch [13/30], Batch [4300/6000], Loss: 0.0228
Epoch [13/30], Batch [4400/6000], Loss: 0.0221
Epoch [13/30], Batch [4500/6000], Loss: 0.0232
Epoch [13/30], Batch [4600/6000], Loss: 0.0572
Epoch [13/30], Batch [4700/6000], Loss: 0.0247
Epoch [13/30], Batch [4800/6000], Loss: 0.1401
Epoch [13/30], Batch [4900/6000], Loss: 0.0258
Epoch [13/30], Batch [5000/6000], Loss: 0.0236
Epoch [13/30], Batch [5100/6000], Loss: 0.0844
Epoch [13/30], Batch [5200/6000], Loss: 0.1987
Epoch [13/30], Batch [5300/6000], Loss: 0.0247
Epoch [13/30], Batch [5400/6000], Loss: 0.0619
Epoch [13/30], Batch [5500/6000], Loss: 0.1028
Epoch [13/30], Batch [5600/6000], Loss: 0.1175
Epoch [13/30], Batch [5700/6000], Loss: 0.0460
Epoch [13/30], Batch [5800/6000], Loss: 0.0247
Epoch [13/30], Batch [5900/6000], Loss: 0.0223
Epoch [13/30], Loss: 0.0531
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.0235
Epoch [14/30], Batch [100/6000], Loss: 0.0242
Epoch [14/30], Batch [200/6000], Loss: 0.1541
Epoch [14/30], Batch [300/6000], Loss: 0.0208
Epoch [14/30], Batch [400/6000], Loss: 0.0337
Epoch [14/30], Batch [500/6000], Loss: 0.0222
Epoch [14/30], Batch [600/6000], Loss: 0.0168
Epoch [14/30], Batch [700/6000], Loss: 0.0339
Epoch [14/30], Batch [800/6000], Loss: 0.0346
Epoch [14/30], Batch [900/6000], Loss: 0.0419
Epoch [14/30], Batch [1000/6000], Loss: 0.0252
Epoch [14/30], Batch [1100/6000], Loss: 0.0598
Epoch [14/30], Batch [1200/6000], Loss: 0.0226
Epoch [14/30], Batch [1300/6000], Loss: 0.0266
Epoch [14/30], Batch [1400/6000], Loss: 0.0244
Epoch [14/30], Batch [1500/6000], Loss: 0.0245
Epoch [14/30], Batch [1600/6000], Loss: 0.0307
Epoch [14/30], Batch [1700/6000], Loss: 0.0244
Epoch [14/30], Batch [1800/6000], Loss: 0.0194
Epoch [14/30], Batch [1900/6000], Loss: 0.0460
Epoch [14/30], Batch [2000/6000], Loss: 0.0257
Epoch [14/30], Batch [2100/6000], Loss: 0.0178
Epoch [14/30], Batch [2200/6000], Loss: 0.0252
Epoch [14/30], Batch [2300/6000], Loss: 0.0197
Epoch [14/30], Batch [2400/6000], Loss: 0.0203
Epoch [14/30], Batch [2500/6000], Loss: 0.0237
Epoch [14/30], Batch [2600/6000], Loss: 0.0542
Epoch [14/30], Batch [2700/6000], Loss: 0.0252
Epoch [14/30], Batch [2800/6000], Loss: 0.0145
Epoch [14/30], Batch [2900/6000], Loss: 0.0233
Epoch [14/30], Batch [3000/6000], Loss: 0.0213
Epoch [14/30], Batch [3100/6000], Loss: 0.0223
Epoch [14/30], Batch [3200/6000], Loss: 0.0285
Epoch [14/30], Batch [3300/6000], Loss: 0.0283
Epoch [14/30], Batch [3400/6000], Loss: 0.0191
Epoch [14/30], Batch [3500/6000], Loss: 0.0220
Epoch [14/30], Batch [3600/6000], Loss: 0.0222
Epoch [14/30], Batch [3700/6000], Loss: 0.0199
Epoch [14/30], Batch [3800/6000], Loss: 0.0318
Epoch [14/30], Batch [3900/6000], Loss: 0.0232
Epoch [14/30], Batch [4000/6000], Loss: 0.1453
Epoch [14/30], Batch [4100/6000], Loss: 0.0235
Epoch [14/30], Batch [4200/6000], Loss: 0.0266
Epoch [14/30], Batch [4300/6000], Loss: 0.0222
Epoch [14/30], Batch [4400/6000], Loss: 0.1597
Epoch [14/30], Batch [4500/6000], Loss: 0.0424
Epoch [14/30], Batch [4600/6000], Loss: 0.0176
Epoch [14/30], Batch [4700/6000], Loss: 0.0339
Epoch [14/30], Batch [4800/6000], Loss: 0.0204
Epoch [14/30], Batch [4900/6000], Loss: 0.0273
Epoch [14/30], Batch [5000/6000], Loss: 0.0273
Epoch [14/30], Batch [5100/6000], Loss: 0.0204
Epoch [14/30], Batch [5200/6000], Loss: 0.0222
Epoch [14/30], Batch [5300/6000], Loss: 0.0295
Epoch [14/30], Batch [5400/6000], Loss: 0.0250
Epoch [14/30], Batch [5500/6000], Loss: 0.0260
Epoch [14/30], Batch [5600/6000], Loss: 0.0595
Epoch [14/30], Batch [5700/6000], Loss: 0.0272
Epoch [14/30], Batch [5800/6000], Loss: 0.0374
Epoch [14/30], Batch [5900/6000], Loss: 0.0347
Epoch [14/30], Loss: 0.0498
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.0273
Epoch [15/30], Batch [100/6000], Loss: 0.0376
Epoch [15/30], Batch [200/6000], Loss: 0.2273
Epoch [15/30], Batch [300/6000], Loss: 0.1349
Epoch [15/30], Batch [400/6000], Loss: 0.0189
Epoch [15/30], Batch [500/6000], Loss: 0.3458
Epoch [15/30], Batch [600/6000], Loss: 0.0243
Epoch [15/30], Batch [700/6000], Loss: 0.0193
Epoch [15/30], Batch [800/6000], Loss: 0.0197
Epoch [15/30], Batch [900/6000], Loss: 0.1866
Epoch [15/30], Batch [1000/6000], Loss: 0.0203
Epoch [15/30], Batch [1100/6000], Loss: 0.0399
Epoch [15/30], Batch [1200/6000], Loss: 0.0228
Epoch [15/30], Batch [1300/6000], Loss: 0.0229
Epoch [15/30], Batch [1400/6000], Loss: 0.0247
Epoch [15/30], Batch [1500/6000], Loss: 0.0288
Epoch [15/30], Batch [1600/6000], Loss: 0.0178
Epoch [15/30], Batch [1700/6000], Loss: 0.0220
Epoch [15/30], Batch [1800/6000], Loss: 0.0250
Epoch [15/30], Batch [1900/6000], Loss: 0.0335
Epoch [15/30], Batch [2000/6000], Loss: 0.0201
Epoch [15/30], Batch [2100/6000], Loss: 0.0169
Epoch [15/30], Batch [2200/6000], Loss: 0.0285
Epoch [15/30], Batch [2300/6000], Loss: 0.0210
Epoch [15/30], Batch [2400/6000], Loss: 0.0356
Epoch [15/30], Batch [2500/6000], Loss: 0.0248
Epoch [15/30], Batch [2600/6000], Loss: 0.0161
Epoch [15/30], Batch [2700/6000], Loss: 0.0342
Epoch [15/30], Batch [2800/6000], Loss: 0.0180
Epoch [15/30], Batch [2900/6000], Loss: 0.0204
Epoch [15/30], Batch [3000/6000], Loss: 0.0229
Epoch [15/30], Batch [3100/6000], Loss: 0.0261
Epoch [15/30], Batch [3200/6000], Loss: 0.0156
Epoch [15/30], Batch [3300/6000], Loss: 0.0128
Epoch [15/30], Batch [3400/6000], Loss: 0.0208
Epoch [15/30], Batch [3500/6000], Loss: 0.0184
Epoch [15/30], Batch [3600/6000], Loss: 0.0148
Epoch [15/30], Batch [3700/6000], Loss: 0.0208
Epoch [15/30], Batch [3800/6000], Loss: 0.0250
Epoch [15/30], Batch [3900/6000], Loss: 0.0636
Epoch [15/30], Batch [4000/6000], Loss: 0.0221
Epoch [15/30], Batch [4100/6000], Loss: 0.0386
Epoch [15/30], Batch [4200/6000], Loss: 0.4867
Epoch [15/30], Batch [4300/6000], Loss: 0.0447
Epoch [15/30], Batch [4400/6000], Loss: 0.0172
Epoch [15/30], Batch [4500/6000], Loss: 0.0368
Epoch [15/30], Batch [4600/6000], Loss: 0.0990
Epoch [15/30], Batch [4700/6000], Loss: 0.0218
Epoch [15/30], Batch [4800/6000], Loss: 0.0264
Epoch [15/30], Batch [4900/6000], Loss: 0.0188
Epoch [15/30], Batch [5000/6000], Loss: 0.0313
Epoch [15/30], Batch [5100/6000], Loss: 0.0190
Epoch [15/30], Batch [5200/6000], Loss: 0.3038
Epoch [15/30], Batch [5300/6000], Loss: 0.0194
Epoch [15/30], Batch [5400/6000], Loss: 0.0167
Epoch [15/30], Batch [5500/6000], Loss: 0.0239
Epoch [15/30], Batch [5600/6000], Loss: 0.2333
Epoch [15/30], Batch [5700/6000], Loss: 0.0204
Epoch [15/30], Batch [5800/6000], Loss: 0.0210
Epoch [15/30], Batch [5900/6000], Loss: 0.0209
Epoch [15/30], Loss: 0.0458
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.0262
Epoch [16/30], Batch [100/6000], Loss: 0.0206
Epoch [16/30], Batch [200/6000], Loss: 0.0205
Epoch [16/30], Batch [300/6000], Loss: 0.0140
Epoch [16/30], Batch [400/6000], Loss: 0.0236
Epoch [16/30], Batch [500/6000], Loss: 0.0252
Epoch [16/30], Batch [600/6000], Loss: 0.0216
Epoch [16/30], Batch [700/6000], Loss: 0.0203
Epoch [16/30], Batch [800/6000], Loss: 0.0267
Epoch [16/30], Batch [900/6000], Loss: 0.0206
Epoch [16/30], Batch [1000/6000], Loss: 0.0213
Epoch [16/30], Batch [1100/6000], Loss: 0.0929
Epoch [16/30], Batch [1200/6000], Loss: 0.0236
Epoch [16/30], Batch [1300/6000], Loss: 0.0892
Epoch [16/30], Batch [1400/6000], Loss: 0.0353
Epoch [16/30], Batch [1500/6000], Loss: 0.0247
Epoch [16/30], Batch [1600/6000], Loss: 0.0224
Epoch [16/30], Batch [1700/6000], Loss: 0.0192
Epoch [16/30], Batch [1800/6000], Loss: 0.0272
Epoch [16/30], Batch [1900/6000], Loss: 0.0225
Epoch [16/30], Batch [2000/6000], Loss: 0.0229
Epoch [16/30], Batch [2100/6000], Loss: 0.0193
Epoch [16/30], Batch [2200/6000], Loss: 0.0206
Epoch [16/30], Batch [2300/6000], Loss: 0.0202
Epoch [16/30], Batch [2400/6000], Loss: 0.0392
Epoch [16/30], Batch [2500/6000], Loss: 0.3393
Epoch [16/30], Batch [2600/6000], Loss: 0.0283
Epoch [16/30], Batch [2700/6000], Loss: 0.0227
Epoch [16/30], Batch [2800/6000], Loss: 0.0417
Epoch [16/30], Batch [2900/6000], Loss: 0.0218
Epoch [16/30], Batch [3000/6000], Loss: 0.0263
Epoch [16/30], Batch [3100/6000], Loss: 0.0183
Epoch [16/30], Batch [3200/6000], Loss: 0.0463
Epoch [16/30], Batch [3300/6000], Loss: 0.0871
Epoch [16/30], Batch [3400/6000], Loss: 0.0243
Epoch [16/30], Batch [3500/6000], Loss: 0.0413
Epoch [16/30], Batch [3600/6000], Loss: 0.0234
Epoch [16/30], Batch [3700/6000], Loss: 0.0929
Epoch [16/30], Batch [3800/6000], Loss: 0.0348
Epoch [16/30], Batch [3900/6000], Loss: 0.0174
Epoch [16/30], Batch [4000/6000], Loss: 0.0188
Epoch [16/30], Batch [4100/6000], Loss: 0.0195
Epoch [16/30], Batch [4200/6000], Loss: 0.0254
Epoch [16/30], Batch [4300/6000], Loss: 0.0733
Epoch [16/30], Batch [4400/6000], Loss: 0.0201
Epoch [16/30], Batch [4500/6000], Loss: 0.0210
Epoch [16/30], Batch [4600/6000], Loss: 0.0194
Epoch [16/30], Batch [4700/6000], Loss: 0.0182
Epoch [16/30], Batch [4800/6000], Loss: 0.1001
Epoch [16/30], Batch [4900/6000], Loss: 0.2812
Epoch [16/30], Batch [5000/6000], Loss: 0.0580
Epoch [16/30], Batch [5100/6000], Loss: 0.0249
Epoch [16/30], Batch [5200/6000], Loss: 0.0228
Epoch [16/30], Batch [5300/6000], Loss: 0.0183
Epoch [16/30], Batch [5400/6000], Loss: 0.0227
Epoch [16/30], Batch [5500/6000], Loss: 0.1139
Epoch [16/30], Batch [5600/6000], Loss: 0.0171
Epoch [16/30], Batch [5700/6000], Loss: 0.0243
Epoch [16/30], Batch [5800/6000], Loss: 0.0386
Epoch [16/30], Batch [5900/6000], Loss: 0.0254
Epoch [16/30], Loss: 0.0436
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.0182
Epoch [17/30], Batch [100/6000], Loss: 0.1663
Epoch [17/30], Batch [200/6000], Loss: 0.0195
Epoch [17/30], Batch [300/6000], Loss: 0.0213
Epoch [17/30], Batch [400/6000], Loss: 0.0224
Epoch [17/30], Batch [500/6000], Loss: 0.0223
Epoch [17/30], Batch [600/6000], Loss: 0.0522
Epoch [17/30], Batch [700/6000], Loss: 0.0180
Epoch [17/30], Batch [800/6000], Loss: 0.0253
Epoch [17/30], Batch [900/6000], Loss: 0.0198
Epoch [17/30], Batch [1000/6000], Loss: 0.0227
Epoch [17/30], Batch [1100/6000], Loss: 0.0247
Epoch [17/30], Batch [1200/6000], Loss: 0.0212
Epoch [17/30], Batch [1300/6000], Loss: 0.0269
Epoch [17/30], Batch [1400/6000], Loss: 0.0272
Epoch [17/30], Batch [1500/6000], Loss: 0.0215
Epoch [17/30], Batch [1600/6000], Loss: 0.0161
Epoch [17/30], Batch [1700/6000], Loss: 0.0185
Epoch [17/30], Batch [1800/6000], Loss: 0.0172
Epoch [17/30], Batch [1900/6000], Loss: 0.0223
Epoch [17/30], Batch [2000/6000], Loss: 0.0228
Epoch [17/30], Batch [2100/6000], Loss: 0.0207
Epoch [17/30], Batch [2200/6000], Loss: 0.0217
Epoch [17/30], Batch [2300/6000], Loss: 0.0333
Epoch [17/30], Batch [2400/6000], Loss: 0.0185
Epoch [17/30], Batch [2500/6000], Loss: 0.0428
Epoch [17/30], Batch [2600/6000], Loss: 0.0697
Epoch [17/30], Batch [2700/6000], Loss: 0.0227
Epoch [17/30], Batch [2800/6000], Loss: 0.0188
Epoch [17/30], Batch [2900/6000], Loss: 0.0225
Epoch [17/30], Batch [3000/6000], Loss: 0.0499
Epoch [17/30], Batch [3100/6000], Loss: 0.0152
Epoch [17/30], Batch [3200/6000], Loss: 0.0215
Epoch [17/30], Batch [3300/6000], Loss: 0.0585
Epoch [17/30], Batch [3400/6000], Loss: 0.0239
Epoch [17/30], Batch [3500/6000], Loss: 0.0237
Epoch [17/30], Batch [3600/6000], Loss: 0.0196
Epoch [17/30], Batch [3700/6000], Loss: 0.0299
Epoch [17/30], Batch [3800/6000], Loss: 0.0169
Epoch [17/30], Batch [3900/6000], Loss: 0.0263
Epoch [17/30], Batch [4000/6000], Loss: 0.0220
Epoch [17/30], Batch [4100/6000], Loss: 0.0349
Epoch [17/30], Batch [4200/6000], Loss: 0.0261
Epoch [17/30], Batch [4300/6000], Loss: 0.0239
Epoch [17/30], Batch [4400/6000], Loss: 0.0268
Epoch [17/30], Batch [4500/6000], Loss: 0.1409
Epoch [17/30], Batch [4600/6000], Loss: 0.0154
Epoch [17/30], Batch [4700/6000], Loss: 0.0278
Epoch [17/30], Batch [4800/6000], Loss: 0.0173
Epoch [17/30], Batch [4900/6000], Loss: 0.0279
Epoch [17/30], Batch [5000/6000], Loss: 0.0217
Epoch [17/30], Batch [5100/6000], Loss: 0.0172
Epoch [17/30], Batch [5200/6000], Loss: 0.0352
Epoch [17/30], Batch [5300/6000], Loss: 0.0177
Epoch [17/30], Batch [5400/6000], Loss: 0.0236
Epoch [17/30], Batch [5500/6000], Loss: 0.0194
Epoch [17/30], Batch [5600/6000], Loss: 0.0240
Epoch [17/30], Batch [5700/6000], Loss: 0.0302
Epoch [17/30], Batch [5800/6000], Loss: 0.0345
Epoch [17/30], Batch [5900/6000], Loss: 0.0251
Epoch [17/30], Loss: 0.0414
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.0270
Epoch [18/30], Batch [100/6000], Loss: 0.0187
Epoch [18/30], Batch [200/6000], Loss: 0.0177
Epoch [18/30], Batch [300/6000], Loss: 0.0218
Epoch [18/30], Batch [400/6000], Loss: 0.0196
Epoch [18/30], Batch [500/6000], Loss: 0.2736
Epoch [18/30], Batch [600/6000], Loss: 0.0176
Epoch [18/30], Batch [700/6000], Loss: 0.0182
Epoch [18/30], Batch [800/6000], Loss: 0.0131
Epoch [18/30], Batch [900/6000], Loss: 0.0206
Epoch [18/30], Batch [1000/6000], Loss: 0.0191
Epoch [18/30], Batch [1100/6000], Loss: 0.0233
Epoch [18/30], Batch [1200/6000], Loss: 0.0192
Epoch [18/30], Batch [1300/6000], Loss: 0.0194
Epoch [18/30], Batch [1400/6000], Loss: 0.0200
Epoch [18/30], Batch [1500/6000], Loss: 0.0201
Epoch [18/30], Batch [1600/6000], Loss: 0.0207
Epoch [18/30], Batch [1700/6000], Loss: 0.0146
Epoch [18/30], Batch [1800/6000], Loss: 0.0259
Epoch [18/30], Batch [1900/6000], Loss: 0.0344
Epoch [18/30], Batch [2000/6000], Loss: 0.0298
Epoch [18/30], Batch [2100/6000], Loss: 0.0219
Epoch [18/30], Batch [2200/6000], Loss: 0.0209
Epoch [18/30], Batch [2300/6000], Loss: 0.0195
Epoch [18/30], Batch [2400/6000], Loss: 0.0244
Epoch [18/30], Batch [2500/6000], Loss: 0.1950
Epoch [18/30], Batch [2600/6000], Loss: 0.0284
Epoch [18/30], Batch [2700/6000], Loss: 0.0223
Epoch [18/30], Batch [2800/6000], Loss: 0.0199
Epoch [18/30], Batch [2900/6000], Loss: 0.0229
Epoch [18/30], Batch [3000/6000], Loss: 0.0171
Epoch [18/30], Batch [3100/6000], Loss: 0.0286
Epoch [18/30], Batch [3200/6000], Loss: 0.0248
Epoch [18/30], Batch [3300/6000], Loss: 0.0208
Epoch [18/30], Batch [3400/6000], Loss: 0.0231
Epoch [18/30], Batch [3500/6000], Loss: 0.0264
Epoch [18/30], Batch [3600/6000], Loss: 0.0302
Epoch [18/30], Batch [3700/6000], Loss: 0.0282
Epoch [18/30], Batch [3800/6000], Loss: 0.0229
Epoch [18/30], Batch [3900/6000], Loss: 0.0197
Epoch [18/30], Batch [4000/6000], Loss: 0.0177
Epoch [18/30], Batch [4100/6000], Loss: 0.0353
Epoch [18/30], Batch [4200/6000], Loss: 0.0393
Epoch [18/30], Batch [4300/6000], Loss: 0.0227
Epoch [18/30], Batch [4400/6000], Loss: 0.0251
Epoch [18/30], Batch [4500/6000], Loss: 0.0206
Epoch [18/30], Batch [4600/6000], Loss: 0.0248
Epoch [18/30], Batch [4700/6000], Loss: 0.0231
Epoch [18/30], Batch [4800/6000], Loss: 0.0169
Epoch [18/30], Batch [4900/6000], Loss: 0.0203
Epoch [18/30], Batch [5000/6000], Loss: 0.0213
Epoch [18/30], Batch [5100/6000], Loss: 0.0193
Epoch [18/30], Batch [5200/6000], Loss: 0.0148
Epoch [18/30], Batch [5300/6000], Loss: 0.0228
Epoch [18/30], Batch [5400/6000], Loss: 0.0845
Epoch [18/30], Batch [5500/6000], Loss: 0.0259
Epoch [18/30], Batch [5600/6000], Loss: 0.2739
Epoch [18/30], Batch [5700/6000], Loss: 0.0178
Epoch [18/30], Batch [5800/6000], Loss: 0.0270
Epoch [18/30], Batch [5900/6000], Loss: 0.0198
Epoch [18/30], Loss: 0.0399
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.0175
Epoch [19/30], Batch [100/6000], Loss: 0.0215
Epoch [19/30], Batch [200/6000], Loss: 0.0191
Epoch [19/30], Batch [300/6000], Loss: 0.0237
Epoch [19/30], Batch [400/6000], Loss: 0.0199
Epoch [19/30], Batch [500/6000], Loss: 0.0194
Epoch [19/30], Batch [600/6000], Loss: 0.0221
Epoch [19/30], Batch [700/6000], Loss: 0.0232
Epoch [19/30], Batch [800/6000], Loss: 0.0418
Epoch [19/30], Batch [900/6000], Loss: 0.0148
Epoch [19/30], Batch [1000/6000], Loss: 0.0179
Epoch [19/30], Batch [1100/6000], Loss: 0.0213
Epoch [19/30], Batch [1200/6000], Loss: 0.0178
Epoch [19/30], Batch [1300/6000], Loss: 0.0181
Epoch [19/30], Batch [1400/6000], Loss: 0.0177
Epoch [19/30], Batch [1500/6000], Loss: 0.0213
Epoch [19/30], Batch [1600/6000], Loss: 0.0225
Epoch [19/30], Batch [1700/6000], Loss: 0.0211
Epoch [19/30], Batch [1800/6000], Loss: 0.0174
Epoch [19/30], Batch [1900/6000], Loss: 0.0369
Epoch [19/30], Batch [2000/6000], Loss: 0.0156
Epoch [19/30], Batch [2100/6000], Loss: 0.0253
Epoch [19/30], Batch [2200/6000], Loss: 0.0201
Epoch [19/30], Batch [2300/6000], Loss: 0.0206
Epoch [19/30], Batch [2400/6000], Loss: 0.0180
Epoch [19/30], Batch [2500/6000], Loss: 0.0195
Epoch [19/30], Batch [2600/6000], Loss: 0.0246
Epoch [19/30], Batch [2700/6000], Loss: 0.0553
Epoch [19/30], Batch [2800/6000], Loss: 0.0617
Epoch [19/30], Batch [2900/6000], Loss: 0.0177
Epoch [19/30], Batch [3000/6000], Loss: 0.0206
Epoch [19/30], Batch [3100/6000], Loss: 0.0406
Epoch [19/30], Batch [3200/6000], Loss: 0.0169
Epoch [19/30], Batch [3300/6000], Loss: 0.0707
Epoch [19/30], Batch [3400/6000], Loss: 0.0221
Epoch [19/30], Batch [3500/6000], Loss: 0.0909
Epoch [19/30], Batch [3600/6000], Loss: 0.0194
Epoch [19/30], Batch [3700/6000], Loss: 0.0174
Epoch [19/30], Batch [3800/6000], Loss: 0.0256
Epoch [19/30], Batch [3900/6000], Loss: 0.0249
Epoch [19/30], Batch [4000/6000], Loss: 0.0188
Epoch [19/30], Batch [4100/6000], Loss: 0.0273
Epoch [19/30], Batch [4200/6000], Loss: 0.0241
Epoch [19/30], Batch [4300/6000], Loss: 0.0182
Epoch [19/30], Batch [4400/6000], Loss: 0.0765
Epoch [19/30], Batch [4500/6000], Loss: 0.0324
Epoch [19/30], Batch [4600/6000], Loss: 0.0320
Epoch [19/30], Batch [4700/6000], Loss: 0.0194
Epoch [19/30], Batch [4800/6000], Loss: 0.0384
Epoch [19/30], Batch [4900/6000], Loss: 0.0232
Epoch [19/30], Batch [5000/6000], Loss: 0.0250
Epoch [19/30], Batch [5100/6000], Loss: 0.0291
Epoch [19/30], Batch [5200/6000], Loss: 0.0284
Epoch [19/30], Batch [5300/6000], Loss: 0.0208
Epoch [19/30], Batch [5400/6000], Loss: 0.0222
Epoch [19/30], Batch [5500/6000], Loss: 0.0217
Epoch [19/30], Batch [5600/6000], Loss: 0.0480
Epoch [19/30], Batch [5700/6000], Loss: 0.0197
Epoch [19/30], Batch [5800/6000], Loss: 0.0880
Epoch [19/30], Batch [5900/6000], Loss: 0.0189
Epoch [19/30], Loss: 0.0370
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.0259
Epoch [20/30], Batch [100/6000], Loss: 0.0480
Epoch [20/30], Batch [200/6000], Loss: 0.0210
Epoch [20/30], Batch [300/6000], Loss: 0.0287
Epoch [20/30], Batch [400/6000], Loss: 0.0194
Epoch [20/30], Batch [500/6000], Loss: 0.0198
Epoch [20/30], Batch [600/6000], Loss: 0.6310
Epoch [20/30], Batch [700/6000], Loss: 0.1147
Epoch [20/30], Batch [800/6000], Loss: 0.0175
Epoch [20/30], Batch [900/6000], Loss: 0.0224
Epoch [20/30], Batch [1000/6000], Loss: 0.0160
Epoch [20/30], Batch [1100/6000], Loss: 0.0198
Epoch [20/30], Batch [1200/6000], Loss: 0.0173
Epoch [20/30], Batch [1300/6000], Loss: 0.0188
Epoch [20/30], Batch [1400/6000], Loss: 0.0200
Epoch [20/30], Batch [1500/6000], Loss: 0.0172
Epoch [20/30], Batch [1600/6000], Loss: 0.0183
Epoch [20/30], Batch [1700/6000], Loss: 0.0203
Epoch [20/30], Batch [1800/6000], Loss: 0.0151
Epoch [20/30], Batch [1900/6000], Loss: 0.0217
Epoch [20/30], Batch [2000/6000], Loss: 0.0143
Epoch [20/30], Batch [2100/6000], Loss: 0.0272
Epoch [20/30], Batch [2200/6000], Loss: 0.0915
Epoch [20/30], Batch [2300/6000], Loss: 0.0160
Epoch [20/30], Batch [2400/6000], Loss: 0.0190
Epoch [20/30], Batch [2500/6000], Loss: 0.0213
Epoch [20/30], Batch [2600/6000], Loss: 0.0760
Epoch [20/30], Batch [2700/6000], Loss: 0.0172
Epoch [20/30], Batch [2800/6000], Loss: 0.0229
Epoch [20/30], Batch [2900/6000], Loss: 0.2677
Epoch [20/30], Batch [3000/6000], Loss: 0.0234
Epoch [20/30], Batch [3100/6000], Loss: 0.0200
Epoch [20/30], Batch [3200/6000], Loss: 0.0269
Epoch [20/30], Batch [3300/6000], Loss: 0.0218
Epoch [20/30], Batch [3400/6000], Loss: 0.0385
Epoch [20/30], Batch [3500/6000], Loss: 0.0234
Epoch [20/30], Batch [3600/6000], Loss: 0.0172
Epoch [20/30], Batch [3700/6000], Loss: 0.0193
Epoch [20/30], Batch [3800/6000], Loss: 0.0212
Epoch [20/30], Batch [3900/6000], Loss: 0.0194
Epoch [20/30], Batch [4000/6000], Loss: 0.0257
Epoch [20/30], Batch [4100/6000], Loss: 0.0160
Epoch [20/30], Batch [4200/6000], Loss: 0.0176
Epoch [20/30], Batch [4300/6000], Loss: 0.0247
Epoch [20/30], Batch [4400/6000], Loss: 0.0215
Epoch [20/30], Batch [4500/6000], Loss: 0.1313
Epoch [20/30], Batch [4600/6000], Loss: 0.1673
Epoch [20/30], Batch [4700/6000], Loss: 0.0188
Epoch [20/30], Batch [4800/6000], Loss: 0.0189
Epoch [20/30], Batch [4900/6000], Loss: 0.0374
Epoch [20/30], Batch [5000/6000], Loss: 0.0202
Epoch [20/30], Batch [5100/6000], Loss: 0.0206
Epoch [20/30], Batch [5200/6000], Loss: 0.0209
Epoch [20/30], Batch [5300/6000], Loss: 0.1778
Epoch [20/30], Batch [5400/6000], Loss: 0.0208
Epoch [20/30], Batch [5500/6000], Loss: 0.0255
Epoch [20/30], Batch [5600/6000], Loss: 0.0193
Epoch [20/30], Batch [5700/6000], Loss: 0.7486
Epoch [20/30], Batch [5800/6000], Loss: 0.0554
Epoch [20/30], Batch [5900/6000], Loss: 0.0191
Epoch [20/30], Loss: 0.0348
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.0192
Epoch [21/30], Batch [100/6000], Loss: 0.0167
Epoch [21/30], Batch [200/6000], Loss: 0.0181
Epoch [21/30], Batch [300/6000], Loss: 0.0199
Epoch [21/30], Batch [400/6000], Loss: 0.0213
Epoch [21/30], Batch [500/6000], Loss: 0.0166
Epoch [21/30], Batch [600/6000], Loss: 0.0244
Epoch [21/30], Batch [700/6000], Loss: 0.0194
Epoch [21/30], Batch [800/6000], Loss: 0.0194
Epoch [21/30], Batch [900/6000], Loss: 0.0206
Epoch [21/30], Batch [1000/6000], Loss: 0.0315
Epoch [21/30], Batch [1100/6000], Loss: 0.0183
Epoch [21/30], Batch [1200/6000], Loss: 0.0170
Epoch [21/30], Batch [1300/6000], Loss: 0.0179
Epoch [21/30], Batch [1400/6000], Loss: 0.0336
Epoch [21/30], Batch [1500/6000], Loss: 0.0204
Epoch [21/30], Batch [1600/6000], Loss: 0.0167
Epoch [21/30], Batch [1700/6000], Loss: 0.0222
Epoch [21/30], Batch [1800/6000], Loss: 0.0145
Epoch [21/30], Batch [1900/6000], Loss: 0.0274
Epoch [21/30], Batch [2000/6000], Loss: 0.0203
Epoch [21/30], Batch [2100/6000], Loss: 0.4313
Epoch [21/30], Batch [2200/6000], Loss: 0.0175
Epoch [21/30], Batch [2300/6000], Loss: 0.0207
Epoch [21/30], Batch [2400/6000], Loss: 0.0158
Epoch [21/30], Batch [2500/6000], Loss: 0.0158
Epoch [21/30], Batch [2600/6000], Loss: 0.0219
Epoch [21/30], Batch [2700/6000], Loss: 0.0216
Epoch [21/30], Batch [2800/6000], Loss: 0.0328
Epoch [21/30], Batch [2900/6000], Loss: 0.0187
Epoch [21/30], Batch [3000/6000], Loss: 0.0183
Epoch [21/30], Batch [3100/6000], Loss: 0.0247
Epoch [21/30], Batch [3200/6000], Loss: 0.0296
Epoch [21/30], Batch [3300/6000], Loss: 0.0236
Epoch [21/30], Batch [3400/6000], Loss: 0.0203
Epoch [21/30], Batch [3500/6000], Loss: 0.1419
Epoch [21/30], Batch [3600/6000], Loss: 0.0210
Epoch [21/30], Batch [3700/6000], Loss: 0.0216
Epoch [21/30], Batch [3800/6000], Loss: 0.0243
Epoch [21/30], Batch [3900/6000], Loss: 0.0566
Epoch [21/30], Batch [4000/6000], Loss: 0.0195
Epoch [21/30], Batch [4100/6000], Loss: 0.0186
Epoch [21/30], Batch [4200/6000], Loss: 0.2072
Epoch [21/30], Batch [4300/6000], Loss: 0.0226
Epoch [21/30], Batch [4400/6000], Loss: 0.0206
Epoch [21/30], Batch [4500/6000], Loss: 0.0263
Epoch [21/30], Batch [4600/6000], Loss: 0.0223
Epoch [21/30], Batch [4700/6000], Loss: 0.0190
Epoch [21/30], Batch [4800/6000], Loss: 0.0197
Epoch [21/30], Batch [4900/6000], Loss: 0.0207
Epoch [21/30], Batch [5000/6000], Loss: 0.0177
Epoch [21/30], Batch [5100/6000], Loss: 0.0222
Epoch [21/30], Batch [5200/6000], Loss: 0.0220
Epoch [21/30], Batch [5300/6000], Loss: 0.0189
Epoch [21/30], Batch [5400/6000], Loss: 0.0375
Epoch [21/30], Batch [5500/6000], Loss: 0.0205
Epoch [21/30], Batch [5600/6000], Loss: 0.0180
Epoch [21/30], Batch [5700/6000], Loss: 0.0237
Epoch [21/30], Batch [5800/6000], Loss: 0.0205
Epoch [21/30], Batch [5900/6000], Loss: 0.0209
Epoch [21/30], Loss: 0.0338
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.0239
Epoch [22/30], Batch [100/6000], Loss: 0.0206
Epoch [22/30], Batch [200/6000], Loss: 0.0178
Epoch [22/30], Batch [300/6000], Loss: 0.0158
Epoch [22/30], Batch [400/6000], Loss: 0.0194
Epoch [22/30], Batch [500/6000], Loss: 0.0191
Epoch [22/30], Batch [600/6000], Loss: 0.0140
Epoch [22/30], Batch [700/6000], Loss: 0.0431
Epoch [22/30], Batch [800/6000], Loss: 0.0223
Epoch [22/30], Batch [900/6000], Loss: 0.1331
Epoch [22/30], Batch [1000/6000], Loss: 0.0189
Epoch [22/30], Batch [1100/6000], Loss: 0.0228
Epoch [22/30], Batch [1200/6000], Loss: 0.0204
Epoch [22/30], Batch [1300/6000], Loss: 0.0144
Epoch [22/30], Batch [1400/6000], Loss: 0.0193
Epoch [22/30], Batch [1500/6000], Loss: 0.0230
Epoch [22/30], Batch [1600/6000], Loss: 0.0176
Epoch [22/30], Batch [1700/6000], Loss: 0.0160
Epoch [22/30], Batch [1800/6000], Loss: 0.0596
Epoch [22/30], Batch [1900/6000], Loss: 0.0199
Epoch [22/30], Batch [2000/6000], Loss: 0.0345
Epoch [22/30], Batch [2100/6000], Loss: 0.1473
Epoch [22/30], Batch [2200/6000], Loss: 0.0182
Epoch [22/30], Batch [2300/6000], Loss: 0.0227
Epoch [22/30], Batch [2400/6000], Loss: 0.0180
Epoch [22/30], Batch [2500/6000], Loss: 0.0170
Epoch [22/30], Batch [2600/6000], Loss: 0.0858
Epoch [22/30], Batch [2700/6000], Loss: 0.0168
Epoch [22/30], Batch [2800/6000], Loss: 0.3023
Epoch [22/30], Batch [2900/6000], Loss: 0.0181
Epoch [22/30], Batch [3000/6000], Loss: 0.1236
Epoch [22/30], Batch [3100/6000], Loss: 0.0195
Epoch [22/30], Batch [3200/6000], Loss: 0.0134
Epoch [22/30], Batch [3300/6000], Loss: 0.0188
Epoch [22/30], Batch [3400/6000], Loss: 0.0209
Epoch [22/30], Batch [3500/6000], Loss: 0.0160
Epoch [22/30], Batch [3600/6000], Loss: 0.0169
Epoch [22/30], Batch [3700/6000], Loss: 0.0238
Epoch [22/30], Batch [3800/6000], Loss: 0.0194
Epoch [22/30], Batch [3900/6000], Loss: 0.0227
Epoch [22/30], Batch [4000/6000], Loss: 0.0192
Epoch [22/30], Batch [4100/6000], Loss: 0.0736
Epoch [22/30], Batch [4200/6000], Loss: 0.4502
Epoch [22/30], Batch [4300/6000], Loss: 0.0255
Epoch [22/30], Batch [4400/6000], Loss: 0.0199
Epoch [22/30], Batch [4500/6000], Loss: 0.0588
Epoch [22/30], Batch [4600/6000], Loss: 0.0182
Epoch [22/30], Batch [4700/6000], Loss: 0.0189
Epoch [22/30], Batch [4800/6000], Loss: 0.0165
Epoch [22/30], Batch [4900/6000], Loss: 0.0417
Epoch [22/30], Batch [5000/6000], Loss: 0.0292
Epoch [22/30], Batch [5100/6000], Loss: 0.0269
Epoch [22/30], Batch [5200/6000], Loss: 0.0180
Epoch [22/30], Batch [5300/6000], Loss: 0.0191
Epoch [22/30], Batch [5400/6000], Loss: 0.0173
Epoch [22/30], Batch [5500/6000], Loss: 0.0279
Epoch [22/30], Batch [5600/6000], Loss: 0.0249
Epoch [22/30], Batch [5700/6000], Loss: 0.0214
Epoch [22/30], Batch [5800/6000], Loss: 0.0249
Epoch [22/30], Batch [5900/6000], Loss: 0.0176
Epoch [22/30], Loss: 0.0336
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.0159
Epoch [23/30], Batch [100/6000], Loss: 0.0172
Epoch [23/30], Batch [200/6000], Loss: 0.0187
Epoch [23/30], Batch [300/6000], Loss: 0.0200
Epoch [23/30], Batch [400/6000], Loss: 0.0173
Epoch [23/30], Batch [500/6000], Loss: 0.0197
Epoch [23/30], Batch [600/6000], Loss: 0.0229
Epoch [23/30], Batch [700/6000], Loss: 0.0144
Epoch [23/30], Batch [800/6000], Loss: 0.0175
Epoch [23/30], Batch [900/6000], Loss: 0.0188
Epoch [23/30], Batch [1000/6000], Loss: 0.0183
Epoch [23/30], Batch [1100/6000], Loss: 0.0181
Epoch [23/30], Batch [1200/6000], Loss: 0.0295
Epoch [23/30], Batch [1300/6000], Loss: 0.0184
Epoch [23/30], Batch [1400/6000], Loss: 0.0204
Epoch [23/30], Batch [1500/6000], Loss: 0.0204
Epoch [23/30], Batch [1600/6000], Loss: 0.0606
Epoch [23/30], Batch [1700/6000], Loss: 0.0206
Epoch [23/30], Batch [1800/6000], Loss: 0.0238
Epoch [23/30], Batch [1900/6000], Loss: 0.0200
Epoch [23/30], Batch [2000/6000], Loss: 0.0229
Epoch [23/30], Batch [2100/6000], Loss: 0.0202
Epoch [23/30], Batch [2200/6000], Loss: 0.0242
Epoch [23/30], Batch [2300/6000], Loss: 0.0218
Epoch [23/30], Batch [2400/6000], Loss: 0.0252
Epoch [23/30], Batch [2500/6000], Loss: 0.0243
Epoch [23/30], Batch [2600/6000], Loss: 0.0273
Epoch [23/30], Batch [2700/6000], Loss: 0.0183
Epoch [23/30], Batch [2800/6000], Loss: 0.0185
Epoch [23/30], Batch [2900/6000], Loss: 0.0204
Epoch [23/30], Batch [3000/6000], Loss: 0.0199
Epoch [23/30], Batch [3100/6000], Loss: 0.0180
Epoch [23/30], Batch [3200/6000], Loss: 0.0206
Epoch [23/30], Batch [3300/6000], Loss: 0.0183
Epoch [23/30], Batch [3400/6000], Loss: 0.0195
Epoch [23/30], Batch [3500/6000], Loss: 0.0171
Epoch [23/30], Batch [3600/6000], Loss: 0.0265
Epoch [23/30], Batch [3700/6000], Loss: 0.0208
Epoch [23/30], Batch [3800/6000], Loss: 0.0173
Epoch [23/30], Batch [3900/6000], Loss: 0.0208
Epoch [23/30], Batch [4000/6000], Loss: 0.0180
Epoch [23/30], Batch [4100/6000], Loss: 0.0368
Epoch [23/30], Batch [4200/6000], Loss: 0.0606
Epoch [23/30], Batch [4300/6000], Loss: 0.0214
Epoch [23/30], Batch [4400/6000], Loss: 0.0148
Epoch [23/30], Batch [4500/6000], Loss: 0.0207
Epoch [23/30], Batch [4600/6000], Loss: 0.0238
Epoch [23/30], Batch [4700/6000], Loss: 0.0217
Epoch [23/30], Batch [4800/6000], Loss: 0.2852
Epoch [23/30], Batch [4900/6000], Loss: 0.0171
Epoch [23/30], Batch [5000/6000], Loss: 0.0265
Epoch [23/30], Batch [5100/6000], Loss: 0.0215
Epoch [23/30], Batch [5200/6000], Loss: 0.0234
Epoch [23/30], Batch [5300/6000], Loss: 0.0168
Epoch [23/30], Batch [5400/6000], Loss: 0.0181
Epoch [23/30], Batch [5500/6000], Loss: 0.0186
Epoch [23/30], Batch [5600/6000], Loss: 0.0187
Epoch [23/30], Batch [5700/6000], Loss: 0.0185
Epoch [23/30], Batch [5800/6000], Loss: 0.0196
Epoch [23/30], Batch [5900/6000], Loss: 0.0197
Epoch [23/30], Loss: 0.0312
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.0155
Epoch [24/30], Batch [100/6000], Loss: 0.0207
Epoch [24/30], Batch [200/6000], Loss: 0.0149
Epoch [24/30], Batch [300/6000], Loss: 0.0166
Epoch [24/30], Batch [400/6000], Loss: 0.0139
Epoch [24/30], Batch [500/6000], Loss: 0.0201
Epoch [24/30], Batch [600/6000], Loss: 0.0205
Epoch [24/30], Batch [700/6000], Loss: 0.0160
Epoch [24/30], Batch [800/6000], Loss: 0.0164
Epoch [24/30], Batch [900/6000], Loss: 0.0509
Epoch [24/30], Batch [1000/6000], Loss: 0.0176
Epoch [24/30], Batch [1100/6000], Loss: 0.0175
Epoch [24/30], Batch [1200/6000], Loss: 0.0160
Epoch [24/30], Batch [1300/6000], Loss: 0.0200
Epoch [24/30], Batch [1400/6000], Loss: 0.0193
Epoch [24/30], Batch [1500/6000], Loss: 0.0196
Epoch [24/30], Batch [1600/6000], Loss: 0.0235
Epoch [24/30], Batch [1700/6000], Loss: 0.0236
Epoch [24/30], Batch [1800/6000], Loss: 0.0176
Epoch [24/30], Batch [1900/6000], Loss: 0.0178
Epoch [24/30], Batch [2000/6000], Loss: 0.0174
Epoch [24/30], Batch [2100/6000], Loss: 0.0197
Epoch [24/30], Batch [2200/6000], Loss: 0.0204
Epoch [24/30], Batch [2300/6000], Loss: 0.0212
Epoch [24/30], Batch [2400/6000], Loss: 0.0136
Epoch [24/30], Batch [2500/6000], Loss: 0.0135
Epoch [24/30], Batch [2600/6000], Loss: 0.0203
Epoch [24/30], Batch [2700/6000], Loss: 0.1261
Epoch [24/30], Batch [2800/6000], Loss: 0.0196
Epoch [24/30], Batch [2900/6000], Loss: 0.0166
Epoch [24/30], Batch [3000/6000], Loss: 0.0145
Epoch [24/30], Batch [3100/6000], Loss: 0.0315
Epoch [24/30], Batch [3200/6000], Loss: 0.0198
Epoch [24/30], Batch [3300/6000], Loss: 0.0128
Epoch [24/30], Batch [3400/6000], Loss: 0.0212
Epoch [24/30], Batch [3500/6000], Loss: 0.0164
Epoch [24/30], Batch [3600/6000], Loss: 0.0186
Epoch [24/30], Batch [3700/6000], Loss: 0.0250
Epoch [24/30], Batch [3800/6000], Loss: 0.0242
Epoch [24/30], Batch [3900/6000], Loss: 0.0154
Epoch [24/30], Batch [4000/6000], Loss: 0.0160
Epoch [24/30], Batch [4100/6000], Loss: 0.0251
Epoch [24/30], Batch [4200/6000], Loss: 0.0184
Epoch [24/30], Batch [4300/6000], Loss: 0.0170
Epoch [24/30], Batch [4400/6000], Loss: 0.0166
Epoch [24/30], Batch [4500/6000], Loss: 0.0304
Epoch [24/30], Batch [4600/6000], Loss: 0.0180
Epoch [24/30], Batch [4700/6000], Loss: 0.0186
Epoch [24/30], Batch [4800/6000], Loss: 0.0204
Epoch [24/30], Batch [4900/6000], Loss: 0.0249
Epoch [24/30], Batch [5000/6000], Loss: 0.0216
Epoch [24/30], Batch [5100/6000], Loss: 0.0153
Epoch [24/30], Batch [5200/6000], Loss: 0.0219
Epoch [24/30], Batch [5300/6000], Loss: 0.0176
Epoch [24/30], Batch [5400/6000], Loss: 0.0146
Epoch [24/30], Batch [5500/6000], Loss: 0.0188
Epoch [24/30], Batch [5600/6000], Loss: 0.0195
Epoch [24/30], Batch [5700/6000], Loss: 0.0192
Epoch [24/30], Batch [5800/6000], Loss: 0.0204
Epoch [24/30], Batch [5900/6000], Loss: 0.0433
Epoch [24/30], Loss: 0.0310
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.0273
Epoch [25/30], Batch [100/6000], Loss: 0.0158
Epoch [25/30], Batch [200/6000], Loss: 0.0250
Epoch [25/30], Batch [300/6000], Loss: 0.0205
Epoch [25/30], Batch [400/6000], Loss: 0.0148
Epoch [25/30], Batch [500/6000], Loss: 0.0193
Epoch [25/30], Batch [600/6000], Loss: 0.0200
Epoch [25/30], Batch [700/6000], Loss: 0.0212
Epoch [25/30], Batch [800/6000], Loss: 0.0196
Epoch [25/30], Batch [900/6000], Loss: 0.0182
Epoch [25/30], Batch [1000/6000], Loss: 0.0156
Epoch [25/30], Batch [1100/6000], Loss: 0.0169
Epoch [25/30], Batch [1200/6000], Loss: 0.0555
Epoch [25/30], Batch [1300/6000], Loss: 0.0172
Epoch [25/30], Batch [1400/6000], Loss: 0.0199
Epoch [25/30], Batch [1500/6000], Loss: 0.0213
Epoch [25/30], Batch [1600/6000], Loss: 0.0181
Epoch [25/30], Batch [1700/6000], Loss: 0.0262
Epoch [25/30], Batch [1800/6000], Loss: 0.0247
Epoch [25/30], Batch [1900/6000], Loss: 0.0196
Epoch [25/30], Batch [2000/6000], Loss: 0.0223
Epoch [25/30], Batch [2100/6000], Loss: 0.0193
Epoch [25/30], Batch [2200/6000], Loss: 0.0159
Epoch [25/30], Batch [2300/6000], Loss: 0.0366
Epoch [25/30], Batch [2400/6000], Loss: 0.0219
Epoch [25/30], Batch [2500/6000], Loss: 0.0157
Epoch [25/30], Batch [2600/6000], Loss: 0.0145
Epoch [25/30], Batch [2700/6000], Loss: 0.0164
Epoch [25/30], Batch [2800/6000], Loss: 0.0252
Epoch [25/30], Batch [2900/6000], Loss: 0.0231
Epoch [25/30], Batch [3000/6000], Loss: 0.0199
Epoch [25/30], Batch [3100/6000], Loss: 0.0205
Epoch [25/30], Batch [3200/6000], Loss: 0.0178
Epoch [25/30], Batch [3300/6000], Loss: 0.0191
Epoch [25/30], Batch [3400/6000], Loss: 0.0324
Epoch [25/30], Batch [3500/6000], Loss: 0.0233
Epoch [25/30], Batch [3600/6000], Loss: 0.0197
Epoch [25/30], Batch [3700/6000], Loss: 0.0161
Epoch [25/30], Batch [3800/6000], Loss: 0.0144
Epoch [25/30], Batch [3900/6000], Loss: 0.0136
Epoch [25/30], Batch [4000/6000], Loss: 0.0204
Epoch [25/30], Batch [4100/6000], Loss: 0.0229
Epoch [25/30], Batch [4200/6000], Loss: 0.0640
Epoch [25/30], Batch [4300/6000], Loss: 0.0185
Epoch [25/30], Batch [4400/6000], Loss: 0.0984
Epoch [25/30], Batch [4500/6000], Loss: 0.0158
Epoch [25/30], Batch [4600/6000], Loss: 0.0190
Epoch [25/30], Batch [4700/6000], Loss: 0.0191
Epoch [25/30], Batch [4800/6000], Loss: 0.0155
Epoch [25/30], Batch [4900/6000], Loss: 0.0176
Epoch [25/30], Batch [5000/6000], Loss: 0.0202
Epoch [25/30], Batch [5100/6000], Loss: 0.0187
Epoch [25/30], Batch [5200/6000], Loss: 0.0197
Epoch [25/30], Batch [5300/6000], Loss: 0.0159
Epoch [25/30], Batch [5400/6000], Loss: 0.0219
Epoch [25/30], Batch [5500/6000], Loss: 0.0172
Epoch [25/30], Batch [5600/6000], Loss: 0.0144
Epoch [25/30], Batch [5700/6000], Loss: 0.0405
Epoch [25/30], Batch [5800/6000], Loss: 0.0235
Epoch [25/30], Batch [5900/6000], Loss: 0.0188
Epoch [25/30], Loss: 0.0308
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.0176
Epoch [26/30], Batch [100/6000], Loss: 0.0220
Epoch [26/30], Batch [200/6000], Loss: 0.0178
Epoch [26/30], Batch [300/6000], Loss: 0.0225
Epoch [26/30], Batch [400/6000], Loss: 0.0176
Epoch [26/30], Batch [500/6000], Loss: 0.0162
Epoch [26/30], Batch [600/6000], Loss: 0.0175
Epoch [26/30], Batch [700/6000], Loss: 0.0133
Epoch [26/30], Batch [800/6000], Loss: 0.0204
Epoch [26/30], Batch [900/6000], Loss: 0.0193
Epoch [26/30], Batch [1000/6000], Loss: 0.0169
Epoch [26/30], Batch [1100/6000], Loss: 0.0240
Epoch [26/30], Batch [1200/6000], Loss: 0.0389
Epoch [26/30], Batch [1300/6000], Loss: 0.0165
Epoch [26/30], Batch [1400/6000], Loss: 0.0225
Epoch [26/30], Batch [1500/6000], Loss: 0.0153
Epoch [26/30], Batch [1600/6000], Loss: 0.0192
Epoch [26/30], Batch [1700/6000], Loss: 0.0176
Epoch [26/30], Batch [1800/6000], Loss: 0.0224
Epoch [26/30], Batch [1900/6000], Loss: 0.0192
Epoch [26/30], Batch [2000/6000], Loss: 0.0227
Epoch [26/30], Batch [2100/6000], Loss: 0.0984
Epoch [26/30], Batch [2200/6000], Loss: 0.0188
Epoch [26/30], Batch [2300/6000], Loss: 0.0188
Epoch [26/30], Batch [2400/6000], Loss: 0.0707
Epoch [26/30], Batch [2500/6000], Loss: 0.0172
Epoch [26/30], Batch [2600/6000], Loss: 0.0167
Epoch [26/30], Batch [2700/6000], Loss: 0.0193
Epoch [26/30], Batch [2800/6000], Loss: 0.0993
Epoch [26/30], Batch [2900/6000], Loss: 0.0174
Epoch [26/30], Batch [3000/6000], Loss: 0.0184
Epoch [26/30], Batch [3100/6000], Loss: 0.0174
Epoch [26/30], Batch [3200/6000], Loss: 0.0308
Epoch [26/30], Batch [3300/6000], Loss: 0.0617
Epoch [26/30], Batch [3400/6000], Loss: 0.0186
Epoch [26/30], Batch [3500/6000], Loss: 0.0224
Epoch [26/30], Batch [3600/6000], Loss: 0.0208
Epoch [26/30], Batch [3700/6000], Loss: 0.0216
Epoch [26/30], Batch [3800/6000], Loss: 0.0175
Epoch [26/30], Batch [3900/6000], Loss: 0.0161
Epoch [26/30], Batch [4000/6000], Loss: 0.0177
Epoch [26/30], Batch [4100/6000], Loss: 0.0152
Epoch [26/30], Batch [4200/6000], Loss: 0.0121
Epoch [26/30], Batch [4300/6000], Loss: 0.0842
Epoch [26/30], Batch [4400/6000], Loss: 0.0133
Epoch [26/30], Batch [4500/6000], Loss: 0.0171
Epoch [26/30], Batch [4600/6000], Loss: 0.0180
Epoch [26/30], Batch [4700/6000], Loss: 0.0186
Epoch [26/30], Batch [4800/6000], Loss: 0.0166
Epoch [26/30], Batch [4900/6000], Loss: 0.0184
Epoch [26/30], Batch [5000/6000], Loss: 0.0179
Epoch [26/30], Batch [5100/6000], Loss: 0.0153
Epoch [26/30], Batch [5200/6000], Loss: 0.0180
Epoch [26/30], Batch [5300/6000], Loss: 0.0162
Epoch [26/30], Batch [5400/6000], Loss: 0.0173
Epoch [26/30], Batch [5500/6000], Loss: 0.0203
Epoch [26/30], Batch [5600/6000], Loss: 0.0177
Epoch [26/30], Batch [5700/6000], Loss: 0.6967
Epoch [26/30], Batch [5800/6000], Loss: 0.0184
Epoch [26/30], Batch [5900/6000], Loss: 0.0162
Epoch [26/30], Loss: 0.0283
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0185
Epoch [27/30], Batch [100/6000], Loss: 0.0168
Epoch [27/30], Batch [200/6000], Loss: 0.0176
Epoch [27/30], Batch [300/6000], Loss: 0.0180
Epoch [27/30], Batch [400/6000], Loss: 0.0208
Epoch [27/30], Batch [500/6000], Loss: 0.0192
Epoch [27/30], Batch [600/6000], Loss: 0.0234
Epoch [27/30], Batch [700/6000], Loss: 0.0185
Epoch [27/30], Batch [800/6000], Loss: 0.0182
Epoch [27/30], Batch [900/6000], Loss: 0.0306
Epoch [27/30], Batch [1000/6000], Loss: 0.0188
Epoch [27/30], Batch [1100/6000], Loss: 0.0171
Epoch [27/30], Batch [1200/6000], Loss: 0.0146
Epoch [27/30], Batch [1300/6000], Loss: 0.0183
Epoch [27/30], Batch [1400/6000], Loss: 0.0185
Epoch [27/30], Batch [1500/6000], Loss: 0.0180
Epoch [27/30], Batch [1600/6000], Loss: 0.0211
Epoch [27/30], Batch [1700/6000], Loss: 0.2289
Epoch [27/30], Batch [1800/6000], Loss: 0.0174
Epoch [27/30], Batch [1900/6000], Loss: 0.0184
Epoch [27/30], Batch [2000/6000], Loss: 0.0183
Epoch [27/30], Batch [2100/6000], Loss: 0.0189
Epoch [27/30], Batch [2200/6000], Loss: 0.0180
Epoch [27/30], Batch [2300/6000], Loss: 0.0198
Epoch [27/30], Batch [2400/6000], Loss: 0.0176
Epoch [27/30], Batch [2500/6000], Loss: 0.0190
Epoch [27/30], Batch [2600/6000], Loss: 0.0201
Epoch [27/30], Batch [2700/6000], Loss: 0.0104
Epoch [27/30], Batch [2800/6000], Loss: 0.0190
Epoch [27/30], Batch [2900/6000], Loss: 0.0225
Epoch [27/30], Batch [3000/6000], Loss: 0.0289
Epoch [27/30], Batch [3100/6000], Loss: 0.0177
Epoch [27/30], Batch [3200/6000], Loss: 0.0162
Epoch [27/30], Batch [3300/6000], Loss: 0.0154
Epoch [27/30], Batch [3400/6000], Loss: 0.0167
Epoch [27/30], Batch [3500/6000], Loss: 0.0164
Epoch [27/30], Batch [3600/6000], Loss: 0.0181
Epoch [27/30], Batch [3700/6000], Loss: 0.0263
Epoch [27/30], Batch [3800/6000], Loss: 0.0177
Epoch [27/30], Batch [3900/6000], Loss: 0.0171
Epoch [27/30], Batch [4000/6000], Loss: 0.0172
Epoch [27/30], Batch [4100/6000], Loss: 0.0184
Epoch [27/30], Batch [4200/6000], Loss: 0.0212
Epoch [27/30], Batch [4300/6000], Loss: 0.0189
Epoch [27/30], Batch [4400/6000], Loss: 0.0200
Epoch [27/30], Batch [4500/6000], Loss: 0.0180
Epoch [27/30], Batch [4600/6000], Loss: 0.1797
Epoch [27/30], Batch [4700/6000], Loss: 0.0217
Epoch [27/30], Batch [4800/6000], Loss: 0.0229
Epoch [27/30], Batch [4900/6000], Loss: 0.0162
Epoch [27/30], Batch [5000/6000], Loss: 0.0193
Epoch [27/30], Batch [5100/6000], Loss: 0.0181
Epoch [27/30], Batch [5200/6000], Loss: 0.0240
Epoch [27/30], Batch [5300/6000], Loss: 0.0237
Epoch [27/30], Batch [5400/6000], Loss: 0.0120
Epoch [27/30], Batch [5500/6000], Loss: 0.0356
Epoch [27/30], Batch [5600/6000], Loss: 0.0223
Epoch [27/30], Batch [5700/6000], Loss: 0.0188
Epoch [27/30], Batch [5800/6000], Loss: 0.0177
Epoch [27/30], Batch [5900/6000], Loss: 0.0193
Epoch [27/30], Loss: 0.0274
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.0213
Epoch [28/30], Batch [100/6000], Loss: 0.0177
Epoch [28/30], Batch [200/6000], Loss: 0.0150
Epoch [28/30], Batch [300/6000], Loss: 0.0175
Epoch [28/30], Batch [400/6000], Loss: 0.0238
Epoch [28/30], Batch [500/6000], Loss: 0.0148
Epoch [28/30], Batch [600/6000], Loss: 0.0184
Epoch [28/30], Batch [700/6000], Loss: 0.0214
Epoch [28/30], Batch [800/6000], Loss: 0.0393
Epoch [28/30], Batch [900/6000], Loss: 0.0135
Epoch [28/30], Batch [1000/6000], Loss: 0.0127
Epoch [28/30], Batch [1100/6000], Loss: 0.0290
Epoch [28/30], Batch [1200/6000], Loss: 0.0192
Epoch [28/30], Batch [1300/6000], Loss: 0.0219
Epoch [28/30], Batch [1400/6000], Loss: 0.0199
Epoch [28/30], Batch [1500/6000], Loss: 0.0228
Epoch [28/30], Batch [1600/6000], Loss: 0.0181
Epoch [28/30], Batch [1700/6000], Loss: 0.0183
Epoch [28/30], Batch [1800/6000], Loss: 0.0226
Epoch [28/30], Batch [1900/6000], Loss: 0.0215
Epoch [28/30], Batch [2000/6000], Loss: 0.0169
Epoch [28/30], Batch [2100/6000], Loss: 0.0216
Epoch [28/30], Batch [2200/6000], Loss: 0.0194
Epoch [28/30], Batch [2300/6000], Loss: 0.0192
Epoch [28/30], Batch [2400/6000], Loss: 0.0229
Epoch [28/30], Batch [2500/6000], Loss: 0.0197
Epoch [28/30], Batch [2600/6000], Loss: 0.0187
Epoch [28/30], Batch [2700/6000], Loss: 0.5681
Epoch [28/30], Batch [2800/6000], Loss: 0.0135
Epoch [28/30], Batch [2900/6000], Loss: 0.0222
Epoch [28/30], Batch [3000/6000], Loss: 0.0144
Epoch [28/30], Batch [3100/6000], Loss: 0.0245
Epoch [28/30], Batch [3200/6000], Loss: 0.0158
Epoch [28/30], Batch [3300/6000], Loss: 0.0215
Epoch [28/30], Batch [3400/6000], Loss: 0.0188
Epoch [28/30], Batch [3500/6000], Loss: 0.0165
Epoch [28/30], Batch [3600/6000], Loss: 0.0193
Epoch [28/30], Batch [3700/6000], Loss: 0.0239
Epoch [28/30], Batch [3800/6000], Loss: 0.0179
Epoch [28/30], Batch [3900/6000], Loss: 0.0166
Epoch [28/30], Batch [4000/6000], Loss: 0.0193
Epoch [28/30], Batch [4100/6000], Loss: 0.0155
Epoch [28/30], Batch [4200/6000], Loss: 0.0202
Epoch [28/30], Batch [4300/6000], Loss: 0.0207
Epoch [28/30], Batch [4400/6000], Loss: 0.0133
Epoch [28/30], Batch [4500/6000], Loss: 0.0202
Epoch [28/30], Batch [4600/6000], Loss: 0.0189
Epoch [28/30], Batch [4700/6000], Loss: 0.0145
Epoch [28/30], Batch [4800/6000], Loss: 0.0994
Epoch [28/30], Batch [4900/6000], Loss: 0.0149
Epoch [28/30], Batch [5000/6000], Loss: 0.0153
Epoch [28/30], Batch [5100/6000], Loss: 0.1222
Epoch [28/30], Batch [5200/6000], Loss: 0.0189
Epoch [28/30], Batch [5300/6000], Loss: 0.0256
Epoch [28/30], Batch [5400/6000], Loss: 0.0196
Epoch [28/30], Batch [5500/6000], Loss: 0.0174
Epoch [28/30], Batch [5600/6000], Loss: 0.0148
Epoch [28/30], Batch [5700/6000], Loss: 0.0138
Epoch [28/30], Batch [5800/6000], Loss: 0.0190
Epoch [28/30], Batch [5900/6000], Loss: 0.0213
Epoch [28/30], Loss: 0.0272
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0170
Epoch [29/30], Batch [100/6000], Loss: 0.0140
Epoch [29/30], Batch [200/6000], Loss: 0.0184
Epoch [29/30], Batch [300/6000], Loss: 0.0229
Epoch [29/30], Batch [400/6000], Loss: 0.0285
Epoch [29/30], Batch [500/6000], Loss: 0.0150
Epoch [29/30], Batch [600/6000], Loss: 0.0145
Epoch [29/30], Batch [700/6000], Loss: 0.0176
Epoch [29/30], Batch [800/6000], Loss: 0.0157
Epoch [29/30], Batch [900/6000], Loss: 0.0182
Epoch [29/30], Batch [1000/6000], Loss: 0.0152
Epoch [29/30], Batch [1100/6000], Loss: 0.0177
Epoch [29/30], Batch [1200/6000], Loss: 0.0149
Epoch [29/30], Batch [1300/6000], Loss: 0.0187
Epoch [29/30], Batch [1400/6000], Loss: 0.0158
Epoch [29/30], Batch [1500/6000], Loss: 0.0230
Epoch [29/30], Batch [1600/6000], Loss: 0.0194
Epoch [29/30], Batch [1700/6000], Loss: 0.0142
Epoch [29/30], Batch [1800/6000], Loss: 0.0162
Epoch [29/30], Batch [1900/6000], Loss: 0.0151
Epoch [29/30], Batch [2000/6000], Loss: 0.0163
Epoch [29/30], Batch [2100/6000], Loss: 0.0141
Epoch [29/30], Batch [2200/6000], Loss: 0.0214
Epoch [29/30], Batch [2300/6000], Loss: 0.0197
Epoch [29/30], Batch [2400/6000], Loss: 0.0169
Epoch [29/30], Batch [2500/6000], Loss: 0.0174
Epoch [29/30], Batch [2600/6000], Loss: 0.1159
Epoch [29/30], Batch [2700/6000], Loss: 0.0233
Epoch [29/30], Batch [2800/6000], Loss: 0.0152
Epoch [29/30], Batch [2900/6000], Loss: 0.0147
Epoch [29/30], Batch [3000/6000], Loss: 0.1157
Epoch [29/30], Batch [3100/6000], Loss: 0.0194
Epoch [29/30], Batch [3200/6000], Loss: 0.0169
Epoch [29/30], Batch [3300/6000], Loss: 0.0200
Epoch [29/30], Batch [3400/6000], Loss: 0.0175
Epoch [29/30], Batch [3500/6000], Loss: 0.0165
Epoch [29/30], Batch [3600/6000], Loss: 0.0211
Epoch [29/30], Batch [3700/6000], Loss: 0.0227
Epoch [29/30], Batch [3800/6000], Loss: 0.0186
Epoch [29/30], Batch [3900/6000], Loss: 0.0295
Epoch [29/30], Batch [4000/6000], Loss: 0.0160
Epoch [29/30], Batch [4100/6000], Loss: 0.0194
Epoch [29/30], Batch [4200/6000], Loss: 0.0168
Epoch [29/30], Batch [4300/6000], Loss: 0.0180
Epoch [29/30], Batch [4400/6000], Loss: 0.0303
Epoch [29/30], Batch [4500/6000], Loss: 0.0192
Epoch [29/30], Batch [4600/6000], Loss: 0.1622
Epoch [29/30], Batch [4700/6000], Loss: 0.0342
Epoch [29/30], Batch [4800/6000], Loss: 0.0181
Epoch [29/30], Batch [4900/6000], Loss: 0.0235
Epoch [29/30], Batch [5000/6000], Loss: 0.0124
Epoch [29/30], Batch [5100/6000], Loss: 0.0218
Epoch [29/30], Batch [5200/6000], Loss: 0.0161
Epoch [29/30], Batch [5300/6000], Loss: 0.0201
Epoch [29/30], Batch [5400/6000], Loss: 0.0168
Epoch [29/30], Batch [5500/6000], Loss: 0.0181
Epoch [29/30], Batch [5600/6000], Loss: 0.0417
Epoch [29/30], Batch [5700/6000], Loss: 0.0165
Epoch [29/30], Batch [5800/6000], Loss: 0.0224
Epoch [29/30], Batch [5900/6000], Loss: 0.0199
Epoch [29/30], Loss: 0.0271
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0186
Epoch [30/30], Batch [100/6000], Loss: 0.0193
Epoch [30/30], Batch [200/6000], Loss: 0.0206
Epoch [30/30], Batch [300/6000], Loss: 0.0139
Epoch [30/30], Batch [400/6000], Loss: 0.0283
Epoch [30/30], Batch [500/6000], Loss: 0.0156
Epoch [30/30], Batch [600/6000], Loss: 0.0172
Epoch [30/30], Batch [700/6000], Loss: 0.0183
Epoch [30/30], Batch [800/6000], Loss: 0.0218
Epoch [30/30], Batch [900/6000], Loss: 0.0207
Epoch [30/30], Batch [1000/6000], Loss: 0.0199
Epoch [30/30], Batch [1100/6000], Loss: 0.0178
Epoch [30/30], Batch [1200/6000], Loss: 0.0151
Epoch [30/30], Batch [1300/6000], Loss: 0.0182
Epoch [30/30], Batch [1400/6000], Loss: 0.0190
Epoch [30/30], Batch [1500/6000], Loss: 0.0178
Epoch [30/30], Batch [1600/6000], Loss: 0.0220
Epoch [30/30], Batch [1700/6000], Loss: 0.0193
Epoch [30/30], Batch [1800/6000], Loss: 0.0150
Epoch [30/30], Batch [1900/6000], Loss: 0.0162
Epoch [30/30], Batch [2000/6000], Loss: 0.0167
Epoch [30/30], Batch [2100/6000], Loss: 0.0185
Epoch [30/30], Batch [2200/6000], Loss: 0.0146
Epoch [30/30], Batch [2300/6000], Loss: 0.0310
Epoch [30/30], Batch [2400/6000], Loss: 0.0166
Epoch [30/30], Batch [2500/6000], Loss: 0.0199
Epoch [30/30], Batch [2600/6000], Loss: 0.0203
Epoch [30/30], Batch [2700/6000], Loss: 0.0191
Epoch [30/30], Batch [2800/6000], Loss: 0.0211
Epoch [30/30], Batch [2900/6000], Loss: 0.0141
Epoch [30/30], Batch [3000/6000], Loss: 0.0170
Epoch [30/30], Batch [3100/6000], Loss: 0.0231
Epoch [30/30], Batch [3200/6000], Loss: 0.0236
Epoch [30/30], Batch [3300/6000], Loss: 0.0188
Epoch [30/30], Batch [3400/6000], Loss: 0.1436
Epoch [30/30], Batch [3500/6000], Loss: 0.0153
Epoch [30/30], Batch [3600/6000], Loss: 0.0176
Epoch [30/30], Batch [3700/6000], Loss: 0.0220
Epoch [30/30], Batch [3800/6000], Loss: 0.0774
Epoch [30/30], Batch [3900/6000], Loss: 0.0188
Epoch [30/30], Batch [4000/6000], Loss: 0.0176
Epoch [30/30], Batch [4100/6000], Loss: 0.0199
Epoch [30/30], Batch [4200/6000], Loss: 0.0173
Epoch [30/30], Batch [4300/6000], Loss: 0.0169
Epoch [30/30], Batch [4400/6000], Loss: 0.0192
Epoch [30/30], Batch [4500/6000], Loss: 0.0251
Epoch [30/30], Batch [4600/6000], Loss: 0.0196
Epoch [30/30], Batch [4700/6000], Loss: 0.0170
Epoch [30/30], Batch [4800/6000], Loss: 0.0163
Epoch [30/30], Batch [4900/6000], Loss: 0.0215
Epoch [30/30], Batch [5000/6000], Loss: 0.0138
Epoch [30/30], Batch [5100/6000], Loss: 0.0135
Epoch [30/30], Batch [5200/6000], Loss: 0.0222
Epoch [30/30], Batch [5300/6000], Loss: 0.0129
Epoch [30/30], Batch [5400/6000], Loss: 0.0189
Epoch [30/30], Batch [5500/6000], Loss: 0.0174
Epoch [30/30], Batch [5600/6000], Loss: 0.0193
Epoch [30/30], Batch [5700/6000], Loss: 0.0173
Epoch [30/30], Batch [5800/6000], Loss: 0.0179
Epoch [30/30], Batch [5900/6000], Loss: 0.0172
Epoch [30/30], Loss: 0.0257
Visualization saved to figures/visualization_0.png
Test Loss: 0.1242, Accuracy: 97.95%
Reconstruction visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0.    0.    0.    0.    0.046 0.    0.    0.954 0.    0.   ]]
Adversarial Training Loop 1/300:
  Label Loss: 0.5107
  Image Loss: 0.0165
  Total Loss: 5.1237
  Image grad max: 1.5822792053222656
  Output probs: [[0.    0.    0.    0.    0.263 0.    0.    0.736 0.    0.   ]]
Adversarial Training Loop 2/300:
  Label Loss: 0.3593
  Image Loss: 0.0164
  Total Loss: 3.6090
  Image grad max: 2.5903232097625732
  Output probs: [[0.    0.    0.    0.    0.27  0.    0.    0.723 0.    0.008]]
Adversarial Training Loop 3/300:
  Label Loss: 0.1896
  Image Loss: 0.0165
  Total Loss: 1.9124
  Image grad max: 3.0045998096466064
  Output probs: [[0.    0.    0.    0.    0.155 0.    0.    0.555 0.001 0.289]]
Adversarial Training Loop 4/300:
  Label Loss: 0.0222
  Image Loss: 0.0166
  Total Loss: 0.2382
  Image grad max: 1.4324630498886108
  Output probs: [[0.    0.    0.    0.    0.013 0.    0.    0.045 0.002 0.94 ]]
Adversarial Training Loop 5/300:
  Label Loss: 0.0890
  Image Loss: 0.0167
  Total Loss: 0.9066
  Image grad max: 2.053607702255249
  Output probs: [[0.    0.    0.    0.    0.003 0.    0.    0.022 0.002 0.973]]
Adversarial Training Loop 6/300:
  Label Loss: 0.1234
  Image Loss: 0.0168
  Total Loss: 1.2512
  Image grad max: 2.123495101928711
  Output probs: [[0.    0.    0.    0.    0.002 0.    0.    0.058 0.002 0.938]]
Adversarial Training Loop 7/300:
  Label Loss: 0.0763
  Image Loss: 0.0167
  Total Loss: 0.7801
  Image grad max: 1.916857123374939
  Output probs: [[0.    0.    0.    0.    0.002 0.    0.    0.411 0.002 0.585]]
Adversarial Training Loop 8/300:
  Label Loss: 0.0020
  Image Loss: 0.0167
  Total Loss: 0.0363
  Image grad max: 0.39394211769104004
  Output probs: [[0.    0.    0.    0.    0.001 0.    0.    0.891 0.001 0.107]]
Adversarial Training Loop 9/300:
  Label Loss: 0.0481
  Image Loss: 0.0166
  Total Loss: 0.4972
  Image grad max: 1.7691465616226196
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.94  0.    0.059]]
Adversarial Training Loop 10/300:
  Label Loss: 0.0752
  Image Loss: 0.0167
  Total Loss: 0.7689
  Image grad max: 1.9511417150497437
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.883 0.001 0.116]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0448
  Image Loss: 0.0167
  Total Loss: 0.4644
  Image grad max: 1.7279115915298462
  Output probs: [[0.    0.    0.    0.    0.001 0.    0.    0.58  0.005 0.414]]
Adversarial Training Loop 12/300:
  Label Loss: 0.0020
  Image Loss: 0.0168
  Total Loss: 0.0369
  Image grad max: 0.3571056127548218
  Output probs: [[0.    0.    0.    0.    0.001 0.    0.    0.215 0.012 0.773]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0205
  Image Loss: 0.0169
  Total Loss: 0.2221
  Image grad max: 1.23538339138031
  Output probs: [[0.    0.    0.    0.    0.001 0.    0.    0.129 0.015 0.856]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0410
  Image Loss: 0.0169
  Total Loss: 0.4272
  Image grad max: 1.5700777769088745
  Output probs: [[0.    0.    0.    0.    0.001 0.    0.    0.17  0.015 0.814]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0296
  Image Loss: 0.0170
  Total Loss: 0.3128
  Image grad max: 1.4037045240402222
  Output probs: [[0.    0.    0.    0.    0.001 0.    0.    0.377 0.012 0.611]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0042
  Image Loss: 0.0170
  Total Loss: 0.0587
  Image grad max: 0.5363998413085938
  Output probs: [[0.    0.    0.    0.    0.001 0.    0.    0.677 0.006 0.317]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0077
  Image Loss: 0.0169
  Total Loss: 0.0938
  Image grad max: 0.7589539885520935
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.802 0.003 0.194]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0236
  Image Loss: 0.0169
  Total Loss: 0.2532
  Image grad max: 1.2965552806854248
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.781 0.003 0.215]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0198
  Image Loss: 0.0170
  Total Loss: 0.2150
  Image grad max: 1.1991018056869507
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.62  0.005 0.374]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0037
  Image Loss: 0.0170
  Total Loss: 0.0540
  Image grad max: 0.5090169310569763
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.384 0.007 0.609]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0034
  Image Loss: 0.0171
  Total Loss: 0.0509
  Image grad max: 0.4915139675140381
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.263 0.007 0.73 ]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0132
  Image Loss: 0.0171
  Total Loss: 0.1489
  Image grad max: 0.986170768737793
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.272 0.006 0.722]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0121
  Image Loss: 0.0171
  Total Loss: 0.1385
  Image grad max: 0.9491117596626282
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.397 0.004 0.598]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0026
  Image Loss: 0.0171
  Total Loss: 0.0427
  Image grad max: 0.42852023243904114
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.581 0.003 0.416]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0017
  Image Loss: 0.0171
  Total Loss: 0.0342
  Image grad max: 0.3371666967868805
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.687 0.002 0.311]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0079
  Image Loss: 0.0171
  Total Loss: 0.0961
  Image grad max: 0.7793588638305664
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.681 0.002 0.317]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0073
  Image Loss: 0.0171
  Total Loss: 0.0905
  Image grad max: 0.7526876330375671
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.573 0.002 0.425]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0013
  Image Loss: 0.0171
  Total Loss: 0.0304
  Image grad max: 0.3024047911167145
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.427 0.002 0.57 ]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0013
  Image Loss: 0.0172
  Total Loss: 0.0299
  Image grad max: 0.3013498783111572
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.348 0.002 0.649]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0050
  Image Loss: 0.0172
  Total Loss: 0.0672
  Image grad max: 0.629218339920044
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.363 0.002 0.635]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0041
  Image Loss: 0.0172
  Total Loss: 0.0579
  Image grad max: 0.5691328644752502
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.46  0.001 0.539]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0005
  Image Loss: 0.0172
  Total Loss: 0.0219
  Image grad max: 0.1668081134557724
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.574 0.001 0.425]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0012
  Image Loss: 0.0172
  Total Loss: 0.0296
  Image grad max: 0.3056451678276062
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.625 0.001 0.373]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0034
  Image Loss: 0.0172
  Total Loss: 0.0512
  Image grad max: 0.5212192535400391
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.596 0.001 0.403]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0020
  Image Loss: 0.0172
  Total Loss: 0.0372
  Image grad max: 0.39835384488105774
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.506 0.001 0.493]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0001
  Image Loss: 0.0172
  Total Loss: 0.0184
  Image grad max: 0.024002835154533386
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.423 0.001 0.576]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0013
  Image Loss: 0.0172
  Total Loss: 0.0301
  Image grad max: 0.31892505288124084
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.401 0.001 0.598]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0021
  Image Loss: 0.0172
  Total Loss: 0.0382
  Image grad max: 0.4134482443332672
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.444 0.001 0.555]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0007
  Image Loss: 0.0172
  Total Loss: 0.0243
  Image grad max: 0.23188339173793793
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.522 0.001 0.477]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0002
  Image Loss: 0.0172
  Total Loss: 0.0191
  Image grad max: 0.09206843376159668
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.575 0.001 0.424]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0012
  Image Loss: 0.0172
  Total Loss: 0.0295
  Image grad max: 0.313737690448761
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.572 0.    0.428]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0011
  Image Loss: 0.0172
  Total Loss: 0.0284
  Image grad max: 0.29836657643318176
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.517 0.    0.482]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0001
  Image Loss: 0.0172
  Total Loss: 0.0186
  Image grad max: 0.07308308780193329
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.458 0.    0.542]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0004
  Image Loss: 0.0172
  Total Loss: 0.0215
  Image grad max: 0.1768236756324768
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.435 0.    0.564]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0009
  Image Loss: 0.0172
  Total Loss: 0.0264
  Image grad max: 0.2714190185070038
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.461 0.    0.539]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0004
  Image Loss: 0.0172
  Total Loss: 0.0209
  Image grad max: 0.1644512265920639
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.513 0.    0.486]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0001
  Image Loss: 0.0172
  Total Loss: 0.0182
  Image grad max: 0.05504487827420235
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.549 0.    0.45 ]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0006
  Image Loss: 0.0172
  Total Loss: 0.0227
  Image grad max: 0.20587781071662903
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.545 0.    0.454]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0005
  Image Loss: 0.0172
  Total Loss: 0.0219
  Image grad max: 0.1893099546432495
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.507 0.    0.493]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0001
  Image Loss: 0.0172
  Total Loss: 0.0178
  Image grad max: 0.028883276507258415
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.468 0.    0.531]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0003
  Image Loss: 0.0172
  Total Loss: 0.0197
  Image grad max: 0.13365691900253296
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.458 0.    0.541]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0004
  Image Loss: 0.0172
  Total Loss: 0.0212
  Image grad max: 0.1755598932504654
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.482 0.    0.518]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0001
  Image Loss: 0.0172
  Total Loss: 0.0183
  Image grad max: 0.07660862803459167
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.517 0.    0.482]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0001
  Image Loss: 0.0172
  Total Loss: 0.0183
  Image grad max: 0.07217179983854294
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.535 0.    0.465]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0003
  Image Loss: 0.0172
  Total Loss: 0.0201
  Image grad max: 0.14522649347782135
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.523 0.    0.477]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0002
  Image Loss: 0.0172
  Total Loss: 0.0187
  Image grad max: 0.09532149881124496
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.493 0.    0.506]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0001
  Image Loss: 0.0172
  Total Loss: 0.0177
  Image grad max: 0.028013668954372406
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.473 0.    0.526]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0002
  Image Loss: 0.0171
  Total Loss: 0.0190
  Image grad max: 0.11247009038925171
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.477 0.    0.523]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0001
  Image Loss: 0.0171
  Total Loss: 0.0186
  Image grad max: 0.09702267497777939
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0176
  Image grad max: 0.002817490603774786
  Output probs: [[0.   0.   0.   0.   0.   0.   0.   0.52 0.   0.48]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0001
  Image Loss: 0.0171
  Total Loss: 0.0183
  Image grad max: 0.08397338539361954
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.521 0.    0.478]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0001
  Image Loss: 0.0171
  Total Loss: 0.0184
  Image grad max: 0.08900897949934006
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.504 0.    0.496]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0175
  Image grad max: 0.01682000420987606
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.486 0.    0.514]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0001
  Image Loss: 0.0171
  Total Loss: 0.0179
  Image grad max: 0.06030182167887688
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.482 0.    0.518]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0001
  Image Loss: 0.0171
  Total Loss: 0.0182
  Image grad max: 0.07708622515201569
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.494 0.    0.506]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0175
  Image grad max: 0.02482043392956257
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.51  0.    0.489]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0001
  Image Loss: 0.0171
  Total Loss: 0.0177
  Image grad max: 0.043379057198762894
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.515 0.    0.484]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0001
  Image Loss: 0.0171
  Total Loss: 0.0179
  Image grad max: 0.0651412159204483
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.506 0.    0.494]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0175
  Image grad max: 0.026146922260522842
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.492 0.    0.507]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0175
  Image grad max: 0.03155278414487839
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.487 0.    0.512]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0001
  Image Loss: 0.0171
  Total Loss: 0.0177
  Image grad max: 0.05391107499599457
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.494 0.    0.506]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0000
  Image Loss: 0.0171
  Total Loss: 0.0175
  Image grad max: 0.025204790756106377
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.506 0.    0.494]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0174
  Image grad max: 0.023767925798892975
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.511 0.    0.489]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0001
  Image Loss: 0.0170
  Total Loss: 0.0176
  Image grad max: 0.04501323401927948
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.505 0.    0.495]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0174
  Image grad max: 0.022094331681728363
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.496 0.    0.504]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0174
  Image grad max: 0.018659260123968124
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.491 0.    0.509]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0175
  Image grad max: 0.03720831498503685
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.495 0.    0.504]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0174
  Image grad max: 0.01895780675113201
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.504 0.    0.496]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0173
  Image grad max: 0.01569158397614956
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.507 0.    0.492]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0174
  Image grad max: 0.031081611290574074
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.503 0.    0.496]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0173
  Image grad max: 0.015185076743364334
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.497 0.    0.503]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0173
  Image grad max: 0.013553096912801266
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.494 0.    0.506]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0173
  Image grad max: 0.02580355666577816
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.497 0.    0.503]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0000
  Image Loss: 0.0170
  Total Loss: 0.0173
  Image grad max: 0.011716358363628387
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.503 0.    0.497]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0173
  Image grad max: 0.012635492719709873
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.505 0.    0.495]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0173
  Image grad max: 0.021507320925593376
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.502 0.    0.498]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.00868257600814104
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.497 0.    0.503]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.011631167493760586
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.496 0.    0.504]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.017723890021443367
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.005651846993714571
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.503 0.    0.497]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.011325427331030369
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.503 0.    0.496]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.014590966515243053
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.003289744956418872
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.497 0.    0.502]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.01050862018018961
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.497 0.    0.502]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0171
  Image grad max: 0.01151585578918457
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0000
  Image Loss: 0.0169
  Total Loss: 0.0171
  Image grad max: 0.002176148584112525
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.502 0.    0.497]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0171
  Image grad max: 0.01015453040599823
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.502 0.    0.498]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0171
  Image grad max: 0.009104125201702118
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0171
  Image grad max: 0.002831481397151947
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.498 0.    0.502]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0171
  Image grad max: 0.008982034400105476
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.498 0.    0.501]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0171
  Image grad max: 0.006265368778258562
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0171
  Image grad max: 0.003424778115004301
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.502 0.    0.498]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0170
  Image grad max: 0.008315359242260456
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0170
  Image grad max: 0.004449030384421349
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0170
  Image grad max: 0.004554341081529856
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.498 0.    0.501]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0170
  Image grad max: 0.006699113175272942
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0000
  Image Loss: 0.0168
  Total Loss: 0.0170
  Image grad max: 0.0031105861999094486
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0170
  Image grad max: 0.00501105934381485
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0170
  Image grad max: 0.005778079852461815
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0170
  Image grad max: 0.0016769784269854426
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0170
  Image grad max: 0.0049829063937067986
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.0044292607344686985
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.00177300488576293
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.004797771573066711
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.00284865265712142
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.003236684249714017
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.004323312547057867
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.0024094239342957735
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0169
  Image grad max: 0.003167323302477598
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0169
  Image grad max: 0.003317086258903146
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.001966258976608515
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.0036433993373066187
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.0029844646342098713
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.0017516713123768568
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.0029613205697387457
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.001518056495115161
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.0029055706690996885
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.0030224602669477463
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0168
  Image grad max: 0.0016362126916646957
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0168
  Image grad max: 0.002362364437431097
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.0016896323068067431
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.0023461319506168365
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.0028336183167994022
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.001960636582225561
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.0018214030424132943
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.001721009612083435
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.001988973468542099
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.002593384822830558
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.0020894992630928755
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0015086239436641335
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0015768909361213446
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.001802123966626823
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.002373728435486555
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0020854203030467033
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0014331465354189277
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0014994177035987377
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.001708372263237834
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0022031168919056654
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.002041348721832037
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.001501482562161982
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.0014498026575893164
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.0016865180805325508
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.002079356461763382
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.0019690152257680893
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.001541381934657693
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.0014090894255787134
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0165
  Image grad max: 0.0016995882615447044
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0164
  Image grad max: 0.002003374043852091
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0164
  Image grad max: 0.0019008544040843844
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0164
  Image grad max: 0.0015662545338273048
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0164
  Image grad max: 0.0014687728835269809
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0164
  Image grad max: 0.0017418224597349763
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0164
  Image grad max: 0.0019972878508269787
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0164
  Image grad max: 0.0018495654221624136
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0164
  Image grad max: 0.0015470186481252313
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0164
  Image grad max: 0.0015104886842891574
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0163
  Image grad max: 0.0017582385335117579
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0163
  Image grad max: 0.0019319959683343768
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0163
  Image grad max: 0.0017843283712863922
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0163
  Image grad max: 0.0015649988781660795
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0163
  Image grad max: 0.0015722783282399178
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0163
  Image grad max: 0.0017798994667828083
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0163
  Image grad max: 0.0018757154466584325
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0163
  Image grad max: 0.0017431782325729728
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0163
  Image grad max: 0.001579573261551559
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0163
  Image grad max: 0.001621540985070169
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0162
  Image grad max: 0.0017864067340269685
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0162
  Image grad max: 0.001828423235565424
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0162
  Image grad max: 0.0016935011371970177
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0162
  Image grad max: 0.0015969338128343225
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0162
  Image grad max: 0.0016706212190911174
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0162
  Image grad max: 0.0017910507740452886
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0162
  Image grad max: 0.0017806250834837556
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0162
  Image grad max: 0.0016578193753957748
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0162
  Image grad max: 0.0015804164577275515
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0161
  Image grad max: 0.001670865691266954
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0161
  Image grad max: 0.0017964578000828624
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0161
  Image grad max: 0.001763143576681614
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0161
  Image grad max: 0.001641096081584692
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0161
  Image grad max: 0.0016176801873371005
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0161
  Image grad max: 0.0017132642678916454
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0161
  Image grad max: 0.0017728946404531598
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0161
  Image grad max: 0.0017185000469908118
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0161
  Image grad max: 0.0016253317007794976
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0160
  Image grad max: 0.0016487849643453956
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0160
  Image grad max: 0.0017336821183562279
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0160
  Image grad max: 0.001750845112837851
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0160
  Image grad max: 0.001674905070103705
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0160
  Image grad max: 0.0016298607224598527
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0160
  Image grad max: 0.0016836627619341016
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0160
  Image grad max: 0.0017427235143259168
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0160
  Image grad max: 0.0017086182488128543
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0160
  Image grad max: 0.0016551779117435217
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0159
  Image grad max: 0.0016514116432517767
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0159
  Image grad max: 0.0017154139932245016
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0159
  Image grad max: 0.0017203399911522865
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0159
  Image grad max: 0.0016748096095398068
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0159
  Image grad max: 0.0016530005959793925
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0159
  Image grad max: 0.0016824997728690505
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0159
  Image grad max: 0.0017142026918008924
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0159
  Image grad max: 0.001692380872555077
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0159
  Image grad max: 0.0016728698974475265
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0158
  Image grad max: 0.0016799927689135075
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0158
  Image grad max: 0.001691604033112526
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0158
  Image grad max: 0.0016849749954417348
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0158
  Image grad max: 0.0016768972855061293
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0158
  Image grad max: 0.0016718603437766433
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0158
  Image grad max: 0.0016920417547225952
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0158
  Image grad max: 0.0016928091645240784
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0158
  Image grad max: 0.0016725577879697084
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0158
  Image grad max: 0.0016673844074830413
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0157
  Image grad max: 0.0016812094254419208
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0157
  Image grad max: 0.0016883722273632884
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0157
  Image grad max: 0.0016775457188487053
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0157
  Image grad max: 0.001669619814492762
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0157
  Image grad max: 0.0016709859482944012
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0157
  Image grad max: 0.0016841139877215028
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0157
  Image grad max: 0.0016754663083702326
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0157
  Image grad max: 0.0016681271372362971
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0157
  Image grad max: 0.0016696471720933914
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0156
  Image grad max: 0.0016806160565465689
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0156
  Image grad max: 0.001676330342888832
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0156
  Image grad max: 0.0016647763550281525
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0156
  Image grad max: 0.0016684872098267078
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0156
  Image grad max: 0.00167802011128515
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0156
  Image grad max: 0.0016737390542402864
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0156
  Image grad max: 0.0016642229165881872
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0156
  Image grad max: 0.0016839549643918872
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0156
  Image grad max: 0.0016943783266469836
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0155
  Image grad max: 0.0016675398219376802
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0155
  Image grad max: 0.0016472444403916597
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0155
  Image grad max: 0.0016633563209325075
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0155
  Image grad max: 0.0016947928816080093
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0155
  Image grad max: 0.0016759332502260804
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0155
  Image grad max: 0.0016462983330711722
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0155
  Image grad max: 0.0016602437244728208
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0155
  Image grad max: 0.0016822160687297583
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0155
  Image grad max: 0.0016597188077867031
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0154
  Image grad max: 0.0016358980210497975
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0154
  Image grad max: 0.0016622619004920125
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0154
  Image grad max: 0.0016953670419752598
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0154
  Image grad max: 0.0016626230208203197
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0154
  Image grad max: 0.0016371856909245253
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0154
  Image grad max: 0.0016549589345231652
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0154
  Image grad max: 0.0017142435535788536
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0154
  Image grad max: 0.0016896793385967612
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0154
  Image grad max: 0.0016233172500506043
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0153
  Image grad max: 0.0016329071950167418
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0153
  Image grad max: 0.0016982367960736156
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0153
  Image grad max: 0.0016881312476471066
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0153
  Image grad max: 0.0016355885891243815
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0153
  Image grad max: 0.001631460851058364
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0153
  Image grad max: 0.0016813484253361821
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0153
  Image grad max: 0.0016947929980233312
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0153
  Image grad max: 0.001648796838708222
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0153
  Image grad max: 0.0016227293526753783
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0152
  Image grad max: 0.001667640870437026
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0152
  Image grad max: 0.0016956250183284283
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0152
  Image grad max: 0.001651199534535408
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0152
  Image grad max: 0.0016242180718109012
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0152
  Image grad max: 0.0016640648245811462
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0152
  Image grad max: 0.0016834241105243564
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0152
  Image grad max: 0.0016571484738960862
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0152
  Image grad max: 0.0016302907606586814
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0152
  Image grad max: 0.0016575661720708013
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0151
  Image grad max: 0.001679146895185113
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0151
  Image grad max: 0.0016493233852088451
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0151
  Image grad max: 0.0016384514747187495
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0151
  Image grad max: 0.0016578228678554296
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0151
  Image grad max: 0.001671348582021892
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0151
  Image grad max: 0.0016442955238744617
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0151
  Image grad max: 0.0016423759516328573
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0151
  Image grad max: 0.0016573596512898803
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0151
  Image grad max: 0.0016642857808619738
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0150
  Image grad max: 0.001647645840421319
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0150
  Image grad max: 0.001639109686948359
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0150
  Image grad max: 0.0016591375460848212
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0150
  Image grad max: 0.0016586952842772007
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0150
  Image grad max: 0.0016442497726529837
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0150
  Image grad max: 0.001643805648200214
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0150
  Image grad max: 0.001652197097428143
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0150
  Image grad max: 0.00165693205781281
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0150
  Image grad max: 0.0016491054557263851
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0149
  Image grad max: 0.0016398238949477673
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0149
  Image grad max: 0.0016445411602035165
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0149
  Image grad max: 0.0016625497955828905
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0149
  Image grad max: 0.001646610675379634
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0149
  Image grad max: 0.0016314228996634483
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0149
  Image grad max: 0.0016501812497153878
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0149
  Image grad max: 0.0016585980774834752
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
Adversarial example training visualization saved to adversarial_figures/adversarial_testing.png
Target label was: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
