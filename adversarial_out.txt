Epoch [1/30], Batch [0/6000], Loss: 7.2607
Epoch [1/30], Batch [100/6000], Loss: 5.2984
Epoch [1/30], Batch [200/6000], Loss: 3.3084
Epoch [1/30], Batch [300/6000], Loss: 3.3251
Epoch [1/30], Batch [400/6000], Loss: 2.1675
Epoch [1/30], Batch [500/6000], Loss: 1.6828
Epoch [1/30], Batch [600/6000], Loss: 2.6037
Epoch [1/30], Batch [700/6000], Loss: 3.5658
Epoch [1/30], Batch [800/6000], Loss: 1.6121
Epoch [1/30], Batch [900/6000], Loss: 3.5447
Epoch [1/30], Batch [1000/6000], Loss: 1.7718
Epoch [1/30], Batch [1100/6000], Loss: 2.6660
Epoch [1/30], Batch [1200/6000], Loss: 1.3377
Epoch [1/30], Batch [1300/6000], Loss: 1.3794
Epoch [1/30], Batch [1400/6000], Loss: 1.3004
Epoch [1/30], Batch [1500/6000], Loss: 3.6210
Epoch [1/30], Batch [1600/6000], Loss: 2.5599
Epoch [1/30], Batch [1700/6000], Loss: 3.1560
Epoch [1/30], Batch [1800/6000], Loss: 1.4693
Epoch [1/30], Batch [1900/6000], Loss: 1.5942
Epoch [1/30], Batch [2000/6000], Loss: 1.8335
Epoch [1/30], Batch [2100/6000], Loss: 4.2760
Epoch [1/30], Batch [2200/6000], Loss: 0.9620
Epoch [1/30], Batch [2300/6000], Loss: 1.9210
Epoch [1/30], Batch [2400/6000], Loss: 1.1579
Epoch [1/30], Batch [2500/6000], Loss: 1.0615
Epoch [1/30], Batch [2600/6000], Loss: 3.4339
Epoch [1/30], Batch [2700/6000], Loss: 0.8795
Epoch [1/30], Batch [2800/6000], Loss: 4.6971
Epoch [1/30], Batch [2900/6000], Loss: 0.8749
Epoch [1/30], Batch [3000/6000], Loss: 1.6848
Epoch [1/30], Batch [3100/6000], Loss: 1.0627
Epoch [1/30], Batch [3200/6000], Loss: 1.2781
Epoch [1/30], Batch [3300/6000], Loss: 0.5246
Epoch [1/30], Batch [3400/6000], Loss: 0.6506
Epoch [1/30], Batch [3500/6000], Loss: 1.7272
Epoch [1/30], Batch [3600/6000], Loss: 3.4823
Epoch [1/30], Batch [3700/6000], Loss: 0.8954
Epoch [1/30], Batch [3800/6000], Loss: 0.9332
Epoch [1/30], Batch [3900/6000], Loss: 0.9642
Epoch [1/30], Batch [4000/6000], Loss: 3.3159
Epoch [1/30], Batch [4100/6000], Loss: 2.8953
Epoch [1/30], Batch [4200/6000], Loss: 0.5769
Epoch [1/30], Batch [4300/6000], Loss: 2.8434
Epoch [1/30], Batch [4400/6000], Loss: 1.0965
Epoch [1/30], Batch [4500/6000], Loss: 2.3188
Epoch [1/30], Batch [4600/6000], Loss: 0.5650
Epoch [1/30], Batch [4700/6000], Loss: 2.3408
Epoch [1/30], Batch [4800/6000], Loss: 2.1246
Epoch [1/30], Batch [4900/6000], Loss: 2.0537
Epoch [1/30], Batch [5000/6000], Loss: 1.7831
Epoch [1/30], Batch [5100/6000], Loss: 1.9261
Epoch [1/30], Batch [5200/6000], Loss: 3.8468
Epoch [1/30], Batch [5300/6000], Loss: 2.6596
Epoch [1/30], Batch [5400/6000], Loss: 0.5837
Epoch [1/30], Batch [5500/6000], Loss: 1.7938
Epoch [1/30], Batch [5600/6000], Loss: 0.5964
Epoch [1/30], Batch [5700/6000], Loss: 0.5525
Epoch [1/30], Batch [5800/6000], Loss: 0.5827
Epoch [1/30], Batch [5900/6000], Loss: 1.0279
Epoch [1/30], Loss: 1.7812
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.4980
Epoch [2/30], Batch [100/6000], Loss: 0.8901
Epoch [2/30], Batch [200/6000], Loss: 4.1406
Epoch [2/30], Batch [300/6000], Loss: 1.1125
Epoch [2/30], Batch [400/6000], Loss: 0.3703
Epoch [2/30], Batch [500/6000], Loss: 0.4983
Epoch [2/30], Batch [600/6000], Loss: 0.7059
Epoch [2/30], Batch [700/6000], Loss: 1.6913
Epoch [2/30], Batch [800/6000], Loss: 0.8410
Epoch [2/30], Batch [900/6000], Loss: 0.3059
Epoch [2/30], Batch [1000/6000], Loss: 1.2559
Epoch [2/30], Batch [1100/6000], Loss: 2.3250
Epoch [2/30], Batch [1200/6000], Loss: 1.1917
Epoch [2/30], Batch [1300/6000], Loss: 0.5651
Epoch [2/30], Batch [1400/6000], Loss: 3.9630
Epoch [2/30], Batch [1500/6000], Loss: 1.7524
Epoch [2/30], Batch [1600/6000], Loss: 0.5069
Epoch [2/30], Batch [1700/6000], Loss: 0.3203
Epoch [2/30], Batch [1800/6000], Loss: 0.8069
Epoch [2/30], Batch [1900/6000], Loss: 1.5231
Epoch [2/30], Batch [2000/6000], Loss: 0.6097
Epoch [2/30], Batch [2100/6000], Loss: 0.6329
Epoch [2/30], Batch [2200/6000], Loss: 1.7504
Epoch [2/30], Batch [2300/6000], Loss: 0.7856
Epoch [2/30], Batch [2400/6000], Loss: 2.3233
Epoch [2/30], Batch [2500/6000], Loss: 0.4265
Epoch [2/30], Batch [2600/6000], Loss: 0.9687
Epoch [2/30], Batch [2700/6000], Loss: 1.0553
Epoch [2/30], Batch [2800/6000], Loss: 0.6602
Epoch [2/30], Batch [2900/6000], Loss: 1.3019
Epoch [2/30], Batch [3000/6000], Loss: 0.6501
Epoch [2/30], Batch [3100/6000], Loss: 0.2772
Epoch [2/30], Batch [3200/6000], Loss: 0.8183
Epoch [2/30], Batch [3300/6000], Loss: 0.4846
Epoch [2/30], Batch [3400/6000], Loss: 1.8559
Epoch [2/30], Batch [3500/6000], Loss: 0.3082
Epoch [2/30], Batch [3600/6000], Loss: 1.7451
Epoch [2/30], Batch [3700/6000], Loss: 2.4230
Epoch [2/30], Batch [3800/6000], Loss: 1.0614
Epoch [2/30], Batch [3900/6000], Loss: 0.6945
Epoch [2/30], Batch [4000/6000], Loss: 1.0865
Epoch [2/30], Batch [4100/6000], Loss: 0.4152
Epoch [2/30], Batch [4200/6000], Loss: 0.8428
Epoch [2/30], Batch [4300/6000], Loss: 2.1098
Epoch [2/30], Batch [4400/6000], Loss: 1.0374
Epoch [2/30], Batch [4500/6000], Loss: 0.5739
Epoch [2/30], Batch [4600/6000], Loss: 1.1327
Epoch [2/30], Batch [4700/6000], Loss: 1.2438
Epoch [2/30], Batch [4800/6000], Loss: 1.3607
Epoch [2/30], Batch [4900/6000], Loss: 3.8343
Epoch [2/30], Batch [5000/6000], Loss: 0.8910
Epoch [2/30], Batch [5100/6000], Loss: 1.3735
Epoch [2/30], Batch [5200/6000], Loss: 0.7496
Epoch [2/30], Batch [5300/6000], Loss: 3.4263
Epoch [2/30], Batch [5400/6000], Loss: 1.6615
Epoch [2/30], Batch [5500/6000], Loss: 0.5291
Epoch [2/30], Batch [5600/6000], Loss: 0.4502
Epoch [2/30], Batch [5700/6000], Loss: 1.2678
Epoch [2/30], Batch [5800/6000], Loss: 1.5561
Epoch [2/30], Batch [5900/6000], Loss: 2.0693
Epoch [2/30], Loss: 1.0914
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.2809
Epoch [3/30], Batch [100/6000], Loss: 0.5302
Epoch [3/30], Batch [200/6000], Loss: 0.4802
Epoch [3/30], Batch [300/6000], Loss: 0.2580
Epoch [3/30], Batch [400/6000], Loss: 3.6600
Epoch [3/30], Batch [500/6000], Loss: 0.2250
Epoch [3/30], Batch [600/6000], Loss: 1.4540
Epoch [3/30], Batch [700/6000], Loss: 0.5197
Epoch [3/30], Batch [800/6000], Loss: 0.2384
Epoch [3/30], Batch [900/6000], Loss: 0.9760
Epoch [3/30], Batch [1000/6000], Loss: 0.6448
Epoch [3/30], Batch [1100/6000], Loss: 2.0336
Epoch [3/30], Batch [1200/6000], Loss: 0.3656
Epoch [3/30], Batch [1300/6000], Loss: 2.5084
Epoch [3/30], Batch [1400/6000], Loss: 0.3686
Epoch [3/30], Batch [1500/6000], Loss: 1.3827
Epoch [3/30], Batch [1600/6000], Loss: 0.3271
Epoch [3/30], Batch [1700/6000], Loss: 0.8149
Epoch [3/30], Batch [1800/6000], Loss: 0.7671
Epoch [3/30], Batch [1900/6000], Loss: 0.3490
Epoch [3/30], Batch [2000/6000], Loss: 0.8562
Epoch [3/30], Batch [2100/6000], Loss: 0.7701
Epoch [3/30], Batch [2200/6000], Loss: 0.2838
Epoch [3/30], Batch [2300/6000], Loss: 1.2432
Epoch [3/30], Batch [2400/6000], Loss: 1.0522
Epoch [3/30], Batch [2500/6000], Loss: 1.9271
Epoch [3/30], Batch [2600/6000], Loss: 0.2495
Epoch [3/30], Batch [2700/6000], Loss: 0.3841
Epoch [3/30], Batch [2800/6000], Loss: 0.2762
Epoch [3/30], Batch [2900/6000], Loss: 0.4206
Epoch [3/30], Batch [3000/6000], Loss: 1.5059
Epoch [3/30], Batch [3100/6000], Loss: 0.2242
Epoch [3/30], Batch [3200/6000], Loss: 0.2578
Epoch [3/30], Batch [3300/6000], Loss: 0.2348
Epoch [3/30], Batch [3400/6000], Loss: 1.9188
Epoch [3/30], Batch [3500/6000], Loss: 0.6402
Epoch [3/30], Batch [3600/6000], Loss: 0.2595
Epoch [3/30], Batch [3700/6000], Loss: 1.9267
Epoch [3/30], Batch [3800/6000], Loss: 0.2158
Epoch [3/30], Batch [3900/6000], Loss: 0.3796
Epoch [3/30], Batch [4000/6000], Loss: 2.0008
Epoch [3/30], Batch [4100/6000], Loss: 0.4225
Epoch [3/30], Batch [4200/6000], Loss: 1.1000
Epoch [3/30], Batch [4300/6000], Loss: 0.8020
Epoch [3/30], Batch [4400/6000], Loss: 0.4500
Epoch [3/30], Batch [4500/6000], Loss: 0.4173
Epoch [3/30], Batch [4600/6000], Loss: 1.9516
Epoch [3/30], Batch [4700/6000], Loss: 0.2977
Epoch [3/30], Batch [4800/6000], Loss: 0.6843
Epoch [3/30], Batch [4900/6000], Loss: 0.2592
Epoch [3/30], Batch [5000/6000], Loss: 3.6285
Epoch [3/30], Batch [5100/6000], Loss: 1.2277
Epoch [3/30], Batch [5200/6000], Loss: 0.2736
Epoch [3/30], Batch [5300/6000], Loss: 1.7653
Epoch [3/30], Batch [5400/6000], Loss: 0.3537
Epoch [3/30], Batch [5500/6000], Loss: 1.0018
Epoch [3/30], Batch [5600/6000], Loss: 0.2291
Epoch [3/30], Batch [5700/6000], Loss: 1.0424
Epoch [3/30], Batch [5800/6000], Loss: 0.4327
Epoch [3/30], Batch [5900/6000], Loss: 0.3336
Epoch [3/30], Loss: 0.8764
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.4250
Epoch [4/30], Batch [100/6000], Loss: 0.5179
Epoch [4/30], Batch [200/6000], Loss: 0.2519
Epoch [4/30], Batch [300/6000], Loss: 0.3575
Epoch [4/30], Batch [400/6000], Loss: 0.6159
Epoch [4/30], Batch [500/6000], Loss: 0.4286
Epoch [4/30], Batch [600/6000], Loss: 2.7242
Epoch [4/30], Batch [700/6000], Loss: 0.2984
Epoch [4/30], Batch [800/6000], Loss: 0.7274
Epoch [4/30], Batch [900/6000], Loss: 0.7142
Epoch [4/30], Batch [1000/6000], Loss: 0.2203
Epoch [4/30], Batch [1100/6000], Loss: 0.6920
Epoch [4/30], Batch [1200/6000], Loss: 0.7370
Epoch [4/30], Batch [1300/6000], Loss: 0.1922
Epoch [4/30], Batch [1400/6000], Loss: 0.3354
Epoch [4/30], Batch [1500/6000], Loss: 0.3212
Epoch [4/30], Batch [1600/6000], Loss: 0.2381
Epoch [4/30], Batch [1700/6000], Loss: 2.5916
Epoch [4/30], Batch [1800/6000], Loss: 0.3043
Epoch [4/30], Batch [1900/6000], Loss: 0.2559
Epoch [4/30], Batch [2000/6000], Loss: 2.2869
Epoch [4/30], Batch [2100/6000], Loss: 0.2991
Epoch [4/30], Batch [2200/6000], Loss: 0.3446
Epoch [4/30], Batch [2300/6000], Loss: 0.3445
Epoch [4/30], Batch [2400/6000], Loss: 1.4171
Epoch [4/30], Batch [2500/6000], Loss: 0.2235
Epoch [4/30], Batch [2600/6000], Loss: 0.5560
Epoch [4/30], Batch [2700/6000], Loss: 0.2496
Epoch [4/30], Batch [2800/6000], Loss: 1.0126
Epoch [4/30], Batch [2900/6000], Loss: 0.2335
Epoch [4/30], Batch [3000/6000], Loss: 1.2542
Epoch [4/30], Batch [3100/6000], Loss: 2.0874
Epoch [4/30], Batch [3200/6000], Loss: 0.4546
Epoch [4/30], Batch [3300/6000], Loss: 0.6939
Epoch [4/30], Batch [3400/6000], Loss: 0.3298
Epoch [4/30], Batch [3500/6000], Loss: 0.7009
Epoch [4/30], Batch [3600/6000], Loss: 0.3575
Epoch [4/30], Batch [3700/6000], Loss: 0.1947
Epoch [4/30], Batch [3800/6000], Loss: 0.2332
Epoch [4/30], Batch [3900/6000], Loss: 0.2934
Epoch [4/30], Batch [4000/6000], Loss: 0.2558
Epoch [4/30], Batch [4100/6000], Loss: 0.8178
Epoch [4/30], Batch [4200/6000], Loss: 0.2291
Epoch [4/30], Batch [4300/6000], Loss: 1.3376
Epoch [4/30], Batch [4400/6000], Loss: 2.4794
Epoch [4/30], Batch [4500/6000], Loss: 2.3376
Epoch [4/30], Batch [4600/6000], Loss: 0.7331
Epoch [4/30], Batch [4700/6000], Loss: 0.2369
Epoch [4/30], Batch [4800/6000], Loss: 0.4849
Epoch [4/30], Batch [4900/6000], Loss: 0.2793
Epoch [4/30], Batch [5000/6000], Loss: 0.2128
Epoch [4/30], Batch [5100/6000], Loss: 0.2638
Epoch [4/30], Batch [5200/6000], Loss: 1.3940
Epoch [4/30], Batch [5300/6000], Loss: 0.2477
Epoch [4/30], Batch [5400/6000], Loss: 1.1268
Epoch [4/30], Batch [5500/6000], Loss: 0.2535
Epoch [4/30], Batch [5600/6000], Loss: 2.4736
Epoch [4/30], Batch [5700/6000], Loss: 1.1263
Epoch [4/30], Batch [5800/6000], Loss: 0.7672
Epoch [4/30], Batch [5900/6000], Loss: 0.2670
Epoch [4/30], Loss: 0.7299
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.3308
Epoch [5/30], Batch [100/6000], Loss: 0.2425
Epoch [5/30], Batch [200/6000], Loss: 0.5140
Epoch [5/30], Batch [300/6000], Loss: 1.3198
Epoch [5/30], Batch [400/6000], Loss: 1.2093
Epoch [5/30], Batch [500/6000], Loss: 0.9761
Epoch [5/30], Batch [600/6000], Loss: 0.3391
Epoch [5/30], Batch [700/6000], Loss: 0.7099
Epoch [5/30], Batch [800/6000], Loss: 0.6841
Epoch [5/30], Batch [900/6000], Loss: 2.9689
Epoch [5/30], Batch [1000/6000], Loss: 0.2419
Epoch [5/30], Batch [1100/6000], Loss: 0.7331
Epoch [5/30], Batch [1200/6000], Loss: 1.2559
Epoch [5/30], Batch [1300/6000], Loss: 0.1977
Epoch [5/30], Batch [1400/6000], Loss: 0.2220
Epoch [5/30], Batch [1500/6000], Loss: 0.6917
Epoch [5/30], Batch [1600/6000], Loss: 1.8940
Epoch [5/30], Batch [1700/6000], Loss: 1.5027
Epoch [5/30], Batch [1800/6000], Loss: 0.8979
Epoch [5/30], Batch [1900/6000], Loss: 2.1664
Epoch [5/30], Batch [2000/6000], Loss: 0.1855
Epoch [5/30], Batch [2100/6000], Loss: 0.2478
Epoch [5/30], Batch [2200/6000], Loss: 0.3740
Epoch [5/30], Batch [2300/6000], Loss: 0.3157
Epoch [5/30], Batch [2400/6000], Loss: 0.4765
Epoch [5/30], Batch [2500/6000], Loss: 0.7349
Epoch [5/30], Batch [2600/6000], Loss: 0.2496
Epoch [5/30], Batch [2700/6000], Loss: 0.6020
Epoch [5/30], Batch [2800/6000], Loss: 0.1802
Epoch [5/30], Batch [2900/6000], Loss: 1.4381
Epoch [5/30], Batch [3000/6000], Loss: 0.2430
Epoch [5/30], Batch [3100/6000], Loss: 1.7046
Epoch [5/30], Batch [3200/6000], Loss: 0.2999
Epoch [5/30], Batch [3300/6000], Loss: 0.2709
Epoch [5/30], Batch [3400/6000], Loss: 0.2302
Epoch [5/30], Batch [3500/6000], Loss: 0.2482
Epoch [5/30], Batch [3600/6000], Loss: 0.2305
Epoch [5/30], Batch [3700/6000], Loss: 2.1082
Epoch [5/30], Batch [3800/6000], Loss: 0.6605
Epoch [5/30], Batch [3900/6000], Loss: 1.0599
Epoch [5/30], Batch [4000/6000], Loss: 0.3904
Epoch [5/30], Batch [4100/6000], Loss: 0.3752
Epoch [5/30], Batch [4200/6000], Loss: 0.7559
Epoch [5/30], Batch [4300/6000], Loss: 1.5970
Epoch [5/30], Batch [4400/6000], Loss: 0.2327
Epoch [5/30], Batch [4500/6000], Loss: 0.2416
Epoch [5/30], Batch [4600/6000], Loss: 0.1532
Epoch [5/30], Batch [4700/6000], Loss: 0.2754
Epoch [5/30], Batch [4800/6000], Loss: 0.1977
Epoch [5/30], Batch [4900/6000], Loss: 2.3130
Epoch [5/30], Batch [5000/6000], Loss: 0.1900
Epoch [5/30], Batch [5100/6000], Loss: 0.6220
Epoch [5/30], Batch [5200/6000], Loss: 0.1754
Epoch [5/30], Batch [5300/6000], Loss: 0.2218
Epoch [5/30], Batch [5400/6000], Loss: 0.2078
Epoch [5/30], Batch [5500/6000], Loss: 0.2523
Epoch [5/30], Batch [5600/6000], Loss: 0.5653
Epoch [5/30], Batch [5700/6000], Loss: 0.2266
Epoch [5/30], Batch [5800/6000], Loss: 0.3728
Epoch [5/30], Batch [5900/6000], Loss: 1.9911
Epoch [5/30], Loss: 0.6339
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 2.7603
Epoch [6/30], Batch [100/6000], Loss: 0.2047
Epoch [6/30], Batch [200/6000], Loss: 0.1960
Epoch [6/30], Batch [300/6000], Loss: 1.6481
Epoch [6/30], Batch [400/6000], Loss: 0.2388
Epoch [6/30], Batch [500/6000], Loss: 1.1702
Epoch [6/30], Batch [600/6000], Loss: 1.2721
Epoch [6/30], Batch [700/6000], Loss: 0.2559
Epoch [6/30], Batch [800/6000], Loss: 0.6976
Epoch [6/30], Batch [900/6000], Loss: 0.2465
Epoch [6/30], Batch [1000/6000], Loss: 0.7464
Epoch [6/30], Batch [1100/6000], Loss: 0.1662
Epoch [6/30], Batch [1200/6000], Loss: 0.2100
Epoch [6/30], Batch [1300/6000], Loss: 0.6288
Epoch [6/30], Batch [1400/6000], Loss: 0.1945
Epoch [6/30], Batch [1500/6000], Loss: 2.0048
Epoch [6/30], Batch [1600/6000], Loss: 0.9073
Epoch [6/30], Batch [1700/6000], Loss: 0.5857
Epoch [6/30], Batch [1800/6000], Loss: 0.2287
Epoch [6/30], Batch [1900/6000], Loss: 0.2306
Epoch [6/30], Batch [2000/6000], Loss: 1.3652
Epoch [6/30], Batch [2100/6000], Loss: 0.2157
Epoch [6/30], Batch [2200/6000], Loss: 0.5615
Epoch [6/30], Batch [2300/6000], Loss: 0.2676
Epoch [6/30], Batch [2400/6000], Loss: 0.2371
Epoch [6/30], Batch [2500/6000], Loss: 0.1927
Epoch [6/30], Batch [2600/6000], Loss: 0.3152
Epoch [6/30], Batch [2700/6000], Loss: 0.5896
Epoch [6/30], Batch [2800/6000], Loss: 1.9278
Epoch [6/30], Batch [2900/6000], Loss: 0.2087
Epoch [6/30], Batch [3000/6000], Loss: 0.5656
Epoch [6/30], Batch [3100/6000], Loss: 0.5767
Epoch [6/30], Batch [3200/6000], Loss: 0.5824
Epoch [6/30], Batch [3300/6000], Loss: 1.2085
Epoch [6/30], Batch [3400/6000], Loss: 0.4633
Epoch [6/30], Batch [3500/6000], Loss: 0.2267
Epoch [6/30], Batch [3600/6000], Loss: 0.2918
Epoch [6/30], Batch [3700/6000], Loss: 0.5708
Epoch [6/30], Batch [3800/6000], Loss: 0.9830
Epoch [6/30], Batch [3900/6000], Loss: 0.3412
Epoch [6/30], Batch [4000/6000], Loss: 0.2706
Epoch [6/30], Batch [4100/6000], Loss: 0.6684
Epoch [6/30], Batch [4200/6000], Loss: 0.6563
Epoch [6/30], Batch [4300/6000], Loss: 0.1774
Epoch [6/30], Batch [4400/6000], Loss: 1.1522
Epoch [6/30], Batch [4500/6000], Loss: 0.3210
Epoch [6/30], Batch [4600/6000], Loss: 0.2024
Epoch [6/30], Batch [4700/6000], Loss: 0.2061
Epoch [6/30], Batch [4800/6000], Loss: 1.5122
Epoch [6/30], Batch [4900/6000], Loss: 0.2720
Epoch [6/30], Batch [5000/6000], Loss: 0.1706
Epoch [6/30], Batch [5100/6000], Loss: 0.2582
Epoch [6/30], Batch [5200/6000], Loss: 1.1519
Epoch [6/30], Batch [5300/6000], Loss: 0.1831
Epoch [6/30], Batch [5400/6000], Loss: 0.3455
Epoch [6/30], Batch [5500/6000], Loss: 0.1926
Epoch [6/30], Batch [5600/6000], Loss: 0.1937
Epoch [6/30], Batch [5700/6000], Loss: 0.2041
Epoch [6/30], Batch [5800/6000], Loss: 0.8121
Epoch [6/30], Batch [5900/6000], Loss: 0.1692
Epoch [6/30], Loss: 0.5705
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 2.0561
Epoch [7/30], Batch [100/6000], Loss: 0.1973
Epoch [7/30], Batch [200/6000], Loss: 0.2261
Epoch [7/30], Batch [300/6000], Loss: 0.3087
Epoch [7/30], Batch [400/6000], Loss: 0.2599
Epoch [7/30], Batch [500/6000], Loss: 0.1957
Epoch [7/30], Batch [600/6000], Loss: 0.5008
Epoch [7/30], Batch [700/6000], Loss: 1.6289
Epoch [7/30], Batch [800/6000], Loss: 0.2857
Epoch [7/30], Batch [900/6000], Loss: 0.1898
Epoch [7/30], Batch [1000/6000], Loss: 0.1932
Epoch [7/30], Batch [1100/6000], Loss: 0.1627
Epoch [7/30], Batch [1200/6000], Loss: 1.2552
Epoch [7/30], Batch [1300/6000], Loss: 0.2284
Epoch [7/30], Batch [1400/6000], Loss: 0.3875
Epoch [7/30], Batch [1500/6000], Loss: 0.1702
Epoch [7/30], Batch [1600/6000], Loss: 0.3377
Epoch [7/30], Batch [1700/6000], Loss: 0.3212
Epoch [7/30], Batch [1800/6000], Loss: 0.2745
Epoch [7/30], Batch [1900/6000], Loss: 0.7326
Epoch [7/30], Batch [2000/6000], Loss: 0.4512
Epoch [7/30], Batch [2100/6000], Loss: 0.6362
Epoch [7/30], Batch [2200/6000], Loss: 0.2139
Epoch [7/30], Batch [2300/6000], Loss: 0.1676
Epoch [7/30], Batch [2400/6000], Loss: 0.1733
Epoch [7/30], Batch [2500/6000], Loss: 2.7695
Epoch [7/30], Batch [2600/6000], Loss: 0.7143
Epoch [7/30], Batch [2700/6000], Loss: 1.0457
Epoch [7/30], Batch [2800/6000], Loss: 0.7316
Epoch [7/30], Batch [2900/6000], Loss: 0.2717
Epoch [7/30], Batch [3000/6000], Loss: 0.2681
Epoch [7/30], Batch [3100/6000], Loss: 0.2020
Epoch [7/30], Batch [3200/6000], Loss: 0.2714
Epoch [7/30], Batch [3300/6000], Loss: 0.2754
Epoch [7/30], Batch [3400/6000], Loss: 0.3316
Epoch [7/30], Batch [3500/6000], Loss: 0.9230
Epoch [7/30], Batch [3600/6000], Loss: 0.1750
Epoch [7/30], Batch [3700/6000], Loss: 0.1712
Epoch [7/30], Batch [3800/6000], Loss: 0.2777
Epoch [7/30], Batch [3900/6000], Loss: 0.9396
Epoch [7/30], Batch [4000/6000], Loss: 1.0951
Epoch [7/30], Batch [4100/6000], Loss: 0.2331
Epoch [7/30], Batch [4200/6000], Loss: 0.1507
Epoch [7/30], Batch [4300/6000], Loss: 0.1640
Epoch [7/30], Batch [4400/6000], Loss: 1.0109
Epoch [7/30], Batch [4500/6000], Loss: 0.6424
Epoch [7/30], Batch [4600/6000], Loss: 0.2343
Epoch [7/30], Batch [4700/6000], Loss: 1.5112
Epoch [7/30], Batch [4800/6000], Loss: 0.5624
Epoch [7/30], Batch [4900/6000], Loss: 0.1898
Epoch [7/30], Batch [5000/6000], Loss: 0.1597
Epoch [7/30], Batch [5100/6000], Loss: 0.3159
Epoch [7/30], Batch [5200/6000], Loss: 1.8683
Epoch [7/30], Batch [5300/6000], Loss: 0.1404
Epoch [7/30], Batch [5400/6000], Loss: 0.5940
Epoch [7/30], Batch [5500/6000], Loss: 0.5381
Epoch [7/30], Batch [5600/6000], Loss: 0.3428
Epoch [7/30], Batch [5700/6000], Loss: 0.3137
Epoch [7/30], Batch [5800/6000], Loss: 1.8070
Epoch [7/30], Batch [5900/6000], Loss: 0.1920
Epoch [7/30], Loss: 0.5129
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.6083
Epoch [8/30], Batch [100/6000], Loss: 0.5827
Epoch [8/30], Batch [200/6000], Loss: 0.9943
Epoch [8/30], Batch [300/6000], Loss: 0.9259
Epoch [8/30], Batch [400/6000], Loss: 0.2385
Epoch [8/30], Batch [500/6000], Loss: 0.4521
Epoch [8/30], Batch [600/6000], Loss: 0.3276
Epoch [8/30], Batch [700/6000], Loss: 0.1613
Epoch [8/30], Batch [800/6000], Loss: 0.2121
Epoch [8/30], Batch [900/6000], Loss: 0.2770
Epoch [8/30], Batch [1000/6000], Loss: 0.2185
Epoch [8/30], Batch [1100/6000], Loss: 0.2340
Epoch [8/30], Batch [1200/6000], Loss: 0.8989
Epoch [8/30], Batch [1300/6000], Loss: 0.2238
Epoch [8/30], Batch [1400/6000], Loss: 1.3903
Epoch [8/30], Batch [1500/6000], Loss: 0.2286
Epoch [8/30], Batch [1600/6000], Loss: 0.8204
Epoch [8/30], Batch [1700/6000], Loss: 0.1435
Epoch [8/30], Batch [1800/6000], Loss: 1.3781
Epoch [8/30], Batch [1900/6000], Loss: 0.6547
Epoch [8/30], Batch [2000/6000], Loss: 2.5418
Epoch [8/30], Batch [2100/6000], Loss: 2.3941
Epoch [8/30], Batch [2200/6000], Loss: 0.7687
Epoch [8/30], Batch [2300/6000], Loss: 0.1573
Epoch [8/30], Batch [2400/6000], Loss: 0.1678
Epoch [8/30], Batch [2500/6000], Loss: 0.3649
Epoch [8/30], Batch [2600/6000], Loss: 0.3799
Epoch [8/30], Batch [2700/6000], Loss: 1.4137
Epoch [8/30], Batch [2800/6000], Loss: 0.2232
Epoch [8/30], Batch [2900/6000], Loss: 2.0701
Epoch [8/30], Batch [3000/6000], Loss: 0.1617
Epoch [8/30], Batch [3100/6000], Loss: 1.7787
Epoch [8/30], Batch [3200/6000], Loss: 0.4288
Epoch [8/30], Batch [3300/6000], Loss: 0.5044
Epoch [8/30], Batch [3400/6000], Loss: 0.2314
Epoch [8/30], Batch [3500/6000], Loss: 0.3208
Epoch [8/30], Batch [3600/6000], Loss: 0.1708
Epoch [8/30], Batch [3700/6000], Loss: 0.1800
Epoch [8/30], Batch [3800/6000], Loss: 1.8455
Epoch [8/30], Batch [3900/6000], Loss: 0.1544
Epoch [8/30], Batch [4000/6000], Loss: 0.4662
Epoch [8/30], Batch [4100/6000], Loss: 0.2656
Epoch [8/30], Batch [4200/6000], Loss: 0.2490
Epoch [8/30], Batch [4300/6000], Loss: 0.7201
Epoch [8/30], Batch [4400/6000], Loss: 0.1803
Epoch [8/30], Batch [4500/6000], Loss: 0.2191
Epoch [8/30], Batch [4600/6000], Loss: 0.2508
Epoch [8/30], Batch [4700/6000], Loss: 0.9350
Epoch [8/30], Batch [4800/6000], Loss: 0.1803
Epoch [8/30], Batch [4900/6000], Loss: 0.2202
Epoch [8/30], Batch [5000/6000], Loss: 0.1960
Epoch [8/30], Batch [5100/6000], Loss: 0.2741
Epoch [8/30], Batch [5200/6000], Loss: 1.8215
Epoch [8/30], Batch [5300/6000], Loss: 0.6441
Epoch [8/30], Batch [5400/6000], Loss: 0.2275
Epoch [8/30], Batch [5500/6000], Loss: 0.4487
Epoch [8/30], Batch [5600/6000], Loss: 1.3325
Epoch [8/30], Batch [5700/6000], Loss: 1.8139
Epoch [8/30], Batch [5800/6000], Loss: 1.0515
Epoch [8/30], Batch [5900/6000], Loss: 0.2688
Epoch [8/30], Loss: 0.4653
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 1.4219
Epoch [9/30], Batch [100/6000], Loss: 0.3256
Epoch [9/30], Batch [200/6000], Loss: 0.2400
Epoch [9/30], Batch [300/6000], Loss: 0.1535
Epoch [9/30], Batch [400/6000], Loss: 0.2007
Epoch [9/30], Batch [500/6000], Loss: 1.0486
Epoch [9/30], Batch [600/6000], Loss: 0.1771
Epoch [9/30], Batch [700/6000], Loss: 0.3326
Epoch [9/30], Batch [800/6000], Loss: 0.1555
Epoch [9/30], Batch [900/6000], Loss: 0.2979
Epoch [9/30], Batch [1000/6000], Loss: 0.1843
Epoch [9/30], Batch [1100/6000], Loss: 0.2028
Epoch [9/30], Batch [1200/6000], Loss: 0.4684
Epoch [9/30], Batch [1300/6000], Loss: 0.1624
Epoch [9/30], Batch [1400/6000], Loss: 0.1752
Epoch [9/30], Batch [1500/6000], Loss: 0.3020
Epoch [9/30], Batch [1600/6000], Loss: 1.1663
Epoch [9/30], Batch [1700/6000], Loss: 0.1475
Epoch [9/30], Batch [1800/6000], Loss: 0.4020
Epoch [9/30], Batch [1900/6000], Loss: 0.1458
Epoch [9/30], Batch [2000/6000], Loss: 0.3548
Epoch [9/30], Batch [2100/6000], Loss: 0.1226
Epoch [9/30], Batch [2200/6000], Loss: 0.2327
Epoch [9/30], Batch [2300/6000], Loss: 0.1531
Epoch [9/30], Batch [2400/6000], Loss: 2.0507
Epoch [9/30], Batch [2500/6000], Loss: 0.2002
Epoch [9/30], Batch [2600/6000], Loss: 0.2785
Epoch [9/30], Batch [2700/6000], Loss: 0.1894
Epoch [9/30], Batch [2800/6000], Loss: 0.1472
Epoch [9/30], Batch [2900/6000], Loss: 0.1829
Epoch [9/30], Batch [3000/6000], Loss: 0.2249
Epoch [9/30], Batch [3100/6000], Loss: 0.1816
Epoch [9/30], Batch [3200/6000], Loss: 0.1760
Epoch [9/30], Batch [3300/6000], Loss: 0.2323
Epoch [9/30], Batch [3400/6000], Loss: 0.1538
Epoch [9/30], Batch [3500/6000], Loss: 0.1769
Epoch [9/30], Batch [3600/6000], Loss: 0.1907
Epoch [9/30], Batch [3700/6000], Loss: 1.6922
Epoch [9/30], Batch [3800/6000], Loss: 0.2924
Epoch [9/30], Batch [3900/6000], Loss: 0.2539
Epoch [9/30], Batch [4000/6000], Loss: 0.1643
Epoch [9/30], Batch [4100/6000], Loss: 0.6953
Epoch [9/30], Batch [4200/6000], Loss: 0.2238
Epoch [9/30], Batch [4300/6000], Loss: 0.1590
Epoch [9/30], Batch [4400/6000], Loss: 0.1460
Epoch [9/30], Batch [4500/6000], Loss: 0.1820
Epoch [9/30], Batch [4600/6000], Loss: 1.1824
Epoch [9/30], Batch [4700/6000], Loss: 0.1707
Epoch [9/30], Batch [4800/6000], Loss: 0.2075
Epoch [9/30], Batch [4900/6000], Loss: 0.1525
Epoch [9/30], Batch [5000/6000], Loss: 0.4345
Epoch [9/30], Batch [5100/6000], Loss: 0.1975
Epoch [9/30], Batch [5200/6000], Loss: 0.7527
Epoch [9/30], Batch [5300/6000], Loss: 0.7139
Epoch [9/30], Batch [5400/6000], Loss: 0.1433
Epoch [9/30], Batch [5500/6000], Loss: 0.1575
Epoch [9/30], Batch [5600/6000], Loss: 0.2757
Epoch [9/30], Batch [5700/6000], Loss: 0.2085
Epoch [9/30], Batch [5800/6000], Loss: 0.6494
Epoch [9/30], Batch [5900/6000], Loss: 0.4306
Epoch [9/30], Loss: 0.4261
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.1422
Epoch [10/30], Batch [100/6000], Loss: 0.3965
Epoch [10/30], Batch [200/6000], Loss: 0.1482
Epoch [10/30], Batch [300/6000], Loss: 0.1476
Epoch [10/30], Batch [400/6000], Loss: 0.1288
Epoch [10/30], Batch [500/6000], Loss: 0.1355
Epoch [10/30], Batch [600/6000], Loss: 0.1501
Epoch [10/30], Batch [700/6000], Loss: 0.3123
Epoch [10/30], Batch [800/6000], Loss: 0.1825
Epoch [10/30], Batch [900/6000], Loss: 1.8079
Epoch [10/30], Batch [1000/6000], Loss: 0.6531
Epoch [10/30], Batch [1100/6000], Loss: 0.1423
Epoch [10/30], Batch [1200/6000], Loss: 0.1263
Epoch [10/30], Batch [1300/6000], Loss: 0.2800
Epoch [10/30], Batch [1400/6000], Loss: 0.1446
Epoch [10/30], Batch [1500/6000], Loss: 0.1904
Epoch [10/30], Batch [1600/6000], Loss: 0.2154
Epoch [10/30], Batch [1700/6000], Loss: 0.6461
Epoch [10/30], Batch [1800/6000], Loss: 0.1773
Epoch [10/30], Batch [1900/6000], Loss: 2.1790
Epoch [10/30], Batch [2000/6000], Loss: 1.4589
Epoch [10/30], Batch [2100/6000], Loss: 0.2523
Epoch [10/30], Batch [2200/6000], Loss: 0.1816
Epoch [10/30], Batch [2300/6000], Loss: 0.7004
Epoch [10/30], Batch [2400/6000], Loss: 0.1899
Epoch [10/30], Batch [2500/6000], Loss: 1.3961
Epoch [10/30], Batch [2600/6000], Loss: 0.3167
Epoch [10/30], Batch [2700/6000], Loss: 0.1724
Epoch [10/30], Batch [2800/6000], Loss: 0.1512
Epoch [10/30], Batch [2900/6000], Loss: 0.1336
Epoch [10/30], Batch [3000/6000], Loss: 0.2355
Epoch [10/30], Batch [3100/6000], Loss: 0.1951
Epoch [10/30], Batch [3200/6000], Loss: 0.2216
Epoch [10/30], Batch [3300/6000], Loss: 0.1654
Epoch [10/30], Batch [3400/6000], Loss: 0.2317
Epoch [10/30], Batch [3500/6000], Loss: 0.9487
Epoch [10/30], Batch [3600/6000], Loss: 0.2477
Epoch [10/30], Batch [3700/6000], Loss: 0.2994
Epoch [10/30], Batch [3800/6000], Loss: 0.3207
Epoch [10/30], Batch [3900/6000], Loss: 0.1595
Epoch [10/30], Batch [4000/6000], Loss: 0.3127
Epoch [10/30], Batch [4100/6000], Loss: 0.7739
Epoch [10/30], Batch [4200/6000], Loss: 0.1429
Epoch [10/30], Batch [4300/6000], Loss: 0.1254
Epoch [10/30], Batch [4400/6000], Loss: 0.2006
Epoch [10/30], Batch [4500/6000], Loss: 0.2157
Epoch [10/30], Batch [4600/6000], Loss: 0.1512
Epoch [10/30], Batch [4700/6000], Loss: 0.1554
Epoch [10/30], Batch [4800/6000], Loss: 0.1596
Epoch [10/30], Batch [4900/6000], Loss: 0.1867
Epoch [10/30], Batch [5000/6000], Loss: 0.1504
Epoch [10/30], Batch [5100/6000], Loss: 0.1788
Epoch [10/30], Batch [5200/6000], Loss: 0.5279
Epoch [10/30], Batch [5300/6000], Loss: 0.3044
Epoch [10/30], Batch [5400/6000], Loss: 0.1360
Epoch [10/30], Batch [5500/6000], Loss: 0.1256
Epoch [10/30], Batch [5600/6000], Loss: 0.1667
Epoch [10/30], Batch [5700/6000], Loss: 0.6228
Epoch [10/30], Batch [5800/6000], Loss: 0.2576
Epoch [10/30], Batch [5900/6000], Loss: 0.1315
Epoch [10/30], Loss: 0.3980
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.1722
Epoch [11/30], Batch [100/6000], Loss: 0.1330
Epoch [11/30], Batch [200/6000], Loss: 0.1723
Epoch [11/30], Batch [300/6000], Loss: 0.2088
Epoch [11/30], Batch [400/6000], Loss: 0.1524
Epoch [11/30], Batch [500/6000], Loss: 0.1707
Epoch [11/30], Batch [600/6000], Loss: 0.1695
Epoch [11/30], Batch [700/6000], Loss: 0.1737
Epoch [11/30], Batch [800/6000], Loss: 0.1462
Epoch [11/30], Batch [900/6000], Loss: 0.1466
Epoch [11/30], Batch [1000/6000], Loss: 0.9606
Epoch [11/30], Batch [1100/6000], Loss: 0.1647
Epoch [11/30], Batch [1200/6000], Loss: 0.1360
Epoch [11/30], Batch [1300/6000], Loss: 0.2516
Epoch [11/30], Batch [1400/6000], Loss: 0.1747
Epoch [11/30], Batch [1500/6000], Loss: 0.1500
Epoch [11/30], Batch [1600/6000], Loss: 1.2891
Epoch [11/30], Batch [1700/6000], Loss: 0.1778
Epoch [11/30], Batch [1800/6000], Loss: 0.1541
Epoch [11/30], Batch [1900/6000], Loss: 0.1674
Epoch [11/30], Batch [2000/6000], Loss: 0.4641
Epoch [11/30], Batch [2100/6000], Loss: 0.2665
Epoch [11/30], Batch [2200/6000], Loss: 0.1770
Epoch [11/30], Batch [2300/6000], Loss: 0.8220
Epoch [11/30], Batch [2400/6000], Loss: 0.3203
Epoch [11/30], Batch [2500/6000], Loss: 0.1357
Epoch [11/30], Batch [2600/6000], Loss: 0.1359
Epoch [11/30], Batch [2700/6000], Loss: 0.1293
Epoch [11/30], Batch [2800/6000], Loss: 0.1751
Epoch [11/30], Batch [2900/6000], Loss: 0.3319
Epoch [11/30], Batch [3000/6000], Loss: 0.3247
Epoch [11/30], Batch [3100/6000], Loss: 0.3167
Epoch [11/30], Batch [3200/6000], Loss: 0.1658
Epoch [11/30], Batch [3300/6000], Loss: 0.7454
Epoch [11/30], Batch [3400/6000], Loss: 0.1305
Epoch [11/30], Batch [3500/6000], Loss: 0.6908
Epoch [11/30], Batch [3600/6000], Loss: 0.1706
Epoch [11/30], Batch [3700/6000], Loss: 0.1568
Epoch [11/30], Batch [3800/6000], Loss: 0.1620
Epoch [11/30], Batch [3900/6000], Loss: 0.1510
Epoch [11/30], Batch [4000/6000], Loss: 0.4016
Epoch [11/30], Batch [4100/6000], Loss: 0.1485
Epoch [11/30], Batch [4200/6000], Loss: 1.7022
Epoch [11/30], Batch [4300/6000], Loss: 0.1433
Epoch [11/30], Batch [4400/6000], Loss: 0.2577
Epoch [11/30], Batch [4500/6000], Loss: 0.5775
Epoch [11/30], Batch [4600/6000], Loss: 0.6884
Epoch [11/30], Batch [4700/6000], Loss: 0.1419
Epoch [11/30], Batch [4800/6000], Loss: 0.1563
Epoch [11/30], Batch [4900/6000], Loss: 0.2912
Epoch [11/30], Batch [5000/6000], Loss: 0.1410
Epoch [11/30], Batch [5100/6000], Loss: 0.1582
Epoch [11/30], Batch [5200/6000], Loss: 0.1924
Epoch [11/30], Batch [5300/6000], Loss: 0.1982
Epoch [11/30], Batch [5400/6000], Loss: 0.1420
Epoch [11/30], Batch [5500/6000], Loss: 0.1668
Epoch [11/30], Batch [5600/6000], Loss: 0.3201
Epoch [11/30], Batch [5700/6000], Loss: 0.5171
Epoch [11/30], Batch [5800/6000], Loss: 0.1444
Epoch [11/30], Batch [5900/6000], Loss: 0.1421
Epoch [11/30], Loss: 0.3743
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.4707
Epoch [12/30], Batch [100/6000], Loss: 0.2815
Epoch [12/30], Batch [200/6000], Loss: 0.1394
Epoch [12/30], Batch [300/6000], Loss: 0.1625
Epoch [12/30], Batch [400/6000], Loss: 0.1757
Epoch [12/30], Batch [500/6000], Loss: 0.1897
Epoch [12/30], Batch [600/6000], Loss: 0.2622
Epoch [12/30], Batch [700/6000], Loss: 0.1392
Epoch [12/30], Batch [800/6000], Loss: 0.1869
Epoch [12/30], Batch [900/6000], Loss: 0.1477
Epoch [12/30], Batch [1000/6000], Loss: 0.3021
Epoch [12/30], Batch [1100/6000], Loss: 0.1325
Epoch [12/30], Batch [1200/6000], Loss: 0.1468
Epoch [12/30], Batch [1300/6000], Loss: 0.8800
Epoch [12/30], Batch [1400/6000], Loss: 0.1455
Epoch [12/30], Batch [1500/6000], Loss: 0.5001
Epoch [12/30], Batch [1600/6000], Loss: 1.7305
Epoch [12/30], Batch [1700/6000], Loss: 0.4452
Epoch [12/30], Batch [1800/6000], Loss: 0.1516
Epoch [12/30], Batch [1900/6000], Loss: 0.4034
Epoch [12/30], Batch [2000/6000], Loss: 0.1401
Epoch [12/30], Batch [2100/6000], Loss: 0.1344
Epoch [12/30], Batch [2200/6000], Loss: 0.1533
Epoch [12/30], Batch [2300/6000], Loss: 0.1498
Epoch [12/30], Batch [2400/6000], Loss: 0.2079
Epoch [12/30], Batch [2500/6000], Loss: 0.4635
Epoch [12/30], Batch [2600/6000], Loss: 0.1657
Epoch [12/30], Batch [2700/6000], Loss: 0.2779
Epoch [12/30], Batch [2800/6000], Loss: 0.5832
Epoch [12/30], Batch [2900/6000], Loss: 0.1583
Epoch [12/30], Batch [3000/6000], Loss: 0.1488
Epoch [12/30], Batch [3100/6000], Loss: 0.1275
Epoch [12/30], Batch [3200/6000], Loss: 0.2512
Epoch [12/30], Batch [3300/6000], Loss: 0.1987
Epoch [12/30], Batch [3400/6000], Loss: 0.1709
Epoch [12/30], Batch [3500/6000], Loss: 0.1556
Epoch [12/30], Batch [3600/6000], Loss: 1.1662
Epoch [12/30], Batch [3700/6000], Loss: 0.1565
Epoch [12/30], Batch [3800/6000], Loss: 0.1354
Epoch [12/30], Batch [3900/6000], Loss: 0.1238
Epoch [12/30], Batch [4000/6000], Loss: 0.1379
Epoch [12/30], Batch [4100/6000], Loss: 0.1440
Epoch [12/30], Batch [4200/6000], Loss: 0.3677
Epoch [12/30], Batch [4300/6000], Loss: 0.3834
Epoch [12/30], Batch [4400/6000], Loss: 0.1713
Epoch [12/30], Batch [4500/6000], Loss: 0.2745
Epoch [12/30], Batch [4600/6000], Loss: 0.1311
Epoch [12/30], Batch [4700/6000], Loss: 0.1476
Epoch [12/30], Batch [4800/6000], Loss: 0.6333
Epoch [12/30], Batch [4900/6000], Loss: 0.2049
Epoch [12/30], Batch [5000/6000], Loss: 0.3714
Epoch [12/30], Batch [5100/6000], Loss: 0.1330
Epoch [12/30], Batch [5200/6000], Loss: 0.1811
Epoch [12/30], Batch [5300/6000], Loss: 0.1248
Epoch [12/30], Batch [5400/6000], Loss: 1.9224
Epoch [12/30], Batch [5500/6000], Loss: 0.1194
Epoch [12/30], Batch [5600/6000], Loss: 0.6193
Epoch [12/30], Batch [5700/6000], Loss: 0.4195
Epoch [12/30], Batch [5800/6000], Loss: 0.7090
Epoch [12/30], Batch [5900/6000], Loss: 0.1538
Epoch [12/30], Loss: 0.3509
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.1726
Epoch [13/30], Batch [100/6000], Loss: 0.1958
Epoch [13/30], Batch [200/6000], Loss: 0.1478
Epoch [13/30], Batch [300/6000], Loss: 0.1601
Epoch [13/30], Batch [400/6000], Loss: 0.4510
Epoch [13/30], Batch [500/6000], Loss: 0.1651
Epoch [13/30], Batch [600/6000], Loss: 0.5084
Epoch [13/30], Batch [700/6000], Loss: 0.2007
Epoch [13/30], Batch [800/6000], Loss: 0.1376
Epoch [13/30], Batch [900/6000], Loss: 0.1477
Epoch [13/30], Batch [1000/6000], Loss: 0.1833
Epoch [13/30], Batch [1100/6000], Loss: 0.1634
Epoch [13/30], Batch [1200/6000], Loss: 0.2333
Epoch [13/30], Batch [1300/6000], Loss: 0.1265
Epoch [13/30], Batch [1400/6000], Loss: 0.1211
Epoch [13/30], Batch [1500/6000], Loss: 0.1392
Epoch [13/30], Batch [1600/6000], Loss: 0.2447
Epoch [13/30], Batch [1700/6000], Loss: 0.1379
Epoch [13/30], Batch [1800/6000], Loss: 1.3223
Epoch [13/30], Batch [1900/6000], Loss: 0.1308
Epoch [13/30], Batch [2000/6000], Loss: 0.1711
Epoch [13/30], Batch [2100/6000], Loss: 0.3205
Epoch [13/30], Batch [2200/6000], Loss: 0.1628
Epoch [13/30], Batch [2300/6000], Loss: 0.1268
Epoch [13/30], Batch [2400/6000], Loss: 0.1624
Epoch [13/30], Batch [2500/6000], Loss: 0.1888
Epoch [13/30], Batch [2600/6000], Loss: 0.1239
Epoch [13/30], Batch [2700/6000], Loss: 0.1423
Epoch [13/30], Batch [2800/6000], Loss: 0.1294
Epoch [13/30], Batch [2900/6000], Loss: 0.9161
Epoch [13/30], Batch [3000/6000], Loss: 0.1347
Epoch [13/30], Batch [3100/6000], Loss: 0.3329
Epoch [13/30], Batch [3200/6000], Loss: 0.2693
Epoch [13/30], Batch [3300/6000], Loss: 0.1364
Epoch [13/30], Batch [3400/6000], Loss: 0.1790
Epoch [13/30], Batch [3500/6000], Loss: 0.2124
Epoch [13/30], Batch [3600/6000], Loss: 0.3509
Epoch [13/30], Batch [3700/6000], Loss: 0.1514
Epoch [13/30], Batch [3800/6000], Loss: 0.1559
Epoch [13/30], Batch [3900/6000], Loss: 0.1219
Epoch [13/30], Batch [4000/6000], Loss: 0.1354
Epoch [13/30], Batch [4100/6000], Loss: 0.3708
Epoch [13/30], Batch [4200/6000], Loss: 0.1476
Epoch [13/30], Batch [4300/6000], Loss: 0.1459
Epoch [13/30], Batch [4400/6000], Loss: 0.1843
Epoch [13/30], Batch [4500/6000], Loss: 0.5512
Epoch [13/30], Batch [4600/6000], Loss: 0.3420
Epoch [13/30], Batch [4700/6000], Loss: 0.1505
Epoch [13/30], Batch [4800/6000], Loss: 0.4457
Epoch [13/30], Batch [4900/6000], Loss: 0.1948
Epoch [13/30], Batch [5000/6000], Loss: 0.1250
Epoch [13/30], Batch [5100/6000], Loss: 0.1372
Epoch [13/30], Batch [5200/6000], Loss: 0.1402
Epoch [13/30], Batch [5300/6000], Loss: 0.1667
Epoch [13/30], Batch [5400/6000], Loss: 0.1406
Epoch [13/30], Batch [5500/6000], Loss: 0.1324
Epoch [13/30], Batch [5600/6000], Loss: 0.1628
Epoch [13/30], Batch [5700/6000], Loss: 0.1506
Epoch [13/30], Batch [5800/6000], Loss: 0.1482
Epoch [13/30], Batch [5900/6000], Loss: 0.1399
Epoch [13/30], Loss: 0.3251
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.1380
Epoch [14/30], Batch [100/6000], Loss: 0.3286
Epoch [14/30], Batch [200/6000], Loss: 0.1349
Epoch [14/30], Batch [300/6000], Loss: 0.1377
Epoch [14/30], Batch [400/6000], Loss: 0.1563
Epoch [14/30], Batch [500/6000], Loss: 0.1483
Epoch [14/30], Batch [600/6000], Loss: 0.1280
Epoch [14/30], Batch [700/6000], Loss: 0.1328
Epoch [14/30], Batch [800/6000], Loss: 0.1337
Epoch [14/30], Batch [900/6000], Loss: 0.1245
Epoch [14/30], Batch [1000/6000], Loss: 0.1436
Epoch [14/30], Batch [1100/6000], Loss: 0.1197
Epoch [14/30], Batch [1200/6000], Loss: 0.1019
Epoch [14/30], Batch [1300/6000], Loss: 0.1754
Epoch [14/30], Batch [1400/6000], Loss: 0.2673
Epoch [14/30], Batch [1500/6000], Loss: 0.1527
Epoch [14/30], Batch [1600/6000], Loss: 0.6579
Epoch [14/30], Batch [1700/6000], Loss: 0.1356
Epoch [14/30], Batch [1800/6000], Loss: 0.1458
Epoch [14/30], Batch [1900/6000], Loss: 0.1219
Epoch [14/30], Batch [2000/6000], Loss: 0.1610
Epoch [14/30], Batch [2100/6000], Loss: 0.8319
Epoch [14/30], Batch [2200/6000], Loss: 0.8681
Epoch [14/30], Batch [2300/6000], Loss: 0.1326
Epoch [14/30], Batch [2400/6000], Loss: 0.1263
Epoch [14/30], Batch [2500/6000], Loss: 0.1276
Epoch [14/30], Batch [2600/6000], Loss: 0.1449
Epoch [14/30], Batch [2700/6000], Loss: 1.2896
Epoch [14/30], Batch [2800/6000], Loss: 0.1312
Epoch [14/30], Batch [2900/6000], Loss: 0.2063
Epoch [14/30], Batch [3000/6000], Loss: 0.1499
Epoch [14/30], Batch [3100/6000], Loss: 0.3482
Epoch [14/30], Batch [3200/6000], Loss: 0.2558
Epoch [14/30], Batch [3300/6000], Loss: 0.1234
Epoch [14/30], Batch [3400/6000], Loss: 0.1727
Epoch [14/30], Batch [3500/6000], Loss: 0.1628
Epoch [14/30], Batch [3600/6000], Loss: 1.6961
Epoch [14/30], Batch [3700/6000], Loss: 0.1301
Epoch [14/30], Batch [3800/6000], Loss: 0.1432
Epoch [14/30], Batch [3900/6000], Loss: 0.1484
Epoch [14/30], Batch [4000/6000], Loss: 0.7471
Epoch [14/30], Batch [4100/6000], Loss: 0.1566
Epoch [14/30], Batch [4200/6000], Loss: 0.1136
Epoch [14/30], Batch [4300/6000], Loss: 0.1362
Epoch [14/30], Batch [4400/6000], Loss: 0.1279
Epoch [14/30], Batch [4500/6000], Loss: 0.1365
Epoch [14/30], Batch [4600/6000], Loss: 0.5159
Epoch [14/30], Batch [4700/6000], Loss: 0.1481
Epoch [14/30], Batch [4800/6000], Loss: 0.1344
Epoch [14/30], Batch [4900/6000], Loss: 0.1836
Epoch [14/30], Batch [5000/6000], Loss: 0.1440
Epoch [14/30], Batch [5100/6000], Loss: 0.1162
Epoch [14/30], Batch [5200/6000], Loss: 0.3798
Epoch [14/30], Batch [5300/6000], Loss: 0.1604
Epoch [14/30], Batch [5400/6000], Loss: 0.1435
Epoch [14/30], Batch [5500/6000], Loss: 0.4509
Epoch [14/30], Batch [5600/6000], Loss: 0.1428
Epoch [14/30], Batch [5700/6000], Loss: 1.6170
Epoch [14/30], Batch [5800/6000], Loss: 0.9817
Epoch [14/30], Batch [5900/6000], Loss: 0.5748
Epoch [14/30], Loss: 0.3075
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.2551
Epoch [15/30], Batch [100/6000], Loss: 0.1521
Epoch [15/30], Batch [200/6000], Loss: 0.3017
Epoch [15/30], Batch [300/6000], Loss: 0.1418
Epoch [15/30], Batch [400/6000], Loss: 0.1658
Epoch [15/30], Batch [500/6000], Loss: 0.1910
Epoch [15/30], Batch [600/6000], Loss: 0.2723
Epoch [15/30], Batch [700/6000], Loss: 0.2798
Epoch [15/30], Batch [800/6000], Loss: 0.1196
Epoch [15/30], Batch [900/6000], Loss: 0.1957
Epoch [15/30], Batch [1000/6000], Loss: 0.1513
Epoch [15/30], Batch [1100/6000], Loss: 0.1443
Epoch [15/30], Batch [1200/6000], Loss: 0.1592
Epoch [15/30], Batch [1300/6000], Loss: 0.1197
Epoch [15/30], Batch [1400/6000], Loss: 0.2663
Epoch [15/30], Batch [1500/6000], Loss: 0.1315
Epoch [15/30], Batch [1600/6000], Loss: 0.1289
Epoch [15/30], Batch [1700/6000], Loss: 0.1365
Epoch [15/30], Batch [1800/6000], Loss: 0.2239
Epoch [15/30], Batch [1900/6000], Loss: 0.1383
Epoch [15/30], Batch [2000/6000], Loss: 1.9934
Epoch [15/30], Batch [2100/6000], Loss: 0.1801
Epoch [15/30], Batch [2200/6000], Loss: 0.1382
Epoch [15/30], Batch [2300/6000], Loss: 0.1487
Epoch [15/30], Batch [2400/6000], Loss: 2.2054
Epoch [15/30], Batch [2500/6000], Loss: 0.1104
Epoch [15/30], Batch [2600/6000], Loss: 0.9019
Epoch [15/30], Batch [2700/6000], Loss: 0.5271
Epoch [15/30], Batch [2800/6000], Loss: 0.5953
Epoch [15/30], Batch [2900/6000], Loss: 0.1218
Epoch [15/30], Batch [3000/6000], Loss: 0.1852
Epoch [15/30], Batch [3100/6000], Loss: 0.3256
Epoch [15/30], Batch [3200/6000], Loss: 0.1511
Epoch [15/30], Batch [3300/6000], Loss: 0.1287
Epoch [15/30], Batch [3400/6000], Loss: 0.3618
Epoch [15/30], Batch [3500/6000], Loss: 2.0792
Epoch [15/30], Batch [3600/6000], Loss: 1.0997
Epoch [15/30], Batch [3700/6000], Loss: 0.1451
Epoch [15/30], Batch [3800/6000], Loss: 0.5140
Epoch [15/30], Batch [3900/6000], Loss: 0.2819
Epoch [15/30], Batch [4000/6000], Loss: 0.1782
Epoch [15/30], Batch [4100/6000], Loss: 0.1692
Epoch [15/30], Batch [4200/6000], Loss: 0.1092
Epoch [15/30], Batch [4300/6000], Loss: 0.6726
Epoch [15/30], Batch [4400/6000], Loss: 0.1657
Epoch [15/30], Batch [4500/6000], Loss: 0.1670
Epoch [15/30], Batch [4600/6000], Loss: 0.1526
Epoch [15/30], Batch [4700/6000], Loss: 1.3402
Epoch [15/30], Batch [4800/6000], Loss: 0.1250
Epoch [15/30], Batch [4900/6000], Loss: 0.1545
Epoch [15/30], Batch [5000/6000], Loss: 0.1065
Epoch [15/30], Batch [5100/6000], Loss: 0.1600
Epoch [15/30], Batch [5200/6000], Loss: 0.1362
Epoch [15/30], Batch [5300/6000], Loss: 0.1454
Epoch [15/30], Batch [5400/6000], Loss: 0.1337
Epoch [15/30], Batch [5500/6000], Loss: 1.5213
Epoch [15/30], Batch [5600/6000], Loss: 0.3761
Epoch [15/30], Batch [5700/6000], Loss: 0.1319
Epoch [15/30], Batch [5800/6000], Loss: 0.1809
Epoch [15/30], Batch [5900/6000], Loss: 0.1151
Epoch [15/30], Loss: 0.2912
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.1651
Epoch [16/30], Batch [100/6000], Loss: 0.2364
Epoch [16/30], Batch [200/6000], Loss: 0.1717
Epoch [16/30], Batch [300/6000], Loss: 0.1811
Epoch [16/30], Batch [400/6000], Loss: 0.1482
Epoch [16/30], Batch [500/6000], Loss: 0.2291
Epoch [16/30], Batch [600/6000], Loss: 0.0921
Epoch [16/30], Batch [700/6000], Loss: 0.1227
Epoch [16/30], Batch [800/6000], Loss: 0.4034
Epoch [16/30], Batch [900/6000], Loss: 0.1647
Epoch [16/30], Batch [1000/6000], Loss: 0.1670
Epoch [16/30], Batch [1100/6000], Loss: 2.5979
Epoch [16/30], Batch [1200/6000], Loss: 0.7134
Epoch [16/30], Batch [1300/6000], Loss: 0.1410
Epoch [16/30], Batch [1400/6000], Loss: 0.1033
Epoch [16/30], Batch [1500/6000], Loss: 0.2422
Epoch [16/30], Batch [1600/6000], Loss: 0.1202
Epoch [16/30], Batch [1700/6000], Loss: 0.7954
Epoch [16/30], Batch [1800/6000], Loss: 0.1270
Epoch [16/30], Batch [1900/6000], Loss: 0.1725
Epoch [16/30], Batch [2000/6000], Loss: 0.1276
Epoch [16/30], Batch [2100/6000], Loss: 0.1390
Epoch [16/30], Batch [2200/6000], Loss: 0.1334
Epoch [16/30], Batch [2300/6000], Loss: 0.4249
Epoch [16/30], Batch [2400/6000], Loss: 0.1259
Epoch [16/30], Batch [2500/6000], Loss: 0.1432
Epoch [16/30], Batch [2600/6000], Loss: 0.1273
Epoch [16/30], Batch [2700/6000], Loss: 0.1329
Epoch [16/30], Batch [2800/6000], Loss: 0.1196
Epoch [16/30], Batch [2900/6000], Loss: 0.1024
Epoch [16/30], Batch [3000/6000], Loss: 0.1192
Epoch [16/30], Batch [3100/6000], Loss: 0.1779
Epoch [16/30], Batch [3200/6000], Loss: 0.1147
Epoch [16/30], Batch [3300/6000], Loss: 0.1287
Epoch [16/30], Batch [3400/6000], Loss: 0.1682
Epoch [16/30], Batch [3500/6000], Loss: 0.1266
Epoch [16/30], Batch [3600/6000], Loss: 0.1455
Epoch [16/30], Batch [3700/6000], Loss: 0.1290
Epoch [16/30], Batch [3800/6000], Loss: 0.1493
Epoch [16/30], Batch [3900/6000], Loss: 0.2117
Epoch [16/30], Batch [4000/6000], Loss: 0.1059
Epoch [16/30], Batch [4100/6000], Loss: 0.1432
Epoch [16/30], Batch [4200/6000], Loss: 0.1292
Epoch [16/30], Batch [4300/6000], Loss: 0.1558
Epoch [16/30], Batch [4400/6000], Loss: 0.4841
Epoch [16/30], Batch [4500/6000], Loss: 0.1097
Epoch [16/30], Batch [4600/6000], Loss: 0.1848
Epoch [16/30], Batch [4700/6000], Loss: 0.2907
Epoch [16/30], Batch [4800/6000], Loss: 0.1247
Epoch [16/30], Batch [4900/6000], Loss: 0.1546
Epoch [16/30], Batch [5000/6000], Loss: 0.1555
Epoch [16/30], Batch [5100/6000], Loss: 0.2061
Epoch [16/30], Batch [5200/6000], Loss: 0.1241
Epoch [16/30], Batch [5300/6000], Loss: 0.1190
Epoch [16/30], Batch [5400/6000], Loss: 0.1824
Epoch [16/30], Batch [5500/6000], Loss: 0.4232
Epoch [16/30], Batch [5600/6000], Loss: 0.1253
Epoch [16/30], Batch [5700/6000], Loss: 0.1128
Epoch [16/30], Batch [5800/6000], Loss: 0.1374
Epoch [16/30], Batch [5900/6000], Loss: 1.7085
Epoch [16/30], Loss: 0.2736
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.2460
Epoch [17/30], Batch [100/6000], Loss: 0.3410
Epoch [17/30], Batch [200/6000], Loss: 0.1409
Epoch [17/30], Batch [300/6000], Loss: 0.0955
Epoch [17/30], Batch [400/6000], Loss: 0.1315
Epoch [17/30], Batch [500/6000], Loss: 0.1499
Epoch [17/30], Batch [600/6000], Loss: 0.1535
Epoch [17/30], Batch [700/6000], Loss: 0.1843
Epoch [17/30], Batch [800/6000], Loss: 0.1355
Epoch [17/30], Batch [900/6000], Loss: 0.1413
Epoch [17/30], Batch [1000/6000], Loss: 0.2407
Epoch [17/30], Batch [1100/6000], Loss: 0.1324
Epoch [17/30], Batch [1200/6000], Loss: 0.1229
Epoch [17/30], Batch [1300/6000], Loss: 0.1146
Epoch [17/30], Batch [1400/6000], Loss: 0.1742
Epoch [17/30], Batch [1500/6000], Loss: 0.1490
Epoch [17/30], Batch [1600/6000], Loss: 0.1362
Epoch [17/30], Batch [1700/6000], Loss: 0.1252
Epoch [17/30], Batch [1800/6000], Loss: 0.2776
Epoch [17/30], Batch [1900/6000], Loss: 0.1384
Epoch [17/30], Batch [2000/6000], Loss: 0.3506
Epoch [17/30], Batch [2100/6000], Loss: 0.1797
Epoch [17/30], Batch [2200/6000], Loss: 0.1256
Epoch [17/30], Batch [2300/6000], Loss: 0.1493
Epoch [17/30], Batch [2400/6000], Loss: 0.1273
Epoch [17/30], Batch [2500/6000], Loss: 0.1514
Epoch [17/30], Batch [2600/6000], Loss: 0.5211
Epoch [17/30], Batch [2700/6000], Loss: 0.1652
Epoch [17/30], Batch [2800/6000], Loss: 0.1411
Epoch [17/30], Batch [2900/6000], Loss: 0.1371
Epoch [17/30], Batch [3000/6000], Loss: 0.2114
Epoch [17/30], Batch [3100/6000], Loss: 0.1386
Epoch [17/30], Batch [3200/6000], Loss: 0.2437
Epoch [17/30], Batch [3300/6000], Loss: 0.1389
Epoch [17/30], Batch [3400/6000], Loss: 0.4696
Epoch [17/30], Batch [3500/6000], Loss: 0.2665
Epoch [17/30], Batch [3600/6000], Loss: 0.3610
Epoch [17/30], Batch [3700/6000], Loss: 0.7108
Epoch [17/30], Batch [3800/6000], Loss: 0.2739
Epoch [17/30], Batch [3900/6000], Loss: 0.4388
Epoch [17/30], Batch [4000/6000], Loss: 0.1362
Epoch [17/30], Batch [4100/6000], Loss: 0.1243
Epoch [17/30], Batch [4200/6000], Loss: 0.1432
Epoch [17/30], Batch [4300/6000], Loss: 0.1291
Epoch [17/30], Batch [4400/6000], Loss: 0.1201
Epoch [17/30], Batch [4500/6000], Loss: 0.1328
Epoch [17/30], Batch [4600/6000], Loss: 0.1316
Epoch [17/30], Batch [4700/6000], Loss: 0.1132
Epoch [17/30], Batch [4800/6000], Loss: 0.4110
Epoch [17/30], Batch [4900/6000], Loss: 0.1720
Epoch [17/30], Batch [5000/6000], Loss: 0.1542
Epoch [17/30], Batch [5100/6000], Loss: 0.1942
Epoch [17/30], Batch [5200/6000], Loss: 0.1176
Epoch [17/30], Batch [5300/6000], Loss: 0.1539
Epoch [17/30], Batch [5400/6000], Loss: 0.9928
Epoch [17/30], Batch [5500/6000], Loss: 0.1174
Epoch [17/30], Batch [5600/6000], Loss: 0.1285
Epoch [17/30], Batch [5700/6000], Loss: 0.1376
Epoch [17/30], Batch [5800/6000], Loss: 0.1221
Epoch [17/30], Batch [5900/6000], Loss: 0.1333
Epoch [17/30], Loss: 0.2588
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.1551
Epoch [18/30], Batch [100/6000], Loss: 0.1268
Epoch [18/30], Batch [200/6000], Loss: 0.1247
Epoch [18/30], Batch [300/6000], Loss: 0.0820
Epoch [18/30], Batch [400/6000], Loss: 0.1277
Epoch [18/30], Batch [500/6000], Loss: 0.1053
Epoch [18/30], Batch [600/6000], Loss: 0.2125
Epoch [18/30], Batch [700/6000], Loss: 0.1207
Epoch [18/30], Batch [800/6000], Loss: 0.1662
Epoch [18/30], Batch [900/6000], Loss: 0.1299
Epoch [18/30], Batch [1000/6000], Loss: 0.1806
Epoch [18/30], Batch [1100/6000], Loss: 0.1270
Epoch [18/30], Batch [1200/6000], Loss: 0.1134
Epoch [18/30], Batch [1300/6000], Loss: 0.1633
Epoch [18/30], Batch [1400/6000], Loss: 0.1373
Epoch [18/30], Batch [1500/6000], Loss: 0.1405
Epoch [18/30], Batch [1600/6000], Loss: 0.1440
Epoch [18/30], Batch [1700/6000], Loss: 0.1335
Epoch [18/30], Batch [1800/6000], Loss: 0.1406
Epoch [18/30], Batch [1900/6000], Loss: 0.1276
Epoch [18/30], Batch [2000/6000], Loss: 0.1575
Epoch [18/30], Batch [2100/6000], Loss: 0.1285
Epoch [18/30], Batch [2200/6000], Loss: 0.7909
Epoch [18/30], Batch [2300/6000], Loss: 0.1501
Epoch [18/30], Batch [2400/6000], Loss: 0.1177
Epoch [18/30], Batch [2500/6000], Loss: 0.1133
Epoch [18/30], Batch [2600/6000], Loss: 0.2483
Epoch [18/30], Batch [2700/6000], Loss: 0.9834
Epoch [18/30], Batch [2800/6000], Loss: 0.1406
Epoch [18/30], Batch [2900/6000], Loss: 0.1462
Epoch [18/30], Batch [3000/6000], Loss: 0.1722
Epoch [18/30], Batch [3100/6000], Loss: 1.0967
Epoch [18/30], Batch [3200/6000], Loss: 0.2256
Epoch [18/30], Batch [3300/6000], Loss: 0.1239
Epoch [18/30], Batch [3400/6000], Loss: 0.2378
Epoch [18/30], Batch [3500/6000], Loss: 0.1121
Epoch [18/30], Batch [3600/6000], Loss: 0.1055
Epoch [18/30], Batch [3700/6000], Loss: 0.1387
Epoch [18/30], Batch [3800/6000], Loss: 0.5355
Epoch [18/30], Batch [3900/6000], Loss: 0.1355
Epoch [18/30], Batch [4000/6000], Loss: 0.1375
Epoch [18/30], Batch [4100/6000], Loss: 0.1510
Epoch [18/30], Batch [4200/6000], Loss: 0.6148
Epoch [18/30], Batch [4300/6000], Loss: 1.0107
Epoch [18/30], Batch [4400/6000], Loss: 0.1542
Epoch [18/30], Batch [4500/6000], Loss: 0.1484
Epoch [18/30], Batch [4600/6000], Loss: 0.7914
Epoch [18/30], Batch [4700/6000], Loss: 0.1210
Epoch [18/30], Batch [4800/6000], Loss: 0.1350
Epoch [18/30], Batch [4900/6000], Loss: 0.1147
Epoch [18/30], Batch [5000/6000], Loss: 0.1324
Epoch [18/30], Batch [5100/6000], Loss: 0.1271
Epoch [18/30], Batch [5200/6000], Loss: 0.2837
Epoch [18/30], Batch [5300/6000], Loss: 0.0987
Epoch [18/30], Batch [5400/6000], Loss: 0.1303
Epoch [18/30], Batch [5500/6000], Loss: 0.2498
Epoch [18/30], Batch [5600/6000], Loss: 0.1505
Epoch [18/30], Batch [5700/6000], Loss: 0.2624
Epoch [18/30], Batch [5800/6000], Loss: 0.1322
Epoch [18/30], Batch [5900/6000], Loss: 0.1254
Epoch [18/30], Loss: 0.2456
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.1516
Epoch [19/30], Batch [100/6000], Loss: 0.1526
Epoch [19/30], Batch [200/6000], Loss: 0.1169
Epoch [19/30], Batch [300/6000], Loss: 0.1147
Epoch [19/30], Batch [400/6000], Loss: 0.1309
Epoch [19/30], Batch [500/6000], Loss: 0.1631
Epoch [19/30], Batch [600/6000], Loss: 0.1318
Epoch [19/30], Batch [700/6000], Loss: 0.1260
Epoch [19/30], Batch [800/6000], Loss: 0.1492
Epoch [19/30], Batch [900/6000], Loss: 0.1185
Epoch [19/30], Batch [1000/6000], Loss: 0.6137
Epoch [19/30], Batch [1100/6000], Loss: 0.2076
Epoch [19/30], Batch [1200/6000], Loss: 0.1376
Epoch [19/30], Batch [1300/6000], Loss: 0.1223
Epoch [19/30], Batch [1400/6000], Loss: 0.1229
Epoch [19/30], Batch [1500/6000], Loss: 0.1048
Epoch [19/30], Batch [1600/6000], Loss: 0.2549
Epoch [19/30], Batch [1700/6000], Loss: 0.1594
Epoch [19/30], Batch [1800/6000], Loss: 0.1172
Epoch [19/30], Batch [1900/6000], Loss: 0.1190
Epoch [19/30], Batch [2000/6000], Loss: 0.1203
Epoch [19/30], Batch [2100/6000], Loss: 0.1271
Epoch [19/30], Batch [2200/6000], Loss: 0.1189
Epoch [19/30], Batch [2300/6000], Loss: 1.7200
Epoch [19/30], Batch [2400/6000], Loss: 0.1577
Epoch [19/30], Batch [2500/6000], Loss: 0.1098
Epoch [19/30], Batch [2600/6000], Loss: 0.1426
Epoch [19/30], Batch [2700/6000], Loss: 0.1236
Epoch [19/30], Batch [2800/6000], Loss: 0.1329
Epoch [19/30], Batch [2900/6000], Loss: 0.2388
Epoch [19/30], Batch [3000/6000], Loss: 0.1333
Epoch [19/30], Batch [3100/6000], Loss: 0.1423
Epoch [19/30], Batch [3200/6000], Loss: 0.5496
Epoch [19/30], Batch [3300/6000], Loss: 0.1307
Epoch [19/30], Batch [3400/6000], Loss: 0.1015
Epoch [19/30], Batch [3500/6000], Loss: 0.1126
Epoch [19/30], Batch [3600/6000], Loss: 0.1376
Epoch [19/30], Batch [3700/6000], Loss: 0.3976
Epoch [19/30], Batch [3800/6000], Loss: 0.1534
Epoch [19/30], Batch [3900/6000], Loss: 0.1624
Epoch [19/30], Batch [4000/6000], Loss: 0.1381
Epoch [19/30], Batch [4100/6000], Loss: 0.1070
Epoch [19/30], Batch [4200/6000], Loss: 0.0956
Epoch [19/30], Batch [4300/6000], Loss: 0.1244
Epoch [19/30], Batch [4400/6000], Loss: 0.1455
Epoch [19/30], Batch [4500/6000], Loss: 0.1328
Epoch [19/30], Batch [4600/6000], Loss: 0.1476
Epoch [19/30], Batch [4700/6000], Loss: 0.2016
Epoch [19/30], Batch [4800/6000], Loss: 0.1396
Epoch [19/30], Batch [4900/6000], Loss: 1.2199
Epoch [19/30], Batch [5000/6000], Loss: 0.1420
Epoch [19/30], Batch [5100/6000], Loss: 0.5928
Epoch [19/30], Batch [5200/6000], Loss: 0.1294
Epoch [19/30], Batch [5300/6000], Loss: 0.1192
Epoch [19/30], Batch [5400/6000], Loss: 0.2213
Epoch [19/30], Batch [5500/6000], Loss: 0.1204
Epoch [19/30], Batch [5600/6000], Loss: 0.1203
Epoch [19/30], Batch [5700/6000], Loss: 1.1342
Epoch [19/30], Batch [5800/6000], Loss: 0.1323
Epoch [19/30], Batch [5900/6000], Loss: 0.1269
Epoch [19/30], Loss: 0.2368
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.1008
Epoch [20/30], Batch [100/6000], Loss: 0.1792
Epoch [20/30], Batch [200/6000], Loss: 0.1188
Epoch [20/30], Batch [300/6000], Loss: 0.1854
Epoch [20/30], Batch [400/6000], Loss: 0.1219
Epoch [20/30], Batch [500/6000], Loss: 0.2283
Epoch [20/30], Batch [600/6000], Loss: 0.1390
Epoch [20/30], Batch [700/6000], Loss: 0.1365
Epoch [20/30], Batch [800/6000], Loss: 0.1766
Epoch [20/30], Batch [900/6000], Loss: 0.1065
Epoch [20/30], Batch [1000/6000], Loss: 0.1198
Epoch [20/30], Batch [1100/6000], Loss: 0.0913
Epoch [20/30], Batch [1200/6000], Loss: 0.1080
Epoch [20/30], Batch [1300/6000], Loss: 0.1104
Epoch [20/30], Batch [1400/6000], Loss: 0.1246
Epoch [20/30], Batch [1500/6000], Loss: 0.2642
Epoch [20/30], Batch [1600/6000], Loss: 0.1294
Epoch [20/30], Batch [1700/6000], Loss: 0.1161
Epoch [20/30], Batch [1800/6000], Loss: 0.1398
Epoch [20/30], Batch [1900/6000], Loss: 0.1059
Epoch [20/30], Batch [2000/6000], Loss: 0.1379
Epoch [20/30], Batch [2100/6000], Loss: 0.1233
Epoch [20/30], Batch [2200/6000], Loss: 0.1427
Epoch [20/30], Batch [2300/6000], Loss: 0.1279
Epoch [20/30], Batch [2400/6000], Loss: 0.1718
Epoch [20/30], Batch [2500/6000], Loss: 0.1373
Epoch [20/30], Batch [2600/6000], Loss: 0.1401
Epoch [20/30], Batch [2700/6000], Loss: 0.8278
Epoch [20/30], Batch [2800/6000], Loss: 0.1249
Epoch [20/30], Batch [2900/6000], Loss: 0.1716
Epoch [20/30], Batch [3000/6000], Loss: 0.1343
Epoch [20/30], Batch [3100/6000], Loss: 0.3270
Epoch [20/30], Batch [3200/6000], Loss: 0.3972
Epoch [20/30], Batch [3300/6000], Loss: 0.1121
Epoch [20/30], Batch [3400/6000], Loss: 0.4370
Epoch [20/30], Batch [3500/6000], Loss: 0.3235
Epoch [20/30], Batch [3600/6000], Loss: 0.1300
Epoch [20/30], Batch [3700/6000], Loss: 0.3647
Epoch [20/30], Batch [3800/6000], Loss: 0.6614
Epoch [20/30], Batch [3900/6000], Loss: 0.1458
Epoch [20/30], Batch [4000/6000], Loss: 0.1139
Epoch [20/30], Batch [4100/6000], Loss: 0.4380
Epoch [20/30], Batch [4200/6000], Loss: 0.1350
Epoch [20/30], Batch [4300/6000], Loss: 0.2305
Epoch [20/30], Batch [4400/6000], Loss: 0.1110
Epoch [20/30], Batch [4500/6000], Loss: 0.5122
Epoch [20/30], Batch [4600/6000], Loss: 0.0925
Epoch [20/30], Batch [4700/6000], Loss: 0.2450
Epoch [20/30], Batch [4800/6000], Loss: 0.1755
Epoch [20/30], Batch [4900/6000], Loss: 0.1606
Epoch [20/30], Batch [5000/6000], Loss: 0.1301
Epoch [20/30], Batch [5100/6000], Loss: 0.1223
Epoch [20/30], Batch [5200/6000], Loss: 0.1225
Epoch [20/30], Batch [5300/6000], Loss: 0.9412
Epoch [20/30], Batch [5400/6000], Loss: 0.1421
Epoch [20/30], Batch [5500/6000], Loss: 0.1186
Epoch [20/30], Batch [5600/6000], Loss: 0.1236
Epoch [20/30], Batch [5700/6000], Loss: 0.1382
Epoch [20/30], Batch [5800/6000], Loss: 0.1173
Epoch [20/30], Batch [5900/6000], Loss: 0.0961
Epoch [20/30], Loss: 0.2254
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.3243
Epoch [21/30], Batch [100/6000], Loss: 0.0985
Epoch [21/30], Batch [200/6000], Loss: 0.1347
Epoch [21/30], Batch [300/6000], Loss: 0.1385
Epoch [21/30], Batch [400/6000], Loss: 0.1558
Epoch [21/30], Batch [500/6000], Loss: 0.1728
Epoch [21/30], Batch [600/6000], Loss: 0.1264
Epoch [21/30], Batch [700/6000], Loss: 0.1667
Epoch [21/30], Batch [800/6000], Loss: 0.1001
Epoch [21/30], Batch [900/6000], Loss: 0.1402
Epoch [21/30], Batch [1000/6000], Loss: 0.1166
Epoch [21/30], Batch [1100/6000], Loss: 0.1356
Epoch [21/30], Batch [1200/6000], Loss: 0.1885
Epoch [21/30], Batch [1300/6000], Loss: 0.1362
Epoch [21/30], Batch [1400/6000], Loss: 0.1357
Epoch [21/30], Batch [1500/6000], Loss: 0.1491
Epoch [21/30], Batch [1600/6000], Loss: 0.2768
Epoch [21/30], Batch [1700/6000], Loss: 0.0962
Epoch [21/30], Batch [1800/6000], Loss: 0.1323
Epoch [21/30], Batch [1900/6000], Loss: 0.1236
Epoch [21/30], Batch [2000/6000], Loss: 0.1104
Epoch [21/30], Batch [2100/6000], Loss: 0.1316
Epoch [21/30], Batch [2200/6000], Loss: 0.1341
Epoch [21/30], Batch [2300/6000], Loss: 1.0626
Epoch [21/30], Batch [2400/6000], Loss: 0.9867
Epoch [21/30], Batch [2500/6000], Loss: 0.2746
Epoch [21/30], Batch [2600/6000], Loss: 0.1426
Epoch [21/30], Batch [2700/6000], Loss: 0.1492
Epoch [21/30], Batch [2800/6000], Loss: 0.1246
Epoch [21/30], Batch [2900/6000], Loss: 0.1492
Epoch [21/30], Batch [3000/6000], Loss: 0.2882
Epoch [21/30], Batch [3100/6000], Loss: 0.1718
Epoch [21/30], Batch [3200/6000], Loss: 0.1243
Epoch [21/30], Batch [3300/6000], Loss: 0.1025
Epoch [21/30], Batch [3400/6000], Loss: 0.1348
Epoch [21/30], Batch [3500/6000], Loss: 0.1419
Epoch [21/30], Batch [3600/6000], Loss: 0.9843
Epoch [21/30], Batch [3700/6000], Loss: 0.1172
Epoch [21/30], Batch [3800/6000], Loss: 0.1418
Epoch [21/30], Batch [3900/6000], Loss: 0.1344
Epoch [21/30], Batch [4000/6000], Loss: 0.1232
Epoch [21/30], Batch [4100/6000], Loss: 0.1195
Epoch [21/30], Batch [4200/6000], Loss: 0.1244
Epoch [21/30], Batch [4300/6000], Loss: 0.2386
Epoch [21/30], Batch [4400/6000], Loss: 0.1275
Epoch [21/30], Batch [4500/6000], Loss: 0.1334
Epoch [21/30], Batch [4600/6000], Loss: 0.1279
Epoch [21/30], Batch [4700/6000], Loss: 0.1414
Epoch [21/30], Batch [4800/6000], Loss: 0.1255
Epoch [21/30], Batch [4900/6000], Loss: 0.1669
Epoch [21/30], Batch [5000/6000], Loss: 0.1173
Epoch [21/30], Batch [5100/6000], Loss: 0.2568
Epoch [21/30], Batch [5200/6000], Loss: 0.1630
Epoch [21/30], Batch [5300/6000], Loss: 0.3463
Epoch [21/30], Batch [5400/6000], Loss: 0.1180
Epoch [21/30], Batch [5500/6000], Loss: 0.3930
Epoch [21/30], Batch [5600/6000], Loss: 0.3455
Epoch [21/30], Batch [5700/6000], Loss: 0.1455
Epoch [21/30], Batch [5800/6000], Loss: 0.1552
Epoch [21/30], Batch [5900/6000], Loss: 0.1702
Epoch [21/30], Loss: 0.2170
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.1422
Epoch [22/30], Batch [100/6000], Loss: 0.1142
Epoch [22/30], Batch [200/6000], Loss: 0.1114
Epoch [22/30], Batch [300/6000], Loss: 0.1051
Epoch [22/30], Batch [400/6000], Loss: 0.3796
Epoch [22/30], Batch [500/6000], Loss: 0.1339
Epoch [22/30], Batch [600/6000], Loss: 0.2278
Epoch [22/30], Batch [700/6000], Loss: 0.1198
Epoch [22/30], Batch [800/6000], Loss: 0.1136
Epoch [22/30], Batch [900/6000], Loss: 0.1191
Epoch [22/30], Batch [1000/6000], Loss: 0.1278
Epoch [22/30], Batch [1100/6000], Loss: 0.1687
Epoch [22/30], Batch [1200/6000], Loss: 0.1226
Epoch [22/30], Batch [1300/6000], Loss: 0.1076
Epoch [22/30], Batch [1400/6000], Loss: 0.1059
Epoch [22/30], Batch [1500/6000], Loss: 0.1679
Epoch [22/30], Batch [1600/6000], Loss: 0.1849
Epoch [22/30], Batch [1700/6000], Loss: 0.1060
Epoch [22/30], Batch [1800/6000], Loss: 0.1106
Epoch [22/30], Batch [1900/6000], Loss: 0.1330
Epoch [22/30], Batch [2000/6000], Loss: 0.1189
Epoch [22/30], Batch [2100/6000], Loss: 0.0927
Epoch [22/30], Batch [2200/6000], Loss: 0.1207
Epoch [22/30], Batch [2300/6000], Loss: 0.1551
Epoch [22/30], Batch [2400/6000], Loss: 0.1246
Epoch [22/30], Batch [2500/6000], Loss: 0.1136
Epoch [22/30], Batch [2600/6000], Loss: 0.5497
Epoch [22/30], Batch [2700/6000], Loss: 0.1160
Epoch [22/30], Batch [2800/6000], Loss: 0.1136
Epoch [22/30], Batch [2900/6000], Loss: 0.1657
Epoch [22/30], Batch [3000/6000], Loss: 0.1189
Epoch [22/30], Batch [3100/6000], Loss: 0.0970
Epoch [22/30], Batch [3200/6000], Loss: 0.1255
Epoch [22/30], Batch [3300/6000], Loss: 0.1379
Epoch [22/30], Batch [3400/6000], Loss: 0.1463
Epoch [22/30], Batch [3500/6000], Loss: 0.2655
Epoch [22/30], Batch [3600/6000], Loss: 0.8001
Epoch [22/30], Batch [3700/6000], Loss: 0.1349
Epoch [22/30], Batch [3800/6000], Loss: 0.4412
Epoch [22/30], Batch [3900/6000], Loss: 0.1448
Epoch [22/30], Batch [4000/6000], Loss: 0.1010
Epoch [22/30], Batch [4100/6000], Loss: 0.1218
Epoch [22/30], Batch [4200/6000], Loss: 0.1995
Epoch [22/30], Batch [4300/6000], Loss: 0.2950
Epoch [22/30], Batch [4400/6000], Loss: 0.1188
Epoch [22/30], Batch [4500/6000], Loss: 0.1287
Epoch [22/30], Batch [4600/6000], Loss: 0.1019
Epoch [22/30], Batch [4700/6000], Loss: 0.1231
Epoch [22/30], Batch [4800/6000], Loss: 0.1288
Epoch [22/30], Batch [4900/6000], Loss: 0.1725
Epoch [22/30], Batch [5000/6000], Loss: 0.1886
Epoch [22/30], Batch [5100/6000], Loss: 0.1117
Epoch [22/30], Batch [5200/6000], Loss: 0.1624
Epoch [22/30], Batch [5300/6000], Loss: 0.1851
Epoch [22/30], Batch [5400/6000], Loss: 0.1171
Epoch [22/30], Batch [5500/6000], Loss: 0.0931
Epoch [22/30], Batch [5600/6000], Loss: 0.2154
Epoch [22/30], Batch [5700/6000], Loss: 0.1273
Epoch [22/30], Batch [5800/6000], Loss: 0.1423
Epoch [22/30], Batch [5900/6000], Loss: 0.1204
Epoch [22/30], Loss: 0.2078
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.1144
Epoch [23/30], Batch [100/6000], Loss: 0.1012
Epoch [23/30], Batch [200/6000], Loss: 0.1043
Epoch [23/30], Batch [300/6000], Loss: 0.1485
Epoch [23/30], Batch [400/6000], Loss: 0.1605
Epoch [23/30], Batch [500/6000], Loss: 0.1026
Epoch [23/30], Batch [600/6000], Loss: 0.1156
Epoch [23/30], Batch [700/6000], Loss: 0.1118
Epoch [23/30], Batch [800/6000], Loss: 0.1273
Epoch [23/30], Batch [900/6000], Loss: 0.1037
Epoch [23/30], Batch [1000/6000], Loss: 0.2873
Epoch [23/30], Batch [1100/6000], Loss: 0.1333
Epoch [23/30], Batch [1200/6000], Loss: 0.1121
Epoch [23/30], Batch [1300/6000], Loss: 0.1115
Epoch [23/30], Batch [1400/6000], Loss: 0.1260
Epoch [23/30], Batch [1500/6000], Loss: 1.2878
Epoch [23/30], Batch [1600/6000], Loss: 0.1185
Epoch [23/30], Batch [1700/6000], Loss: 0.1244
Epoch [23/30], Batch [1800/6000], Loss: 0.1351
Epoch [23/30], Batch [1900/6000], Loss: 0.1520
Epoch [23/30], Batch [2000/6000], Loss: 0.1616
Epoch [23/30], Batch [2100/6000], Loss: 1.7218
Epoch [23/30], Batch [2200/6000], Loss: 0.1031
Epoch [23/30], Batch [2300/6000], Loss: 0.1914
Epoch [23/30], Batch [2400/6000], Loss: 0.1444
Epoch [23/30], Batch [2500/6000], Loss: 0.1260
Epoch [23/30], Batch [2600/6000], Loss: 0.1085
Epoch [23/30], Batch [2700/6000], Loss: 0.1275
Epoch [23/30], Batch [2800/6000], Loss: 0.1747
Epoch [23/30], Batch [2900/6000], Loss: 0.1364
Epoch [23/30], Batch [3000/6000], Loss: 0.1086
Epoch [23/30], Batch [3100/6000], Loss: 0.1290
Epoch [23/30], Batch [3200/6000], Loss: 0.1388
Epoch [23/30], Batch [3300/6000], Loss: 0.1335
Epoch [23/30], Batch [3400/6000], Loss: 0.1294
Epoch [23/30], Batch [3500/6000], Loss: 0.3663
Epoch [23/30], Batch [3600/6000], Loss: 0.1256
Epoch [23/30], Batch [3700/6000], Loss: 0.2841
Epoch [23/30], Batch [3800/6000], Loss: 0.1144
Epoch [23/30], Batch [3900/6000], Loss: 0.2082
Epoch [23/30], Batch [4000/6000], Loss: 0.0906
Epoch [23/30], Batch [4100/6000], Loss: 0.1040
Epoch [23/30], Batch [4200/6000], Loss: 0.2528
Epoch [23/30], Batch [4300/6000], Loss: 0.1198
Epoch [23/30], Batch [4400/6000], Loss: 0.1148
Epoch [23/30], Batch [4500/6000], Loss: 0.1235
Epoch [23/30], Batch [4600/6000], Loss: 0.1176
Epoch [23/30], Batch [4700/6000], Loss: 0.1734
Epoch [23/30], Batch [4800/6000], Loss: 0.1385
Epoch [23/30], Batch [4900/6000], Loss: 0.1225
Epoch [23/30], Batch [5000/6000], Loss: 0.1189
Epoch [23/30], Batch [5100/6000], Loss: 0.1231
Epoch [23/30], Batch [5200/6000], Loss: 0.2645
Epoch [23/30], Batch [5300/6000], Loss: 0.1110
Epoch [23/30], Batch [5400/6000], Loss: 0.5566
Epoch [23/30], Batch [5500/6000], Loss: 0.1091
Epoch [23/30], Batch [5600/6000], Loss: 0.1200
Epoch [23/30], Batch [5700/6000], Loss: 0.0864
Epoch [23/30], Batch [5800/6000], Loss: 0.1921
Epoch [23/30], Batch [5900/6000], Loss: 0.1248
Epoch [23/30], Loss: 0.1997
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.1443
Epoch [24/30], Batch [100/6000], Loss: 0.1289
Epoch [24/30], Batch [200/6000], Loss: 0.1277
Epoch [24/30], Batch [300/6000], Loss: 0.1104
Epoch [24/30], Batch [400/6000], Loss: 0.1462
Epoch [24/30], Batch [500/6000], Loss: 0.1699
Epoch [24/30], Batch [600/6000], Loss: 0.0914
Epoch [24/30], Batch [700/6000], Loss: 0.0970
Epoch [24/30], Batch [800/6000], Loss: 0.1132
Epoch [24/30], Batch [900/6000], Loss: 0.1157
Epoch [24/30], Batch [1000/6000], Loss: 0.1237
Epoch [24/30], Batch [1100/6000], Loss: 0.1416
Epoch [24/30], Batch [1200/6000], Loss: 0.1098
Epoch [24/30], Batch [1300/6000], Loss: 0.1468
Epoch [24/30], Batch [1400/6000], Loss: 0.6334
Epoch [24/30], Batch [1500/6000], Loss: 0.7558
Epoch [24/30], Batch [1600/6000], Loss: 0.1569
Epoch [24/30], Batch [1700/6000], Loss: 0.1228
Epoch [24/30], Batch [1800/6000], Loss: 0.1198
Epoch [24/30], Batch [1900/6000], Loss: 0.1063
Epoch [24/30], Batch [2000/6000], Loss: 0.1480
Epoch [24/30], Batch [2100/6000], Loss: 0.6123
Epoch [24/30], Batch [2200/6000], Loss: 0.1340
Epoch [24/30], Batch [2300/6000], Loss: 0.1268
Epoch [24/30], Batch [2400/6000], Loss: 0.0969
Epoch [24/30], Batch [2500/6000], Loss: 0.1282
Epoch [24/30], Batch [2600/6000], Loss: 0.2298
Epoch [24/30], Batch [2700/6000], Loss: 0.1341
Epoch [24/30], Batch [2800/6000], Loss: 0.1195
Epoch [24/30], Batch [2900/6000], Loss: 0.1093
Epoch [24/30], Batch [3000/6000], Loss: 0.1382
Epoch [24/30], Batch [3100/6000], Loss: 0.1203
Epoch [24/30], Batch [3200/6000], Loss: 0.1119
Epoch [24/30], Batch [3300/6000], Loss: 0.1030
Epoch [24/30], Batch [3400/6000], Loss: 0.1312
Epoch [24/30], Batch [3500/6000], Loss: 0.1334
Epoch [24/30], Batch [3600/6000], Loss: 0.1159
Epoch [24/30], Batch [3700/6000], Loss: 2.7151
Epoch [24/30], Batch [3800/6000], Loss: 0.1101
Epoch [24/30], Batch [3900/6000], Loss: 0.1339
Epoch [24/30], Batch [4000/6000], Loss: 0.1550
Epoch [24/30], Batch [4100/6000], Loss: 0.1258
Epoch [24/30], Batch [4200/6000], Loss: 0.1129
Epoch [24/30], Batch [4300/6000], Loss: 0.6558
Epoch [24/30], Batch [4400/6000], Loss: 0.1214
Epoch [24/30], Batch [4500/6000], Loss: 0.1296
Epoch [24/30], Batch [4600/6000], Loss: 0.1198
Epoch [24/30], Batch [4700/6000], Loss: 0.4638
Epoch [24/30], Batch [4800/6000], Loss: 0.1465
Epoch [24/30], Batch [4900/6000], Loss: 0.1230
Epoch [24/30], Batch [5000/6000], Loss: 0.1214
Epoch [24/30], Batch [5100/6000], Loss: 0.1383
Epoch [24/30], Batch [5200/6000], Loss: 0.1160
Epoch [24/30], Batch [5300/6000], Loss: 0.2496
Epoch [24/30], Batch [5400/6000], Loss: 0.1406
Epoch [24/30], Batch [5500/6000], Loss: 0.8415
Epoch [24/30], Batch [5600/6000], Loss: 0.1412
Epoch [24/30], Batch [5700/6000], Loss: 0.1870
Epoch [24/30], Batch [5800/6000], Loss: 0.1247
Epoch [24/30], Batch [5900/6000], Loss: 0.1111
Epoch [24/30], Loss: 0.1952
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.1226
Epoch [25/30], Batch [100/6000], Loss: 0.1337
Epoch [25/30], Batch [200/6000], Loss: 0.4357
Epoch [25/30], Batch [300/6000], Loss: 0.0747
Epoch [25/30], Batch [400/6000], Loss: 0.0993
Epoch [25/30], Batch [500/6000], Loss: 0.1266
Epoch [25/30], Batch [600/6000], Loss: 0.1040
Epoch [25/30], Batch [700/6000], Loss: 0.1165
Epoch [25/30], Batch [800/6000], Loss: 0.1161
Epoch [25/30], Batch [900/6000], Loss: 0.0998
Epoch [25/30], Batch [1000/6000], Loss: 0.1314
Epoch [25/30], Batch [1100/6000], Loss: 0.3993
Epoch [25/30], Batch [1200/6000], Loss: 0.1357
Epoch [25/30], Batch [1300/6000], Loss: 0.1176
Epoch [25/30], Batch [1400/6000], Loss: 0.1452
Epoch [25/30], Batch [1500/6000], Loss: 0.1375
Epoch [25/30], Batch [1600/6000], Loss: 0.1519
Epoch [25/30], Batch [1700/6000], Loss: 0.1461
Epoch [25/30], Batch [1800/6000], Loss: 0.5260
Epoch [25/30], Batch [1900/6000], Loss: 0.1346
Epoch [25/30], Batch [2000/6000], Loss: 0.1129
Epoch [25/30], Batch [2100/6000], Loss: 0.1399
Epoch [25/30], Batch [2200/6000], Loss: 0.1065
Epoch [25/30], Batch [2300/6000], Loss: 0.1014
Epoch [25/30], Batch [2400/6000], Loss: 0.1122
Epoch [25/30], Batch [2500/6000], Loss: 0.0866
Epoch [25/30], Batch [2600/6000], Loss: 0.0897
Epoch [25/30], Batch [2700/6000], Loss: 0.2291
Epoch [25/30], Batch [2800/6000], Loss: 0.1590
Epoch [25/30], Batch [2900/6000], Loss: 0.1410
Epoch [25/30], Batch [3000/6000], Loss: 0.1405
Epoch [25/30], Batch [3100/6000], Loss: 0.1268
Epoch [25/30], Batch [3200/6000], Loss: 0.0991
Epoch [25/30], Batch [3300/6000], Loss: 0.1364
Epoch [25/30], Batch [3400/6000], Loss: 0.1274
Epoch [25/30], Batch [3500/6000], Loss: 0.1517
Epoch [25/30], Batch [3600/6000], Loss: 0.1261
Epoch [25/30], Batch [3700/6000], Loss: 0.1186
Epoch [25/30], Batch [3800/6000], Loss: 0.2383
Epoch [25/30], Batch [3900/6000], Loss: 0.1127
Epoch [25/30], Batch [4000/6000], Loss: 0.1102
Epoch [25/30], Batch [4100/6000], Loss: 0.1104
Epoch [25/30], Batch [4200/6000], Loss: 0.1185
Epoch [25/30], Batch [4300/6000], Loss: 0.1316
Epoch [25/30], Batch [4400/6000], Loss: 0.1085
Epoch [25/30], Batch [4500/6000], Loss: 0.1330
Epoch [25/30], Batch [4600/6000], Loss: 0.1014
Epoch [25/30], Batch [4700/6000], Loss: 0.1482
Epoch [25/30], Batch [4800/6000], Loss: 0.2193
Epoch [25/30], Batch [4900/6000], Loss: 0.1028
Epoch [25/30], Batch [5000/6000], Loss: 0.1249
Epoch [25/30], Batch [5100/6000], Loss: 0.1272
Epoch [25/30], Batch [5200/6000], Loss: 0.1125
Epoch [25/30], Batch [5300/6000], Loss: 0.0984
Epoch [25/30], Batch [5400/6000], Loss: 0.1288
Epoch [25/30], Batch [5500/6000], Loss: 0.5489
Epoch [25/30], Batch [5600/6000], Loss: 0.4590
Epoch [25/30], Batch [5700/6000], Loss: 0.1072
Epoch [25/30], Batch [5800/6000], Loss: 0.1151
Epoch [25/30], Batch [5900/6000], Loss: 0.1626
Epoch [25/30], Loss: 0.1845
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.1181
Epoch [26/30], Batch [100/6000], Loss: 0.1457
Epoch [26/30], Batch [200/6000], Loss: 0.1695
Epoch [26/30], Batch [300/6000], Loss: 0.1056
Epoch [26/30], Batch [400/6000], Loss: 0.1113
Epoch [26/30], Batch [500/6000], Loss: 0.1165
Epoch [26/30], Batch [600/6000], Loss: 0.1214
Epoch [26/30], Batch [700/6000], Loss: 0.0844
Epoch [26/30], Batch [800/6000], Loss: 0.1467
Epoch [26/30], Batch [900/6000], Loss: 0.1048
Epoch [26/30], Batch [1000/6000], Loss: 0.1363
Epoch [26/30], Batch [1100/6000], Loss: 0.1676
Epoch [26/30], Batch [1200/6000], Loss: 0.1427
Epoch [26/30], Batch [1300/6000], Loss: 0.1330
Epoch [26/30], Batch [1400/6000], Loss: 0.1115
Epoch [26/30], Batch [1500/6000], Loss: 0.1967
Epoch [26/30], Batch [1600/6000], Loss: 0.1061
Epoch [26/30], Batch [1700/6000], Loss: 0.1259
Epoch [26/30], Batch [1800/6000], Loss: 0.1122
Epoch [26/30], Batch [1900/6000], Loss: 0.0943
Epoch [26/30], Batch [2000/6000], Loss: 0.1190
Epoch [26/30], Batch [2100/6000], Loss: 0.1224
Epoch [26/30], Batch [2200/6000], Loss: 0.1308
Epoch [26/30], Batch [2300/6000], Loss: 0.1159
Epoch [26/30], Batch [2400/6000], Loss: 0.0985
Epoch [26/30], Batch [2500/6000], Loss: 0.1297
Epoch [26/30], Batch [2600/6000], Loss: 0.1002
Epoch [26/30], Batch [2700/6000], Loss: 0.1574
Epoch [26/30], Batch [2800/6000], Loss: 0.1029
Epoch [26/30], Batch [2900/6000], Loss: 0.0922
Epoch [26/30], Batch [3000/6000], Loss: 0.1280
Epoch [26/30], Batch [3100/6000], Loss: 0.0952
Epoch [26/30], Batch [3200/6000], Loss: 0.1590
Epoch [26/30], Batch [3300/6000], Loss: 0.0961
Epoch [26/30], Batch [3400/6000], Loss: 0.1504
Epoch [26/30], Batch [3500/6000], Loss: 0.1255
Epoch [26/30], Batch [3600/6000], Loss: 0.1018
Epoch [26/30], Batch [3700/6000], Loss: 0.7468
Epoch [26/30], Batch [3800/6000], Loss: 0.0810
Epoch [26/30], Batch [3900/6000], Loss: 0.8427
Epoch [26/30], Batch [4000/6000], Loss: 0.0980
Epoch [26/30], Batch [4100/6000], Loss: 0.1140
Epoch [26/30], Batch [4200/6000], Loss: 0.0956
Epoch [26/30], Batch [4300/6000], Loss: 0.1385
Epoch [26/30], Batch [4400/6000], Loss: 0.1222
Epoch [26/30], Batch [4500/6000], Loss: 0.1739
Epoch [26/30], Batch [4600/6000], Loss: 0.1699
Epoch [26/30], Batch [4700/6000], Loss: 0.2502
Epoch [26/30], Batch [4800/6000], Loss: 0.1158
Epoch [26/30], Batch [4900/6000], Loss: 0.1934
Epoch [26/30], Batch [5000/6000], Loss: 0.1361
Epoch [26/30], Batch [5100/6000], Loss: 0.1072
Epoch [26/30], Batch [5200/6000], Loss: 0.1210
Epoch [26/30], Batch [5300/6000], Loss: 0.1223
Epoch [26/30], Batch [5400/6000], Loss: 0.1264
Epoch [26/30], Batch [5500/6000], Loss: 0.1096
Epoch [26/30], Batch [5600/6000], Loss: 0.1648
Epoch [26/30], Batch [5700/6000], Loss: 0.1226
Epoch [26/30], Batch [5800/6000], Loss: 0.1436
Epoch [26/30], Batch [5900/6000], Loss: 0.1626
Epoch [26/30], Loss: 0.1824
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.1262
Epoch [27/30], Batch [100/6000], Loss: 0.1081
Epoch [27/30], Batch [200/6000], Loss: 0.2336
Epoch [27/30], Batch [300/6000], Loss: 0.1237
Epoch [27/30], Batch [400/6000], Loss: 0.1645
Epoch [27/30], Batch [500/6000], Loss: 0.1070
Epoch [27/30], Batch [600/6000], Loss: 0.1613
Epoch [27/30], Batch [700/6000], Loss: 0.1100
Epoch [27/30], Batch [800/6000], Loss: 0.1144
Epoch [27/30], Batch [900/6000], Loss: 0.1147
Epoch [27/30], Batch [1000/6000], Loss: 0.1348
Epoch [27/30], Batch [1100/6000], Loss: 0.1003
Epoch [27/30], Batch [1200/6000], Loss: 0.1242
Epoch [27/30], Batch [1300/6000], Loss: 0.0807
Epoch [27/30], Batch [1400/6000], Loss: 0.1190
Epoch [27/30], Batch [1500/6000], Loss: 0.1716
Epoch [27/30], Batch [1600/6000], Loss: 0.1243
Epoch [27/30], Batch [1700/6000], Loss: 0.1224
Epoch [27/30], Batch [1800/6000], Loss: 0.1124
Epoch [27/30], Batch [1900/6000], Loss: 0.1114
Epoch [27/30], Batch [2000/6000], Loss: 0.1410
Epoch [27/30], Batch [2100/6000], Loss: 0.1162
Epoch [27/30], Batch [2200/6000], Loss: 0.1173
Epoch [27/30], Batch [2300/6000], Loss: 0.2468
Epoch [27/30], Batch [2400/6000], Loss: 0.1079
Epoch [27/30], Batch [2500/6000], Loss: 0.1410
Epoch [27/30], Batch [2600/6000], Loss: 2.2035
Epoch [27/30], Batch [2700/6000], Loss: 0.1164
Epoch [27/30], Batch [2800/6000], Loss: 0.2001
Epoch [27/30], Batch [2900/6000], Loss: 0.1120
Epoch [27/30], Batch [3000/6000], Loss: 0.0901
Epoch [27/30], Batch [3100/6000], Loss: 0.1096
Epoch [27/30], Batch [3200/6000], Loss: 0.0812
Epoch [27/30], Batch [3300/6000], Loss: 0.1174
Epoch [27/30], Batch [3400/6000], Loss: 1.8286
Epoch [27/30], Batch [3500/6000], Loss: 0.1825
Epoch [27/30], Batch [3600/6000], Loss: 0.1333
Epoch [27/30], Batch [3700/6000], Loss: 0.1010
Epoch [27/30], Batch [3800/6000], Loss: 0.1248
Epoch [27/30], Batch [3900/6000], Loss: 0.1262
Epoch [27/30], Batch [4000/6000], Loss: 0.0990
Epoch [27/30], Batch [4100/6000], Loss: 0.1403
Epoch [27/30], Batch [4200/6000], Loss: 0.0930
Epoch [27/30], Batch [4300/6000], Loss: 0.6747
Epoch [27/30], Batch [4400/6000], Loss: 0.1364
Epoch [27/30], Batch [4500/6000], Loss: 0.1173
Epoch [27/30], Batch [4600/6000], Loss: 0.1169
Epoch [27/30], Batch [4700/6000], Loss: 0.0969
Epoch [27/30], Batch [4800/6000], Loss: 0.1568
Epoch [27/30], Batch [4900/6000], Loss: 0.1753
Epoch [27/30], Batch [5000/6000], Loss: 0.0993
Epoch [27/30], Batch [5100/6000], Loss: 0.1097
Epoch [27/30], Batch [5200/6000], Loss: 0.1367
Epoch [27/30], Batch [5300/6000], Loss: 0.1365
Epoch [27/30], Batch [5400/6000], Loss: 0.1140
Epoch [27/30], Batch [5500/6000], Loss: 0.1087
Epoch [27/30], Batch [5600/6000], Loss: 0.1183
Epoch [27/30], Batch [5700/6000], Loss: 0.1217
Epoch [27/30], Batch [5800/6000], Loss: 0.1263
Epoch [27/30], Batch [5900/6000], Loss: 0.1136
Epoch [27/30], Loss: 0.1777
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.1207
Epoch [28/30], Batch [100/6000], Loss: 0.1054
Epoch [28/30], Batch [200/6000], Loss: 0.1067
Epoch [28/30], Batch [300/6000], Loss: 0.0908
Epoch [28/30], Batch [400/6000], Loss: 0.1078
Epoch [28/30], Batch [500/6000], Loss: 0.0965
Epoch [28/30], Batch [600/6000], Loss: 0.1037
Epoch [28/30], Batch [700/6000], Loss: 0.2067
Epoch [28/30], Batch [800/6000], Loss: 0.1156
Epoch [28/30], Batch [900/6000], Loss: 0.1269
Epoch [28/30], Batch [1000/6000], Loss: 0.1264
Epoch [28/30], Batch [1100/6000], Loss: 0.0931
Epoch [28/30], Batch [1200/6000], Loss: 0.0835
Epoch [28/30], Batch [1300/6000], Loss: 0.2218
Epoch [28/30], Batch [1400/6000], Loss: 0.1085
Epoch [28/30], Batch [1500/6000], Loss: 0.1095
Epoch [28/30], Batch [1600/6000], Loss: 0.0940
Epoch [28/30], Batch [1700/6000], Loss: 0.1522
Epoch [28/30], Batch [1800/6000], Loss: 0.1180
Epoch [28/30], Batch [1900/6000], Loss: 0.0981
Epoch [28/30], Batch [2000/6000], Loss: 0.1936
Epoch [28/30], Batch [2100/6000], Loss: 0.1033
Epoch [28/30], Batch [2200/6000], Loss: 0.1069
Epoch [28/30], Batch [2300/6000], Loss: 0.1085
Epoch [28/30], Batch [2400/6000], Loss: 0.0942
Epoch [28/30], Batch [2500/6000], Loss: 0.3844
Epoch [28/30], Batch [2600/6000], Loss: 0.1330
Epoch [28/30], Batch [2700/6000], Loss: 0.1236
Epoch [28/30], Batch [2800/6000], Loss: 0.0932
Epoch [28/30], Batch [2900/6000], Loss: 0.1249
Epoch [28/30], Batch [3000/6000], Loss: 0.1249
Epoch [28/30], Batch [3100/6000], Loss: 0.1086
Epoch [28/30], Batch [3200/6000], Loss: 0.1122
Epoch [28/30], Batch [3300/6000], Loss: 0.0811
Epoch [28/30], Batch [3400/6000], Loss: 0.1119
Epoch [28/30], Batch [3500/6000], Loss: 0.1298
Epoch [28/30], Batch [3600/6000], Loss: 0.0808
Epoch [28/30], Batch [3700/6000], Loss: 0.1095
Epoch [28/30], Batch [3800/6000], Loss: 0.1041
Epoch [28/30], Batch [3900/6000], Loss: 0.1088
Epoch [28/30], Batch [4000/6000], Loss: 0.1032
Epoch [28/30], Batch [4100/6000], Loss: 0.1064
Epoch [28/30], Batch [4200/6000], Loss: 0.2606
Epoch [28/30], Batch [4300/6000], Loss: 0.1145
Epoch [28/30], Batch [4400/6000], Loss: 0.1288
Epoch [28/30], Batch [4500/6000], Loss: 0.1237
Epoch [28/30], Batch [4600/6000], Loss: 0.1047
Epoch [28/30], Batch [4700/6000], Loss: 0.1321
Epoch [28/30], Batch [4800/6000], Loss: 2.4158
Epoch [28/30], Batch [4900/6000], Loss: 0.1054
Epoch [28/30], Batch [5000/6000], Loss: 0.1222
Epoch [28/30], Batch [5100/6000], Loss: 0.1288
Epoch [28/30], Batch [5200/6000], Loss: 0.1108
Epoch [28/30], Batch [5300/6000], Loss: 0.1641
Epoch [28/30], Batch [5400/6000], Loss: 0.1054
Epoch [28/30], Batch [5500/6000], Loss: 0.1059
Epoch [28/30], Batch [5600/6000], Loss: 0.1718
Epoch [28/30], Batch [5700/6000], Loss: 0.1237
Epoch [28/30], Batch [5800/6000], Loss: 0.1068
Epoch [28/30], Batch [5900/6000], Loss: 0.1012
Epoch [28/30], Loss: 0.1704
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0967
Epoch [29/30], Batch [100/6000], Loss: 0.9915
Epoch [29/30], Batch [200/6000], Loss: 0.1077
Epoch [29/30], Batch [300/6000], Loss: 0.1195
Epoch [29/30], Batch [400/6000], Loss: 0.1270
Epoch [29/30], Batch [500/6000], Loss: 0.1060
Epoch [29/30], Batch [600/6000], Loss: 0.0926
Epoch [29/30], Batch [700/6000], Loss: 0.1344
Epoch [29/30], Batch [800/6000], Loss: 0.0922
Epoch [29/30], Batch [900/6000], Loss: 0.1169
Epoch [29/30], Batch [1000/6000], Loss: 0.0787
Epoch [29/30], Batch [1100/6000], Loss: 0.1182
Epoch [29/30], Batch [1200/6000], Loss: 0.1368
Epoch [29/30], Batch [1300/6000], Loss: 0.1257
Epoch [29/30], Batch [1400/6000], Loss: 1.1542
Epoch [29/30], Batch [1500/6000], Loss: 0.0997
Epoch [29/30], Batch [1600/6000], Loss: 0.1043
Epoch [29/30], Batch [1700/6000], Loss: 0.1016
Epoch [29/30], Batch [1800/6000], Loss: 0.1290
Epoch [29/30], Batch [1900/6000], Loss: 0.1312
Epoch [29/30], Batch [2000/6000], Loss: 0.0731
Epoch [29/30], Batch [2100/6000], Loss: 0.1741
Epoch [29/30], Batch [2200/6000], Loss: 0.2883
Epoch [29/30], Batch [2300/6000], Loss: 0.1089
Epoch [29/30], Batch [2400/6000], Loss: 0.1090
Epoch [29/30], Batch [2500/6000], Loss: 0.0921
Epoch [29/30], Batch [2600/6000], Loss: 0.0871
Epoch [29/30], Batch [2700/6000], Loss: 0.2102
Epoch [29/30], Batch [2800/6000], Loss: 0.1241
Epoch [29/30], Batch [2900/6000], Loss: 0.1075
Epoch [29/30], Batch [3000/6000], Loss: 0.1052
Epoch [29/30], Batch [3100/6000], Loss: 0.1777
Epoch [29/30], Batch [3200/6000], Loss: 0.1082
Epoch [29/30], Batch [3300/6000], Loss: 0.1137
Epoch [29/30], Batch [3400/6000], Loss: 0.1329
Epoch [29/30], Batch [3500/6000], Loss: 0.1236
Epoch [29/30], Batch [3600/6000], Loss: 0.1205
Epoch [29/30], Batch [3700/6000], Loss: 0.0811
Epoch [29/30], Batch [3800/6000], Loss: 0.1048
Epoch [29/30], Batch [3900/6000], Loss: 0.1370
Epoch [29/30], Batch [4000/6000], Loss: 0.0907
Epoch [29/30], Batch [4100/6000], Loss: 0.1183
Epoch [29/30], Batch [4200/6000], Loss: 0.1206
Epoch [29/30], Batch [4300/6000], Loss: 0.1213
Epoch [29/30], Batch [4400/6000], Loss: 0.1438
Epoch [29/30], Batch [4500/6000], Loss: 0.0820
Epoch [29/30], Batch [4600/6000], Loss: 0.1480
Epoch [29/30], Batch [4700/6000], Loss: 1.1762
Epoch [29/30], Batch [4800/6000], Loss: 0.0926
Epoch [29/30], Batch [4900/6000], Loss: 0.1232
Epoch [29/30], Batch [5000/6000], Loss: 1.9859
Epoch [29/30], Batch [5100/6000], Loss: 0.1278
Epoch [29/30], Batch [5200/6000], Loss: 0.3341
Epoch [29/30], Batch [5300/6000], Loss: 0.1351
Epoch [29/30], Batch [5400/6000], Loss: 0.4086
Epoch [29/30], Batch [5500/6000], Loss: 0.1739
Epoch [29/30], Batch [5600/6000], Loss: 0.0883
Epoch [29/30], Batch [5700/6000], Loss: 0.1332
Epoch [29/30], Batch [5800/6000], Loss: 0.4304
Epoch [29/30], Batch [5900/6000], Loss: 0.1486
Epoch [29/30], Loss: 0.1689
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0960
Epoch [30/30], Batch [100/6000], Loss: 0.0982
Epoch [30/30], Batch [200/6000], Loss: 0.0997
Epoch [30/30], Batch [300/6000], Loss: 0.1070
Epoch [30/30], Batch [400/6000], Loss: 0.0971
Epoch [30/30], Batch [500/6000], Loss: 0.6087
Epoch [30/30], Batch [600/6000], Loss: 0.1105
Epoch [30/30], Batch [700/6000], Loss: 0.0888
Epoch [30/30], Batch [800/6000], Loss: 0.2298
Epoch [30/30], Batch [900/6000], Loss: 0.1081
Epoch [30/30], Batch [1000/6000], Loss: 0.0942
Epoch [30/30], Batch [1100/6000], Loss: 0.3052
Epoch [30/30], Batch [1200/6000], Loss: 0.0874
Epoch [30/30], Batch [1300/6000], Loss: 0.1116
Epoch [30/30], Batch [1400/6000], Loss: 0.1056
Epoch [30/30], Batch [1500/6000], Loss: 0.1275
Epoch [30/30], Batch [1600/6000], Loss: 0.1392
Epoch [30/30], Batch [1700/6000], Loss: 0.1360
Epoch [30/30], Batch [1800/6000], Loss: 0.4356
Epoch [30/30], Batch [1900/6000], Loss: 0.1559
Epoch [30/30], Batch [2000/6000], Loss: 0.1384
Epoch [30/30], Batch [2100/6000], Loss: 0.1237
Epoch [30/30], Batch [2200/6000], Loss: 0.0947
Epoch [30/30], Batch [2300/6000], Loss: 0.0879
Epoch [30/30], Batch [2400/6000], Loss: 0.1008
Epoch [30/30], Batch [2500/6000], Loss: 0.1111
Epoch [30/30], Batch [2600/6000], Loss: 0.1270
Epoch [30/30], Batch [2700/6000], Loss: 0.1670
Epoch [30/30], Batch [2800/6000], Loss: 0.1655
Epoch [30/30], Batch [2900/6000], Loss: 0.0901
Epoch [30/30], Batch [3000/6000], Loss: 0.1068
Epoch [30/30], Batch [3100/6000], Loss: 0.4190
Epoch [30/30], Batch [3200/6000], Loss: 0.0982
Epoch [30/30], Batch [3300/6000], Loss: 0.1132
Epoch [30/30], Batch [3400/6000], Loss: 0.1485
Epoch [30/30], Batch [3500/6000], Loss: 0.1125
Epoch [30/30], Batch [3600/6000], Loss: 0.1050
Epoch [30/30], Batch [3700/6000], Loss: 0.0826
Epoch [30/30], Batch [3800/6000], Loss: 0.1246
Epoch [30/30], Batch [3900/6000], Loss: 0.1462
Epoch [30/30], Batch [4000/6000], Loss: 0.1545
Epoch [30/30], Batch [4100/6000], Loss: 0.1618
Epoch [30/30], Batch [4200/6000], Loss: 0.1270
Epoch [30/30], Batch [4300/6000], Loss: 0.1062
Epoch [30/30], Batch [4400/6000], Loss: 0.1233
Epoch [30/30], Batch [4500/6000], Loss: 0.1288
Epoch [30/30], Batch [4600/6000], Loss: 0.1218
Epoch [30/30], Batch [4700/6000], Loss: 0.1325
Epoch [30/30], Batch [4800/6000], Loss: 0.2226
Epoch [30/30], Batch [4900/6000], Loss: 0.1294
Epoch [30/30], Batch [5000/6000], Loss: 0.1269
Epoch [30/30], Batch [5100/6000], Loss: 0.1373
Epoch [30/30], Batch [5200/6000], Loss: 0.0932
Epoch [30/30], Batch [5300/6000], Loss: 0.0996
Epoch [30/30], Batch [5400/6000], Loss: 0.1131
Epoch [30/30], Batch [5500/6000], Loss: 0.1093
Epoch [30/30], Batch [5600/6000], Loss: 0.1971
Epoch [30/30], Batch [5700/6000], Loss: 0.1011
Epoch [30/30], Batch [5800/6000], Loss: 0.1115
Epoch [30/30], Batch [5900/6000], Loss: 0.4370
Epoch [30/30], Loss: 0.1629
Visualization saved to figures/visualization_0.png
Test Loss: 0.1169, Accuracy: 98.17%
Visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 0.6099
  Image Loss: 0.0377
  Total Loss: 6.1368
  Image grad max: 0.533020555973053
  Output probs: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 2/300:
  Label Loss: 0.5739
  Image Loss: 0.0373
  Total Loss: 5.7763
  Image grad max: 0.5648632049560547
  Output probs: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 3/300:
  Label Loss: 0.5404
  Image Loss: 0.0369
  Total Loss: 5.4407
  Image grad max: 0.5614980459213257
  Output probs: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 4/300:
  Label Loss: 0.5070
  Image Loss: 0.0365
  Total Loss: 5.1063
  Image grad max: 0.5708469152450562
  Output probs: [[0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 5/300:
  Label Loss: 0.4726
  Image Loss: 0.0363
  Total Loss: 4.7620
  Image grad max: 0.5845218300819397
  Output probs: [[0.    0.    0.001 0.    0.999 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 6/300:
  Label Loss: 0.4371
  Image Loss: 0.0361
  Total Loss: 4.4075
  Image grad max: 0.6030079126358032
  Output probs: [[0.    0.    0.003 0.    0.997 0.    0.    0.    0.    0.001]]
Adversarial Training Loop 7/300:
  Label Loss: 0.4005
  Image Loss: 0.0361
  Total Loss: 4.0416
  Image grad max: 0.617250919342041
  Output probs: [[0.    0.    0.009 0.    0.989 0.    0.    0.    0.    0.001]]
Adversarial Training Loop 8/300:
  Label Loss: 0.3630
  Image Loss: 0.0362
  Total Loss: 3.6660
  Image grad max: 0.6149802207946777
  Output probs: [[0.    0.    0.03  0.    0.964 0.    0.    0.    0.002 0.003]]
Adversarial Training Loop 9/300:
  Label Loss: 0.3261
  Image Loss: 0.0363
  Total Loss: 3.2969
  Image grad max: 0.563664436340332
  Output probs: [[0.    0.    0.092 0.    0.893 0.    0.001 0.001 0.008 0.005]]
Adversarial Training Loop 10/300:
  Label Loss: 0.2943
  Image Loss: 0.0365
  Total Loss: 2.9790
  Image grad max: 0.42239952087402344
  Output probs: [[0.    0.    0.209 0.    0.754 0.001 0.001 0.001 0.025 0.009]]
Adversarial Training Loop 11/300:
  Label Loss: 0.2754
  Image Loss: 0.0367
  Total Loss: 2.7906
  Image grad max: 0.2337101846933365
  Output probs: [[0.001 0.001 0.333 0.001 0.596 0.001 0.002 0.002 0.052 0.011]]
Adversarial Training Loop 12/300:
  Label Loss: 0.2690
  Image Loss: 0.0369
  Total Loss: 2.7273
  Image grad max: 0.17376890778541565
  Output probs: [[0.002 0.001 0.415 0.002 0.479 0.002 0.002 0.002 0.082 0.013]]
Adversarial Training Loop 13/300:
  Label Loss: 0.2677
  Image Loss: 0.0372
  Total Loss: 2.7145
  Image grad max: 0.20960380136966705
  Output probs: [[0.002 0.001 0.451 0.002 0.415 0.002 0.003 0.002 0.107 0.014]]
Adversarial Training Loop 14/300:
  Label Loss: 0.2653
  Image Loss: 0.0374
  Total Loss: 2.6908
  Image grad max: 0.24222077429294586
  Output probs: [[0.003 0.001 0.456 0.003 0.39  0.003 0.004 0.002 0.123 0.015]]
Adversarial Training Loop 15/300:
  Label Loss: 0.2598
  Image Loss: 0.0376
  Total Loss: 2.6354
  Image grad max: 0.2543225884437561
  Output probs: [[0.004 0.001 0.444 0.003 0.393 0.003 0.004 0.002 0.13  0.015]]
Adversarial Training Loop 16/300:
  Label Loss: 0.2513
  Image Loss: 0.0377
  Total Loss: 2.5509
  Image grad max: 0.2565118670463562
  Output probs: [[0.005 0.002 0.421 0.003 0.414 0.003 0.005 0.002 0.129 0.016]]
Adversarial Training Loop 17/300:
  Label Loss: 0.2409
  Image Loss: 0.0379
  Total Loss: 2.4466
  Image grad max: 0.24947777390480042
  Output probs: [[0.006 0.002 0.389 0.003 0.45  0.003 0.006 0.003 0.123 0.017]]
Adversarial Training Loop 18/300:
  Label Loss: 0.2296
  Image Loss: 0.0380
  Total Loss: 2.3336
  Image grad max: 0.23526069521903992
  Output probs: [[0.006 0.002 0.351 0.003 0.495 0.003 0.006 0.003 0.113 0.017]]
Adversarial Training Loop 19/300:
  Label Loss: 0.2181
  Image Loss: 0.0382
  Total Loss: 2.2191
  Image grad max: 0.21471020579338074
  Output probs: [[0.007 0.002 0.309 0.002 0.548 0.004 0.007 0.003 0.101 0.018]]
Adversarial Training Loop 20/300:
  Label Loss: 0.2070
  Image Loss: 0.0383
  Total Loss: 2.1084
  Image grad max: 0.1890602707862854
  Output probs: [[0.007 0.002 0.265 0.002 0.604 0.003 0.008 0.003 0.088 0.018]]
Adversarial Training Loop 21/300:
  Label Loss: 0.1967
  Image Loss: 0.0385
  Total Loss: 2.0050
  Image grad max: 0.15951037406921387
  Output probs: [[0.007 0.002 0.221 0.002 0.66  0.003 0.009 0.002 0.075 0.018]]
Adversarial Training Loop 22/300:
  Label Loss: 0.1870
  Image Loss: 0.0387
  Total Loss: 1.9089
  Image grad max: 0.15008701384067535
  Output probs: [[0.007 0.002 0.182 0.001 0.711 0.003 0.01  0.002 0.064 0.017]]
Adversarial Training Loop 23/300:
  Label Loss: 0.1781
  Image Loss: 0.0389
  Total Loss: 1.8200
  Image grad max: 0.17355595529079437
  Output probs: [[0.008 0.002 0.148 0.001 0.753 0.003 0.011 0.002 0.055 0.017]]
Adversarial Training Loop 24/300:
  Label Loss: 0.1695
  Image Loss: 0.0392
  Total Loss: 1.7344
  Image grad max: 0.19856637716293335
  Output probs: [[0.008 0.002 0.119 0.001 0.787 0.003 0.013 0.002 0.048 0.016]]
Adversarial Training Loop 25/300:
  Label Loss: 0.1606
  Image Loss: 0.0395
  Total Loss: 1.6452
  Image grad max: 0.22760529816150665
  Output probs: [[0.01  0.002 0.098 0.001 0.809 0.003 0.015 0.002 0.044 0.016]]
Adversarial Training Loop 26/300:
  Label Loss: 0.1506
  Image Loss: 0.0398
  Total Loss: 1.5461
  Image grad max: 0.25671231746673584
  Output probs: [[0.012 0.002 0.084 0.001 0.818 0.003 0.019 0.002 0.043 0.016]]
Adversarial Training Loop 27/300:
  Label Loss: 0.1394
  Image Loss: 0.0402
  Total Loss: 1.4337
  Image grad max: 0.28798460960388184
  Output probs: [[0.017 0.002 0.077 0.001 0.812 0.004 0.025 0.002 0.045 0.016]]
Adversarial Training Loop 28/300:
  Label Loss: 0.1263
  Image Loss: 0.0406
  Total Loss: 1.3034
  Image grad max: 0.30370140075683594
  Output probs: [[0.026 0.002 0.073 0.001 0.791 0.004 0.034 0.002 0.051 0.017]]
Adversarial Training Loop 29/300:
  Label Loss: 0.1117
  Image Loss: 0.0410
  Total Loss: 1.1582
  Image grad max: 0.305763840675354
  Output probs: [[0.04  0.002 0.071 0.001 0.754 0.005 0.048 0.001 0.059 0.018]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0966
  Image Loss: 0.0415
  Total Loss: 1.0074
  Image grad max: 0.29423704743385315
  Output probs: [[0.065 0.003 0.071 0.001 0.696 0.006 0.071 0.001 0.069 0.018]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0814
  Image Loss: 0.0421
  Total Loss: 0.8559
  Image grad max: 0.2883134186267853
  Output probs: [[0.106 0.003 0.071 0.001 0.609 0.006 0.107 0.001 0.08  0.016]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0672
  Image Loss: 0.0427
  Total Loss: 0.7145
  Image grad max: 0.3179514408111572
  Output probs: [[0.165 0.002 0.066 0.002 0.49  0.007 0.165 0.001 0.088 0.014]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0563
  Image Loss: 0.0433
  Total Loss: 0.6065
  Image grad max: 0.29767829179763794
  Output probs: [[0.226 0.002 0.055 0.001 0.364 0.006 0.248 0.001 0.087 0.01 ]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0510
  Image Loss: 0.0439
  Total Loss: 0.5536
  Image grad max: 0.3581116795539856
  Output probs: [[0.253 0.001 0.041 0.001 0.27  0.005 0.345 0.    0.076 0.006]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0493
  Image Loss: 0.0445
  Total Loss: 0.5378
  Image grad max: 0.4315507709980011
  Output probs: [[0.233 0.001 0.029 0.001 0.22  0.004 0.446 0.    0.062 0.004]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0468
  Image Loss: 0.0451
  Total Loss: 0.5132
  Image grad max: 0.4412078857421875
  Output probs: [[0.185 0.001 0.021 0.    0.208 0.003 0.529 0.    0.05  0.003]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0411
  Image Loss: 0.0455
  Total Loss: 0.4562
  Image grad max: 0.4455905258655548
  Output probs: [[0.134 0.001 0.016 0.    0.239 0.003 0.564 0.    0.041 0.002]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0308
  Image Loss: 0.0458
  Total Loss: 0.3540
  Image grad max: 0.4093269407749176
  Output probs: [[0.089 0.001 0.013 0.    0.327 0.002 0.531 0.    0.035 0.002]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0182
  Image Loss: 0.0460
  Total Loss: 0.2283
  Image grad max: 0.26101285219192505
  Output probs: [[0.054 0.001 0.01  0.    0.463 0.002 0.44  0.    0.028 0.002]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0103
  Image Loss: 0.0462
  Total Loss: 0.1490
  Image grad max: 0.08582496643066406
  Output probs: [[0.031 0.001 0.008 0.    0.59  0.002 0.346 0.    0.021 0.002]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0102
  Image Loss: 0.0463
  Total Loss: 0.1481
  Image grad max: 0.25488585233688354
  Output probs: [[0.019 0.001 0.006 0.    0.652 0.002 0.302 0.    0.016 0.002]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0118
  Image Loss: 0.0465
  Total Loss: 0.1649
  Image grad max: 0.3433438837528229
  Output probs: [[0.014 0.001 0.004 0.    0.638 0.001 0.327 0.    0.013 0.001]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0091
  Image Loss: 0.0468
  Total Loss: 0.1381
  Image grad max: 0.33293360471725464
  Output probs: [[0.012 0.001 0.003 0.    0.544 0.001 0.425 0.    0.013 0.001]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0039
  Image Loss: 0.0472
  Total Loss: 0.0861
  Image grad max: 0.15524239838123322
  Output probs: [[0.01  0.    0.002 0.    0.414 0.001 0.561 0.    0.011 0.001]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0038
  Image Loss: 0.0475
  Total Loss: 0.0853
  Image grad max: 0.1693357229232788
  Output probs: [[0.008 0.    0.002 0.    0.334 0.001 0.644 0.    0.01  0.   ]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0074
  Image Loss: 0.0478
  Total Loss: 0.1221
  Image grad max: 0.4002472758293152
  Output probs: [[0.006 0.    0.002 0.    0.356 0.001 0.626 0.    0.009 0.   ]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0058
  Image Loss: 0.0480
  Total Loss: 0.1062
  Image grad max: 0.3506063222885132
  Output probs: [[0.005 0.    0.002 0.    0.462 0.001 0.521 0.    0.008 0.   ]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0019
  Image Loss: 0.0480
  Total Loss: 0.0667
  Image grad max: 0.07106205075979233
  Output probs: [[0.004 0.    0.002 0.    0.579 0.001 0.407 0.    0.007 0.001]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0030
  Image Loss: 0.0480
  Total Loss: 0.0781
  Image grad max: 0.21912840008735657
  Output probs: [[0.003 0.    0.001 0.    0.627 0.001 0.361 0.    0.006 0.001]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0051
  Image Loss: 0.0481
  Total Loss: 0.0987
  Image grad max: 0.32773756980895996
  Output probs: [[0.003 0.    0.001 0.    0.593 0.001 0.395 0.    0.006 0.   ]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0033
  Image Loss: 0.0482
  Total Loss: 0.0810
  Image grad max: 0.25219056010246277
  Output probs: [[0.003 0.    0.001 0.    0.5   0.001 0.488 0.    0.006 0.   ]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0012
  Image Loss: 0.0484
  Total Loss: 0.0602
  Image grad max: 0.022352974861860275
  Output probs: [[0.003 0.    0.001 0.    0.411 0.001 0.579 0.    0.006 0.   ]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0026
  Image Loss: 0.0486
  Total Loss: 0.0742
  Image grad max: 0.2263847291469574
  Output probs: [[0.003 0.    0.001 0.    0.388 0.001 0.602 0.    0.006 0.   ]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0034
  Image Loss: 0.0487
  Total Loss: 0.0828
  Image grad max: 0.2941635847091675
  Output probs: [[0.002 0.    0.001 0.    0.444 0.001 0.546 0.    0.006 0.   ]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0015
  Image Loss: 0.0487
  Total Loss: 0.0640
  Image grad max: 0.13656564056873322
  Output probs: [[0.002 0.    0.001 0.    0.532 0.001 0.458 0.    0.005 0.   ]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0012
  Image Loss: 0.0487
  Total Loss: 0.0607
  Image grad max: 0.10224054008722305
  Output probs: [[0.002 0.    0.001 0.    0.584 0.001 0.407 0.    0.005 0.   ]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0025
  Image Loss: 0.0487
  Total Loss: 0.0735
  Image grad max: 0.23384034633636475
  Output probs: [[0.002 0.    0.001 0.    0.571 0.001 0.421 0.    0.005 0.   ]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0020
  Image Loss: 0.0487
  Total Loss: 0.0686
  Image grad max: 0.2004619836807251
  Output probs: [[0.002 0.    0.001 0.    0.506 0.    0.485 0.    0.005 0.   ]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0009
  Image Loss: 0.0488
  Total Loss: 0.0573
  Image grad max: 0.03224944323301315
  Output probs: [[0.002 0.    0.001 0.    0.441 0.    0.551 0.    0.005 0.   ]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0014
  Image Loss: 0.0489
  Total Loss: 0.0632
  Image grad max: 0.15146993100643158
  Output probs: [[0.002 0.    0.001 0.    0.424 0.    0.568 0.    0.005 0.   ]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0018
  Image Loss: 0.0489
  Total Loss: 0.0673
  Image grad max: 0.20022355020046234
  Output probs: [[0.002 0.    0.001 0.    0.465 0.    0.527 0.    0.004 0.   ]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0010
  Image Loss: 0.0489
  Total Loss: 0.0585
  Image grad max: 0.08344142884016037
  Output probs: [[0.002 0.    0.001 0.    0.527 0.    0.465 0.    0.004 0.   ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0009
  Image Loss: 0.0488
  Total Loss: 0.0581
  Image grad max: 0.08745943754911423
  Output probs: [[0.001 0.    0.001 0.    0.558 0.    0.435 0.    0.004 0.   ]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0015
  Image Loss: 0.0488
  Total Loss: 0.0637
  Image grad max: 0.16961544752120972
  Output probs: [[0.001 0.    0.001 0.    0.54  0.    0.453 0.    0.004 0.   ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0011
  Image Loss: 0.0488
  Total Loss: 0.0595
  Image grad max: 0.12030009180307388
  Output probs: [[0.001 0.    0.001 0.    0.489 0.    0.504 0.    0.004 0.   ]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0007
  Image Loss: 0.0488
  Total Loss: 0.0559
  Image grad max: 0.020056793466210365
  Output probs: [[0.001 0.    0.001 0.    0.451 0.    0.543 0.    0.004 0.   ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0011
  Image Loss: 0.0489
  Total Loss: 0.0600
  Image grad max: 0.12812407314777374
  Output probs: [[0.001 0.    0.001 0.    0.454 0.    0.539 0.    0.004 0.   ]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0010
  Image Loss: 0.0488
  Total Loss: 0.0592
  Image grad max: 0.11827433109283447
  Output probs: [[0.001 0.    0.001 0.    0.494 0.    0.499 0.    0.004 0.   ]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0007
  Image Loss: 0.0488
  Total Loss: 0.0553
  Image grad max: 0.0079965153709054
  Output probs: [[0.001 0.    0.001 0.    0.532 0.    0.461 0.    0.004 0.   ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0009
  Image Loss: 0.0487
  Total Loss: 0.0576
  Image grad max: 0.10077402740716934
  Output probs: [[0.001 0.    0.001 0.    0.537 0.    0.457 0.    0.004 0.   ]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0010
  Image Loss: 0.0486
  Total Loss: 0.0582
  Image grad max: 0.11333804577589035
  Output probs: [[0.001 0.    0.001 0.    0.508 0.    0.486 0.    0.004 0.   ]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0006
  Image Loss: 0.0486
  Total Loss: 0.0551
  Image grad max: 0.03329337015748024
  Output probs: [[0.001 0.    0.    0.    0.473 0.    0.521 0.    0.004 0.   ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0007
  Image Loss: 0.0486
  Total Loss: 0.0560
  Image grad max: 0.0660598948597908
  Output probs: [[0.001 0.    0.    0.    0.463 0.    0.531 0.    0.004 0.   ]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0008
  Image Loss: 0.0486
  Total Loss: 0.0570
  Image grad max: 0.09421470761299133
  Output probs: [[0.001 0.    0.    0.    0.485 0.    0.509 0.    0.004 0.   ]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0006
  Image Loss: 0.0485
  Total Loss: 0.0548
  Image grad max: 0.03143144026398659
  Output probs: [[0.001 0.    0.    0.    0.516 0.    0.478 0.    0.004 0.   ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0007
  Image Loss: 0.0485
  Total Loss: 0.0551
  Image grad max: 0.0561990886926651
  Output probs: [[0.001 0.    0.    0.    0.527 0.    0.467 0.    0.003 0.   ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0008
  Image Loss: 0.0484
  Total Loss: 0.0560
  Image grad max: 0.08582279086112976
  Output probs: [[0.001 0.    0.    0.    0.51  0.    0.484 0.    0.003 0.   ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0006
  Image Loss: 0.0484
  Total Loss: 0.0545
  Image grad max: 0.038694750517606735
  Output probs: [[0.001 0.    0.    0.    0.484 0.    0.511 0.    0.003 0.   ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0006
  Image Loss: 0.0483
  Total Loss: 0.0544
  Image grad max: 0.03636008873581886
  Output probs: [[0.001 0.    0.    0.    0.473 0.    0.521 0.    0.003 0.   ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0007
  Image Loss: 0.0483
  Total Loss: 0.0551
  Image grad max: 0.0670272558927536
  Output probs: [[0.001 0.    0.    0.    0.487 0.    0.507 0.    0.003 0.   ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0006
  Image Loss: 0.0482
  Total Loss: 0.0540
  Image grad max: 0.027475809678435326
  Output probs: [[0.001 0.    0.    0.    0.51  0.    0.485 0.    0.003 0.   ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0006
  Image Loss: 0.0481
  Total Loss: 0.0540
  Image grad max: 0.038061901926994324
  Output probs: [[0.001 0.    0.    0.    0.519 0.    0.476 0.    0.003 0.   ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0006
  Image Loss: 0.0481
  Total Loss: 0.0544
  Image grad max: 0.06273318082094193
  Output probs: [[0.001 0.    0.    0.    0.506 0.    0.488 0.    0.003 0.   ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0006
  Image Loss: 0.0480
  Total Loss: 0.0536
  Image grad max: 0.02786790020763874
  Output probs: [[0.001 0.    0.    0.    0.487 0.    0.507 0.    0.003 0.   ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0006
  Image Loss: 0.0480
  Total Loss: 0.0535
  Image grad max: 0.02732795849442482
  Output probs: [[0.001 0.    0.    0.    0.481 0.    0.514 0.    0.003 0.   ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0006
  Image Loss: 0.0479
  Total Loss: 0.0538
  Image grad max: 0.04693126305937767
  Output probs: [[0.001 0.    0.    0.    0.492 0.    0.503 0.    0.003 0.   ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0005
  Image Loss: 0.0478
  Total Loss: 0.0532
  Image grad max: 0.014744237996637821
  Output probs: [[0.001 0.    0.    0.    0.508 0.    0.486 0.    0.003 0.   ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0005
  Image Loss: 0.0478
  Total Loss: 0.0532
  Image grad max: 0.0333762988448143
  Output probs: [[0.001 0.    0.    0.    0.512 0.    0.482 0.    0.003 0.   ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0006
  Image Loss: 0.0477
  Total Loss: 0.0533
  Image grad max: 0.044914163649082184
  Output probs: [[0.001 0.    0.    0.    0.501 0.    0.494 0.    0.003 0.   ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0005
  Image Loss: 0.0476
  Total Loss: 0.0528
  Image grad max: 0.012964447028934956
  Output probs: [[0.001 0.    0.    0.    0.488 0.    0.507 0.    0.003 0.   ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0005
  Image Loss: 0.0476
  Total Loss: 0.0528
  Image grad max: 0.025777874514460564
  Output probs: [[0.001 0.    0.    0.    0.486 0.    0.509 0.    0.003 0.   ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0005
  Image Loss: 0.0475
  Total Loss: 0.0528
  Image grad max: 0.03056083433330059
  Output probs: [[0.001 0.    0.    0.    0.497 0.    0.498 0.    0.003 0.   ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0005
  Image Loss: 0.0474
  Total Loss: 0.0524
  Image grad max: 0.0031037982553243637
  Output probs: [[0.001 0.    0.    0.    0.508 0.    0.487 0.    0.003 0.   ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0005
  Image Loss: 0.0473
  Total Loss: 0.0525
  Image grad max: 0.03169434890151024
  Output probs: [[0.001 0.    0.    0.    0.507 0.    0.488 0.    0.003 0.   ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0005
  Image Loss: 0.0473
  Total Loss: 0.0523
  Image grad max: 0.028717070817947388
  Output probs: [[0.001 0.    0.    0.    0.497 0.    0.499 0.    0.003 0.   ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0005
  Image Loss: 0.0472
  Total Loss: 0.0521
  Image grad max: 0.004245679825544357
  Output probs: [[0.001 0.    0.    0.    0.489 0.    0.506 0.    0.003 0.   ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0005
  Image Loss: 0.0471
  Total Loss: 0.0521
  Image grad max: 0.02359556034207344
  Output probs: [[0.001 0.    0.    0.    0.492 0.    0.503 0.    0.003 0.   ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0005
  Image Loss: 0.0471
  Total Loss: 0.0519
  Image grad max: 0.014881751500070095
  Output probs: [[0.001 0.    0.    0.    0.502 0.    0.494 0.    0.003 0.   ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0005
  Image Loss: 0.0470
  Total Loss: 0.0517
  Image grad max: 0.013334715738892555
  Output probs: [[0.001 0.    0.    0.    0.506 0.    0.489 0.    0.003 0.   ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0005
  Image Loss: 0.0469
  Total Loss: 0.0517
  Image grad max: 0.02689574845135212
  Output probs: [[0.001 0.    0.    0.    0.501 0.    0.494 0.    0.003 0.   ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0005
  Image Loss: 0.0468
  Total Loss: 0.0515
  Image grad max: 0.012626297771930695
  Output probs: [[0.001 0.    0.    0.    0.493 0.    0.502 0.    0.003 0.   ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0005
  Image Loss: 0.0468
  Total Loss: 0.0514
  Image grad max: 0.011707031168043613
  Output probs: [[0.001 0.    0.    0.    0.492 0.    0.504 0.    0.003 0.   ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0005
  Image Loss: 0.0467
  Total Loss: 0.0513
  Image grad max: 0.01650676317512989
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.498 0.    0.003 0.   ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0005
  Image Loss: 0.0466
  Total Loss: 0.0511
  Image grad max: 0.002941995393484831
  Output probs: [[0.001 0.    0.    0.    0.504 0.    0.492 0.    0.003 0.   ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0005
  Image Loss: 0.0465
  Total Loss: 0.0511
  Image grad max: 0.01910579577088356
  Output probs: [[0.001 0.    0.    0.    0.503 0.    0.493 0.    0.003 0.   ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0004
  Image Loss: 0.0465
  Total Loss: 0.0509
  Image grad max: 0.016707615926861763
  Output probs: [[0.001 0.    0.    0.    0.497 0.    0.499 0.    0.003 0.   ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0004
  Image Loss: 0.0464
  Total Loss: 0.0508
  Image grad max: 0.004108855966478586
  Output probs: [[0.001 0.    0.    0.    0.493 0.    0.503 0.    0.003 0.   ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0004
  Image Loss: 0.0463
  Total Loss: 0.0507
  Image grad max: 0.013029665686190128
  Output probs: [[0.001 0.    0.    0.    0.496 0.    0.5   0.    0.003 0.   ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0004
  Image Loss: 0.0462
  Total Loss: 0.0506
  Image grad max: 0.006016078405082226
  Output probs: [[0.001 0.    0.    0.    0.501 0.    0.494 0.    0.003 0.   ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0004
  Image Loss: 0.0462
  Total Loss: 0.0505
  Image grad max: 0.012058954685926437
  Output probs: [[0.001 0.    0.    0.    0.503 0.    0.493 0.    0.003 0.   ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0004
  Image Loss: 0.0461
  Total Loss: 0.0504
  Image grad max: 0.015834972262382507
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.003 0.   ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0004
  Image Loss: 0.0460
  Total Loss: 0.0502
  Image grad max: 0.0036427821032702923
  Output probs: [[0.001 0.    0.    0.    0.495 0.    0.501 0.    0.003 0.   ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0004
  Image Loss: 0.0459
  Total Loss: 0.0501
  Image grad max: 0.009020943194627762
  Output probs: [[0.001 0.    0.    0.    0.496 0.    0.5   0.    0.003 0.   ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0004
  Image Loss: 0.0459
  Total Loss: 0.0500
  Image grad max: 0.006891095079481602
  Output probs: [[0.001 0.    0.    0.    0.5   0.    0.496 0.    0.003 0.   ]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0004
  Image Loss: 0.0458
  Total Loss: 0.0499
  Image grad max: 0.0073554376140236855
  Output probs: [[0.001 0.    0.    0.    0.502 0.    0.494 0.    0.003 0.   ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0004
  Image Loss: 0.0457
  Total Loss: 0.0498
  Image grad max: 0.0133322449401021
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0004
  Image Loss: 0.0456
  Total Loss: 0.0497
  Image grad max: 0.005587809719145298
  Output probs: [[0.001 0.    0.    0.    0.496 0.    0.5   0.    0.002 0.   ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0004
  Image Loss: 0.0455
  Total Loss: 0.0496
  Image grad max: 0.006405786611139774
  Output probs: [[0.001 0.    0.    0.    0.496 0.    0.5   0.    0.002 0.   ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0004
  Image Loss: 0.0455
  Total Loss: 0.0495
  Image grad max: 0.006240963935852051
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0004
  Image Loss: 0.0454
  Total Loss: 0.0493
  Image grad max: 0.00501374201849103
  Output probs: [[0.001 0.    0.    0.    0.501 0.    0.495 0.    0.002 0.   ]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0004
  Image Loss: 0.0453
  Total Loss: 0.0492
  Image grad max: 0.010899200104176998
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0004
  Image Loss: 0.0452
  Total Loss: 0.0491
  Image grad max: 0.005630507133901119
  Output probs: [[0.001 0.    0.    0.    0.497 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0004
  Image Loss: 0.0451
  Total Loss: 0.0490
  Image grad max: 0.004843247123062611
  Output probs: [[0.001 0.    0.    0.    0.497 0.    0.5   0.    0.002 0.   ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0004
  Image Loss: 0.0451
  Total Loss: 0.0489
  Image grad max: 0.005135736893862486
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0004
  Image Loss: 0.0450
  Total Loss: 0.0488
  Image grad max: 0.0042337472550570965
  Output probs: [[0.001 0.    0.    0.    0.501 0.    0.496 0.    0.002 0.   ]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0004
  Image Loss: 0.0449
  Total Loss: 0.0487
  Image grad max: 0.008985387161374092
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0004
  Image Loss: 0.0448
  Total Loss: 0.0486
  Image grad max: 0.004795517772436142
  Output probs: [[0.001 0.    0.    0.    0.497 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0004
  Image Loss: 0.0448
  Total Loss: 0.0485
  Image grad max: 0.004026640206575394
  Output probs: [[0.001 0.    0.    0.    0.497 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0004
  Image Loss: 0.0447
  Total Loss: 0.0484
  Image grad max: 0.003980305045843124
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0004
  Image Loss: 0.0446
  Total Loss: 0.0482
  Image grad max: 0.00427162554115057
  Output probs: [[0.001 0.    0.    0.    0.5   0.    0.496 0.    0.002 0.   ]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0004
  Image Loss: 0.0445
  Total Loss: 0.0481
  Image grad max: 0.007505909539759159
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0004
  Image Loss: 0.0444
  Total Loss: 0.0480
  Image grad max: 0.0036067445762455463
  Output probs: [[0.001 0.    0.    0.    0.497 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0004
  Image Loss: 0.0444
  Total Loss: 0.0479
  Image grad max: 0.003569586668163538
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0004
  Image Loss: 0.0443
  Total Loss: 0.0478
  Image grad max: 0.0030737584456801414
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0004
  Image Loss: 0.0442
  Total Loss: 0.0477
  Image grad max: 0.004628503695130348
  Output probs: [[0.001 0.    0.    0.    0.5   0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0003
  Image Loss: 0.0441
  Total Loss: 0.0476
  Image grad max: 0.006155841983854771
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0003
  Image Loss: 0.0440
  Total Loss: 0.0475
  Image grad max: 0.0023449696600437164
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0003
  Image Loss: 0.0440
  Total Loss: 0.0474
  Image grad max: 0.003269660985097289
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0003
  Image Loss: 0.0439
  Total Loss: 0.0473
  Image grad max: 0.002551669254899025
  Output probs: [[0.001 0.    0.    0.    0.5   0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0003
  Image Loss: 0.0438
  Total Loss: 0.0472
  Image grad max: 0.004891512915492058
  Output probs: [[0.001 0.    0.    0.    0.5   0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0003
  Image Loss: 0.0437
  Total Loss: 0.0471
  Image grad max: 0.004794565495103598
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0003
  Image Loss: 0.0436
  Total Loss: 0.0470
  Image grad max: 0.002474225824698806
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0003
  Image Loss: 0.0436
  Total Loss: 0.0469
  Image grad max: 0.0029574683867394924
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0003
  Image Loss: 0.0435
  Total Loss: 0.0467
  Image grad max: 0.002402200363576412
  Output probs: [[0.001 0.    0.    0.    0.5   0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0003
  Image Loss: 0.0434
  Total Loss: 0.0466
  Image grad max: 0.0047874669544398785
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0003
  Image Loss: 0.0433
  Total Loss: 0.0465
  Image grad max: 0.0032657692208886147
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0003
  Image Loss: 0.0432
  Total Loss: 0.0464
  Image grad max: 0.002651108196005225
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0003
  Image Loss: 0.0432
  Total Loss: 0.0463
  Image grad max: 0.002541345078498125
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0003
  Image Loss: 0.0431
  Total Loss: 0.0462
  Image grad max: 0.003453778102993965
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.497 0.    0.002 0.   ]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0003
  Image Loss: 0.0430
  Total Loss: 0.0461
  Image grad max: 0.004085618536919355
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0003
  Image Loss: 0.0429
  Total Loss: 0.0460
  Image grad max: 0.0022015776485204697
  Output probs: [[0.001 0.    0.    0.    0.498 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0003
  Image Loss: 0.0428
  Total Loss: 0.0459
  Image grad max: 0.002597590209916234
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0003
  Image Loss: 0.0428
  Total Loss: 0.0458
  Image grad max: 0.002218707464635372
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0003
  Image Loss: 0.0427
  Total Loss: 0.0457
  Image grad max: 0.0038021577056497335
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0003
  Image Loss: 0.0426
  Total Loss: 0.0456
  Image grad max: 0.002890834817662835
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0003
  Image Loss: 0.0425
  Total Loss: 0.0455
  Image grad max: 0.002409113571047783
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0003
  Image Loss: 0.0424
  Total Loss: 0.0454
  Image grad max: 0.0023194458335638046
  Output probs: [[0.001 0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0003
  Image Loss: 0.0424
  Total Loss: 0.0453
  Image grad max: 0.003058023750782013
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0003
  Image Loss: 0.0423
  Total Loss: 0.0452
  Image grad max: 0.0032682197634130716
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0003
  Image Loss: 0.0422
  Total Loss: 0.0451
  Image grad max: 0.0021739110816270113
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0003
  Image Loss: 0.0421
  Total Loss: 0.0450
  Image grad max: 0.0023510735481977463
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0003
  Image Loss: 0.0420
  Total Loss: 0.0449
  Image grad max: 0.002365141175687313
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0003
  Image Loss: 0.0420
  Total Loss: 0.0448
  Image grad max: 0.0031897583976387978
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0003
  Image Loss: 0.0419
  Total Loss: 0.0447
  Image grad max: 0.0022132217418402433
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0003
  Image Loss: 0.0418
  Total Loss: 0.0446
  Image grad max: 0.0022855831775814295
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0003
  Image Loss: 0.0417
  Total Loss: 0.0445
  Image grad max: 0.0021185199730098248
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0003
  Image Loss: 0.0416
  Total Loss: 0.0444
  Image grad max: 0.002876084530726075
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0003
  Image Loss: 0.0416
  Total Loss: 0.0443
  Image grad max: 0.002463381038978696
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0003
  Image Loss: 0.0415
  Total Loss: 0.0442
  Image grad max: 0.002180488547310233
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0003
  Image Loss: 0.0414
  Total Loss: 0.0441
  Image grad max: 0.0021153264679014683
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0003
  Image Loss: 0.0413
  Total Loss: 0.0440
  Image grad max: 0.0025860655587166548
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0003
  Image Loss: 0.0413
  Total Loss: 0.0439
  Image grad max: 0.0025223898701369762
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0003
  Image Loss: 0.0412
  Total Loss: 0.0438
  Image grad max: 0.0020967319142073393
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0003
  Image Loss: 0.0411
  Total Loss: 0.0437
  Image grad max: 0.0021113096736371517
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0003
  Image Loss: 0.0410
  Total Loss: 0.0436
  Image grad max: 0.0023554733488708735
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0003
  Image Loss: 0.0409
  Total Loss: 0.0435
  Image grad max: 0.002477609785273671
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0003
  Image Loss: 0.0409
  Total Loss: 0.0434
  Image grad max: 0.0020480682142078876
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0003
  Image Loss: 0.0408
  Total Loss: 0.0433
  Image grad max: 0.0020861162338405848
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0003
  Image Loss: 0.0407
  Total Loss: 0.0432
  Image grad max: 0.0022066389210522175
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0002
  Image Loss: 0.0406
  Total Loss: 0.0431
  Image grad max: 0.0023885539267212152
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0002
  Image Loss: 0.0405
  Total Loss: 0.0430
  Image grad max: 0.002034683246165514
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0002
  Image Loss: 0.0405
  Total Loss: 0.0429
  Image grad max: 0.0020365952514111996
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0002
  Image Loss: 0.0404
  Total Loss: 0.0428
  Image grad max: 0.002145119709894061
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0002
  Image Loss: 0.0403
  Total Loss: 0.0427
  Image grad max: 0.002247351221740246
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0002
  Image Loss: 0.0402
  Total Loss: 0.0426
  Image grad max: 0.002013299148529768
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0002
  Image Loss: 0.0401
  Total Loss: 0.0425
  Image grad max: 0.002005618764087558
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0002
  Image Loss: 0.0401
  Total Loss: 0.0424
  Image grad max: 0.002093656687065959
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.002 0.   ]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0002
  Image Loss: 0.0400
  Total Loss: 0.0423
  Image grad max: 0.0021567109506577253
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0002
  Image Loss: 0.0399
  Total Loss: 0.0422
  Image grad max: 0.001990403514355421
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.002 0.   ]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0002
  Image Loss: 0.0398
  Total Loss: 0.0422
  Image grad max: 0.0019785191398113966
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.001 0.   ]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0002
  Image Loss: 0.0398
  Total Loss: 0.0421
  Image grad max: 0.0020598210394382477
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.001 0.   ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0002
  Image Loss: 0.0397
  Total Loss: 0.0420
  Image grad max: 0.0020681105088442564
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0002
  Image Loss: 0.0396
  Total Loss: 0.0419
  Image grad max: 0.001967331161722541
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0002
  Image Loss: 0.0395
  Total Loss: 0.0418
  Image grad max: 0.0019620570819824934
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.498 0.    0.001 0.   ]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0002
  Image Loss: 0.0394
  Total Loss: 0.0417
  Image grad max: 0.0020661281887441874
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0002
  Image Loss: 0.0394
  Total Loss: 0.0416
  Image grad max: 0.00198221392929554
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0002
  Image Loss: 0.0393
  Total Loss: 0.0415
  Image grad max: 0.001942543894983828
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0002
  Image Loss: 0.0392
  Total Loss: 0.0414
  Image grad max: 0.0019505340605974197
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0002
  Image Loss: 0.0391
  Total Loss: 0.0413
  Image grad max: 0.0020285663194954395
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0002
  Image Loss: 0.0391
  Total Loss: 0.0412
  Image grad max: 0.0019492140272632241
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0002
  Image Loss: 0.0390
  Total Loss: 0.0411
  Image grad max: 0.0019241789123043418
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0002
  Image Loss: 0.0389
  Total Loss: 0.0410
  Image grad max: 0.0019383777398616076
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0002
  Image Loss: 0.0388
  Total Loss: 0.0409
  Image grad max: 0.001982672605663538
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0002
  Image Loss: 0.0387
  Total Loss: 0.0408
  Image grad max: 0.0019163137767463923
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0002
  Image Loss: 0.0387
  Total Loss: 0.0408
  Image grad max: 0.001900439732708037
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0002
  Image Loss: 0.0386
  Total Loss: 0.0407
  Image grad max: 0.001924262847751379
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0002
  Image Loss: 0.0385
  Total Loss: 0.0406
  Image grad max: 0.0019217662047594786
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0002
  Image Loss: 0.0384
  Total Loss: 0.0405
  Image grad max: 0.0018890302162617445
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0002
  Image Loss: 0.0384
  Total Loss: 0.0404
  Image grad max: 0.0018879196140915155
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0002
  Image Loss: 0.0383
  Total Loss: 0.0403
  Image grad max: 0.0019060962367802858
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0002
  Image Loss: 0.0382
  Total Loss: 0.0402
  Image grad max: 0.0018914727261289954
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0002
  Image Loss: 0.0381
  Total Loss: 0.0401
  Image grad max: 0.0018761756364256144
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0002
  Image Loss: 0.0381
  Total Loss: 0.0400
  Image grad max: 0.001875721151009202
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0002
  Image Loss: 0.0380
  Total Loss: 0.0399
  Image grad max: 0.001876777270808816
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0002
  Image Loss: 0.0379
  Total Loss: 0.0399
  Image grad max: 0.0018660343484953046
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0002
  Image Loss: 0.0378
  Total Loss: 0.0398
  Image grad max: 0.0018561723409220576
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0002
  Image Loss: 0.0377
  Total Loss: 0.0397
  Image grad max: 0.0018593643326312304
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0002
  Image Loss: 0.0377
  Total Loss: 0.0396
  Image grad max: 0.0018547172658145428
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0002
  Image Loss: 0.0376
  Total Loss: 0.0395
  Image grad max: 0.001842427416704595
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0002
  Image Loss: 0.0375
  Total Loss: 0.0394
  Image grad max: 0.0018390386831015348
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0002
  Image Loss: 0.0374
  Total Loss: 0.0393
  Image grad max: 0.0018421150743961334
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0002
  Image Loss: 0.0374
  Total Loss: 0.0392
  Image grad max: 0.0018351447070017457
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0002
  Image Loss: 0.0373
  Total Loss: 0.0391
  Image grad max: 0.0018206870881840587
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0002
  Image Loss: 0.0372
  Total Loss: 0.0391
  Image grad max: 0.001821810845285654
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0002
  Image Loss: 0.0371
  Total Loss: 0.0390
  Image grad max: 0.001823273254558444
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0002
  Image Loss: 0.0371
  Total Loss: 0.0389
  Image grad max: 0.0018103709444403648
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0002
  Image Loss: 0.0370
  Total Loss: 0.0388
  Image grad max: 0.0018032474908977747
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0002
  Image Loss: 0.0369
  Total Loss: 0.0387
  Image grad max: 0.0018073205137625337
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0002
  Image Loss: 0.0368
  Total Loss: 0.0386
  Image grad max: 0.0018007946200668812
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0002
  Image Loss: 0.0368
  Total Loss: 0.0385
  Image grad max: 0.0017907714936882257
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0002
  Image Loss: 0.0367
  Total Loss: 0.0385
  Image grad max: 0.001789840403944254
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0002
  Image Loss: 0.0366
  Total Loss: 0.0384
  Image grad max: 0.0017886352725327015
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0002
  Image Loss: 0.0365
  Total Loss: 0.0383
  Image grad max: 0.001777941593900323
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0002
  Image Loss: 0.0365
  Total Loss: 0.0382
  Image grad max: 0.0017738493625074625
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0002
  Image Loss: 0.0364
  Total Loss: 0.0381
  Image grad max: 0.0017760094488039613
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0002
  Image Loss: 0.0363
  Total Loss: 0.0380
  Image grad max: 0.0017652205424383283
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0002
  Image Loss: 0.0363
  Total Loss: 0.0379
  Image grad max: 0.0017687182407826185
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0002
  Image Loss: 0.0362
  Total Loss: 0.0379
  Image grad max: 0.0017618726706132293
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0002
  Image Loss: 0.0361
  Total Loss: 0.0378
  Image grad max: 0.0017541758716106415
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0002
  Image Loss: 0.0360
  Total Loss: 0.0377
  Image grad max: 0.0017594365635886788
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0002
  Image Loss: 0.0360
  Total Loss: 0.0376
  Image grad max: 0.0017459969967603683
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0002
  Image Loss: 0.0359
  Total Loss: 0.0375
  Image grad max: 0.001748265465721488
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0002
  Image Loss: 0.0358
  Total Loss: 0.0374
  Image grad max: 0.001747777801938355
  Output probs: [[0.    0.    0.    0.    0.499 0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0002
  Image Loss: 0.0357
  Total Loss: 0.0374
  Image grad max: 0.0017523455899208784
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0002
  Image Loss: 0.0357
  Total Loss: 0.0373
  Image grad max: 0.0017343431245535612
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0002
  Image Loss: 0.0356
  Total Loss: 0.0372
  Image grad max: 0.0017324478831142187
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0002
  Image Loss: 0.0355
  Total Loss: 0.0371
  Image grad max: 0.0017460588132962584
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0002
  Image Loss: 0.0354
  Total Loss: 0.0370
  Image grad max: 0.001719578867778182
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0002
  Image Loss: 0.0354
  Total Loss: 0.0369
  Image grad max: 0.0017163618467748165
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0002
  Image Loss: 0.0353
  Total Loss: 0.0369
  Image grad max: 0.0017369234701618552
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0002
  Image Loss: 0.0352
  Total Loss: 0.0368
  Image grad max: 0.0017186551121994853
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0002
  Image Loss: 0.0352
  Total Loss: 0.0367
  Image grad max: 0.0017052976181730628
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0002
  Image Loss: 0.0351
  Total Loss: 0.0366
  Image grad max: 0.001728338305838406
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0002
  Image Loss: 0.0350
  Total Loss: 0.0365
  Image grad max: 0.0017142391297966242
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0002
  Image Loss: 0.0349
  Total Loss: 0.0365
  Image grad max: 0.0016964944079518318
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0002
  Image Loss: 0.0349
  Total Loss: 0.0364
  Image grad max: 0.001712713623419404
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0001
  Image Loss: 0.0348
  Total Loss: 0.0363
  Image grad max: 0.001719304476864636
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0001
  Image Loss: 0.0347
  Total Loss: 0.0362
  Image grad max: 0.0016837355215102434
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0001
  Image Loss: 0.0347
  Total Loss: 0.0361
  Image grad max: 0.001700314343906939
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0001
  Image Loss: 0.0346
  Total Loss: 0.0361
  Image grad max: 0.0017104789149016142
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0001
  Image Loss: 0.0345
  Total Loss: 0.0360
  Image grad max: 0.0016819003503769636
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0001
  Image Loss: 0.0344
  Total Loss: 0.0359
  Image grad max: 0.001686939736828208
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0001
  Image Loss: 0.0344
  Total Loss: 0.0358
  Image grad max: 0.0017015429912135005
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0001
  Image Loss: 0.0343
  Total Loss: 0.0357
  Image grad max: 0.0016766507178544998
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0001
  Image Loss: 0.0342
  Total Loss: 0.0357
  Image grad max: 0.001680742483586073
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0001
  Image Loss: 0.0342
  Total Loss: 0.0356
  Image grad max: 0.0016900170594453812
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0001
  Image Loss: 0.0341
  Total Loss: 0.0355
  Image grad max: 0.0016714953817427158
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0001
  Image Loss: 0.0340
  Total Loss: 0.0354
  Image grad max: 0.0016737249679863453
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0001
  Image Loss: 0.0339
  Total Loss: 0.0353
  Image grad max: 0.0016796806594356894
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0001
  Image Loss: 0.0339
  Total Loss: 0.0353
  Image grad max: 0.001665600691922009
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0001
  Image Loss: 0.0338
  Total Loss: 0.0352
  Image grad max: 0.0016676108352839947
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0001
  Image Loss: 0.0337
  Total Loss: 0.0351
  Image grad max: 0.001669651479460299
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0001
  Image Loss: 0.0337
  Total Loss: 0.0350
  Image grad max: 0.001660532085224986
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0001
  Image Loss: 0.0336
  Total Loss: 0.0350
  Image grad max: 0.0016582795651629567
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0001
  Image Loss: 0.0335
  Total Loss: 0.0349
  Image grad max: 0.0016299558337777853
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0001
  Image Loss: 0.0335
  Total Loss: 0.0348
  Image grad max: 0.001685121445916593
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0001
  Image Loss: 0.0334
  Total Loss: 0.0347
  Image grad max: 0.001642429968342185
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0001
  Image Loss: 0.0333
  Total Loss: 0.0346
  Image grad max: 0.001633085310459137
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0001
  Image Loss: 0.0333
  Total Loss: 0.0346
  Image grad max: 0.0016738934209570289
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0001
  Image Loss: 0.0332
  Total Loss: 0.0345
  Image grad max: 0.0016316239489242435
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0001
  Image Loss: 0.0331
  Total Loss: 0.0344
  Image grad max: 0.0016291884239763021
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0001
  Image Loss: 0.0330
  Total Loss: 0.0343
  Image grad max: 0.0016663814894855022
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0001
  Image Loss: 0.0330
  Total Loss: 0.0343
  Image grad max: 0.001607399550266564
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0001
  Image Loss: 0.0329
  Total Loss: 0.0342
  Image grad max: 0.0016410041134804487
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0001
  Image Loss: 0.0328
  Total Loss: 0.0341
  Image grad max: 0.0016471970593556762
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0001
  Image Loss: 0.0328
  Total Loss: 0.0340
  Image grad max: 0.0016072498401626945
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0001
  Image Loss: 0.0327
  Total Loss: 0.0340
  Image grad max: 0.0016408174997195601
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0001
  Image Loss: 0.0326
  Total Loss: 0.0339
  Image grad max: 0.0016235017683357
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0001
  Image Loss: 0.0326
  Total Loss: 0.0338
  Image grad max: 0.0015926278429105878
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0001
  Image Loss: 0.0325
  Total Loss: 0.0337
  Image grad max: 0.0016583283431828022
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0001
  Image Loss: 0.0324
  Total Loss: 0.0337
  Image grad max: 0.0015980423195287585
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0001
  Image Loss: 0.0324
  Total Loss: 0.0336
  Image grad max: 0.0016054334118962288
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0001
  Image Loss: 0.0323
  Total Loss: 0.0335
  Image grad max: 0.001641712267883122
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0001
  Image Loss: 0.0322
  Total Loss: 0.0334
  Image grad max: 0.001581299933604896
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0001
  Image Loss: 0.0322
  Total Loss: 0.0334
  Image grad max: 0.0016185336280614138
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0001
  Image Loss: 0.0321
  Total Loss: 0.0333
  Image grad max: 0.0016155963530763984
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0001
  Image Loss: 0.0320
  Total Loss: 0.0332
  Image grad max: 0.001579351257532835
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0001
  Image Loss: 0.0320
  Total Loss: 0.0332
  Image grad max: 0.001619366928935051
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0001
  Image Loss: 0.0319
  Total Loss: 0.0331
  Image grad max: 0.0015916392439976335
  Output probs: [[0.    0.    0.    0.    0.5   0.    0.499 0.    0.001 0.   ]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0001
  Image Loss: 0.0318
  Total Loss: 0.0330
  Image grad max: 0.001585433492437005
Visualization saved to adversarial_figures/adversarial_training.png
Visualization saved to adversarial_figures/adversarial_testing.png
