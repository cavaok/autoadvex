Epoch [1/30], Batch [0/6000], Loss: 2.3949
Epoch [1/30], Batch [100/6000], Loss: 1.2591
Epoch [1/30], Batch [200/6000], Loss: 0.6631
Epoch [1/30], Batch [300/6000], Loss: 0.6611
Epoch [1/30], Batch [400/6000], Loss: 0.8737
Epoch [1/30], Batch [500/6000], Loss: 0.9550
Epoch [1/30], Batch [600/6000], Loss: 0.7258
Epoch [1/30], Batch [700/6000], Loss: 0.3110
Epoch [1/30], Batch [800/6000], Loss: 0.0868
Epoch [1/30], Batch [900/6000], Loss: 1.0740
Epoch [1/30], Batch [1000/6000], Loss: 0.1698
Epoch [1/30], Batch [1100/6000], Loss: 0.0801
Epoch [1/30], Batch [1200/6000], Loss: 0.3640
Epoch [1/30], Batch [1300/6000], Loss: 0.1595
Epoch [1/30], Batch [1400/6000], Loss: 0.3561
Epoch [1/30], Batch [1500/6000], Loss: 0.1395
Epoch [1/30], Batch [1600/6000], Loss: 0.1168
Epoch [1/30], Batch [1700/6000], Loss: 0.1380
Epoch [1/30], Batch [1800/6000], Loss: 0.1620
Epoch [1/30], Batch [1900/6000], Loss: 0.1035
Epoch [1/30], Batch [2000/6000], Loss: 0.7240
Epoch [1/30], Batch [2100/6000], Loss: 0.4156
Epoch [1/30], Batch [2200/6000], Loss: 0.8208
Epoch [1/30], Batch [2300/6000], Loss: 0.1876
Epoch [1/30], Batch [2400/6000], Loss: 1.0149
Epoch [1/30], Batch [2500/6000], Loss: 0.7590
Epoch [1/30], Batch [2600/6000], Loss: 0.2354
Epoch [1/30], Batch [2700/6000], Loss: 0.1139
Epoch [1/30], Batch [2800/6000], Loss: 0.2095
Epoch [1/30], Batch [2900/6000], Loss: 0.8055
Epoch [1/30], Batch [3000/6000], Loss: 0.5099
Epoch [1/30], Batch [3100/6000], Loss: 0.1465
Epoch [1/30], Batch [3200/6000], Loss: 0.0957
Epoch [1/30], Batch [3300/6000], Loss: 0.5968
Epoch [1/30], Batch [3400/6000], Loss: 0.3061
Epoch [1/30], Batch [3500/6000], Loss: 0.4606
Epoch [1/30], Batch [3600/6000], Loss: 0.1140
Epoch [1/30], Batch [3700/6000], Loss: 0.1543
Epoch [1/30], Batch [3800/6000], Loss: 0.2359
Epoch [1/30], Batch [3900/6000], Loss: 0.2348
Epoch [1/30], Batch [4000/6000], Loss: 0.1483
Epoch [1/30], Batch [4100/6000], Loss: 0.1964
Epoch [1/30], Batch [4200/6000], Loss: 0.1496
Epoch [1/30], Batch [4300/6000], Loss: 0.9021
Epoch [1/30], Batch [4400/6000], Loss: 0.0850
Epoch [1/30], Batch [4500/6000], Loss: 0.6859
Epoch [1/30], Batch [4600/6000], Loss: 0.5223
Epoch [1/30], Batch [4700/6000], Loss: 0.1544
Epoch [1/30], Batch [4800/6000], Loss: 0.0468
Epoch [1/30], Batch [4900/6000], Loss: 0.1181
Epoch [1/30], Batch [5000/6000], Loss: 0.1119
Epoch [1/30], Batch [5100/6000], Loss: 0.0528
Epoch [1/30], Batch [5200/6000], Loss: 0.0692
Epoch [1/30], Batch [5300/6000], Loss: 0.1818
Epoch [1/30], Batch [5400/6000], Loss: 0.3696
Epoch [1/30], Batch [5500/6000], Loss: 0.1912
Epoch [1/30], Batch [5600/6000], Loss: 0.1997
Epoch [1/30], Batch [5700/6000], Loss: 0.2214
Epoch [1/30], Batch [5800/6000], Loss: 0.2388
Epoch [1/30], Batch [5900/6000], Loss: 0.3550
Epoch [1/30], Loss: 0.3931
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.2385
Epoch [2/30], Batch [100/6000], Loss: 0.1345
Epoch [2/30], Batch [200/6000], Loss: 0.1169
Epoch [2/30], Batch [300/6000], Loss: 0.0470
Epoch [2/30], Batch [400/6000], Loss: 0.1656
Epoch [2/30], Batch [500/6000], Loss: 0.0552
Epoch [2/30], Batch [600/6000], Loss: 0.0567
Epoch [2/30], Batch [700/6000], Loss: 0.0955
Epoch [2/30], Batch [800/6000], Loss: 0.0582
Epoch [2/30], Batch [900/6000], Loss: 0.0452
Epoch [2/30], Batch [1000/6000], Loss: 0.2086
Epoch [2/30], Batch [1100/6000], Loss: 0.1928
Epoch [2/30], Batch [1200/6000], Loss: 0.0903
Epoch [2/30], Batch [1300/6000], Loss: 0.1087
Epoch [2/30], Batch [1400/6000], Loss: 0.2127
Epoch [2/30], Batch [1500/6000], Loss: 0.3048
Epoch [2/30], Batch [1600/6000], Loss: 0.0450
Epoch [2/30], Batch [1700/6000], Loss: 0.0398
Epoch [2/30], Batch [1800/6000], Loss: 0.1862
Epoch [2/30], Batch [1900/6000], Loss: 0.0778
Epoch [2/30], Batch [2000/6000], Loss: 0.3393
Epoch [2/30], Batch [2100/6000], Loss: 0.2554
Epoch [2/30], Batch [2200/6000], Loss: 0.6766
Epoch [2/30], Batch [2300/6000], Loss: 0.3595
Epoch [2/30], Batch [2400/6000], Loss: 0.5052
Epoch [2/30], Batch [2500/6000], Loss: 0.2111
Epoch [2/30], Batch [2600/6000], Loss: 0.0519
Epoch [2/30], Batch [2700/6000], Loss: 0.0675
Epoch [2/30], Batch [2800/6000], Loss: 0.3794
Epoch [2/30], Batch [2900/6000], Loss: 0.2007
Epoch [2/30], Batch [3000/6000], Loss: 0.0991
Epoch [2/30], Batch [3100/6000], Loss: 0.2588
Epoch [2/30], Batch [3200/6000], Loss: 0.1616
Epoch [2/30], Batch [3300/6000], Loss: 0.0785
Epoch [2/30], Batch [3400/6000], Loss: 0.0426
Epoch [2/30], Batch [3500/6000], Loss: 0.1769
Epoch [2/30], Batch [3600/6000], Loss: 0.1428
Epoch [2/30], Batch [3700/6000], Loss: 0.4830
Epoch [2/30], Batch [3800/6000], Loss: 0.0544
Epoch [2/30], Batch [3900/6000], Loss: 0.1182
Epoch [2/30], Batch [4000/6000], Loss: 0.0943
Epoch [2/30], Batch [4100/6000], Loss: 0.1617
Epoch [2/30], Batch [4200/6000], Loss: 0.0540
Epoch [2/30], Batch [4300/6000], Loss: 0.5933
Epoch [2/30], Batch [4400/6000], Loss: 0.0792
Epoch [2/30], Batch [4500/6000], Loss: 0.0393
Epoch [2/30], Batch [4600/6000], Loss: 0.4695
Epoch [2/30], Batch [4700/6000], Loss: 0.3243
Epoch [2/30], Batch [4800/6000], Loss: 0.1439
Epoch [2/30], Batch [4900/6000], Loss: 0.1279
Epoch [2/30], Batch [5000/6000], Loss: 0.2822
Epoch [2/30], Batch [5100/6000], Loss: 0.4770
Epoch [2/30], Batch [5200/6000], Loss: 0.3745
Epoch [2/30], Batch [5300/6000], Loss: 0.4416
Epoch [2/30], Batch [5400/6000], Loss: 0.0554
Epoch [2/30], Batch [5500/6000], Loss: 0.0537
Epoch [2/30], Batch [5600/6000], Loss: 1.0916
Epoch [2/30], Batch [5700/6000], Loss: 0.0759
Epoch [2/30], Batch [5800/6000], Loss: 0.0647
Epoch [2/30], Batch [5900/6000], Loss: 0.0581
Epoch [2/30], Loss: 0.2240
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.0366
Epoch [3/30], Batch [100/6000], Loss: 0.3147
Epoch [3/30], Batch [200/6000], Loss: 0.1017
Epoch [3/30], Batch [300/6000], Loss: 0.0723
Epoch [3/30], Batch [400/6000], Loss: 0.1075
Epoch [3/30], Batch [500/6000], Loss: 0.2721
Epoch [3/30], Batch [600/6000], Loss: 0.2094
Epoch [3/30], Batch [700/6000], Loss: 0.0659
Epoch [3/30], Batch [800/6000], Loss: 0.1595
Epoch [3/30], Batch [900/6000], Loss: 0.0832
Epoch [3/30], Batch [1000/6000], Loss: 0.2543
Epoch [3/30], Batch [1100/6000], Loss: 0.1118
Epoch [3/30], Batch [1200/6000], Loss: 0.0355
Epoch [3/30], Batch [1300/6000], Loss: 0.0489
Epoch [3/30], Batch [1400/6000], Loss: 0.1814
Epoch [3/30], Batch [1500/6000], Loss: 0.0671
Epoch [3/30], Batch [1600/6000], Loss: 0.0453
Epoch [3/30], Batch [1700/6000], Loss: 0.1933
Epoch [3/30], Batch [1800/6000], Loss: 0.0596
Epoch [3/30], Batch [1900/6000], Loss: 0.2267
Epoch [3/30], Batch [2000/6000], Loss: 0.0382
Epoch [3/30], Batch [2100/6000], Loss: 0.1797
Epoch [3/30], Batch [2200/6000], Loss: 0.2356
Epoch [3/30], Batch [2300/6000], Loss: 0.1623
Epoch [3/30], Batch [2400/6000], Loss: 0.0448
Epoch [3/30], Batch [2500/6000], Loss: 0.8196
Epoch [3/30], Batch [2600/6000], Loss: 0.1028
Epoch [3/30], Batch [2700/6000], Loss: 0.1153
Epoch [3/30], Batch [2800/6000], Loss: 0.0565
Epoch [3/30], Batch [2900/6000], Loss: 0.0677
Epoch [3/30], Batch [3000/6000], Loss: 0.0309
Epoch [3/30], Batch [3100/6000], Loss: 0.0352
Epoch [3/30], Batch [3200/6000], Loss: 0.0398
Epoch [3/30], Batch [3300/6000], Loss: 0.0428
Epoch [3/30], Batch [3400/6000], Loss: 0.0708
Epoch [3/30], Batch [3500/6000], Loss: 0.0864
Epoch [3/30], Batch [3600/6000], Loss: 0.6388
Epoch [3/30], Batch [3700/6000], Loss: 0.0525
Epoch [3/30], Batch [3800/6000], Loss: 0.2423
Epoch [3/30], Batch [3900/6000], Loss: 0.0463
Epoch [3/30], Batch [4000/6000], Loss: 0.0458
Epoch [3/30], Batch [4100/6000], Loss: 0.0571
Epoch [3/30], Batch [4200/6000], Loss: 0.1544
Epoch [3/30], Batch [4300/6000], Loss: 0.0782
Epoch [3/30], Batch [4400/6000], Loss: 0.2600
Epoch [3/30], Batch [4500/6000], Loss: 0.1002
Epoch [3/30], Batch [4600/6000], Loss: 0.0638
Epoch [3/30], Batch [4700/6000], Loss: 0.6016
Epoch [3/30], Batch [4800/6000], Loss: 0.0370
Epoch [3/30], Batch [4900/6000], Loss: 0.0442
Epoch [3/30], Batch [5000/6000], Loss: 0.0470
Epoch [3/30], Batch [5100/6000], Loss: 0.0463
Epoch [3/30], Batch [5200/6000], Loss: 0.0523
Epoch [3/30], Batch [5300/6000], Loss: 0.0327
Epoch [3/30], Batch [5400/6000], Loss: 0.0436
Epoch [3/30], Batch [5500/6000], Loss: 0.0925
Epoch [3/30], Batch [5600/6000], Loss: 0.0830
Epoch [3/30], Batch [5700/6000], Loss: 0.1270
Epoch [3/30], Batch [5800/6000], Loss: 0.2404
Epoch [3/30], Batch [5900/6000], Loss: 0.3601
Epoch [3/30], Loss: 0.1715
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.0396
Epoch [4/30], Batch [100/6000], Loss: 0.0819
Epoch [4/30], Batch [200/6000], Loss: 0.0353
Epoch [4/30], Batch [300/6000], Loss: 0.0414
Epoch [4/30], Batch [400/6000], Loss: 0.0411
Epoch [4/30], Batch [500/6000], Loss: 0.6002
Epoch [4/30], Batch [600/6000], Loss: 0.0514
Epoch [4/30], Batch [700/6000], Loss: 0.0463
Epoch [4/30], Batch [800/6000], Loss: 0.0567
Epoch [4/30], Batch [900/6000], Loss: 0.0816
Epoch [4/30], Batch [1000/6000], Loss: 0.0411
Epoch [4/30], Batch [1100/6000], Loss: 0.2741
Epoch [4/30], Batch [1200/6000], Loss: 0.0604
Epoch [4/30], Batch [1300/6000], Loss: 0.0827
Epoch [4/30], Batch [1400/6000], Loss: 0.0891
Epoch [4/30], Batch [1500/6000], Loss: 0.2786
Epoch [4/30], Batch [1600/6000], Loss: 0.0533
Epoch [4/30], Batch [1700/6000], Loss: 0.0736
Epoch [4/30], Batch [1800/6000], Loss: 0.1346
Epoch [4/30], Batch [1900/6000], Loss: 0.2156
Epoch [4/30], Batch [2000/6000], Loss: 0.0519
Epoch [4/30], Batch [2100/6000], Loss: 0.1038
Epoch [4/30], Batch [2200/6000], Loss: 0.0402
Epoch [4/30], Batch [2300/6000], Loss: 0.1533
Epoch [4/30], Batch [2400/6000], Loss: 0.1857
Epoch [4/30], Batch [2500/6000], Loss: 0.1152
Epoch [4/30], Batch [2600/6000], Loss: 0.0381
Epoch [4/30], Batch [2700/6000], Loss: 0.0450
Epoch [4/30], Batch [2800/6000], Loss: 0.1241
Epoch [4/30], Batch [2900/6000], Loss: 0.0910
Epoch [4/30], Batch [3000/6000], Loss: 0.1743
Epoch [4/30], Batch [3100/6000], Loss: 0.0807
Epoch [4/30], Batch [3200/6000], Loss: 0.0524
Epoch [4/30], Batch [3300/6000], Loss: 0.0858
Epoch [4/30], Batch [3400/6000], Loss: 0.4010
Epoch [4/30], Batch [3500/6000], Loss: 0.0397
Epoch [4/30], Batch [3600/6000], Loss: 0.0268
Epoch [4/30], Batch [3700/6000], Loss: 0.0414
Epoch [4/30], Batch [3800/6000], Loss: 0.0754
Epoch [4/30], Batch [3900/6000], Loss: 0.9411
Epoch [4/30], Batch [4000/6000], Loss: 0.3179
Epoch [4/30], Batch [4100/6000], Loss: 0.1693
Epoch [4/30], Batch [4200/6000], Loss: 0.0415
Epoch [4/30], Batch [4300/6000], Loss: 0.0297
Epoch [4/30], Batch [4400/6000], Loss: 0.0481
Epoch [4/30], Batch [4500/6000], Loss: 0.0654
Epoch [4/30], Batch [4600/6000], Loss: 0.0417
Epoch [4/30], Batch [4700/6000], Loss: 0.0666
Epoch [4/30], Batch [4800/6000], Loss: 0.0266
Epoch [4/30], Batch [4900/6000], Loss: 0.1512
Epoch [4/30], Batch [5000/6000], Loss: 0.0615
Epoch [4/30], Batch [5100/6000], Loss: 0.0529
Epoch [4/30], Batch [5200/6000], Loss: 0.3351
Epoch [4/30], Batch [5300/6000], Loss: 0.1687
Epoch [4/30], Batch [5400/6000], Loss: 0.0441
Epoch [4/30], Batch [5500/6000], Loss: 0.2808
Epoch [4/30], Batch [5600/6000], Loss: 0.0413
Epoch [4/30], Batch [5700/6000], Loss: 0.0695
Epoch [4/30], Batch [5800/6000], Loss: 0.0506
Epoch [4/30], Batch [5900/6000], Loss: 0.0474
Epoch [4/30], Loss: 0.1427
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.1973
Epoch [5/30], Batch [100/6000], Loss: 0.0351
Epoch [5/30], Batch [200/6000], Loss: 0.3211
Epoch [5/30], Batch [300/6000], Loss: 0.0501
Epoch [5/30], Batch [400/6000], Loss: 0.1422
Epoch [5/30], Batch [500/6000], Loss: 0.0373
Epoch [5/30], Batch [600/6000], Loss: 0.2130
Epoch [5/30], Batch [700/6000], Loss: 0.2557
Epoch [5/30], Batch [800/6000], Loss: 0.0743
Epoch [5/30], Batch [900/6000], Loss: 0.3084
Epoch [5/30], Batch [1000/6000], Loss: 0.6905
Epoch [5/30], Batch [1100/6000], Loss: 0.0623
Epoch [5/30], Batch [1200/6000], Loss: 0.3133
Epoch [5/30], Batch [1300/6000], Loss: 0.1913
Epoch [5/30], Batch [1400/6000], Loss: 0.4725
Epoch [5/30], Batch [1500/6000], Loss: 0.1108
Epoch [5/30], Batch [1600/6000], Loss: 0.0425
Epoch [5/30], Batch [1700/6000], Loss: 0.0394
Epoch [5/30], Batch [1800/6000], Loss: 0.0307
Epoch [5/30], Batch [1900/6000], Loss: 0.0400
Epoch [5/30], Batch [2000/6000], Loss: 0.0235
Epoch [5/30], Batch [2100/6000], Loss: 0.0300
Epoch [5/30], Batch [2200/6000], Loss: 0.1614
Epoch [5/30], Batch [2300/6000], Loss: 0.0312
Epoch [5/30], Batch [2400/6000], Loss: 0.0339
Epoch [5/30], Batch [2500/6000], Loss: 0.3426
Epoch [5/30], Batch [2600/6000], Loss: 0.0683
Epoch [5/30], Batch [2700/6000], Loss: 0.0512
Epoch [5/30], Batch [2800/6000], Loss: 0.0595
Epoch [5/30], Batch [2900/6000], Loss: 0.1176
Epoch [5/30], Batch [3000/6000], Loss: 0.0301
Epoch [5/30], Batch [3100/6000], Loss: 0.0400
Epoch [5/30], Batch [3200/6000], Loss: 0.0977
Epoch [5/30], Batch [3300/6000], Loss: 0.3413
Epoch [5/30], Batch [3400/6000], Loss: 0.0393
Epoch [5/30], Batch [3500/6000], Loss: 0.0561
Epoch [5/30], Batch [3600/6000], Loss: 0.0294
Epoch [5/30], Batch [3700/6000], Loss: 0.0601
Epoch [5/30], Batch [3800/6000], Loss: 0.0500
Epoch [5/30], Batch [3900/6000], Loss: 0.0456
Epoch [5/30], Batch [4000/6000], Loss: 0.0255
Epoch [5/30], Batch [4100/6000], Loss: 0.0250
Epoch [5/30], Batch [4200/6000], Loss: 0.1134
Epoch [5/30], Batch [4300/6000], Loss: 0.3010
Epoch [5/30], Batch [4400/6000], Loss: 0.0444
Epoch [5/30], Batch [4500/6000], Loss: 0.0232
Epoch [5/30], Batch [4600/6000], Loss: 0.0393
Epoch [5/30], Batch [4700/6000], Loss: 0.0315
Epoch [5/30], Batch [4800/6000], Loss: 0.0306
Epoch [5/30], Batch [4900/6000], Loss: 0.0642
Epoch [5/30], Batch [5000/6000], Loss: 0.2313
Epoch [5/30], Batch [5100/6000], Loss: 0.0680
Epoch [5/30], Batch [5200/6000], Loss: 0.1035
Epoch [5/30], Batch [5300/6000], Loss: 0.0455
Epoch [5/30], Batch [5400/6000], Loss: 0.0318
Epoch [5/30], Batch [5500/6000], Loss: 0.0238
Epoch [5/30], Batch [5600/6000], Loss: 0.0337
Epoch [5/30], Batch [5700/6000], Loss: 0.0429
Epoch [5/30], Batch [5800/6000], Loss: 0.0649
Epoch [5/30], Batch [5900/6000], Loss: 0.0368
Epoch [5/30], Loss: 0.1217
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.0281
Epoch [6/30], Batch [100/6000], Loss: 0.0369
Epoch [6/30], Batch [200/6000], Loss: 0.1554
Epoch [6/30], Batch [300/6000], Loss: 0.1580
Epoch [6/30], Batch [400/6000], Loss: 0.0428
Epoch [6/30], Batch [500/6000], Loss: 0.0308
Epoch [6/30], Batch [600/6000], Loss: 0.0366
Epoch [6/30], Batch [700/6000], Loss: 0.0311
Epoch [6/30], Batch [800/6000], Loss: 0.0995
Epoch [6/30], Batch [900/6000], Loss: 0.1510
Epoch [6/30], Batch [1000/6000], Loss: 0.0292
Epoch [6/30], Batch [1100/6000], Loss: 0.0524
Epoch [6/30], Batch [1200/6000], Loss: 0.0493
Epoch [6/30], Batch [1300/6000], Loss: 0.0324
Epoch [6/30], Batch [1400/6000], Loss: 0.0278
Epoch [6/30], Batch [1500/6000], Loss: 0.0238
Epoch [6/30], Batch [1600/6000], Loss: 0.0291
Epoch [6/30], Batch [1700/6000], Loss: 0.0345
Epoch [6/30], Batch [1800/6000], Loss: 0.2873
Epoch [6/30], Batch [1900/6000], Loss: 0.0253
Epoch [6/30], Batch [2000/6000], Loss: 0.5774
Epoch [6/30], Batch [2100/6000], Loss: 0.0805
Epoch [6/30], Batch [2200/6000], Loss: 0.0340
Epoch [6/30], Batch [2300/6000], Loss: 0.0262
Epoch [6/30], Batch [2400/6000], Loss: 0.0836
Epoch [6/30], Batch [2500/6000], Loss: 0.0388
Epoch [6/30], Batch [2600/6000], Loss: 0.0228
Epoch [6/30], Batch [2700/6000], Loss: 0.0268
Epoch [6/30], Batch [2800/6000], Loss: 0.0477
Epoch [6/30], Batch [2900/6000], Loss: 0.0259
Epoch [6/30], Batch [3000/6000], Loss: 0.0817
Epoch [6/30], Batch [3100/6000], Loss: 0.0188
Epoch [6/30], Batch [3200/6000], Loss: 0.0370
Epoch [6/30], Batch [3300/6000], Loss: 0.0263
Epoch [6/30], Batch [3400/6000], Loss: 0.0687
Epoch [6/30], Batch [3500/6000], Loss: 0.0665
Epoch [6/30], Batch [3600/6000], Loss: 0.1067
Epoch [6/30], Batch [3700/6000], Loss: 0.0384
Epoch [6/30], Batch [3800/6000], Loss: 0.0681
Epoch [6/30], Batch [3900/6000], Loss: 0.1441
Epoch [6/30], Batch [4000/6000], Loss: 0.0516
Epoch [6/30], Batch [4100/6000], Loss: 0.1218
Epoch [6/30], Batch [4200/6000], Loss: 0.1510
Epoch [6/30], Batch [4300/6000], Loss: 0.0343
Epoch [6/30], Batch [4400/6000], Loss: 0.0481
Epoch [6/30], Batch [4500/6000], Loss: 0.0306
Epoch [6/30], Batch [4600/6000], Loss: 0.0339
Epoch [6/30], Batch [4700/6000], Loss: 0.0276
Epoch [6/30], Batch [4800/6000], Loss: 0.0376
Epoch [6/30], Batch [4900/6000], Loss: 0.2360
Epoch [6/30], Batch [5000/6000], Loss: 0.0577
Epoch [6/30], Batch [5100/6000], Loss: 0.0450
Epoch [6/30], Batch [5200/6000], Loss: 0.0391
Epoch [6/30], Batch [5300/6000], Loss: 0.0403
Epoch [6/30], Batch [5400/6000], Loss: 0.0227
Epoch [6/30], Batch [5500/6000], Loss: 0.0508
Epoch [6/30], Batch [5600/6000], Loss: 0.0249
Epoch [6/30], Batch [5700/6000], Loss: 0.0677
Epoch [6/30], Batch [5800/6000], Loss: 0.0595
Epoch [6/30], Batch [5900/6000], Loss: 0.0296
Epoch [6/30], Loss: 0.1065
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.0288
Epoch [7/30], Batch [100/6000], Loss: 0.1441
Epoch [7/30], Batch [200/6000], Loss: 0.1266
Epoch [7/30], Batch [300/6000], Loss: 0.1270
Epoch [7/30], Batch [400/6000], Loss: 0.1465
Epoch [7/30], Batch [500/6000], Loss: 0.1094
Epoch [7/30], Batch [600/6000], Loss: 0.1869
Epoch [7/30], Batch [700/6000], Loss: 0.0281
Epoch [7/30], Batch [800/6000], Loss: 0.0259
Epoch [7/30], Batch [900/6000], Loss: 0.0249
Epoch [7/30], Batch [1000/6000], Loss: 0.0324
Epoch [7/30], Batch [1100/6000], Loss: 0.0666
Epoch [7/30], Batch [1200/6000], Loss: 0.2798
Epoch [7/30], Batch [1300/6000], Loss: 0.0477
Epoch [7/30], Batch [1400/6000], Loss: 0.2899
Epoch [7/30], Batch [1500/6000], Loss: 0.0314
Epoch [7/30], Batch [1600/6000], Loss: 0.0379
Epoch [7/30], Batch [1700/6000], Loss: 0.0289
Epoch [7/30], Batch [1800/6000], Loss: 0.0204
Epoch [7/30], Batch [1900/6000], Loss: 0.0259
Epoch [7/30], Batch [2000/6000], Loss: 0.8926
Epoch [7/30], Batch [2100/6000], Loss: 0.0368
Epoch [7/30], Batch [2200/6000], Loss: 0.4056
Epoch [7/30], Batch [2300/6000], Loss: 0.0627
Epoch [7/30], Batch [2400/6000], Loss: 0.0280
Epoch [7/30], Batch [2500/6000], Loss: 0.0313
Epoch [7/30], Batch [2600/6000], Loss: 0.0390
Epoch [7/30], Batch [2700/6000], Loss: 0.0384
Epoch [7/30], Batch [2800/6000], Loss: 0.0331
Epoch [7/30], Batch [2900/6000], Loss: 0.0396
Epoch [7/30], Batch [3000/6000], Loss: 0.0465
Epoch [7/30], Batch [3100/6000], Loss: 0.0327
Epoch [7/30], Batch [3200/6000], Loss: 0.0401
Epoch [7/30], Batch [3300/6000], Loss: 0.1091
Epoch [7/30], Batch [3400/6000], Loss: 0.0470
Epoch [7/30], Batch [3500/6000], Loss: 0.0902
Epoch [7/30], Batch [3600/6000], Loss: 0.0295
Epoch [7/30], Batch [3700/6000], Loss: 0.0346
Epoch [7/30], Batch [3800/6000], Loss: 0.0401
Epoch [7/30], Batch [3900/6000], Loss: 0.0223
Epoch [7/30], Batch [4000/6000], Loss: 0.0628
Epoch [7/30], Batch [4100/6000], Loss: 0.0246
Epoch [7/30], Batch [4200/6000], Loss: 0.3814
Epoch [7/30], Batch [4300/6000], Loss: 0.1426
Epoch [7/30], Batch [4400/6000], Loss: 0.0208
Epoch [7/30], Batch [4500/6000], Loss: 0.0239
Epoch [7/30], Batch [4600/6000], Loss: 0.0483
Epoch [7/30], Batch [4700/6000], Loss: 0.0414
Epoch [7/30], Batch [4800/6000], Loss: 0.1974
Epoch [7/30], Batch [4900/6000], Loss: 0.0226
Epoch [7/30], Batch [5000/6000], Loss: 0.1274
Epoch [7/30], Batch [5100/6000], Loss: 0.0640
Epoch [7/30], Batch [5200/6000], Loss: 0.0544
Epoch [7/30], Batch [5300/6000], Loss: 0.0464
Epoch [7/30], Batch [5400/6000], Loss: 0.0212
Epoch [7/30], Batch [5500/6000], Loss: 0.0193
Epoch [7/30], Batch [5600/6000], Loss: 0.0777
Epoch [7/30], Batch [5700/6000], Loss: 0.1717
Epoch [7/30], Batch [5800/6000], Loss: 0.0290
Epoch [7/30], Batch [5900/6000], Loss: 0.0295
Epoch [7/30], Loss: 0.0945
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.0278
Epoch [8/30], Batch [100/6000], Loss: 0.0253
Epoch [8/30], Batch [200/6000], Loss: 0.1659
Epoch [8/30], Batch [300/6000], Loss: 0.8885
Epoch [8/30], Batch [400/6000], Loss: 0.0236
Epoch [8/30], Batch [500/6000], Loss: 0.0343
Epoch [8/30], Batch [600/6000], Loss: 0.0293
Epoch [8/30], Batch [700/6000], Loss: 0.0577
Epoch [8/30], Batch [800/6000], Loss: 0.0236
Epoch [8/30], Batch [900/6000], Loss: 0.1339
Epoch [8/30], Batch [1000/6000], Loss: 0.0224
Epoch [8/30], Batch [1100/6000], Loss: 0.0484
Epoch [8/30], Batch [1200/6000], Loss: 0.3375
Epoch [8/30], Batch [1300/6000], Loss: 0.0289
Epoch [8/30], Batch [1400/6000], Loss: 0.0196
Epoch [8/30], Batch [1500/6000], Loss: 0.0234
Epoch [8/30], Batch [1600/6000], Loss: 0.0295
Epoch [8/30], Batch [1700/6000], Loss: 0.1800
Epoch [8/30], Batch [1800/6000], Loss: 0.2696
Epoch [8/30], Batch [1900/6000], Loss: 0.1222
Epoch [8/30], Batch [2000/6000], Loss: 0.0726
Epoch [8/30], Batch [2100/6000], Loss: 0.0467
Epoch [8/30], Batch [2200/6000], Loss: 0.0804
Epoch [8/30], Batch [2300/6000], Loss: 0.0248
Epoch [8/30], Batch [2400/6000], Loss: 0.0289
Epoch [8/30], Batch [2500/6000], Loss: 0.0565
Epoch [8/30], Batch [2600/6000], Loss: 0.0261
Epoch [8/30], Batch [2700/6000], Loss: 0.0232
Epoch [8/30], Batch [2800/6000], Loss: 0.0280
Epoch [8/30], Batch [2900/6000], Loss: 0.1222
Epoch [8/30], Batch [3000/6000], Loss: 0.0320
Epoch [8/30], Batch [3100/6000], Loss: 0.0452
Epoch [8/30], Batch [3200/6000], Loss: 0.0479
Epoch [8/30], Batch [3300/6000], Loss: 0.0601
Epoch [8/30], Batch [3400/6000], Loss: 0.0316
Epoch [8/30], Batch [3500/6000], Loss: 0.0457
Epoch [8/30], Batch [3600/6000], Loss: 0.0306
Epoch [8/30], Batch [3700/6000], Loss: 0.0680
Epoch [8/30], Batch [3800/6000], Loss: 0.2253
Epoch [8/30], Batch [3900/6000], Loss: 0.1785
Epoch [8/30], Batch [4000/6000], Loss: 0.1079
Epoch [8/30], Batch [4100/6000], Loss: 0.0313
Epoch [8/30], Batch [4200/6000], Loss: 0.0330
Epoch [8/30], Batch [4300/6000], Loss: 0.0251
Epoch [8/30], Batch [4400/6000], Loss: 0.0466
Epoch [8/30], Batch [4500/6000], Loss: 0.1181
Epoch [8/30], Batch [4600/6000], Loss: 0.0381
Epoch [8/30], Batch [4700/6000], Loss: 0.0333
Epoch [8/30], Batch [4800/6000], Loss: 0.5835
Epoch [8/30], Batch [4900/6000], Loss: 0.0217
Epoch [8/30], Batch [5000/6000], Loss: 0.0235
Epoch [8/30], Batch [5100/6000], Loss: 0.0331
Epoch [8/30], Batch [5200/6000], Loss: 0.1810
Epoch [8/30], Batch [5300/6000], Loss: 0.0320
Epoch [8/30], Batch [5400/6000], Loss: 0.0277
Epoch [8/30], Batch [5500/6000], Loss: 0.1951
Epoch [8/30], Batch [5600/6000], Loss: 0.0265
Epoch [8/30], Batch [5700/6000], Loss: 0.0168
Epoch [8/30], Batch [5800/6000], Loss: 0.0292
Epoch [8/30], Batch [5900/6000], Loss: 0.0264
Epoch [8/30], Loss: 0.0832
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.0318
Epoch [9/30], Batch [100/6000], Loss: 0.0439
Epoch [9/30], Batch [200/6000], Loss: 0.0823
Epoch [9/30], Batch [300/6000], Loss: 0.0199
Epoch [9/30], Batch [400/6000], Loss: 0.1571
Epoch [9/30], Batch [500/6000], Loss: 0.0217
Epoch [9/30], Batch [600/6000], Loss: 0.0479
Epoch [9/30], Batch [700/6000], Loss: 0.0307
Epoch [9/30], Batch [800/6000], Loss: 0.0224
Epoch [9/30], Batch [900/6000], Loss: 0.1384
Epoch [9/30], Batch [1000/6000], Loss: 0.3365
Epoch [9/30], Batch [1100/6000], Loss: 0.0570
Epoch [9/30], Batch [1200/6000], Loss: 0.0336
Epoch [9/30], Batch [1300/6000], Loss: 0.0243
Epoch [9/30], Batch [1400/6000], Loss: 0.0249
Epoch [9/30], Batch [1500/6000], Loss: 0.0222
Epoch [9/30], Batch [1600/6000], Loss: 0.3142
Epoch [9/30], Batch [1700/6000], Loss: 0.0328
Epoch [9/30], Batch [1800/6000], Loss: 0.0247
Epoch [9/30], Batch [1900/6000], Loss: 0.0345
Epoch [9/30], Batch [2000/6000], Loss: 0.1703
Epoch [9/30], Batch [2100/6000], Loss: 0.0241
Epoch [9/30], Batch [2200/6000], Loss: 0.0380
Epoch [9/30], Batch [2300/6000], Loss: 0.0257
Epoch [9/30], Batch [2400/6000], Loss: 0.4018
Epoch [9/30], Batch [2500/6000], Loss: 0.1669
Epoch [9/30], Batch [2600/6000], Loss: 0.0365
Epoch [9/30], Batch [2700/6000], Loss: 0.0486
Epoch [9/30], Batch [2800/6000], Loss: 0.3403
Epoch [9/30], Batch [2900/6000], Loss: 0.0828
Epoch [9/30], Batch [3000/6000], Loss: 0.2373
Epoch [9/30], Batch [3100/6000], Loss: 0.0517
Epoch [9/30], Batch [3200/6000], Loss: 0.0357
Epoch [9/30], Batch [3300/6000], Loss: 0.1029
Epoch [9/30], Batch [3400/6000], Loss: 0.1620
Epoch [9/30], Batch [3500/6000], Loss: 0.0299
Epoch [9/30], Batch [3600/6000], Loss: 0.0554
Epoch [9/30], Batch [3700/6000], Loss: 0.0249
Epoch [9/30], Batch [3800/6000], Loss: 0.0212
Epoch [9/30], Batch [3900/6000], Loss: 0.0265
Epoch [9/30], Batch [4000/6000], Loss: 0.0225
Epoch [9/30], Batch [4100/6000], Loss: 0.0489
Epoch [9/30], Batch [4200/6000], Loss: 0.0309
Epoch [9/30], Batch [4300/6000], Loss: 0.0276
Epoch [9/30], Batch [4400/6000], Loss: 0.1168
Epoch [9/30], Batch [4500/6000], Loss: 0.0284
Epoch [9/30], Batch [4600/6000], Loss: 0.0370
Epoch [9/30], Batch [4700/6000], Loss: 0.0446
Epoch [9/30], Batch [4800/6000], Loss: 0.2890
Epoch [9/30], Batch [4900/6000], Loss: 0.0241
Epoch [9/30], Batch [5000/6000], Loss: 0.0242
Epoch [9/30], Batch [5100/6000], Loss: 0.0498
Epoch [9/30], Batch [5200/6000], Loss: 0.0212
Epoch [9/30], Batch [5300/6000], Loss: 0.0486
Epoch [9/30], Batch [5400/6000], Loss: 0.1664
Epoch [9/30], Batch [5500/6000], Loss: 0.0205
Epoch [9/30], Batch [5600/6000], Loss: 0.3039
Epoch [9/30], Batch [5700/6000], Loss: 0.0281
Epoch [9/30], Batch [5800/6000], Loss: 0.0301
Epoch [9/30], Batch [5900/6000], Loss: 0.0251
Epoch [9/30], Loss: 0.0750
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.0800
Epoch [10/30], Batch [100/6000], Loss: 0.1973
Epoch [10/30], Batch [200/6000], Loss: 0.0818
Epoch [10/30], Batch [300/6000], Loss: 0.0708
Epoch [10/30], Batch [400/6000], Loss: 0.0617
Epoch [10/30], Batch [500/6000], Loss: 0.0229
Epoch [10/30], Batch [600/6000], Loss: 0.0251
Epoch [10/30], Batch [700/6000], Loss: 0.0600
Epoch [10/30], Batch [800/6000], Loss: 0.0245
Epoch [10/30], Batch [900/6000], Loss: 0.0275
Epoch [10/30], Batch [1000/6000], Loss: 0.0332
Epoch [10/30], Batch [1100/6000], Loss: 0.0726
Epoch [10/30], Batch [1200/6000], Loss: 0.0313
Epoch [10/30], Batch [1300/6000], Loss: 0.0294
Epoch [10/30], Batch [1400/6000], Loss: 0.0323
Epoch [10/30], Batch [1500/6000], Loss: 0.2026
Epoch [10/30], Batch [1600/6000], Loss: 0.0256
Epoch [10/30], Batch [1700/6000], Loss: 0.0335
Epoch [10/30], Batch [1800/6000], Loss: 0.0282
Epoch [10/30], Batch [1900/6000], Loss: 0.0264
Epoch [10/30], Batch [2000/6000], Loss: 0.0504
Epoch [10/30], Batch [2100/6000], Loss: 0.0410
Epoch [10/30], Batch [2200/6000], Loss: 0.1600
Epoch [10/30], Batch [2300/6000], Loss: 0.0209
Epoch [10/30], Batch [2400/6000], Loss: 0.0540
Epoch [10/30], Batch [2500/6000], Loss: 0.0242
Epoch [10/30], Batch [2600/6000], Loss: 0.0253
Epoch [10/30], Batch [2700/6000], Loss: 0.0202
Epoch [10/30], Batch [2800/6000], Loss: 0.0249
Epoch [10/30], Batch [2900/6000], Loss: 0.0312
Epoch [10/30], Batch [3000/6000], Loss: 0.0264
Epoch [10/30], Batch [3100/6000], Loss: 0.3320
Epoch [10/30], Batch [3200/6000], Loss: 0.0210
Epoch [10/30], Batch [3300/6000], Loss: 0.0278
Epoch [10/30], Batch [3400/6000], Loss: 0.0223
Epoch [10/30], Batch [3500/6000], Loss: 0.0215
Epoch [10/30], Batch [3600/6000], Loss: 0.1275
Epoch [10/30], Batch [3700/6000], Loss: 0.0273
Epoch [10/30], Batch [3800/6000], Loss: 0.0476
Epoch [10/30], Batch [3900/6000], Loss: 0.0442
Epoch [10/30], Batch [4000/6000], Loss: 0.1160
Epoch [10/30], Batch [4100/6000], Loss: 0.0305
Epoch [10/30], Batch [4200/6000], Loss: 0.2099
Epoch [10/30], Batch [4300/6000], Loss: 0.0256
Epoch [10/30], Batch [4400/6000], Loss: 0.0263
Epoch [10/30], Batch [4500/6000], Loss: 0.0284
Epoch [10/30], Batch [4600/6000], Loss: 0.0185
Epoch [10/30], Batch [4700/6000], Loss: 0.0419
Epoch [10/30], Batch [4800/6000], Loss: 0.0330
Epoch [10/30], Batch [4900/6000], Loss: 0.0517
Epoch [10/30], Batch [5000/6000], Loss: 0.0263
Epoch [10/30], Batch [5100/6000], Loss: 0.0211
Epoch [10/30], Batch [5200/6000], Loss: 0.0228
Epoch [10/30], Batch [5300/6000], Loss: 0.0202
Epoch [10/30], Batch [5400/6000], Loss: 0.0562
Epoch [10/30], Batch [5500/6000], Loss: 0.0242
Epoch [10/30], Batch [5600/6000], Loss: 0.0247
Epoch [10/30], Batch [5700/6000], Loss: 0.0564
Epoch [10/30], Batch [5800/6000], Loss: 0.0317
Epoch [10/30], Batch [5900/6000], Loss: 0.0231
Epoch [10/30], Loss: 0.0684
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.0234
Epoch [11/30], Batch [100/6000], Loss: 0.0232
Epoch [11/30], Batch [200/6000], Loss: 0.0297
Epoch [11/30], Batch [300/6000], Loss: 0.0211
Epoch [11/30], Batch [400/6000], Loss: 0.0187
Epoch [11/30], Batch [500/6000], Loss: 0.1195
Epoch [11/30], Batch [600/6000], Loss: 0.0245
Epoch [11/30], Batch [700/6000], Loss: 0.0155
Epoch [11/30], Batch [800/6000], Loss: 0.0285
Epoch [11/30], Batch [900/6000], Loss: 0.7465
Epoch [11/30], Batch [1000/6000], Loss: 0.0405
Epoch [11/30], Batch [1100/6000], Loss: 0.0689
Epoch [11/30], Batch [1200/6000], Loss: 0.0410
Epoch [11/30], Batch [1300/6000], Loss: 0.0297
Epoch [11/30], Batch [1400/6000], Loss: 0.0279
Epoch [11/30], Batch [1500/6000], Loss: 0.0291
Epoch [11/30], Batch [1600/6000], Loss: 0.0276
Epoch [11/30], Batch [1700/6000], Loss: 0.0230
Epoch [11/30], Batch [1800/6000], Loss: 0.1990
Epoch [11/30], Batch [1900/6000], Loss: 0.0229
Epoch [11/30], Batch [2000/6000], Loss: 0.0232
Epoch [11/30], Batch [2100/6000], Loss: 0.0329
Epoch [11/30], Batch [2200/6000], Loss: 0.0190
Epoch [11/30], Batch [2300/6000], Loss: 0.0273
Epoch [11/30], Batch [2400/6000], Loss: 0.0252
Epoch [11/30], Batch [2500/6000], Loss: 0.0670
Epoch [11/30], Batch [2600/6000], Loss: 0.0265
Epoch [11/30], Batch [2700/6000], Loss: 0.0221
Epoch [11/30], Batch [2800/6000], Loss: 0.0582
Epoch [11/30], Batch [2900/6000], Loss: 0.0836
Epoch [11/30], Batch [3000/6000], Loss: 0.0250
Epoch [11/30], Batch [3100/6000], Loss: 0.0275
Epoch [11/30], Batch [3200/6000], Loss: 0.3434
Epoch [11/30], Batch [3300/6000], Loss: 0.0228
Epoch [11/30], Batch [3400/6000], Loss: 0.0236
Epoch [11/30], Batch [3500/6000], Loss: 0.0322
Epoch [11/30], Batch [3600/6000], Loss: 0.0206
Epoch [11/30], Batch [3700/6000], Loss: 0.0393
Epoch [11/30], Batch [3800/6000], Loss: 0.2088
Epoch [11/30], Batch [3900/6000], Loss: 0.0404
Epoch [11/30], Batch [4000/6000], Loss: 0.0275
Epoch [11/30], Batch [4100/6000], Loss: 0.0280
Epoch [11/30], Batch [4200/6000], Loss: 0.0259
Epoch [11/30], Batch [4300/6000], Loss: 0.0867
Epoch [11/30], Batch [4400/6000], Loss: 0.1284
Epoch [11/30], Batch [4500/6000], Loss: 0.0232
Epoch [11/30], Batch [4600/6000], Loss: 0.0270
Epoch [11/30], Batch [4700/6000], Loss: 0.0223
Epoch [11/30], Batch [4800/6000], Loss: 0.0344
Epoch [11/30], Batch [4900/6000], Loss: 0.0395
Epoch [11/30], Batch [5000/6000], Loss: 0.0294
Epoch [11/30], Batch [5100/6000], Loss: 0.0647
Epoch [11/30], Batch [5200/6000], Loss: 0.0400
Epoch [11/30], Batch [5300/6000], Loss: 0.0217
Epoch [11/30], Batch [5400/6000], Loss: 0.0239
Epoch [11/30], Batch [5500/6000], Loss: 0.0333
Epoch [11/30], Batch [5600/6000], Loss: 0.0317
Epoch [11/30], Batch [5700/6000], Loss: 0.1430
Epoch [11/30], Batch [5800/6000], Loss: 0.0462
Epoch [11/30], Batch [5900/6000], Loss: 0.2328
Epoch [11/30], Loss: 0.0634
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.0223
Epoch [12/30], Batch [100/6000], Loss: 0.0501
Epoch [12/30], Batch [200/6000], Loss: 0.0268
Epoch [12/30], Batch [300/6000], Loss: 0.0706
Epoch [12/30], Batch [400/6000], Loss: 0.0255
Epoch [12/30], Batch [500/6000], Loss: 0.0403
Epoch [12/30], Batch [600/6000], Loss: 0.0214
Epoch [12/30], Batch [700/6000], Loss: 0.0220
Epoch [12/30], Batch [800/6000], Loss: 0.1560
Epoch [12/30], Batch [900/6000], Loss: 0.0214
Epoch [12/30], Batch [1000/6000], Loss: 0.0280
Epoch [12/30], Batch [1100/6000], Loss: 0.0230
Epoch [12/30], Batch [1200/6000], Loss: 0.0162
Epoch [12/30], Batch [1300/6000], Loss: 0.0225
Epoch [12/30], Batch [1400/6000], Loss: 0.0277
Epoch [12/30], Batch [1500/6000], Loss: 0.3421
Epoch [12/30], Batch [1600/6000], Loss: 0.0241
Epoch [12/30], Batch [1700/6000], Loss: 0.0252
Epoch [12/30], Batch [1800/6000], Loss: 0.0540
Epoch [12/30], Batch [1900/6000], Loss: 0.0458
Epoch [12/30], Batch [2000/6000], Loss: 0.0220
Epoch [12/30], Batch [2100/6000], Loss: 0.0862
Epoch [12/30], Batch [2200/6000], Loss: 0.0226
Epoch [12/30], Batch [2300/6000], Loss: 0.3361
Epoch [12/30], Batch [2400/6000], Loss: 0.0350
Epoch [12/30], Batch [2500/6000], Loss: 0.0226
Epoch [12/30], Batch [2600/6000], Loss: 0.0287
Epoch [12/30], Batch [2700/6000], Loss: 0.0203
Epoch [12/30], Batch [2800/6000], Loss: 0.0266
Epoch [12/30], Batch [2900/6000], Loss: 0.0205
Epoch [12/30], Batch [3000/6000], Loss: 0.0276
Epoch [12/30], Batch [3100/6000], Loss: 0.0222
Epoch [12/30], Batch [3200/6000], Loss: 0.0233
Epoch [12/30], Batch [3300/6000], Loss: 0.0598
Epoch [12/30], Batch [3400/6000], Loss: 0.0319
Epoch [12/30], Batch [3500/6000], Loss: 0.0650
Epoch [12/30], Batch [3600/6000], Loss: 0.2529
Epoch [12/30], Batch [3700/6000], Loss: 0.0248
Epoch [12/30], Batch [3800/6000], Loss: 0.0230
Epoch [12/30], Batch [3900/6000], Loss: 0.0623
Epoch [12/30], Batch [4000/6000], Loss: 0.0309
Epoch [12/30], Batch [4100/6000], Loss: 0.0339
Epoch [12/30], Batch [4200/6000], Loss: 0.0238
Epoch [12/30], Batch [4300/6000], Loss: 0.0643
Epoch [12/30], Batch [4400/6000], Loss: 0.0228
Epoch [12/30], Batch [4500/6000], Loss: 0.0227
Epoch [12/30], Batch [4600/6000], Loss: 0.0215
Epoch [12/30], Batch [4700/6000], Loss: 0.2626
Epoch [12/30], Batch [4800/6000], Loss: 0.0220
Epoch [12/30], Batch [4900/6000], Loss: 0.2565
Epoch [12/30], Batch [5000/6000], Loss: 0.0218
Epoch [12/30], Batch [5100/6000], Loss: 0.1106
Epoch [12/30], Batch [5200/6000], Loss: 0.0227
Epoch [12/30], Batch [5300/6000], Loss: 0.0191
Epoch [12/30], Batch [5400/6000], Loss: 0.0191
Epoch [12/30], Batch [5500/6000], Loss: 0.0369
Epoch [12/30], Batch [5600/6000], Loss: 0.0261
Epoch [12/30], Batch [5700/6000], Loss: 0.0213
Epoch [12/30], Batch [5800/6000], Loss: 0.0224
Epoch [12/30], Batch [5900/6000], Loss: 0.0221
Epoch [12/30], Loss: 0.0572
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.0253
Epoch [13/30], Batch [100/6000], Loss: 0.0180
Epoch [13/30], Batch [200/6000], Loss: 0.0192
Epoch [13/30], Batch [300/6000], Loss: 0.1291
Epoch [13/30], Batch [400/6000], Loss: 0.0300
Epoch [13/30], Batch [500/6000], Loss: 0.0239
Epoch [13/30], Batch [600/6000], Loss: 0.0233
Epoch [13/30], Batch [700/6000], Loss: 0.0214
Epoch [13/30], Batch [800/6000], Loss: 0.0575
Epoch [13/30], Batch [900/6000], Loss: 0.4517
Epoch [13/30], Batch [1000/6000], Loss: 0.0609
Epoch [13/30], Batch [1100/6000], Loss: 0.0350
Epoch [13/30], Batch [1200/6000], Loss: 0.0504
Epoch [13/30], Batch [1300/6000], Loss: 0.0355
Epoch [13/30], Batch [1400/6000], Loss: 0.0237
Epoch [13/30], Batch [1500/6000], Loss: 0.0185
Epoch [13/30], Batch [1600/6000], Loss: 0.0277
Epoch [13/30], Batch [1700/6000], Loss: 0.0184
Epoch [13/30], Batch [1800/6000], Loss: 0.0242
Epoch [13/30], Batch [1900/6000], Loss: 0.0353
Epoch [13/30], Batch [2000/6000], Loss: 0.0455
Epoch [13/30], Batch [2100/6000], Loss: 0.0255
Epoch [13/30], Batch [2200/6000], Loss: 0.0196
Epoch [13/30], Batch [2300/6000], Loss: 0.0174
Epoch [13/30], Batch [2400/6000], Loss: 0.0283
Epoch [13/30], Batch [2500/6000], Loss: 0.0405
Epoch [13/30], Batch [2600/6000], Loss: 0.0259
Epoch [13/30], Batch [2700/6000], Loss: 0.0251
Epoch [13/30], Batch [2800/6000], Loss: 0.0202
Epoch [13/30], Batch [2900/6000], Loss: 0.0234
Epoch [13/30], Batch [3000/6000], Loss: 0.0238
Epoch [13/30], Batch [3100/6000], Loss: 0.0240
Epoch [13/30], Batch [3200/6000], Loss: 0.0330
Epoch [13/30], Batch [3300/6000], Loss: 0.0176
Epoch [13/30], Batch [3400/6000], Loss: 0.0498
Epoch [13/30], Batch [3500/6000], Loss: 0.0273
Epoch [13/30], Batch [3600/6000], Loss: 0.0317
Epoch [13/30], Batch [3700/6000], Loss: 0.0224
Epoch [13/30], Batch [3800/6000], Loss: 0.0236
Epoch [13/30], Batch [3900/6000], Loss: 0.0197
Epoch [13/30], Batch [4000/6000], Loss: 0.0267
Epoch [13/30], Batch [4100/6000], Loss: 0.0275
Epoch [13/30], Batch [4200/6000], Loss: 0.0673
Epoch [13/30], Batch [4300/6000], Loss: 0.0418
Epoch [13/30], Batch [4400/6000], Loss: 0.0732
Epoch [13/30], Batch [4500/6000], Loss: 0.0276
Epoch [13/30], Batch [4600/6000], Loss: 0.0234
Epoch [13/30], Batch [4700/6000], Loss: 0.0378
Epoch [13/30], Batch [4800/6000], Loss: 0.0179
Epoch [13/30], Batch [4900/6000], Loss: 0.0255
Epoch [13/30], Batch [5000/6000], Loss: 0.0194
Epoch [13/30], Batch [5100/6000], Loss: 0.0236
Epoch [13/30], Batch [5200/6000], Loss: 0.3443
Epoch [13/30], Batch [5300/6000], Loss: 0.1325
Epoch [13/30], Batch [5400/6000], Loss: 0.0207
Epoch [13/30], Batch [5500/6000], Loss: 0.0291
Epoch [13/30], Batch [5600/6000], Loss: 0.0191
Epoch [13/30], Batch [5700/6000], Loss: 0.0272
Epoch [13/30], Batch [5800/6000], Loss: 0.0286
Epoch [13/30], Batch [5900/6000], Loss: 0.0243
Epoch [13/30], Loss: 0.0549
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.0248
Epoch [14/30], Batch [100/6000], Loss: 0.0184
Epoch [14/30], Batch [200/6000], Loss: 0.0207
Epoch [14/30], Batch [300/6000], Loss: 0.0197
Epoch [14/30], Batch [400/6000], Loss: 0.0204
Epoch [14/30], Batch [500/6000], Loss: 0.0428
Epoch [14/30], Batch [600/6000], Loss: 0.0238
Epoch [14/30], Batch [700/6000], Loss: 0.0229
Epoch [14/30], Batch [800/6000], Loss: 0.2104
Epoch [14/30], Batch [900/6000], Loss: 0.0204
Epoch [14/30], Batch [1000/6000], Loss: 0.0234
Epoch [14/30], Batch [1100/6000], Loss: 0.0197
Epoch [14/30], Batch [1200/6000], Loss: 0.0175
Epoch [14/30], Batch [1300/6000], Loss: 0.0478
Epoch [14/30], Batch [1400/6000], Loss: 0.0251
Epoch [14/30], Batch [1500/6000], Loss: 0.0260
Epoch [14/30], Batch [1600/6000], Loss: 0.0265
Epoch [14/30], Batch [1700/6000], Loss: 0.0234
Epoch [14/30], Batch [1800/6000], Loss: 0.0355
Epoch [14/30], Batch [1900/6000], Loss: 0.0293
Epoch [14/30], Batch [2000/6000], Loss: 0.0239
Epoch [14/30], Batch [2100/6000], Loss: 0.0217
Epoch [14/30], Batch [2200/6000], Loss: 0.0202
Epoch [14/30], Batch [2300/6000], Loss: 0.0251
Epoch [14/30], Batch [2400/6000], Loss: 0.0257
Epoch [14/30], Batch [2500/6000], Loss: 0.0758
Epoch [14/30], Batch [2600/6000], Loss: 0.0235
Epoch [14/30], Batch [2700/6000], Loss: 0.0253
Epoch [14/30], Batch [2800/6000], Loss: 0.0458
Epoch [14/30], Batch [2900/6000], Loss: 0.4845
Epoch [14/30], Batch [3000/6000], Loss: 0.0272
Epoch [14/30], Batch [3100/6000], Loss: 0.0270
Epoch [14/30], Batch [3200/6000], Loss: 0.0213
Epoch [14/30], Batch [3300/6000], Loss: 0.0221
Epoch [14/30], Batch [3400/6000], Loss: 0.0313
Epoch [14/30], Batch [3500/6000], Loss: 0.0291
Epoch [14/30], Batch [3600/6000], Loss: 0.0264
Epoch [14/30], Batch [3700/6000], Loss: 0.2669
Epoch [14/30], Batch [3800/6000], Loss: 0.0190
Epoch [14/30], Batch [3900/6000], Loss: 0.0244
Epoch [14/30], Batch [4000/6000], Loss: 0.0272
Epoch [14/30], Batch [4100/6000], Loss: 0.0199
Epoch [14/30], Batch [4200/6000], Loss: 0.0269
Epoch [14/30], Batch [4300/6000], Loss: 0.0261
Epoch [14/30], Batch [4400/6000], Loss: 0.0227
Epoch [14/30], Batch [4500/6000], Loss: 0.0226
Epoch [14/30], Batch [4600/6000], Loss: 0.0243
Epoch [14/30], Batch [4700/6000], Loss: 0.0392
Epoch [14/30], Batch [4800/6000], Loss: 0.0225
Epoch [14/30], Batch [4900/6000], Loss: 0.0266
Epoch [14/30], Batch [5000/6000], Loss: 0.0265
Epoch [14/30], Batch [5100/6000], Loss: 0.2639
Epoch [14/30], Batch [5200/6000], Loss: 0.0188
Epoch [14/30], Batch [5300/6000], Loss: 0.0165
Epoch [14/30], Batch [5400/6000], Loss: 0.0256
Epoch [14/30], Batch [5500/6000], Loss: 0.0318
Epoch [14/30], Batch [5600/6000], Loss: 0.3157
Epoch [14/30], Batch [5700/6000], Loss: 0.0203
Epoch [14/30], Batch [5800/6000], Loss: 0.0402
Epoch [14/30], Batch [5900/6000], Loss: 0.0245
Epoch [14/30], Loss: 0.0497
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.2698
Epoch [15/30], Batch [100/6000], Loss: 0.0239
Epoch [15/30], Batch [200/6000], Loss: 0.5519
Epoch [15/30], Batch [300/6000], Loss: 0.0265
Epoch [15/30], Batch [400/6000], Loss: 0.0209
Epoch [15/30], Batch [500/6000], Loss: 0.0234
Epoch [15/30], Batch [600/6000], Loss: 0.0174
Epoch [15/30], Batch [700/6000], Loss: 0.1080
Epoch [15/30], Batch [800/6000], Loss: 0.0428
Epoch [15/30], Batch [900/6000], Loss: 0.0180
Epoch [15/30], Batch [1000/6000], Loss: 0.0191
Epoch [15/30], Batch [1100/6000], Loss: 0.0231
Epoch [15/30], Batch [1200/6000], Loss: 0.0213
Epoch [15/30], Batch [1300/6000], Loss: 0.0220
Epoch [15/30], Batch [1400/6000], Loss: 0.0666
Epoch [15/30], Batch [1500/6000], Loss: 0.0283
Epoch [15/30], Batch [1600/6000], Loss: 0.0188
Epoch [15/30], Batch [1700/6000], Loss: 0.0168
Epoch [15/30], Batch [1800/6000], Loss: 0.0384
Epoch [15/30], Batch [1900/6000], Loss: 0.0266
Epoch [15/30], Batch [2000/6000], Loss: 0.0298
Epoch [15/30], Batch [2100/6000], Loss: 0.0205
Epoch [15/30], Batch [2200/6000], Loss: 0.0274
Epoch [15/30], Batch [2300/6000], Loss: 0.0167
Epoch [15/30], Batch [2400/6000], Loss: 0.0224
Epoch [15/30], Batch [2500/6000], Loss: 0.0287
Epoch [15/30], Batch [2600/6000], Loss: 0.0216
Epoch [15/30], Batch [2700/6000], Loss: 0.1325
Epoch [15/30], Batch [2800/6000], Loss: 0.0246
Epoch [15/30], Batch [2900/6000], Loss: 0.0238
Epoch [15/30], Batch [3000/6000], Loss: 0.0208
Epoch [15/30], Batch [3100/6000], Loss: 0.0221
Epoch [15/30], Batch [3200/6000], Loss: 0.0607
Epoch [15/30], Batch [3300/6000], Loss: 0.0758
Epoch [15/30], Batch [3400/6000], Loss: 0.0192
Epoch [15/30], Batch [3500/6000], Loss: 0.0329
Epoch [15/30], Batch [3600/6000], Loss: 0.3424
Epoch [15/30], Batch [3700/6000], Loss: 0.0148
Epoch [15/30], Batch [3800/6000], Loss: 0.0646
Epoch [15/30], Batch [3900/6000], Loss: 0.0219
Epoch [15/30], Batch [4000/6000], Loss: 0.0286
Epoch [15/30], Batch [4100/6000], Loss: 0.0285
Epoch [15/30], Batch [4200/6000], Loss: 0.0195
Epoch [15/30], Batch [4300/6000], Loss: 0.0202
Epoch [15/30], Batch [4400/6000], Loss: 0.0204
Epoch [15/30], Batch [4500/6000], Loss: 0.4778
Epoch [15/30], Batch [4600/6000], Loss: 0.0208
Epoch [15/30], Batch [4700/6000], Loss: 0.0238
Epoch [15/30], Batch [4800/6000], Loss: 0.0274
Epoch [15/30], Batch [4900/6000], Loss: 0.0185
Epoch [15/30], Batch [5000/6000], Loss: 0.0206
Epoch [15/30], Batch [5100/6000], Loss: 0.0206
Epoch [15/30], Batch [5200/6000], Loss: 0.0267
Epoch [15/30], Batch [5300/6000], Loss: 0.1156
Epoch [15/30], Batch [5400/6000], Loss: 0.0276
Epoch [15/30], Batch [5500/6000], Loss: 0.0233
Epoch [15/30], Batch [5600/6000], Loss: 0.0332
Epoch [15/30], Batch [5700/6000], Loss: 0.0251
Epoch [15/30], Batch [5800/6000], Loss: 0.0217
Epoch [15/30], Batch [5900/6000], Loss: 0.0223
Epoch [15/30], Loss: 0.0461
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.0195
Epoch [16/30], Batch [100/6000], Loss: 0.0218
Epoch [16/30], Batch [200/6000], Loss: 0.0256
Epoch [16/30], Batch [300/6000], Loss: 0.0174
Epoch [16/30], Batch [400/6000], Loss: 0.0393
Epoch [16/30], Batch [500/6000], Loss: 0.0211
Epoch [16/30], Batch [600/6000], Loss: 0.0328
Epoch [16/30], Batch [700/6000], Loss: 0.0375
Epoch [16/30], Batch [800/6000], Loss: 0.0195
Epoch [16/30], Batch [900/6000], Loss: 0.0243
Epoch [16/30], Batch [1000/6000], Loss: 0.0175
Epoch [16/30], Batch [1100/6000], Loss: 0.0240
Epoch [16/30], Batch [1200/6000], Loss: 0.0238
Epoch [16/30], Batch [1300/6000], Loss: 0.0231
Epoch [16/30], Batch [1400/6000], Loss: 0.0232
Epoch [16/30], Batch [1500/6000], Loss: 0.0270
Epoch [16/30], Batch [1600/6000], Loss: 0.0217
Epoch [16/30], Batch [1700/6000], Loss: 0.0243
Epoch [16/30], Batch [1800/6000], Loss: 0.0182
Epoch [16/30], Batch [1900/6000], Loss: 0.1353
Epoch [16/30], Batch [2000/6000], Loss: 0.0180
Epoch [16/30], Batch [2100/6000], Loss: 0.0238
Epoch [16/30], Batch [2200/6000], Loss: 0.0209
Epoch [16/30], Batch [2300/6000], Loss: 0.0178
Epoch [16/30], Batch [2400/6000], Loss: 0.0239
Epoch [16/30], Batch [2500/6000], Loss: 0.0218
Epoch [16/30], Batch [2600/6000], Loss: 0.0242
Epoch [16/30], Batch [2700/6000], Loss: 0.0270
Epoch [16/30], Batch [2800/6000], Loss: 0.0506
Epoch [16/30], Batch [2900/6000], Loss: 0.0539
Epoch [16/30], Batch [3000/6000], Loss: 0.0208
Epoch [16/30], Batch [3100/6000], Loss: 0.0898
Epoch [16/30], Batch [3200/6000], Loss: 0.0165
Epoch [16/30], Batch [3300/6000], Loss: 0.0723
Epoch [16/30], Batch [3400/6000], Loss: 0.0556
Epoch [16/30], Batch [3500/6000], Loss: 0.0334
Epoch [16/30], Batch [3600/6000], Loss: 0.0220
Epoch [16/30], Batch [3700/6000], Loss: 0.0253
Epoch [16/30], Batch [3800/6000], Loss: 0.0268
Epoch [16/30], Batch [3900/6000], Loss: 0.0217
Epoch [16/30], Batch [4000/6000], Loss: 0.0365
Epoch [16/30], Batch [4100/6000], Loss: 0.0263
Epoch [16/30], Batch [4200/6000], Loss: 0.0178
Epoch [16/30], Batch [4300/6000], Loss: 0.0184
Epoch [16/30], Batch [4400/6000], Loss: 0.0190
Epoch [16/30], Batch [4500/6000], Loss: 0.1029
Epoch [16/30], Batch [4600/6000], Loss: 0.2390
Epoch [16/30], Batch [4700/6000], Loss: 0.0534
Epoch [16/30], Batch [4800/6000], Loss: 0.1011
Epoch [16/30], Batch [4900/6000], Loss: 0.0224
Epoch [16/30], Batch [5000/6000], Loss: 0.0587
Epoch [16/30], Batch [5100/6000], Loss: 0.0189
Epoch [16/30], Batch [5200/6000], Loss: 0.1924
Epoch [16/30], Batch [5300/6000], Loss: 0.0198
Epoch [16/30], Batch [5400/6000], Loss: 0.0161
Epoch [16/30], Batch [5500/6000], Loss: 0.0243
Epoch [16/30], Batch [5600/6000], Loss: 0.7202
Epoch [16/30], Batch [5700/6000], Loss: 0.0177
Epoch [16/30], Batch [5800/6000], Loss: 0.0206
Epoch [16/30], Batch [5900/6000], Loss: 0.0189
Epoch [16/30], Loss: 0.0438
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.0225
Epoch [17/30], Batch [100/6000], Loss: 0.1017
Epoch [17/30], Batch [200/6000], Loss: 0.0252
Epoch [17/30], Batch [300/6000], Loss: 0.0315
Epoch [17/30], Batch [400/6000], Loss: 0.0226
Epoch [17/30], Batch [500/6000], Loss: 0.0184
Epoch [17/30], Batch [600/6000], Loss: 0.0662
Epoch [17/30], Batch [700/6000], Loss: 0.0226
Epoch [17/30], Batch [800/6000], Loss: 0.0212
Epoch [17/30], Batch [900/6000], Loss: 0.0232
Epoch [17/30], Batch [1000/6000], Loss: 0.0206
Epoch [17/30], Batch [1100/6000], Loss: 0.0200
Epoch [17/30], Batch [1200/6000], Loss: 0.0196
Epoch [17/30], Batch [1300/6000], Loss: 0.0275
Epoch [17/30], Batch [1400/6000], Loss: 0.0239
Epoch [17/30], Batch [1500/6000], Loss: 0.0196
Epoch [17/30], Batch [1600/6000], Loss: 0.0212
Epoch [17/30], Batch [1700/6000], Loss: 0.0228
Epoch [17/30], Batch [1800/6000], Loss: 0.0191
Epoch [17/30], Batch [1900/6000], Loss: 0.0201
Epoch [17/30], Batch [2000/6000], Loss: 0.0178
Epoch [17/30], Batch [2100/6000], Loss: 0.0648
Epoch [17/30], Batch [2200/6000], Loss: 0.0199
Epoch [17/30], Batch [2300/6000], Loss: 0.0218
Epoch [17/30], Batch [2400/6000], Loss: 0.0281
Epoch [17/30], Batch [2500/6000], Loss: 0.0223
Epoch [17/30], Batch [2600/6000], Loss: 0.0345
Epoch [17/30], Batch [2700/6000], Loss: 0.0191
Epoch [17/30], Batch [2800/6000], Loss: 0.0472
Epoch [17/30], Batch [2900/6000], Loss: 0.0200
Epoch [17/30], Batch [3000/6000], Loss: 0.0173
Epoch [17/30], Batch [3100/6000], Loss: 0.0223
Epoch [17/30], Batch [3200/6000], Loss: 0.0194
Epoch [17/30], Batch [3300/6000], Loss: 0.0295
Epoch [17/30], Batch [3400/6000], Loss: 0.0195
Epoch [17/30], Batch [3500/6000], Loss: 0.0255
Epoch [17/30], Batch [3600/6000], Loss: 0.0649
Epoch [17/30], Batch [3700/6000], Loss: 0.0229
Epoch [17/30], Batch [3800/6000], Loss: 0.0184
Epoch [17/30], Batch [3900/6000], Loss: 0.0470
Epoch [17/30], Batch [4000/6000], Loss: 0.0171
Epoch [17/30], Batch [4100/6000], Loss: 0.0286
Epoch [17/30], Batch [4200/6000], Loss: 0.0404
Epoch [17/30], Batch [4300/6000], Loss: 0.0175
Epoch [17/30], Batch [4400/6000], Loss: 0.0423
Epoch [17/30], Batch [4500/6000], Loss: 0.0293
Epoch [17/30], Batch [4600/6000], Loss: 0.0260
Epoch [17/30], Batch [4700/6000], Loss: 0.0994
Epoch [17/30], Batch [4800/6000], Loss: 0.0217
Epoch [17/30], Batch [4900/6000], Loss: 0.0237
Epoch [17/30], Batch [5000/6000], Loss: 0.0208
Epoch [17/30], Batch [5100/6000], Loss: 0.0180
Epoch [17/30], Batch [5200/6000], Loss: 0.0172
Epoch [17/30], Batch [5300/6000], Loss: 0.0207
Epoch [17/30], Batch [5400/6000], Loss: 0.0197
Epoch [17/30], Batch [5500/6000], Loss: 0.0257
Epoch [17/30], Batch [5600/6000], Loss: 0.0237
Epoch [17/30], Batch [5700/6000], Loss: 0.0215
Epoch [17/30], Batch [5800/6000], Loss: 0.0200
Epoch [17/30], Batch [5900/6000], Loss: 0.0191
Epoch [17/30], Loss: 0.0404
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.0190
Epoch [18/30], Batch [100/6000], Loss: 0.0220
Epoch [18/30], Batch [200/6000], Loss: 0.0243
Epoch [18/30], Batch [300/6000], Loss: 0.0256
Epoch [18/30], Batch [400/6000], Loss: 0.0215
Epoch [18/30], Batch [500/6000], Loss: 0.0207
Epoch [18/30], Batch [600/6000], Loss: 0.0195
Epoch [18/30], Batch [700/6000], Loss: 0.0190
Epoch [18/30], Batch [800/6000], Loss: 0.0188
Epoch [18/30], Batch [900/6000], Loss: 0.0184
Epoch [18/30], Batch [1000/6000], Loss: 0.0231
Epoch [18/30], Batch [1100/6000], Loss: 0.0197
Epoch [18/30], Batch [1200/6000], Loss: 0.0289
Epoch [18/30], Batch [1300/6000], Loss: 0.0279
Epoch [18/30], Batch [1400/6000], Loss: 0.0186
Epoch [18/30], Batch [1500/6000], Loss: 0.0198
Epoch [18/30], Batch [1600/6000], Loss: 0.0220
Epoch [18/30], Batch [1700/6000], Loss: 0.0209
Epoch [18/30], Batch [1800/6000], Loss: 0.0210
Epoch [18/30], Batch [1900/6000], Loss: 0.0217
Epoch [18/30], Batch [2000/6000], Loss: 0.0202
Epoch [18/30], Batch [2100/6000], Loss: 0.0181
Epoch [18/30], Batch [2200/6000], Loss: 0.0286
Epoch [18/30], Batch [2300/6000], Loss: 0.0201
Epoch [18/30], Batch [2400/6000], Loss: 0.0289
Epoch [18/30], Batch [2500/6000], Loss: 0.0294
Epoch [18/30], Batch [2600/6000], Loss: 0.0198
Epoch [18/30], Batch [2700/6000], Loss: 0.0228
Epoch [18/30], Batch [2800/6000], Loss: 0.0217
Epoch [18/30], Batch [2900/6000], Loss: 0.0157
Epoch [18/30], Batch [3000/6000], Loss: 0.0191
Epoch [18/30], Batch [3100/6000], Loss: 0.0240
Epoch [18/30], Batch [3200/6000], Loss: 0.0206
Epoch [18/30], Batch [3300/6000], Loss: 0.0226
Epoch [18/30], Batch [3400/6000], Loss: 0.0196
Epoch [18/30], Batch [3500/6000], Loss: 0.0182
Epoch [18/30], Batch [3600/6000], Loss: 0.0332
Epoch [18/30], Batch [3700/6000], Loss: 0.0240
Epoch [18/30], Batch [3800/6000], Loss: 0.0207
Epoch [18/30], Batch [3900/6000], Loss: 0.0248
Epoch [18/30], Batch [4000/6000], Loss: 0.0238
Epoch [18/30], Batch [4100/6000], Loss: 0.0191
Epoch [18/30], Batch [4200/6000], Loss: 0.0182
Epoch [18/30], Batch [4300/6000], Loss: 0.0181
Epoch [18/30], Batch [4400/6000], Loss: 0.0432
Epoch [18/30], Batch [4500/6000], Loss: 0.0243
Epoch [18/30], Batch [4600/6000], Loss: 0.0181
Epoch [18/30], Batch [4700/6000], Loss: 0.0421
Epoch [18/30], Batch [4800/6000], Loss: 0.0207
Epoch [18/30], Batch [4900/6000], Loss: 0.0194
Epoch [18/30], Batch [5000/6000], Loss: 0.0207
Epoch [18/30], Batch [5100/6000], Loss: 0.0218
Epoch [18/30], Batch [5200/6000], Loss: 0.0716
Epoch [18/30], Batch [5300/6000], Loss: 0.0322
Epoch [18/30], Batch [5400/6000], Loss: 0.1862
Epoch [18/30], Batch [5500/6000], Loss: 0.0506
Epoch [18/30], Batch [5600/6000], Loss: 0.0264
Epoch [18/30], Batch [5700/6000], Loss: 0.0373
Epoch [18/30], Batch [5800/6000], Loss: 0.0146
Epoch [18/30], Batch [5900/6000], Loss: 0.1107
Epoch [18/30], Loss: 0.0394
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.0236
Epoch [19/30], Batch [100/6000], Loss: 0.0212
Epoch [19/30], Batch [200/6000], Loss: 0.0207
Epoch [19/30], Batch [300/6000], Loss: 0.0369
Epoch [19/30], Batch [400/6000], Loss: 0.0178
Epoch [19/30], Batch [500/6000], Loss: 0.0167
Epoch [19/30], Batch [600/6000], Loss: 0.0239
Epoch [19/30], Batch [700/6000], Loss: 0.0248
Epoch [19/30], Batch [800/6000], Loss: 0.0243
Epoch [19/30], Batch [900/6000], Loss: 0.0315
Epoch [19/30], Batch [1000/6000], Loss: 0.0192
Epoch [19/30], Batch [1100/6000], Loss: 0.0170
Epoch [19/30], Batch [1200/6000], Loss: 0.0744
Epoch [19/30], Batch [1300/6000], Loss: 0.0200
Epoch [19/30], Batch [1400/6000], Loss: 0.0213
Epoch [19/30], Batch [1500/6000], Loss: 0.0262
Epoch [19/30], Batch [1600/6000], Loss: 0.0224
Epoch [19/30], Batch [1700/6000], Loss: 0.0237
Epoch [19/30], Batch [1800/6000], Loss: 0.0173
Epoch [19/30], Batch [1900/6000], Loss: 0.0163
Epoch [19/30], Batch [2000/6000], Loss: 0.0187
Epoch [19/30], Batch [2100/6000], Loss: 0.0209
Epoch [19/30], Batch [2200/6000], Loss: 0.0202
Epoch [19/30], Batch [2300/6000], Loss: 0.0251
Epoch [19/30], Batch [2400/6000], Loss: 0.0248
Epoch [19/30], Batch [2500/6000], Loss: 0.0203
Epoch [19/30], Batch [2600/6000], Loss: 0.0192
Epoch [19/30], Batch [2700/6000], Loss: 0.0199
Epoch [19/30], Batch [2800/6000], Loss: 0.0241
Epoch [19/30], Batch [2900/6000], Loss: 0.0210
Epoch [19/30], Batch [3000/6000], Loss: 0.0271
Epoch [19/30], Batch [3100/6000], Loss: 0.0192
Epoch [19/30], Batch [3200/6000], Loss: 0.0226
Epoch [19/30], Batch [3300/6000], Loss: 0.0194
Epoch [19/30], Batch [3400/6000], Loss: 0.0220
Epoch [19/30], Batch [3500/6000], Loss: 0.1945
Epoch [19/30], Batch [3600/6000], Loss: 0.0222
Epoch [19/30], Batch [3700/6000], Loss: 0.0249
Epoch [19/30], Batch [3800/6000], Loss: 0.1744
Epoch [19/30], Batch [3900/6000], Loss: 0.0495
Epoch [19/30], Batch [4000/6000], Loss: 0.0342
Epoch [19/30], Batch [4100/6000], Loss: 0.0155
Epoch [19/30], Batch [4200/6000], Loss: 0.0259
Epoch [19/30], Batch [4300/6000], Loss: 0.0422
Epoch [19/30], Batch [4400/6000], Loss: 0.0201
Epoch [19/30], Batch [4500/6000], Loss: 0.0215
Epoch [19/30], Batch [4600/6000], Loss: 0.0347
Epoch [19/30], Batch [4700/6000], Loss: 0.0205
Epoch [19/30], Batch [4800/6000], Loss: 0.0124
Epoch [19/30], Batch [4900/6000], Loss: 0.0194
Epoch [19/30], Batch [5000/6000], Loss: 0.0216
Epoch [19/30], Batch [5100/6000], Loss: 0.0475
Epoch [19/30], Batch [5200/6000], Loss: 0.0183
Epoch [19/30], Batch [5300/6000], Loss: 0.0171
Epoch [19/30], Batch [5400/6000], Loss: 0.0160
Epoch [19/30], Batch [5500/6000], Loss: 0.0275
Epoch [19/30], Batch [5600/6000], Loss: 0.0236
Epoch [19/30], Batch [5700/6000], Loss: 0.0219
Epoch [19/30], Batch [5800/6000], Loss: 0.0225
Epoch [19/30], Batch [5900/6000], Loss: 0.0568
Epoch [19/30], Loss: 0.0366
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.0169
Epoch [20/30], Batch [100/6000], Loss: 0.0189
Epoch [20/30], Batch [200/6000], Loss: 0.0207
Epoch [20/30], Batch [300/6000], Loss: 0.0267
Epoch [20/30], Batch [400/6000], Loss: 0.0204
Epoch [20/30], Batch [500/6000], Loss: 0.0222
Epoch [20/30], Batch [600/6000], Loss: 0.0194
Epoch [20/30], Batch [700/6000], Loss: 0.0155
Epoch [20/30], Batch [800/6000], Loss: 0.0241
Epoch [20/30], Batch [900/6000], Loss: 0.0245
Epoch [20/30], Batch [1000/6000], Loss: 0.0385
Epoch [20/30], Batch [1100/6000], Loss: 0.0213
Epoch [20/30], Batch [1200/6000], Loss: 0.0915
Epoch [20/30], Batch [1300/6000], Loss: 0.0256
Epoch [20/30], Batch [1400/6000], Loss: 0.0184
Epoch [20/30], Batch [1500/6000], Loss: 0.0198
Epoch [20/30], Batch [1600/6000], Loss: 0.0195
Epoch [20/30], Batch [1700/6000], Loss: 0.0182
Epoch [20/30], Batch [1800/6000], Loss: 0.0223
Epoch [20/30], Batch [1900/6000], Loss: 0.0305
Epoch [20/30], Batch [2000/6000], Loss: 0.0228
Epoch [20/30], Batch [2100/6000], Loss: 0.0133
Epoch [20/30], Batch [2200/6000], Loss: 0.0237
Epoch [20/30], Batch [2300/6000], Loss: 0.0168
Epoch [20/30], Batch [2400/6000], Loss: 0.0375
Epoch [20/30], Batch [2500/6000], Loss: 0.0188
Epoch [20/30], Batch [2600/6000], Loss: 0.0738
Epoch [20/30], Batch [2700/6000], Loss: 0.0226
Epoch [20/30], Batch [2800/6000], Loss: 0.0266
Epoch [20/30], Batch [2900/6000], Loss: 0.0231
Epoch [20/30], Batch [3000/6000], Loss: 0.0196
Epoch [20/30], Batch [3100/6000], Loss: 0.0163
Epoch [20/30], Batch [3200/6000], Loss: 0.0220
Epoch [20/30], Batch [3300/6000], Loss: 0.0205
Epoch [20/30], Batch [3400/6000], Loss: 0.0251
Epoch [20/30], Batch [3500/6000], Loss: 0.1555
Epoch [20/30], Batch [3600/6000], Loss: 0.0174
Epoch [20/30], Batch [3700/6000], Loss: 0.0220
Epoch [20/30], Batch [3800/6000], Loss: 0.0211
Epoch [20/30], Batch [3900/6000], Loss: 0.0426
Epoch [20/30], Batch [4000/6000], Loss: 0.0224
Epoch [20/30], Batch [4100/6000], Loss: 0.0228
Epoch [20/30], Batch [4200/6000], Loss: 0.0202
Epoch [20/30], Batch [4300/6000], Loss: 0.0214
Epoch [20/30], Batch [4400/6000], Loss: 0.0280
Epoch [20/30], Batch [4500/6000], Loss: 0.0190
Epoch [20/30], Batch [4600/6000], Loss: 0.0203
Epoch [20/30], Batch [4700/6000], Loss: 0.0222
Epoch [20/30], Batch [4800/6000], Loss: 0.0204
Epoch [20/30], Batch [4900/6000], Loss: 0.0793
Epoch [20/30], Batch [5000/6000], Loss: 0.0287
Epoch [20/30], Batch [5100/6000], Loss: 0.0209
Epoch [20/30], Batch [5200/6000], Loss: 0.0258
Epoch [20/30], Batch [5300/6000], Loss: 0.0230
Epoch [20/30], Batch [5400/6000], Loss: 0.0190
Epoch [20/30], Batch [5500/6000], Loss: 0.0229
Epoch [20/30], Batch [5600/6000], Loss: 0.0237
Epoch [20/30], Batch [5700/6000], Loss: 0.0254
Epoch [20/30], Batch [5800/6000], Loss: 0.0218
Epoch [20/30], Batch [5900/6000], Loss: 0.3535
Epoch [20/30], Loss: 0.0359
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.0184
Epoch [21/30], Batch [100/6000], Loss: 0.0223
Epoch [21/30], Batch [200/6000], Loss: 0.0181
Epoch [21/30], Batch [300/6000], Loss: 0.0186
Epoch [21/30], Batch [400/6000], Loss: 0.0309
Epoch [21/30], Batch [500/6000], Loss: 0.0214
Epoch [21/30], Batch [600/6000], Loss: 0.0179
Epoch [21/30], Batch [700/6000], Loss: 0.0817
Epoch [21/30], Batch [800/6000], Loss: 0.0222
Epoch [21/30], Batch [900/6000], Loss: 0.0188
Epoch [21/30], Batch [1000/6000], Loss: 0.0161
Epoch [21/30], Batch [1100/6000], Loss: 0.0213
Epoch [21/30], Batch [1200/6000], Loss: 0.0234
Epoch [21/30], Batch [1300/6000], Loss: 0.0170
Epoch [21/30], Batch [1400/6000], Loss: 0.0224
Epoch [21/30], Batch [1500/6000], Loss: 0.1146
Epoch [21/30], Batch [1600/6000], Loss: 0.3092
Epoch [21/30], Batch [1700/6000], Loss: 0.0180
Epoch [21/30], Batch [1800/6000], Loss: 0.0203
Epoch [21/30], Batch [1900/6000], Loss: 0.0225
Epoch [21/30], Batch [2000/6000], Loss: 0.0254
Epoch [21/30], Batch [2100/6000], Loss: 0.0166
Epoch [21/30], Batch [2200/6000], Loss: 0.0295
Epoch [21/30], Batch [2300/6000], Loss: 0.0287
Epoch [21/30], Batch [2400/6000], Loss: 0.2588
Epoch [21/30], Batch [2500/6000], Loss: 0.0187
Epoch [21/30], Batch [2600/6000], Loss: 0.0331
Epoch [21/30], Batch [2700/6000], Loss: 0.0198
Epoch [21/30], Batch [2800/6000], Loss: 0.0266
Epoch [21/30], Batch [2900/6000], Loss: 0.0217
Epoch [21/30], Batch [3000/6000], Loss: 0.0266
Epoch [21/30], Batch [3100/6000], Loss: 0.1278
Epoch [21/30], Batch [3200/6000], Loss: 0.0306
Epoch [21/30], Batch [3300/6000], Loss: 0.2377
Epoch [21/30], Batch [3400/6000], Loss: 0.0322
Epoch [21/30], Batch [3500/6000], Loss: 0.0206
Epoch [21/30], Batch [3600/6000], Loss: 0.0184
Epoch [21/30], Batch [3700/6000], Loss: 0.0209
Epoch [21/30], Batch [3800/6000], Loss: 0.0246
Epoch [21/30], Batch [3900/6000], Loss: 0.0284
Epoch [21/30], Batch [4000/6000], Loss: 0.0128
Epoch [21/30], Batch [4100/6000], Loss: 0.0140
Epoch [21/30], Batch [4200/6000], Loss: 0.0216
Epoch [21/30], Batch [4300/6000], Loss: 0.0208
Epoch [21/30], Batch [4400/6000], Loss: 0.0185
Epoch [21/30], Batch [4500/6000], Loss: 0.0242
Epoch [21/30], Batch [4600/6000], Loss: 0.0195
Epoch [21/30], Batch [4700/6000], Loss: 0.0205
Epoch [21/30], Batch [4800/6000], Loss: 0.0207
Epoch [21/30], Batch [4900/6000], Loss: 0.0212
Epoch [21/30], Batch [5000/6000], Loss: 0.0319
Epoch [21/30], Batch [5100/6000], Loss: 0.0201
Epoch [21/30], Batch [5200/6000], Loss: 0.0184
Epoch [21/30], Batch [5300/6000], Loss: 0.0211
Epoch [21/30], Batch [5400/6000], Loss: 0.0253
Epoch [21/30], Batch [5500/6000], Loss: 0.0191
Epoch [21/30], Batch [5600/6000], Loss: 0.0176
Epoch [21/30], Batch [5700/6000], Loss: 0.0177
Epoch [21/30], Batch [5800/6000], Loss: 0.0186
Epoch [21/30], Batch [5900/6000], Loss: 0.0254
Epoch [21/30], Loss: 0.0336
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.0294
Epoch [22/30], Batch [100/6000], Loss: 0.0240
Epoch [22/30], Batch [200/6000], Loss: 0.0188
Epoch [22/30], Batch [300/6000], Loss: 0.0204
Epoch [22/30], Batch [400/6000], Loss: 0.0192
Epoch [22/30], Batch [500/6000], Loss: 0.0167
Epoch [22/30], Batch [600/6000], Loss: 0.0191
Epoch [22/30], Batch [700/6000], Loss: 0.0190
Epoch [22/30], Batch [800/6000], Loss: 0.0178
Epoch [22/30], Batch [900/6000], Loss: 0.0153
Epoch [22/30], Batch [1000/6000], Loss: 0.0164
Epoch [22/30], Batch [1100/6000], Loss: 0.0183
Epoch [22/30], Batch [1200/6000], Loss: 0.0209
Epoch [22/30], Batch [1300/6000], Loss: 0.0523
Epoch [22/30], Batch [1400/6000], Loss: 0.0206
Epoch [22/30], Batch [1500/6000], Loss: 0.0189
Epoch [22/30], Batch [1600/6000], Loss: 0.0216
Epoch [22/30], Batch [1700/6000], Loss: 0.0241
Epoch [22/30], Batch [1800/6000], Loss: 0.0203
Epoch [22/30], Batch [1900/6000], Loss: 0.0504
Epoch [22/30], Batch [2000/6000], Loss: 0.0202
Epoch [22/30], Batch [2100/6000], Loss: 0.0207
Epoch [22/30], Batch [2200/6000], Loss: 0.0187
Epoch [22/30], Batch [2300/6000], Loss: 0.0248
Epoch [22/30], Batch [2400/6000], Loss: 0.0209
Epoch [22/30], Batch [2500/6000], Loss: 0.0292
Epoch [22/30], Batch [2600/6000], Loss: 0.0209
Epoch [22/30], Batch [2700/6000], Loss: 0.0205
Epoch [22/30], Batch [2800/6000], Loss: 0.0898
Epoch [22/30], Batch [2900/6000], Loss: 0.0189
Epoch [22/30], Batch [3000/6000], Loss: 0.0184
Epoch [22/30], Batch [3100/6000], Loss: 0.0190
Epoch [22/30], Batch [3200/6000], Loss: 0.0272
Epoch [22/30], Batch [3300/6000], Loss: 0.0209
Epoch [22/30], Batch [3400/6000], Loss: 0.0222
Epoch [22/30], Batch [3500/6000], Loss: 0.0210
Epoch [22/30], Batch [3600/6000], Loss: 0.0183
Epoch [22/30], Batch [3700/6000], Loss: 0.0770
Epoch [22/30], Batch [3800/6000], Loss: 0.0176
Epoch [22/30], Batch [3900/6000], Loss: 0.0165
Epoch [22/30], Batch [4000/6000], Loss: 0.0213
Epoch [22/30], Batch [4100/6000], Loss: 0.2624
Epoch [22/30], Batch [4200/6000], Loss: 0.0163
Epoch [22/30], Batch [4300/6000], Loss: 0.0216
Epoch [22/30], Batch [4400/6000], Loss: 0.0349
Epoch [22/30], Batch [4500/6000], Loss: 0.0182
Epoch [22/30], Batch [4600/6000], Loss: 0.0223
Epoch [22/30], Batch [4700/6000], Loss: 0.0166
Epoch [22/30], Batch [4800/6000], Loss: 0.0222
Epoch [22/30], Batch [4900/6000], Loss: 0.0195
Epoch [22/30], Batch [5000/6000], Loss: 0.0188
Epoch [22/30], Batch [5100/6000], Loss: 0.0240
Epoch [22/30], Batch [5200/6000], Loss: 0.0242
Epoch [22/30], Batch [5300/6000], Loss: 0.0266
Epoch [22/30], Batch [5400/6000], Loss: 0.0180
Epoch [22/30], Batch [5500/6000], Loss: 0.0187
Epoch [22/30], Batch [5600/6000], Loss: 0.0184
Epoch [22/30], Batch [5700/6000], Loss: 0.0185
Epoch [22/30], Batch [5800/6000], Loss: 0.0202
Epoch [22/30], Batch [5900/6000], Loss: 0.0190
Epoch [22/30], Loss: 0.0334
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.0382
Epoch [23/30], Batch [100/6000], Loss: 0.0202
Epoch [23/30], Batch [200/6000], Loss: 0.0205
Epoch [23/30], Batch [300/6000], Loss: 0.0184
Epoch [23/30], Batch [400/6000], Loss: 0.0216
Epoch [23/30], Batch [500/6000], Loss: 0.0209
Epoch [23/30], Batch [600/6000], Loss: 0.0193
Epoch [23/30], Batch [700/6000], Loss: 0.0164
Epoch [23/30], Batch [800/6000], Loss: 0.0257
Epoch [23/30], Batch [900/6000], Loss: 0.0208
Epoch [23/30], Batch [1000/6000], Loss: 0.0176
Epoch [23/30], Batch [1100/6000], Loss: 0.0184
Epoch [23/30], Batch [1200/6000], Loss: 0.0172
Epoch [23/30], Batch [1300/6000], Loss: 0.0193
Epoch [23/30], Batch [1400/6000], Loss: 0.1208
Epoch [23/30], Batch [1500/6000], Loss: 0.0232
Epoch [23/30], Batch [1600/6000], Loss: 0.0199
Epoch [23/30], Batch [1700/6000], Loss: 0.0156
Epoch [23/30], Batch [1800/6000], Loss: 0.0190
Epoch [23/30], Batch [1900/6000], Loss: 0.0165
Epoch [23/30], Batch [2000/6000], Loss: 0.0153
Epoch [23/30], Batch [2100/6000], Loss: 0.0245
Epoch [23/30], Batch [2200/6000], Loss: 0.0169
Epoch [23/30], Batch [2300/6000], Loss: 0.0196
Epoch [23/30], Batch [2400/6000], Loss: 0.0384
Epoch [23/30], Batch [2500/6000], Loss: 0.0197
Epoch [23/30], Batch [2600/6000], Loss: 0.0173
Epoch [23/30], Batch [2700/6000], Loss: 0.0190
Epoch [23/30], Batch [2800/6000], Loss: 0.0373
Epoch [23/30], Batch [2900/6000], Loss: 0.0158
Epoch [23/30], Batch [3000/6000], Loss: 0.0191
Epoch [23/30], Batch [3100/6000], Loss: 0.0375
Epoch [23/30], Batch [3200/6000], Loss: 0.0208
Epoch [23/30], Batch [3300/6000], Loss: 0.0346
Epoch [23/30], Batch [3400/6000], Loss: 0.0186
Epoch [23/30], Batch [3500/6000], Loss: 0.0138
Epoch [23/30], Batch [3600/6000], Loss: 0.0182
Epoch [23/30], Batch [3700/6000], Loss: 0.0369
Epoch [23/30], Batch [3800/6000], Loss: 0.0198
Epoch [23/30], Batch [3900/6000], Loss: 0.0193
Epoch [23/30], Batch [4000/6000], Loss: 0.0175
Epoch [23/30], Batch [4100/6000], Loss: 0.0193
Epoch [23/30], Batch [4200/6000], Loss: 0.0210
Epoch [23/30], Batch [4300/6000], Loss: 0.0203
Epoch [23/30], Batch [4400/6000], Loss: 0.0208
Epoch [23/30], Batch [4500/6000], Loss: 0.0255
Epoch [23/30], Batch [4600/6000], Loss: 0.0221
Epoch [23/30], Batch [4700/6000], Loss: 0.0194
Epoch [23/30], Batch [4800/6000], Loss: 0.0254
Epoch [23/30], Batch [4900/6000], Loss: 0.0183
Epoch [23/30], Batch [5000/6000], Loss: 0.0271
Epoch [23/30], Batch [5100/6000], Loss: 0.0191
Epoch [23/30], Batch [5200/6000], Loss: 0.0183
Epoch [23/30], Batch [5300/6000], Loss: 0.0232
Epoch [23/30], Batch [5400/6000], Loss: 0.0158
Epoch [23/30], Batch [5500/6000], Loss: 0.0212
Epoch [23/30], Batch [5600/6000], Loss: 0.0186
Epoch [23/30], Batch [5700/6000], Loss: 0.0232
Epoch [23/30], Batch [5800/6000], Loss: 0.0181
Epoch [23/30], Batch [5900/6000], Loss: 0.0170
Epoch [23/30], Loss: 0.0320
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.0189
Epoch [24/30], Batch [100/6000], Loss: 0.0352
Epoch [24/30], Batch [200/6000], Loss: 0.0161
Epoch [24/30], Batch [300/6000], Loss: 0.0241
Epoch [24/30], Batch [400/6000], Loss: 0.0174
Epoch [24/30], Batch [500/6000], Loss: 0.0200
Epoch [24/30], Batch [600/6000], Loss: 0.0169
Epoch [24/30], Batch [700/6000], Loss: 0.0196
Epoch [24/30], Batch [800/6000], Loss: 0.0291
Epoch [24/30], Batch [900/6000], Loss: 0.0188
Epoch [24/30], Batch [1000/6000], Loss: 0.0160
Epoch [24/30], Batch [1100/6000], Loss: 0.0212
Epoch [24/30], Batch [1200/6000], Loss: 0.0199
Epoch [24/30], Batch [1300/6000], Loss: 0.0175
Epoch [24/30], Batch [1400/6000], Loss: 0.0203
Epoch [24/30], Batch [1500/6000], Loss: 0.0184
Epoch [24/30], Batch [1600/6000], Loss: 0.0224
Epoch [24/30], Batch [1700/6000], Loss: 0.0197
Epoch [24/30], Batch [1800/6000], Loss: 0.0180
Epoch [24/30], Batch [1900/6000], Loss: 0.0184
Epoch [24/30], Batch [2000/6000], Loss: 0.0154
Epoch [24/30], Batch [2100/6000], Loss: 0.0189
Epoch [24/30], Batch [2200/6000], Loss: 0.0227
Epoch [24/30], Batch [2300/6000], Loss: 0.0188
Epoch [24/30], Batch [2400/6000], Loss: 0.0196
Epoch [24/30], Batch [2500/6000], Loss: 0.0193
Epoch [24/30], Batch [2600/6000], Loss: 0.0183
Epoch [24/30], Batch [2700/6000], Loss: 0.0298
Epoch [24/30], Batch [2800/6000], Loss: 0.0136
Epoch [24/30], Batch [2900/6000], Loss: 0.0222
Epoch [24/30], Batch [3000/6000], Loss: 0.5115
Epoch [24/30], Batch [3100/6000], Loss: 0.0270
Epoch [24/30], Batch [3200/6000], Loss: 0.0801
Epoch [24/30], Batch [3300/6000], Loss: 0.0228
Epoch [24/30], Batch [3400/6000], Loss: 0.0188
Epoch [24/30], Batch [3500/6000], Loss: 0.0219
Epoch [24/30], Batch [3600/6000], Loss: 0.0149
Epoch [24/30], Batch [3700/6000], Loss: 0.0143
Epoch [24/30], Batch [3800/6000], Loss: 0.0164
Epoch [24/30], Batch [3900/6000], Loss: 0.0196
Epoch [24/30], Batch [4000/6000], Loss: 0.0185
Epoch [24/30], Batch [4100/6000], Loss: 0.0216
Epoch [24/30], Batch [4200/6000], Loss: 0.0226
Epoch [24/30], Batch [4300/6000], Loss: 0.0194
Epoch [24/30], Batch [4400/6000], Loss: 0.0767
Epoch [24/30], Batch [4500/6000], Loss: 0.0176
Epoch [24/30], Batch [4600/6000], Loss: 0.0229
Epoch [24/30], Batch [4700/6000], Loss: 0.0209
Epoch [24/30], Batch [4800/6000], Loss: 0.0573
Epoch [24/30], Batch [4900/6000], Loss: 0.0330
Epoch [24/30], Batch [5000/6000], Loss: 0.0159
Epoch [24/30], Batch [5100/6000], Loss: 0.0165
Epoch [24/30], Batch [5200/6000], Loss: 0.0235
Epoch [24/30], Batch [5300/6000], Loss: 0.0192
Epoch [24/30], Batch [5400/6000], Loss: 0.0205
Epoch [24/30], Batch [5500/6000], Loss: 0.0179
Epoch [24/30], Batch [5600/6000], Loss: 0.0140
Epoch [24/30], Batch [5700/6000], Loss: 0.0194
Epoch [24/30], Batch [5800/6000], Loss: 0.0212
Epoch [24/30], Batch [5900/6000], Loss: 0.0229
Epoch [24/30], Loss: 0.0310
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.0174
Epoch [25/30], Batch [100/6000], Loss: 0.0305
Epoch [25/30], Batch [200/6000], Loss: 0.0180
Epoch [25/30], Batch [300/6000], Loss: 0.0177
Epoch [25/30], Batch [400/6000], Loss: 0.0409
Epoch [25/30], Batch [500/6000], Loss: 0.0219
Epoch [25/30], Batch [600/6000], Loss: 0.0218
Epoch [25/30], Batch [700/6000], Loss: 0.0183
Epoch [25/30], Batch [800/6000], Loss: 0.0243
Epoch [25/30], Batch [900/6000], Loss: 0.0148
Epoch [25/30], Batch [1000/6000], Loss: 0.0168
Epoch [25/30], Batch [1100/6000], Loss: 0.0143
Epoch [25/30], Batch [1200/6000], Loss: 0.0176
Epoch [25/30], Batch [1300/6000], Loss: 0.0227
Epoch [25/30], Batch [1400/6000], Loss: 0.0171
Epoch [25/30], Batch [1500/6000], Loss: 0.0389
Epoch [25/30], Batch [1600/6000], Loss: 0.0211
Epoch [25/30], Batch [1700/6000], Loss: 0.0196
Epoch [25/30], Batch [1800/6000], Loss: 0.0187
Epoch [25/30], Batch [1900/6000], Loss: 0.0205
Epoch [25/30], Batch [2000/6000], Loss: 0.0229
Epoch [25/30], Batch [2100/6000], Loss: 0.0223
Epoch [25/30], Batch [2200/6000], Loss: 0.0223
Epoch [25/30], Batch [2300/6000], Loss: 0.0160
Epoch [25/30], Batch [2400/6000], Loss: 0.0211
Epoch [25/30], Batch [2500/6000], Loss: 0.0162
Epoch [25/30], Batch [2600/6000], Loss: 0.0257
Epoch [25/30], Batch [2700/6000], Loss: 0.0170
Epoch [25/30], Batch [2800/6000], Loss: 0.0202
Epoch [25/30], Batch [2900/6000], Loss: 0.0162
Epoch [25/30], Batch [3000/6000], Loss: 0.0160
Epoch [25/30], Batch [3100/6000], Loss: 0.0197
Epoch [25/30], Batch [3200/6000], Loss: 0.0206
Epoch [25/30], Batch [3300/6000], Loss: 0.0168
Epoch [25/30], Batch [3400/6000], Loss: 0.0153
Epoch [25/30], Batch [3500/6000], Loss: 0.0219
Epoch [25/30], Batch [3600/6000], Loss: 0.0278
Epoch [25/30], Batch [3700/6000], Loss: 0.0218
Epoch [25/30], Batch [3800/6000], Loss: 0.0174
Epoch [25/30], Batch [3900/6000], Loss: 0.0200
Epoch [25/30], Batch [4000/6000], Loss: 0.0184
Epoch [25/30], Batch [4100/6000], Loss: 0.0161
Epoch [25/30], Batch [4200/6000], Loss: 0.0177
Epoch [25/30], Batch [4300/6000], Loss: 0.0177
Epoch [25/30], Batch [4400/6000], Loss: 0.0209
Epoch [25/30], Batch [4500/6000], Loss: 0.0164
Epoch [25/30], Batch [4600/6000], Loss: 0.0161
Epoch [25/30], Batch [4700/6000], Loss: 0.0187
Epoch [25/30], Batch [4800/6000], Loss: 0.0196
Epoch [25/30], Batch [4900/6000], Loss: 0.0178
Epoch [25/30], Batch [5000/6000], Loss: 0.0297
Epoch [25/30], Batch [5100/6000], Loss: 0.0175
Epoch [25/30], Batch [5200/6000], Loss: 0.0212
Epoch [25/30], Batch [5300/6000], Loss: 0.0207
Epoch [25/30], Batch [5400/6000], Loss: 0.0248
Epoch [25/30], Batch [5500/6000], Loss: 0.1492
Epoch [25/30], Batch [5600/6000], Loss: 0.0217
Epoch [25/30], Batch [5700/6000], Loss: 0.0156
Epoch [25/30], Batch [5800/6000], Loss: 0.1287
Epoch [25/30], Batch [5900/6000], Loss: 0.0181
Epoch [25/30], Loss: 0.0290
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.0237
Epoch [26/30], Batch [100/6000], Loss: 0.0317
Epoch [26/30], Batch [200/6000], Loss: 0.0202
Epoch [26/30], Batch [300/6000], Loss: 0.0154
Epoch [26/30], Batch [400/6000], Loss: 0.0181
Epoch [26/30], Batch [500/6000], Loss: 0.0171
Epoch [26/30], Batch [600/6000], Loss: 0.0215
Epoch [26/30], Batch [700/6000], Loss: 0.0168
Epoch [26/30], Batch [800/6000], Loss: 0.0182
Epoch [26/30], Batch [900/6000], Loss: 0.0137
Epoch [26/30], Batch [1000/6000], Loss: 0.0156
Epoch [26/30], Batch [1100/6000], Loss: 0.0190
Epoch [26/30], Batch [1200/6000], Loss: 0.0171
Epoch [26/30], Batch [1300/6000], Loss: 0.0171
Epoch [26/30], Batch [1400/6000], Loss: 0.0191
Epoch [26/30], Batch [1500/6000], Loss: 0.0598
Epoch [26/30], Batch [1600/6000], Loss: 0.0176
Epoch [26/30], Batch [1700/6000], Loss: 0.0153
Epoch [26/30], Batch [1800/6000], Loss: 0.0178
Epoch [26/30], Batch [1900/6000], Loss: 0.0196
Epoch [26/30], Batch [2000/6000], Loss: 0.0160
Epoch [26/30], Batch [2100/6000], Loss: 0.0347
Epoch [26/30], Batch [2200/6000], Loss: 0.0195
Epoch [26/30], Batch [2300/6000], Loss: 0.0183
Epoch [26/30], Batch [2400/6000], Loss: 0.0862
Epoch [26/30], Batch [2500/6000], Loss: 0.0167
Epoch [26/30], Batch [2600/6000], Loss: 0.0194
Epoch [26/30], Batch [2700/6000], Loss: 0.0192
Epoch [26/30], Batch [2800/6000], Loss: 0.0188
Epoch [26/30], Batch [2900/6000], Loss: 0.0170
Epoch [26/30], Batch [3000/6000], Loss: 0.0214
Epoch [26/30], Batch [3100/6000], Loss: 0.0215
Epoch [26/30], Batch [3200/6000], Loss: 0.0216
Epoch [26/30], Batch [3300/6000], Loss: 0.3879
Epoch [26/30], Batch [3400/6000], Loss: 0.0174
Epoch [26/30], Batch [3500/6000], Loss: 0.0201
Epoch [26/30], Batch [3600/6000], Loss: 0.0187
Epoch [26/30], Batch [3700/6000], Loss: 0.0166
Epoch [26/30], Batch [3800/6000], Loss: 0.0171
Epoch [26/30], Batch [3900/6000], Loss: 0.0190
Epoch [26/30], Batch [4000/6000], Loss: 0.0169
Epoch [26/30], Batch [4100/6000], Loss: 0.0806
Epoch [26/30], Batch [4200/6000], Loss: 0.0209
Epoch [26/30], Batch [4300/6000], Loss: 0.0269
Epoch [26/30], Batch [4400/6000], Loss: 0.0146
Epoch [26/30], Batch [4500/6000], Loss: 0.0226
Epoch [26/30], Batch [4600/6000], Loss: 0.0166
Epoch [26/30], Batch [4700/6000], Loss: 0.0207
Epoch [26/30], Batch [4800/6000], Loss: 0.0196
Epoch [26/30], Batch [4900/6000], Loss: 0.0167
Epoch [26/30], Batch [5000/6000], Loss: 0.0194
Epoch [26/30], Batch [5100/6000], Loss: 0.0150
Epoch [26/30], Batch [5200/6000], Loss: 0.0203
Epoch [26/30], Batch [5300/6000], Loss: 0.0178
Epoch [26/30], Batch [5400/6000], Loss: 0.0152
Epoch [26/30], Batch [5500/6000], Loss: 0.0180
Epoch [26/30], Batch [5600/6000], Loss: 0.0149
Epoch [26/30], Batch [5700/6000], Loss: 0.0161
Epoch [26/30], Batch [5800/6000], Loss: 0.0204
Epoch [26/30], Batch [5900/6000], Loss: 0.0121
Epoch [26/30], Loss: 0.0284
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0183
Epoch [27/30], Batch [100/6000], Loss: 0.0209
Epoch [27/30], Batch [200/6000], Loss: 0.0208
Epoch [27/30], Batch [300/6000], Loss: 0.0179
Epoch [27/30], Batch [400/6000], Loss: 0.0190
Epoch [27/30], Batch [500/6000], Loss: 0.0179
Epoch [27/30], Batch [600/6000], Loss: 0.0213
Epoch [27/30], Batch [700/6000], Loss: 0.0185
Epoch [27/30], Batch [800/6000], Loss: 0.0211
Epoch [27/30], Batch [900/6000], Loss: 0.0176
Epoch [27/30], Batch [1000/6000], Loss: 0.0193
Epoch [27/30], Batch [1100/6000], Loss: 0.0166
Epoch [27/30], Batch [1200/6000], Loss: 0.0133
Epoch [27/30], Batch [1300/6000], Loss: 0.0185
Epoch [27/30], Batch [1400/6000], Loss: 0.0172
Epoch [27/30], Batch [1500/6000], Loss: 0.0159
Epoch [27/30], Batch [1600/6000], Loss: 0.0185
Epoch [27/30], Batch [1700/6000], Loss: 0.0152
Epoch [27/30], Batch [1800/6000], Loss: 0.0165
Epoch [27/30], Batch [1900/6000], Loss: 0.0157
Epoch [27/30], Batch [2000/6000], Loss: 0.0353
Epoch [27/30], Batch [2100/6000], Loss: 0.0178
Epoch [27/30], Batch [2200/6000], Loss: 0.0192
Epoch [27/30], Batch [2300/6000], Loss: 0.0195
Epoch [27/30], Batch [2400/6000], Loss: 0.0162
Epoch [27/30], Batch [2500/6000], Loss: 0.0178
Epoch [27/30], Batch [2600/6000], Loss: 0.0191
Epoch [27/30], Batch [2700/6000], Loss: 0.0194
Epoch [27/30], Batch [2800/6000], Loss: 0.0190
Epoch [27/30], Batch [2900/6000], Loss: 0.0189
Epoch [27/30], Batch [3000/6000], Loss: 0.0181
Epoch [27/30], Batch [3100/6000], Loss: 0.0195
Epoch [27/30], Batch [3200/6000], Loss: 0.0170
Epoch [27/30], Batch [3300/6000], Loss: 0.0199
Epoch [27/30], Batch [3400/6000], Loss: 0.0171
Epoch [27/30], Batch [3500/6000], Loss: 0.0182
Epoch [27/30], Batch [3600/6000], Loss: 0.0284
Epoch [27/30], Batch [3700/6000], Loss: 0.0179
Epoch [27/30], Batch [3800/6000], Loss: 0.0203
Epoch [27/30], Batch [3900/6000], Loss: 0.0177
Epoch [27/30], Batch [4000/6000], Loss: 0.0272
Epoch [27/30], Batch [4100/6000], Loss: 0.0165
Epoch [27/30], Batch [4200/6000], Loss: 0.0178
Epoch [27/30], Batch [4300/6000], Loss: 0.0337
Epoch [27/30], Batch [4400/6000], Loss: 0.0175
Epoch [27/30], Batch [4500/6000], Loss: 0.0184
Epoch [27/30], Batch [4600/6000], Loss: 0.0161
Epoch [27/30], Batch [4700/6000], Loss: 0.0141
Epoch [27/30], Batch [4800/6000], Loss: 0.4502
Epoch [27/30], Batch [4900/6000], Loss: 0.0150
Epoch [27/30], Batch [5000/6000], Loss: 0.0143
Epoch [27/30], Batch [5100/6000], Loss: 0.0233
Epoch [27/30], Batch [5200/6000], Loss: 0.0175
Epoch [27/30], Batch [5300/6000], Loss: 0.0168
Epoch [27/30], Batch [5400/6000], Loss: 0.0213
Epoch [27/30], Batch [5500/6000], Loss: 0.0136
Epoch [27/30], Batch [5600/6000], Loss: 0.0151
Epoch [27/30], Batch [5700/6000], Loss: 0.0200
Epoch [27/30], Batch [5800/6000], Loss: 0.0190
Epoch [27/30], Batch [5900/6000], Loss: 0.0220
Epoch [27/30], Loss: 0.0279
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.0192
Epoch [28/30], Batch [100/6000], Loss: 0.0176
Epoch [28/30], Batch [200/6000], Loss: 0.0165
Epoch [28/30], Batch [300/6000], Loss: 0.0167
Epoch [28/30], Batch [400/6000], Loss: 0.0255
Epoch [28/30], Batch [500/6000], Loss: 0.0185
Epoch [28/30], Batch [600/6000], Loss: 0.0182
Epoch [28/30], Batch [700/6000], Loss: 0.0161
Epoch [28/30], Batch [800/6000], Loss: 0.0196
Epoch [28/30], Batch [900/6000], Loss: 0.0186
Epoch [28/30], Batch [1000/6000], Loss: 0.0226
Epoch [28/30], Batch [1100/6000], Loss: 0.0240
Epoch [28/30], Batch [1200/6000], Loss: 0.0178
Epoch [28/30], Batch [1300/6000], Loss: 0.0195
Epoch [28/30], Batch [1400/6000], Loss: 0.0169
Epoch [28/30], Batch [1500/6000], Loss: 0.0172
Epoch [28/30], Batch [1600/6000], Loss: 0.0175
Epoch [28/30], Batch [1700/6000], Loss: 0.0119
Epoch [28/30], Batch [1800/6000], Loss: 0.0206
Epoch [28/30], Batch [1900/6000], Loss: 0.0178
Epoch [28/30], Batch [2000/6000], Loss: 0.0159
Epoch [28/30], Batch [2100/6000], Loss: 0.0177
Epoch [28/30], Batch [2200/6000], Loss: 0.0255
Epoch [28/30], Batch [2300/6000], Loss: 0.0160
Epoch [28/30], Batch [2400/6000], Loss: 0.0196
Epoch [28/30], Batch [2500/6000], Loss: 0.0196
Epoch [28/30], Batch [2600/6000], Loss: 0.0188
Epoch [28/30], Batch [2700/6000], Loss: 0.0182
Epoch [28/30], Batch [2800/6000], Loss: 0.0199
Epoch [28/30], Batch [2900/6000], Loss: 0.0197
Epoch [28/30], Batch [3000/6000], Loss: 0.0647
Epoch [28/30], Batch [3100/6000], Loss: 0.0226
Epoch [28/30], Batch [3200/6000], Loss: 0.0188
Epoch [28/30], Batch [3300/6000], Loss: 0.0248
Epoch [28/30], Batch [3400/6000], Loss: 0.0221
Epoch [28/30], Batch [3500/6000], Loss: 0.1523
Epoch [28/30], Batch [3600/6000], Loss: 0.0217
Epoch [28/30], Batch [3700/6000], Loss: 0.0194
Epoch [28/30], Batch [3800/6000], Loss: 0.0175
Epoch [28/30], Batch [3900/6000], Loss: 0.0199
Epoch [28/30], Batch [4000/6000], Loss: 0.0140
Epoch [28/30], Batch [4100/6000], Loss: 0.0158
Epoch [28/30], Batch [4200/6000], Loss: 0.0180
Epoch [28/30], Batch [4300/6000], Loss: 0.0186
Epoch [28/30], Batch [4400/6000], Loss: 0.0626
Epoch [28/30], Batch [4500/6000], Loss: 0.0628
Epoch [28/30], Batch [4600/6000], Loss: 0.0245
Epoch [28/30], Batch [4700/6000], Loss: 0.0176
Epoch [28/30], Batch [4800/6000], Loss: 0.0212
Epoch [28/30], Batch [4900/6000], Loss: 0.0199
Epoch [28/30], Batch [5000/6000], Loss: 0.0134
Epoch [28/30], Batch [5100/6000], Loss: 0.0121
Epoch [28/30], Batch [5200/6000], Loss: 0.0221
Epoch [28/30], Batch [5300/6000], Loss: 0.0190
Epoch [28/30], Batch [5400/6000], Loss: 0.0943
Epoch [28/30], Batch [5500/6000], Loss: 0.0167
Epoch [28/30], Batch [5600/6000], Loss: 0.0191
Epoch [28/30], Batch [5700/6000], Loss: 0.0194
Epoch [28/30], Batch [5800/6000], Loss: 0.0323
Epoch [28/30], Batch [5900/6000], Loss: 0.0220
Epoch [28/30], Loss: 0.0272
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0144
Epoch [29/30], Batch [100/6000], Loss: 0.0241
Epoch [29/30], Batch [200/6000], Loss: 0.0170
Epoch [29/30], Batch [300/6000], Loss: 0.0130
Epoch [29/30], Batch [400/6000], Loss: 0.0166
Epoch [29/30], Batch [500/6000], Loss: 0.0192
Epoch [29/30], Batch [600/6000], Loss: 0.0207
Epoch [29/30], Batch [700/6000], Loss: 0.0214
Epoch [29/30], Batch [800/6000], Loss: 0.0182
Epoch [29/30], Batch [900/6000], Loss: 0.0157
Epoch [29/30], Batch [1000/6000], Loss: 0.0157
Epoch [29/30], Batch [1100/6000], Loss: 0.0124
Epoch [29/30], Batch [1200/6000], Loss: 0.0187
Epoch [29/30], Batch [1300/6000], Loss: 0.0164
Epoch [29/30], Batch [1400/6000], Loss: 0.0160
Epoch [29/30], Batch [1500/6000], Loss: 0.0159
Epoch [29/30], Batch [1600/6000], Loss: 0.0173
Epoch [29/30], Batch [1700/6000], Loss: 0.0305
Epoch [29/30], Batch [1800/6000], Loss: 0.0182
Epoch [29/30], Batch [1900/6000], Loss: 0.0136
Epoch [29/30], Batch [2000/6000], Loss: 0.0200
Epoch [29/30], Batch [2100/6000], Loss: 0.0146
Epoch [29/30], Batch [2200/6000], Loss: 0.0177
Epoch [29/30], Batch [2300/6000], Loss: 0.0212
Epoch [29/30], Batch [2400/6000], Loss: 0.0193
Epoch [29/30], Batch [2500/6000], Loss: 0.0193
Epoch [29/30], Batch [2600/6000], Loss: 0.0155
Epoch [29/30], Batch [2700/6000], Loss: 0.0158
Epoch [29/30], Batch [2800/6000], Loss: 0.0219
Epoch [29/30], Batch [2900/6000], Loss: 0.0157
Epoch [29/30], Batch [3000/6000], Loss: 0.0190
Epoch [29/30], Batch [3100/6000], Loss: 0.0212
Epoch [29/30], Batch [3200/6000], Loss: 0.0150
Epoch [29/30], Batch [3300/6000], Loss: 0.0177
Epoch [29/30], Batch [3400/6000], Loss: 0.0165
Epoch [29/30], Batch [3500/6000], Loss: 0.0151
Epoch [29/30], Batch [3600/6000], Loss: 0.0132
Epoch [29/30], Batch [3700/6000], Loss: 0.0179
Epoch [29/30], Batch [3800/6000], Loss: 0.0162
Epoch [29/30], Batch [3900/6000], Loss: 0.0156
Epoch [29/30], Batch [4000/6000], Loss: 0.0171
Epoch [29/30], Batch [4100/6000], Loss: 0.0171
Epoch [29/30], Batch [4200/6000], Loss: 0.0167
Epoch [29/30], Batch [4300/6000], Loss: 0.0336
Epoch [29/30], Batch [4400/6000], Loss: 0.0191
Epoch [29/30], Batch [4500/6000], Loss: 0.0203
Epoch [29/30], Batch [4600/6000], Loss: 0.0169
Epoch [29/30], Batch [4700/6000], Loss: 0.0233
Epoch [29/30], Batch [4800/6000], Loss: 0.0164
Epoch [29/30], Batch [4900/6000], Loss: 0.0151
Epoch [29/30], Batch [5000/6000], Loss: 0.0226
Epoch [29/30], Batch [5100/6000], Loss: 0.0190
Epoch [29/30], Batch [5200/6000], Loss: 0.0175
Epoch [29/30], Batch [5300/6000], Loss: 0.0153
Epoch [29/30], Batch [5400/6000], Loss: 0.0185
Epoch [29/30], Batch [5500/6000], Loss: 0.0331
Epoch [29/30], Batch [5600/6000], Loss: 0.0196
Epoch [29/30], Batch [5700/6000], Loss: 0.1100
Epoch [29/30], Batch [5800/6000], Loss: 0.0158
Epoch [29/30], Batch [5900/6000], Loss: 0.0176
Epoch [29/30], Loss: 0.0279
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0201
Epoch [30/30], Batch [100/6000], Loss: 0.0182
Epoch [30/30], Batch [200/6000], Loss: 0.0169
Epoch [30/30], Batch [300/6000], Loss: 0.0205
Epoch [30/30], Batch [400/6000], Loss: 0.0184
Epoch [30/30], Batch [500/6000], Loss: 0.0166
Epoch [30/30], Batch [600/6000], Loss: 0.0154
Epoch [30/30], Batch [700/6000], Loss: 0.0167
Epoch [30/30], Batch [800/6000], Loss: 0.0177
Epoch [30/30], Batch [900/6000], Loss: 0.0194
Epoch [30/30], Batch [1000/6000], Loss: 0.0199
Epoch [30/30], Batch [1100/6000], Loss: 0.0154
Epoch [30/30], Batch [1200/6000], Loss: 0.0197
Epoch [30/30], Batch [1300/6000], Loss: 0.0184
Epoch [30/30], Batch [1400/6000], Loss: 0.0196
Epoch [30/30], Batch [1500/6000], Loss: 0.0151
Epoch [30/30], Batch [1600/6000], Loss: 0.0176
Epoch [30/30], Batch [1700/6000], Loss: 0.0197
Epoch [30/30], Batch [1800/6000], Loss: 0.0267
Epoch [30/30], Batch [1900/6000], Loss: 0.0189
Epoch [30/30], Batch [2000/6000], Loss: 0.0125
Epoch [30/30], Batch [2100/6000], Loss: 0.0197
Epoch [30/30], Batch [2200/6000], Loss: 0.0171
Epoch [30/30], Batch [2300/6000], Loss: 0.0203
Epoch [30/30], Batch [2400/6000], Loss: 0.0140
Epoch [30/30], Batch [2500/6000], Loss: 0.0194
Epoch [30/30], Batch [2600/6000], Loss: 0.0199
Epoch [30/30], Batch [2700/6000], Loss: 0.0484
Epoch [30/30], Batch [2800/6000], Loss: 0.0175
Epoch [30/30], Batch [2900/6000], Loss: 0.0191
Epoch [30/30], Batch [3000/6000], Loss: 0.0157
Epoch [30/30], Batch [3100/6000], Loss: 0.0210
Epoch [30/30], Batch [3200/6000], Loss: 0.0174
Epoch [30/30], Batch [3300/6000], Loss: 0.0137
Epoch [30/30], Batch [3400/6000], Loss: 0.0201
Epoch [30/30], Batch [3500/6000], Loss: 0.0186
Epoch [30/30], Batch [3600/6000], Loss: 0.0190
Epoch [30/30], Batch [3700/6000], Loss: 0.0190
Epoch [30/30], Batch [3800/6000], Loss: 0.0158
Epoch [30/30], Batch [3900/6000], Loss: 0.0169
Epoch [30/30], Batch [4000/6000], Loss: 0.0186
Epoch [30/30], Batch [4100/6000], Loss: 0.0189
Epoch [30/30], Batch [4200/6000], Loss: 0.0197
Epoch [30/30], Batch [4300/6000], Loss: 0.0162
Epoch [30/30], Batch [4400/6000], Loss: 0.0208
Epoch [30/30], Batch [4500/6000], Loss: 0.0176
Epoch [30/30], Batch [4600/6000], Loss: 0.0162
Epoch [30/30], Batch [4700/6000], Loss: 0.0153
Epoch [30/30], Batch [4800/6000], Loss: 0.0192
Epoch [30/30], Batch [4900/6000], Loss: 0.0181
Epoch [30/30], Batch [5000/6000], Loss: 0.0225
Epoch [30/30], Batch [5100/6000], Loss: 0.0171
Epoch [30/30], Batch [5200/6000], Loss: 0.0192
Epoch [30/30], Batch [5300/6000], Loss: 0.0188
Epoch [30/30], Batch [5400/6000], Loss: 0.0425
Epoch [30/30], Batch [5500/6000], Loss: 0.0193
Epoch [30/30], Batch [5600/6000], Loss: 0.0157
Epoch [30/30], Batch [5700/6000], Loss: 0.0177
Epoch [30/30], Batch [5800/6000], Loss: 0.0186
Epoch [30/30], Batch [5900/6000], Loss: 0.0124
Epoch [30/30], Loss: 0.0266
Visualization saved to figures/visualization_0.png
Test Loss: 0.1011, Accuracy: 98.25%
Reconstruction visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0.    0.007 0.68  0.    0.002 0.    0.    0.309 0.    0.003]]
Adversarial Training Loop 1/300:
  Label Loss: 0.3938
  Image Loss: 0.0224
  Total Loss: 3.9605
  Image grad max: 3.152604579925537
  Output probs: [[0.    0.006 0.398 0.003 0.019 0.    0.    0.46  0.047 0.067]]
Adversarial Training Loop 2/300:
  Label Loss: 0.1223
  Image Loss: 0.0227
  Total Loss: 1.2453
  Image grad max: 1.9452201128005981
  Output probs: [[0.    0.001 0.048 0.003 0.013 0.    0.    0.051 0.821 0.064]]
Adversarial Training Loop 3/300:
  Label Loss: 0.0898
  Image Loss: 0.0231
  Total Loss: 0.9209
  Image grad max: 2.308171272277832
  Output probs: [[0.    0.    0.013 0.001 0.012 0.    0.    0.035 0.892 0.047]]
Adversarial Training Loop 4/300:
  Label Loss: 0.1040
  Image Loss: 0.0232
  Total Loss: 1.0631
  Image grad max: 2.685396671295166
  Output probs: [[0.    0.001 0.015 0.002 0.044 0.    0.    0.202 0.624 0.111]]
Adversarial Training Loop 5/300:
  Label Loss: 0.0343
  Image Loss: 0.0232
  Total Loss: 0.3659
  Image grad max: 1.2907634973526
  Output probs: [[0.    0.002 0.009 0.001 0.071 0.    0.    0.696 0.123 0.097]]
Adversarial Training Loop 6/300:
  Label Loss: 0.0536
  Image Loss: 0.0232
  Total Loss: 0.5587
  Image grad max: 2.2379584312438965
  Output probs: [[0.    0.002 0.006 0.001 0.065 0.    0.    0.79  0.085 0.051]]
Adversarial Training Loop 7/300:
  Label Loss: 0.0656
  Image Loss: 0.0233
  Total Loss: 0.6792
  Image grad max: 2.7488558292388916
  Output probs: [[0.    0.002 0.006 0.001 0.065 0.    0.    0.631 0.26  0.035]]
Adversarial Training Loop 8/300:
  Label Loss: 0.0211
  Image Loss: 0.0233
  Total Loss: 0.2341
  Image grad max: 1.47007155418396
  Output probs: [[0.    0.001 0.004 0.001 0.034 0.    0.    0.225 0.721 0.014]]
Adversarial Training Loop 9/300:
  Label Loss: 0.0216
  Image Loss: 0.0235
  Total Loss: 0.2391
  Image grad max: 1.7211377620697021
  Output probs: [[0.    0.001 0.003 0.    0.02  0.    0.    0.128 0.842 0.006]]
Adversarial Training Loop 10/300:
  Label Loss: 0.0422
  Image Loss: 0.0235
  Total Loss: 0.4451
  Image grad max: 2.527364730834961
  Output probs: [[0.    0.001 0.003 0.    0.028 0.    0.    0.232 0.731 0.005]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0195
  Image Loss: 0.0235
  Total Loss: 0.2182
  Image grad max: 1.7691715955734253
  Output probs: [[0.    0.002 0.003 0.    0.041 0.    0.    0.557 0.391 0.005]]
Adversarial Training Loop 12/300:
  Label Loss: 0.0069
  Image Loss: 0.0235
  Total Loss: 0.0927
  Image grad max: 0.6769750118255615
  Output probs: [[0.    0.002 0.003 0.    0.039 0.    0.    0.74  0.213 0.003]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0232
  Image Loss: 0.0236
  Total Loss: 0.2554
  Image grad max: 2.0248160362243652
  Output probs: [[0.    0.002 0.003 0.    0.035 0.    0.    0.712 0.246 0.002]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0178
  Image Loss: 0.0236
  Total Loss: 0.2017
  Image grad max: 1.7807598114013672
  Output probs: [[0.    0.002 0.003 0.    0.028 0.    0.    0.502 0.463 0.002]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0036
  Image Loss: 0.0236
  Total Loss: 0.0595
  Image grad max: 0.18144099414348602
  Output probs: [[0.    0.001 0.002 0.    0.018 0.    0.    0.29  0.687 0.001]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0114
  Image Loss: 0.0237
  Total Loss: 0.1372
  Image grad max: 1.4168450832366943
  Output probs: [[0.    0.001 0.002 0.    0.015 0.    0.    0.259 0.722 0.001]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0146
  Image Loss: 0.0237
  Total Loss: 0.1693
  Image grad max: 1.6568281650543213
  Output probs: [[0.    0.002 0.002 0.    0.018 0.    0.    0.384 0.593 0.001]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0046
  Image Loss: 0.0237
  Total Loss: 0.0697
  Image grad max: 0.7403356432914734
  Output probs: [[0.    0.002 0.002 0.    0.02  0.    0.    0.583 0.392 0.001]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0045
  Image Loss: 0.0237
  Total Loss: 0.0689
  Image grad max: 0.7305381894111633
  Output probs: [[0.    0.002 0.002 0.    0.019 0.    0.    0.668 0.308 0.001]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0098
  Image Loss: 0.0237
  Total Loss: 0.1214
  Image grad max: 1.3544422388076782
  Output probs: [[0.    0.002 0.002 0.    0.017 0.    0.    0.613 0.365 0.   ]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0056
  Image Loss: 0.0238
  Total Loss: 0.0796
  Image grad max: 0.9370996356010437
  Output probs: [[0.    0.002 0.002 0.    0.014 0.    0.    0.459 0.523 0.   ]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0021
  Image Loss: 0.0238
  Total Loss: 0.0446
  Image grad max: 0.21675628423690796
  Output probs: [[0.    0.002 0.002 0.    0.011 0.    0.    0.348 0.637 0.   ]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0060
  Image Loss: 0.0238
  Total Loss: 0.0836
  Image grad max: 1.0315781831741333
  Output probs: [[0.    0.002 0.002 0.    0.011 0.    0.    0.359 0.627 0.   ]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0053
  Image Loss: 0.0238
  Total Loss: 0.0767
  Image grad max: 0.9603934288024902
  Output probs: [[0.    0.002 0.002 0.    0.011 0.    0.    0.469 0.516 0.   ]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0017
  Image Loss: 0.0238
  Total Loss: 0.0408
  Image grad max: 0.15752507746219635
  Output probs: [[0.    0.002 0.002 0.    0.012 0.    0.    0.58  0.404 0.   ]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0033
  Image Loss: 0.0239
  Total Loss: 0.0565
  Image grad max: 0.6576316952705383
  Output probs: [[0.    0.002 0.002 0.    0.011 0.    0.    0.603 0.381 0.   ]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0042
  Image Loss: 0.0239
  Total Loss: 0.0660
  Image grad max: 0.8297265768051147
  Output probs: [[0.    0.002 0.002 0.    0.01  0.    0.    0.536 0.45  0.   ]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0018
  Image Loss: 0.0239
  Total Loss: 0.0423
  Image grad max: 0.3272705078125
  Output probs: [[0.    0.002 0.002 0.    0.009 0.    0.    0.436 0.551 0.   ]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0020
  Image Loss: 0.0239
  Total Loss: 0.0434
  Image grad max: 0.40779006481170654
  Output probs: [[0.    0.002 0.002 0.    0.008 0.    0.    0.396 0.593 0.   ]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0032
  Image Loss: 0.0239
  Total Loss: 0.0557
  Image grad max: 0.7044326663017273
  Output probs: [[0.    0.002 0.002 0.    0.008 0.    0.    0.438 0.55  0.   ]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0018
  Image Loss: 0.0239
  Total Loss: 0.0422
  Image grad max: 0.40065425634384155
  Output probs: [[0.    0.002 0.002 0.    0.008 0.    0.    0.52  0.467 0.   ]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0014
  Image Loss: 0.0239
  Total Loss: 0.0377
  Image grad max: 0.20389801263809204
  Output probs: [[0.    0.002 0.002 0.    0.008 0.    0.    0.566 0.422 0.   ]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0023
  Image Loss: 0.0239
  Total Loss: 0.0469
  Image grad max: 0.5336719155311584
  Output probs: [[0.    0.002 0.002 0.    0.008 0.    0.    0.545 0.444 0.   ]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0017
  Image Loss: 0.0239
  Total Loss: 0.0410
  Image grad max: 0.37823376059532166
  Output probs: [[0.    0.002 0.002 0.    0.007 0.    0.    0.479 0.51  0.   ]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0011
  Image Loss: 0.0239
  Total Loss: 0.0353
  Image grad max: 0.10463714599609375
  Output probs: [[0.    0.002 0.002 0.    0.006 0.    0.    0.434 0.556 0.   ]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0018
  Image Loss: 0.0240
  Total Loss: 0.0418
  Image grad max: 0.4382389485836029
  Output probs: [[0.    0.002 0.002 0.    0.006 0.    0.    0.446 0.544 0.   ]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0015
  Image Loss: 0.0240
  Total Loss: 0.0388
  Image grad max: 0.34932681918144226
  Output probs: [[0.    0.002 0.002 0.    0.006 0.    0.    0.497 0.492 0.   ]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0010
  Image Loss: 0.0240
  Total Loss: 0.0342
  Image grad max: 0.029818067327141762
  Output probs: [[0.    0.002 0.002 0.    0.006 0.    0.    0.537 0.453 0.   ]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0014
  Image Loss: 0.0240
  Total Loss: 0.0378
  Image grad max: 0.3123306632041931
  Output probs: [[0.    0.002 0.002 0.    0.006 0.    0.    0.533 0.457 0.   ]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0013
  Image Loss: 0.0240
  Total Loss: 0.0369
  Image grad max: 0.2826455533504486
  Output probs: [[0.    0.002 0.002 0.    0.006 0.    0.    0.493 0.497 0.   ]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0010
  Image Loss: 0.0240
  Total Loss: 0.0335
  Image grad max: 0.009623418562114239
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.459 0.533 0.   ]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0012
  Image Loss: 0.0240
  Total Loss: 0.0358
  Image grad max: 0.2622729241847992
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.461 0.53  0.   ]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0011
  Image Loss: 0.0240
  Total Loss: 0.0353
  Image grad max: 0.24573193490505219
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.493 0.498 0.   ]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0009
  Image Loss: 0.0240
  Total Loss: 0.0330
  Image grad max: 0.013328155502676964
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.52  0.471 0.   ]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0010
  Image Loss: 0.0240
  Total Loss: 0.0342
  Image grad max: 0.18765442073345184
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.52  0.471 0.   ]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0010
  Image Loss: 0.0240
  Total Loss: 0.0341
  Image grad max: 0.18547534942626953
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.495 0.496 0.   ]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0009
  Image Loss: 0.0240
  Total Loss: 0.0325
  Image grad max: 0.011657781898975372
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.472 0.52  0.   ]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0009
  Image Loss: 0.0240
  Total Loss: 0.0334
  Image grad max: 0.1687777042388916
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.473 0.519 0.   ]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0009
  Image Loss: 0.0240
  Total Loss: 0.0331
  Image grad max: 0.15926505625247955
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.494 0.498 0.   ]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0008
  Image Loss: 0.0240
  Total Loss: 0.0321
  Image grad max: 0.008184904232621193
  Output probs: [[0.    0.002 0.002 0.    0.005 0.    0.    0.512 0.48  0.   ]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0009
  Image Loss: 0.0240
  Total Loss: 0.0327
  Image grad max: 0.12432820349931717
  Output probs: [[0.    0.002 0.002 0.    0.004 0.    0.    0.511 0.481 0.   ]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0008
  Image Loss: 0.0240
  Total Loss: 0.0325
  Image grad max: 0.1138700470328331
  Output probs: [[0.    0.002 0.002 0.    0.004 0.    0.    0.493 0.499 0.   ]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0008
  Image Loss: 0.0240
  Total Loss: 0.0318
  Image grad max: 0.014508849941194057
  Output probs: [[0.    0.002 0.002 0.    0.004 0.    0.    0.479 0.513 0.   ]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0008
  Image Loss: 0.0240
  Total Loss: 0.0322
  Image grad max: 0.11996472626924515
  Output probs: [[0.    0.002 0.002 0.    0.004 0.    0.    0.482 0.51  0.   ]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0008
  Image Loss: 0.0240
  Total Loss: 0.0319
  Image grad max: 0.09715218096971512
  Output probs: [[0.    0.002 0.002 0.    0.004 0.    0.    0.497 0.495 0.   ]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0315
  Image grad max: 0.016779787838459015
  Output probs: [[0.    0.002 0.002 0.    0.004 0.    0.    0.508 0.485 0.   ]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0008
  Image Loss: 0.0240
  Total Loss: 0.0318
  Image grad max: 0.08957205712795258
  Output probs: [[0.    0.002 0.002 0.    0.004 0.    0.    0.504 0.489 0.   ]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0315
  Image grad max: 0.062072694301605225
  Output probs: [[0.    0.002 0.002 0.    0.004 0.    0.    0.491 0.502 0.   ]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0313
  Image grad max: 0.033867549151182175
  Output probs: [[0.    0.001 0.002 0.    0.004 0.    0.    0.484 0.509 0.   ]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0314
  Image grad max: 0.08925388008356094
  Output probs: [[0.    0.001 0.002 0.    0.004 0.    0.    0.489 0.504 0.   ]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0311
  Image grad max: 0.05158492550253868
  Output probs: [[0.    0.001 0.002 0.    0.004 0.    0.    0.5   0.493 0.   ]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0310
  Image grad max: 0.029886558651924133
  Output probs: [[0.    0.001 0.002 0.    0.004 0.    0.    0.504 0.489 0.   ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0311
  Image grad max: 0.06396631896495819
  Output probs: [[0.    0.001 0.002 0.    0.004 0.    0.    0.499 0.495 0.   ]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0309
  Image grad max: 0.021315673366189003
  Output probs: [[0.    0.001 0.002 0.    0.004 0.    0.    0.49  0.504 0.   ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0308
  Image grad max: 0.046837326139211655
  Output probs: [[0.    0.001 0.002 0.    0.003 0.    0.    0.488 0.506 0.   ]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0308
  Image grad max: 0.061946164816617966
  Output probs: [[0.    0.001 0.002 0.    0.003 0.    0.    0.494 0.499 0.   ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0306
  Image grad max: 0.014179248362779617
  Output probs: [[0.    0.001 0.002 0.    0.003 0.    0.    0.501 0.492 0.   ]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0306
  Image grad max: 0.03747883066534996
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.501 0.493 0.   ]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0007
  Image Loss: 0.0240
  Total Loss: 0.0306
  Image grad max: 0.03603816777467728
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.495 0.499 0.   ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0304
  Image grad max: 0.011499246582388878
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.49  0.504 0.   ]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0304
  Image grad max: 0.04738562926650047
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.492 0.502 0.   ]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0303
  Image grad max: 0.03199433535337448
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.498 0.496 0.   ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0303
  Image grad max: 0.01490271557122469
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.501 0.493 0.   ]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0302
  Image grad max: 0.0316941998898983
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.497 0.497 0.   ]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0301
  Image grad max: 0.011908788233995438
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.492 0.502 0.   ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0301
  Image grad max: 0.029278498142957687
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.492 0.502 0.   ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0300
  Image grad max: 0.0337924100458622
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.496 0.498 0.   ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0300
  Image grad max: 0.005496346857398748
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.499 0.495 0.   ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0299
  Image grad max: 0.02195560932159424
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.498 0.496 0.   ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0299
  Image grad max: 0.015210932120680809
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.494 0.5   0.   ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0298
  Image grad max: 0.01654321327805519
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.493 0.502 0.   ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0298
  Image grad max: 0.029052764177322388
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.495 0.499 0.   ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0297
  Image grad max: 0.010741853155195713
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.498 0.496 0.   ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0297
  Image grad max: 0.014823919162154198
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.498 0.496 0.   ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0296
  Image grad max: 0.014501720666885376
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.495 0.499 0.   ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0296
  Image grad max: 0.009247862733900547
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.494 0.501 0.   ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0006
  Image Loss: 0.0240
  Total Loss: 0.0295
  Image grad max: 0.023173101246356964
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.495 0.499 0.   ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0005
  Image Loss: 0.0240
  Total Loss: 0.0295
  Image grad max: 0.012284099124372005
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.498 0.497 0.   ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0005
  Image Loss: 0.0240
  Total Loss: 0.0294
  Image grad max: 0.011040106415748596
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.498 0.497 0.   ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0005
  Image Loss: 0.0240
  Total Loss: 0.0294
  Image grad max: 0.012631195597350597
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0005
  Image Loss: 0.0240
  Total Loss: 0.0293
  Image grad max: 0.006254778243601322
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.494 0.5   0.   ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0005
  Image Loss: 0.0240
  Total Loss: 0.0293
  Image grad max: 0.018636269494891167
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.495 0.499 0.   ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0005
  Image Loss: 0.0240
  Total Loss: 0.0292
  Image grad max: 0.011217896826565266
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.497 0.497 0.   ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0005
  Image Loss: 0.0240
  Total Loss: 0.0292
  Image grad max: 0.008924216963350773
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.498 0.497 0.   ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0005
  Image Loss: 0.0240
  Total Loss: 0.0292
  Image grad max: 0.01049993745982647
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0291
  Image grad max: 0.005686811171472073
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.495 0.5   0.   ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0291
  Image grad max: 0.015361002646386623
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0290
  Image grad max: 0.009033412672579288
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0290
  Image grad max: 0.007896496914327145
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.498 0.497 0.   ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0290
  Image grad max: 0.008494985289871693
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0289
  Image grad max: 0.006312093697488308
  Output probs: [[0.    0.001 0.001 0.    0.003 0.    0.    0.495 0.5   0.   ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0289
  Image grad max: 0.012847096659243107
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0288
  Image grad max: 0.006498595234006643
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0288
  Image grad max: 0.0073135243728756905
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0288
  Image grad max: 0.006589486729353666
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0287
  Image grad max: 0.007237412501126528
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.496 0.5   0.   ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0287
  Image grad max: 0.010555396787822247
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0287
  Image grad max: 0.004561366979032755
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0286
  Image grad max: 0.006718564312905073
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0286
  Image grad max: 0.00474484683945775
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0286
  Image grad max: 0.00789023470133543
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0285
  Image grad max: 0.008180176839232445
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0285
  Image grad max: 0.003992853220552206
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0285
  Image grad max: 0.005815928801894188
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0284
  Image grad max: 0.004120797850191593
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0005
  Image Loss: 0.0239
  Total Loss: 0.0284
  Image grad max: 0.007752102799713612
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0004
  Image Loss: 0.0239
  Total Loss: 0.0284
  Image grad max: 0.005709851626306772
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0004
  Image Loss: 0.0239
  Total Loss: 0.0283
  Image grad max: 0.004573869053274393
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0004
  Image Loss: 0.0239
  Total Loss: 0.0283
  Image grad max: 0.00463678315281868
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0283
  Image grad max: 0.005010172259062529
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.496 0.499 0.   ]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0282
  Image grad max: 0.006813633255660534
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0282
  Image grad max: 0.00405639735981822
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.498 0.   ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0282
  Image grad max: 0.004559521563351154
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0281
  Image grad max: 0.003797720419242978
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0281
  Image grad max: 0.0058627743273973465
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0281
  Image grad max: 0.005099338013678789
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0280
  Image grad max: 0.0036320246290415525
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0280
  Image grad max: 0.0036864883732050657
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0280
  Image grad max: 0.0044279987923800945
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0279
  Image grad max: 0.005317288916558027
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0279
  Image grad max: 0.003752992255613208
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0279
  Image grad max: 0.003579311305657029
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0279
  Image grad max: 0.0037900700699537992
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0278
  Image grad max: 0.004846254363656044
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0278
  Image grad max: 0.004015729296952486
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0278
  Image grad max: 0.003478715429082513
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0004
  Image Loss: 0.0238
  Total Loss: 0.0277
  Image grad max: 0.003543226746842265
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0277
  Image grad max: 0.00419903127476573
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0277
  Image grad max: 0.004059772472828627
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0277
  Image grad max: 0.0035018406342715025
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0276
  Image grad max: 0.0034364250022917986
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0276
  Image grad max: 0.003781844163313508
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0276
  Image grad max: 0.003997362684458494
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0276
  Image grad max: 0.0035805855877697468
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0275
  Image grad max: 0.003368512960150838
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0275
  Image grad max: 0.0035640080459415913
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0275
  Image grad max: 0.003835858777165413
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0275
  Image grad max: 0.0036129180807620287
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0274
  Image grad max: 0.0033214360009878874
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0274
  Image grad max: 0.003413732396438718
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0274
  Image grad max: 0.0036832313053309917
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0004
  Image Loss: 0.0237
  Total Loss: 0.0274
  Image grad max: 0.0035917507484555244
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0273
  Image grad max: 0.003305983729660511
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0273
  Image grad max: 0.0033130275551229715
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0273
  Image grad max: 0.0035514880437403917
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0273
  Image grad max: 0.0035800456535071135
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0272
  Image grad max: 0.0033353609032928944
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0272
  Image grad max: 0.0032212594524025917
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0272
  Image grad max: 0.0034452308900654316
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0272
  Image grad max: 0.0035265202168375254
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0271
  Image grad max: 0.003280877834185958
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0271
  Image grad max: 0.003155536949634552
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0004
  Image Loss: 0.0236
  Total Loss: 0.0271
  Image grad max: 0.003337656380608678
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0003
  Image Loss: 0.0236
  Total Loss: 0.0271
  Image grad max: 0.0034377016127109528
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0003
  Image Loss: 0.0236
  Total Loss: 0.0270
  Image grad max: 0.003244092920795083
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0003
  Image Loss: 0.0236
  Total Loss: 0.0270
  Image grad max: 0.0030982457101345062
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0003
  Image Loss: 0.0236
  Total Loss: 0.0270
  Image grad max: 0.0032590804621577263
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.497 0.499 0.   ]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0270
  Image grad max: 0.003381256014108658
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0269
  Image grad max: 0.0032041200902312994
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0269
  Image grad max: 0.00303651113063097
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0269
  Image grad max: 0.00318546942435205
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0269
  Image grad max: 0.0033139409497380257
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0269
  Image grad max: 0.003159384708851576
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0268
  Image grad max: 0.0030126203782856464
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0268
  Image grad max: 0.003122184192761779
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0268
  Image grad max: 0.0032380742486566305
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0268
  Image grad max: 0.0031024240888655186
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0267
  Image grad max: 0.0029807814862579107
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0267
  Image grad max: 0.0030830418691039085
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0003
  Image Loss: 0.0235
  Total Loss: 0.0267
  Image grad max: 0.0031586948316544294
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0267
  Image grad max: 0.003040342591702938
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0267
  Image grad max: 0.0029575973749160767
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0266
  Image grad max: 0.0030364603735506535
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0266
  Image grad max: 0.0030871389899402857
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0266
  Image grad max: 0.002991674467921257
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0266
  Image grad max: 0.002921165432780981
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0266
  Image grad max: 0.0029949203599244356
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0265
  Image grad max: 0.0030246872920542955
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0265
  Image grad max: 0.0029334481805562973
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0265
  Image grad max: 0.0028924988582730293
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0265
  Image grad max: 0.0029564909636974335
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0264
  Image grad max: 0.0029577293898910284
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0003
  Image Loss: 0.0234
  Total Loss: 0.0264
  Image grad max: 0.0028851102106273174
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0264
  Image grad max: 0.0028647619765251875
  Output probs: [[0.    0.001 0.001 0.    0.002 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0264
  Image grad max: 0.002911912277340889
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0264
  Image grad max: 0.002938252640888095
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0263
  Image grad max: 0.0029252476524561644
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0263
  Image grad max: 0.00280431448481977
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0263
  Image grad max: 0.0028035547584295273
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0263
  Image grad max: 0.0028991568833589554
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0263
  Image grad max: 0.0028593302704393864
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0262
  Image grad max: 0.0027577655855566263
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0262
  Image grad max: 0.0027883253060281277
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0262
  Image grad max: 0.0028382213786244392
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0003
  Image Loss: 0.0233
  Total Loss: 0.0262
  Image grad max: 0.0027839080430567265
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0262
  Image grad max: 0.002735250163823366
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0261
  Image grad max: 0.0027744693215936422
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0261
  Image grad max: 0.0027590221725404263
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0261
  Image grad max: 0.0027333423495292664
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0261
  Image grad max: 0.0027378720697015524
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0261
  Image grad max: 0.0027321968227624893
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0260
  Image grad max: 0.0027161200996488333
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0260
  Image grad max: 0.0026688864454627037
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0260
  Image grad max: 0.002702867379412055
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0260
  Image grad max: 0.0027494977694004774
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0003
  Image Loss: 0.0232
  Total Loss: 0.0260
  Image grad max: 0.002669400069862604
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0260
  Image grad max: 0.002656512428075075
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0259
  Image grad max: 0.0026935350615531206
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0259
  Image grad max: 0.002687853528186679
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0259
  Image grad max: 0.002646272536367178
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0259
  Image grad max: 0.0026457272469997406
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0259
  Image grad max: 0.00267150835134089
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0258
  Image grad max: 0.0026467826683074236
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0258
  Image grad max: 0.0026222725864499807
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0258
  Image grad max: 0.0026426068507134914
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0258
  Image grad max: 0.0026790981646627188
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0003
  Image Loss: 0.0231
  Total Loss: 0.0258
  Image grad max: 0.0026144771836698055
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0257
  Image grad max: 0.0026050563901662827
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0257
  Image grad max: 0.0026366515085101128
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0257
  Image grad max: 0.0026247967034578323
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0257
  Image grad max: 0.002590116113424301
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0257
  Image grad max: 0.0026044039987027645
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0257
  Image grad max: 0.0026213647797703743
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0256
  Image grad max: 0.0025917766615748405
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0256
  Image grad max: 0.0025789516512304544
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0256
  Image grad max: 0.0026000288780778646
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0256
  Image grad max: 0.0025945003144443035
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0003
  Image Loss: 0.0230
  Total Loss: 0.0256
  Image grad max: 0.002568989060819149
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0255
  Image grad max: 0.002573389559984207
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0255
  Image grad max: 0.0025872066617012024
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0255
  Image grad max: 0.002567751333117485
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0255
  Image grad max: 0.0025522783398628235
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0255
  Image grad max: 0.0025686798617243767
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0255
  Image grad max: 0.00256755156442523
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0254
  Image grad max: 0.0025418722070753574
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0254
  Image grad max: 0.0025470727123320103
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0254
  Image grad max: 0.0025599529035389423
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0254
  Image grad max: 0.0025374935939908028
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0003
  Image Loss: 0.0229
  Total Loss: 0.0254
  Image grad max: 0.0025270404294133186
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0003
  Image Loss: 0.0228
  Total Loss: 0.0253
  Image grad max: 0.002541932975873351
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0253
  Image grad max: 0.0025353352539241314
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0253
  Image grad max: 0.002535523846745491
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0253
  Image grad max: 0.0025266194716095924
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0253
  Image grad max: 0.0025150510482490063
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0253
  Image grad max: 0.0025136666372418404
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0252
  Image grad max: 0.0025148794520646334
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0252
  Image grad max: 0.002507557161152363
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0252
  Image grad max: 0.002498861402273178
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0002
  Image Loss: 0.0228
  Total Loss: 0.0252
  Image grad max: 0.002500767819583416
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0252
  Image grad max: 0.0024993191473186016
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0252
  Image grad max: 0.0024901642464101315
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0251
  Image grad max: 0.0024855597876012325
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0251
  Image grad max: 0.002488555386662483
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0251
  Image grad max: 0.0024813483469188213
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0251
  Image grad max: 0.002473267260938883
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0251
  Image grad max: 0.0024755962658673525
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0250
  Image grad max: 0.0024721708614379168
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0250
  Image grad max: 0.0024635042063891888
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0002
  Image Loss: 0.0227
  Total Loss: 0.0250
  Image grad max: 0.002460925839841366
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0250
  Image grad max: 0.002462735865265131
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0250
  Image grad max: 0.0024546815548092127
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0250
  Image grad max: 0.002448461949825287
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0249
  Image grad max: 0.00245048850774765
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0249
  Image grad max: 0.002446152735501528
  Output probs: [[0.    0.001 0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0249
  Image grad max: 0.002438521245494485
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0249
  Image grad max: 0.002436581766232848
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0249
  Image grad max: 0.002437284681946039
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0249
  Image grad max: 0.0024302033707499504
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0002
  Image Loss: 0.0226
  Total Loss: 0.0248
  Image grad max: 0.0024153124541044235
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0248
  Image grad max: 0.002426745602861047
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0248
  Image grad max: 0.002422953024506569
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0248
  Image grad max: 0.0024110255762934685
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0248
  Image grad max: 0.0024258361663669348
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0248
  Image grad max: 0.002408399246633053
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0247
  Image grad max: 0.002395820105448365
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0247
  Image grad max: 0.00241225422360003
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0247
  Image grad max: 0.0024026704486459494
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0002
  Image Loss: 0.0225
  Total Loss: 0.0247
  Image grad max: 0.002397846430540085
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0247
  Image grad max: 0.0024076420813798904
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0247
  Image grad max: 0.0023812600411474705
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0246
  Image grad max: 0.00238251150585711
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0246
  Image grad max: 0.002396703464910388
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0246
  Image grad max: 0.0023821573704481125
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0246
  Image grad max: 0.0023703868500888348
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0246
  Image grad max: 0.0023808858823031187
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0246
  Image grad max: 0.0023741687182337046
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0245
  Image grad max: 0.002362076658755541
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0002
  Image Loss: 0.0224
  Total Loss: 0.0245
  Image grad max: 0.00236849975772202
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0002
  Image Loss: 0.0223
  Total Loss: 0.0245
  Image grad max: 0.002365082735195756
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.    0.498 0.499 0.   ]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0002
  Image Loss: 0.0223
  Total Loss: 0.0245
  Image grad max: 0.002354155294597149
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
Adversarial example training visualization saved to adversarial_figures/adversarial_testing.png
Target label was: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.5 0. ]]
