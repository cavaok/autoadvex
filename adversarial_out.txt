Epoch [1/30], Batch [0/6000], Loss: 2.4333
Epoch [1/30], Batch [100/6000], Loss: 1.4283
Epoch [1/30], Batch [200/6000], Loss: 0.7733
Epoch [1/30], Batch [300/6000], Loss: 0.4679
Epoch [1/30], Batch [400/6000], Loss: 1.4769
Epoch [1/30], Batch [500/6000], Loss: 0.3604
Epoch [1/30], Batch [600/6000], Loss: 0.4549
Epoch [1/30], Batch [700/6000], Loss: 0.7009
Epoch [1/30], Batch [800/6000], Loss: 0.7681
Epoch [1/30], Batch [900/6000], Loss: 0.1342
Epoch [1/30], Batch [1000/6000], Loss: 0.2810
Epoch [1/30], Batch [1100/6000], Loss: 0.3106
Epoch [1/30], Batch [1200/6000], Loss: 0.5463
Epoch [1/30], Batch [1300/6000], Loss: 0.2739
Epoch [1/30], Batch [1400/6000], Loss: 0.1064
Epoch [1/30], Batch [1500/6000], Loss: 0.1874
Epoch [1/30], Batch [1600/6000], Loss: 0.2083
Epoch [1/30], Batch [1700/6000], Loss: 0.7397
Epoch [1/30], Batch [1800/6000], Loss: 0.2857
Epoch [1/30], Batch [1900/6000], Loss: 0.2512
Epoch [1/30], Batch [2000/6000], Loss: 0.3378
Epoch [1/30], Batch [2100/6000], Loss: 1.5139
Epoch [1/30], Batch [2200/6000], Loss: 0.0618
Epoch [1/30], Batch [2300/6000], Loss: 1.2189
Epoch [1/30], Batch [2400/6000], Loss: 0.6585
Epoch [1/30], Batch [2500/6000], Loss: 0.1314
Epoch [1/30], Batch [2600/6000], Loss: 0.3023
Epoch [1/30], Batch [2700/6000], Loss: 0.1728
Epoch [1/30], Batch [2800/6000], Loss: 0.2660
Epoch [1/30], Batch [2900/6000], Loss: 0.7524
Epoch [1/30], Batch [3000/6000], Loss: 0.4195
Epoch [1/30], Batch [3100/6000], Loss: 0.1625
Epoch [1/30], Batch [3200/6000], Loss: 0.4423
Epoch [1/30], Batch [3300/6000], Loss: 0.1477
Epoch [1/30], Batch [3400/6000], Loss: 0.3385
Epoch [1/30], Batch [3500/6000], Loss: 0.3315
Epoch [1/30], Batch [3600/6000], Loss: 0.0587
Epoch [1/30], Batch [3700/6000], Loss: 0.6829
Epoch [1/30], Batch [3800/6000], Loss: 0.9447
Epoch [1/30], Batch [3900/6000], Loss: 0.4041
Epoch [1/30], Batch [4000/6000], Loss: 0.1748
Epoch [1/30], Batch [4100/6000], Loss: 0.0674
Epoch [1/30], Batch [4200/6000], Loss: 0.1540
Epoch [1/30], Batch [4300/6000], Loss: 0.1836
Epoch [1/30], Batch [4400/6000], Loss: 0.1323
Epoch [1/30], Batch [4500/6000], Loss: 0.1519
Epoch [1/30], Batch [4600/6000], Loss: 0.1082
Epoch [1/30], Batch [4700/6000], Loss: 0.3500
Epoch [1/30], Batch [4800/6000], Loss: 0.1816
Epoch [1/30], Batch [4900/6000], Loss: 0.2621
Epoch [1/30], Batch [5000/6000], Loss: 0.6944
Epoch [1/30], Batch [5100/6000], Loss: 0.1581
Epoch [1/30], Batch [5200/6000], Loss: 0.8609
Epoch [1/30], Batch [5300/6000], Loss: 0.1088
Epoch [1/30], Batch [5400/6000], Loss: 0.1014
Epoch [1/30], Batch [5500/6000], Loss: 0.1301
Epoch [1/30], Batch [5600/6000], Loss: 0.0655
Epoch [1/30], Batch [5700/6000], Loss: 0.0709
Epoch [1/30], Batch [5800/6000], Loss: 0.4179
Epoch [1/30], Batch [5900/6000], Loss: 0.2336
Epoch [1/30], Loss: 0.4044
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.1691
Epoch [2/30], Batch [100/6000], Loss: 0.0991
Epoch [2/30], Batch [200/6000], Loss: 0.4202
Epoch [2/30], Batch [300/6000], Loss: 0.4551
Epoch [2/30], Batch [400/6000], Loss: 0.5742
Epoch [2/30], Batch [500/6000], Loss: 0.2535
Epoch [2/30], Batch [600/6000], Loss: 0.1016
Epoch [2/30], Batch [700/6000], Loss: 0.1871
Epoch [2/30], Batch [800/6000], Loss: 0.3348
Epoch [2/30], Batch [900/6000], Loss: 0.0490
Epoch [2/30], Batch [1000/6000], Loss: 0.0630
Epoch [2/30], Batch [1100/6000], Loss: 0.4532
Epoch [2/30], Batch [1200/6000], Loss: 0.2483
Epoch [2/30], Batch [1300/6000], Loss: 0.2395
Epoch [2/30], Batch [1400/6000], Loss: 0.3998
Epoch [2/30], Batch [1500/6000], Loss: 0.1164
Epoch [2/30], Batch [1600/6000], Loss: 0.2660
Epoch [2/30], Batch [1700/6000], Loss: 0.0943
Epoch [2/30], Batch [1800/6000], Loss: 0.1616
Epoch [2/30], Batch [1900/6000], Loss: 0.0661
Epoch [2/30], Batch [2000/6000], Loss: 0.0527
Epoch [2/30], Batch [2100/6000], Loss: 0.0446
Epoch [2/30], Batch [2200/6000], Loss: 0.4819
Epoch [2/30], Batch [2300/6000], Loss: 0.2714
Epoch [2/30], Batch [2400/6000], Loss: 0.1314
Epoch [2/30], Batch [2500/6000], Loss: 0.1512
Epoch [2/30], Batch [2600/6000], Loss: 0.1035
Epoch [2/30], Batch [2700/6000], Loss: 0.0489
Epoch [2/30], Batch [2800/6000], Loss: 0.2386
Epoch [2/30], Batch [2900/6000], Loss: 0.2393
Epoch [2/30], Batch [3000/6000], Loss: 0.0728
Epoch [2/30], Batch [3100/6000], Loss: 0.5967
Epoch [2/30], Batch [3200/6000], Loss: 0.3381
Epoch [2/30], Batch [3300/6000], Loss: 0.1081
Epoch [2/30], Batch [3400/6000], Loss: 0.0523
Epoch [2/30], Batch [3500/6000], Loss: 0.0796
Epoch [2/30], Batch [3600/6000], Loss: 0.7345
Epoch [2/30], Batch [3700/6000], Loss: 0.0498
Epoch [2/30], Batch [3800/6000], Loss: 0.0622
Epoch [2/30], Batch [3900/6000], Loss: 0.1785
Epoch [2/30], Batch [4000/6000], Loss: 0.1464
Epoch [2/30], Batch [4100/6000], Loss: 0.2422
Epoch [2/30], Batch [4200/6000], Loss: 0.1506
Epoch [2/30], Batch [4300/6000], Loss: 0.5340
Epoch [2/30], Batch [4400/6000], Loss: 0.4448
Epoch [2/30], Batch [4500/6000], Loss: 0.0592
Epoch [2/30], Batch [4600/6000], Loss: 0.1815
Epoch [2/30], Batch [4700/6000], Loss: 0.0884
Epoch [2/30], Batch [4800/6000], Loss: 0.2641
Epoch [2/30], Batch [4900/6000], Loss: 0.0905
Epoch [2/30], Batch [5000/6000], Loss: 0.0881
Epoch [2/30], Batch [5100/6000], Loss: 0.1215
Epoch [2/30], Batch [5200/6000], Loss: 0.1222
Epoch [2/30], Batch [5300/6000], Loss: 0.0668
Epoch [2/30], Batch [5400/6000], Loss: 0.0990
Epoch [2/30], Batch [5500/6000], Loss: 0.4373
Epoch [2/30], Batch [5600/6000], Loss: 0.0769
Epoch [2/30], Batch [5700/6000], Loss: 0.0711
Epoch [2/30], Batch [5800/6000], Loss: 0.3115
Epoch [2/30], Batch [5900/6000], Loss: 0.3145
Epoch [2/30], Loss: 0.2299
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.0570
Epoch [3/30], Batch [100/6000], Loss: 0.0391
Epoch [3/30], Batch [200/6000], Loss: 0.5772
Epoch [3/30], Batch [300/6000], Loss: 0.2766
Epoch [3/30], Batch [400/6000], Loss: 0.0796
Epoch [3/30], Batch [500/6000], Loss: 0.0646
Epoch [3/30], Batch [600/6000], Loss: 0.0418
Epoch [3/30], Batch [700/6000], Loss: 0.0499
Epoch [3/30], Batch [800/6000], Loss: 0.0658
Epoch [3/30], Batch [900/6000], Loss: 0.0447
Epoch [3/30], Batch [1000/6000], Loss: 0.3074
Epoch [3/30], Batch [1100/6000], Loss: 0.1114
Epoch [3/30], Batch [1200/6000], Loss: 1.1597
Epoch [3/30], Batch [1300/6000], Loss: 0.0854
Epoch [3/30], Batch [1400/6000], Loss: 0.0380
Epoch [3/30], Batch [1500/6000], Loss: 0.0767
Epoch [3/30], Batch [1600/6000], Loss: 0.1316
Epoch [3/30], Batch [1700/6000], Loss: 0.0594
Epoch [3/30], Batch [1800/6000], Loss: 0.0859
Epoch [3/30], Batch [1900/6000], Loss: 0.0489
Epoch [3/30], Batch [2000/6000], Loss: 0.0847
Epoch [3/30], Batch [2100/6000], Loss: 0.1104
Epoch [3/30], Batch [2200/6000], Loss: 0.1684
Epoch [3/30], Batch [2300/6000], Loss: 0.0480
Epoch [3/30], Batch [2400/6000], Loss: 0.3513
Epoch [3/30], Batch [2500/6000], Loss: 0.0401
Epoch [3/30], Batch [2600/6000], Loss: 0.1805
Epoch [3/30], Batch [2700/6000], Loss: 0.3807
Epoch [3/30], Batch [2800/6000], Loss: 0.1281
Epoch [3/30], Batch [2900/6000], Loss: 0.0342
Epoch [3/30], Batch [3000/6000], Loss: 0.2970
Epoch [3/30], Batch [3100/6000], Loss: 0.4633
Epoch [3/30], Batch [3200/6000], Loss: 0.1186
Epoch [3/30], Batch [3300/6000], Loss: 0.0488
Epoch [3/30], Batch [3400/6000], Loss: 0.3205
Epoch [3/30], Batch [3500/6000], Loss: 0.0723
Epoch [3/30], Batch [3600/6000], Loss: 0.1028
Epoch [3/30], Batch [3700/6000], Loss: 0.1660
Epoch [3/30], Batch [3800/6000], Loss: 0.1217
Epoch [3/30], Batch [3900/6000], Loss: 0.0962
Epoch [3/30], Batch [4000/6000], Loss: 0.0394
Epoch [3/30], Batch [4100/6000], Loss: 0.4452
Epoch [3/30], Batch [4200/6000], Loss: 0.1963
Epoch [3/30], Batch [4300/6000], Loss: 0.0448
Epoch [3/30], Batch [4400/6000], Loss: 0.0430
Epoch [3/30], Batch [4500/6000], Loss: 0.0679
Epoch [3/30], Batch [4600/6000], Loss: 0.0450
Epoch [3/30], Batch [4700/6000], Loss: 0.0839
Epoch [3/30], Batch [4800/6000], Loss: 0.2129
Epoch [3/30], Batch [4900/6000], Loss: 0.0548
Epoch [3/30], Batch [5000/6000], Loss: 0.0453
Epoch [3/30], Batch [5100/6000], Loss: 0.0498
Epoch [3/30], Batch [5200/6000], Loss: 0.2029
Epoch [3/30], Batch [5300/6000], Loss: 0.0356
Epoch [3/30], Batch [5400/6000], Loss: 0.0498
Epoch [3/30], Batch [5500/6000], Loss: 0.1966
Epoch [3/30], Batch [5600/6000], Loss: 0.0453
Epoch [3/30], Batch [5700/6000], Loss: 0.1169
Epoch [3/30], Batch [5800/6000], Loss: 0.2297
Epoch [3/30], Batch [5900/6000], Loss: 0.0679
Epoch [3/30], Loss: 0.1750
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.1120
Epoch [4/30], Batch [100/6000], Loss: 0.4265
Epoch [4/30], Batch [200/6000], Loss: 0.1197
Epoch [4/30], Batch [300/6000], Loss: 0.2046
Epoch [4/30], Batch [400/6000], Loss: 0.0626
Epoch [4/30], Batch [500/6000], Loss: 0.1052
Epoch [4/30], Batch [600/6000], Loss: 0.0480
Epoch [4/30], Batch [700/6000], Loss: 0.0416
Epoch [4/30], Batch [800/6000], Loss: 0.1549
Epoch [4/30], Batch [900/6000], Loss: 0.3164
Epoch [4/30], Batch [1000/6000], Loss: 0.1135
Epoch [4/30], Batch [1100/6000], Loss: 0.1125
Epoch [4/30], Batch [1200/6000], Loss: 0.0545
Epoch [4/30], Batch [1300/6000], Loss: 0.1320
Epoch [4/30], Batch [1400/6000], Loss: 0.0797
Epoch [4/30], Batch [1500/6000], Loss: 0.0347
Epoch [4/30], Batch [1600/6000], Loss: 0.0386
Epoch [4/30], Batch [1700/6000], Loss: 0.0575
Epoch [4/30], Batch [1800/6000], Loss: 0.2379
Epoch [4/30], Batch [1900/6000], Loss: 0.0491
Epoch [4/30], Batch [2000/6000], Loss: 0.0331
Epoch [4/30], Batch [2100/6000], Loss: 0.0410
Epoch [4/30], Batch [2200/6000], Loss: 0.2559
Epoch [4/30], Batch [2300/6000], Loss: 0.0453
Epoch [4/30], Batch [2400/6000], Loss: 0.0664
Epoch [4/30], Batch [2500/6000], Loss: 0.0449
Epoch [4/30], Batch [2600/6000], Loss: 0.0324
Epoch [4/30], Batch [2700/6000], Loss: 0.0846
Epoch [4/30], Batch [2800/6000], Loss: 0.5491
Epoch [4/30], Batch [2900/6000], Loss: 0.0639
Epoch [4/30], Batch [3000/6000], Loss: 0.0952
Epoch [4/30], Batch [3100/6000], Loss: 0.0484
Epoch [4/30], Batch [3200/6000], Loss: 0.0623
Epoch [4/30], Batch [3300/6000], Loss: 0.0341
Epoch [4/30], Batch [3400/6000], Loss: 0.0689
Epoch [4/30], Batch [3500/6000], Loss: 0.1243
Epoch [4/30], Batch [3600/6000], Loss: 0.2485
Epoch [4/30], Batch [3700/6000], Loss: 0.0297
Epoch [4/30], Batch [3800/6000], Loss: 0.0450
Epoch [4/30], Batch [3900/6000], Loss: 0.0547
Epoch [4/30], Batch [4000/6000], Loss: 0.0507
Epoch [4/30], Batch [4100/6000], Loss: 0.0415
Epoch [4/30], Batch [4200/6000], Loss: 0.1688
Epoch [4/30], Batch [4300/6000], Loss: 0.0388
Epoch [4/30], Batch [4400/6000], Loss: 0.0315
Epoch [4/30], Batch [4500/6000], Loss: 0.0323
Epoch [4/30], Batch [4600/6000], Loss: 0.0424
Epoch [4/30], Batch [4700/6000], Loss: 0.3488
Epoch [4/30], Batch [4800/6000], Loss: 0.1338
Epoch [4/30], Batch [4900/6000], Loss: 0.2595
Epoch [4/30], Batch [5000/6000], Loss: 0.0297
Epoch [4/30], Batch [5100/6000], Loss: 0.0380
Epoch [4/30], Batch [5200/6000], Loss: 0.7919
Epoch [4/30], Batch [5300/6000], Loss: 0.0283
Epoch [4/30], Batch [5400/6000], Loss: 0.0312
Epoch [4/30], Batch [5500/6000], Loss: 0.0484
Epoch [4/30], Batch [5600/6000], Loss: 0.0356
Epoch [4/30], Batch [5700/6000], Loss: 0.0411
Epoch [4/30], Batch [5800/6000], Loss: 0.2775
Epoch [4/30], Batch [5900/6000], Loss: 0.0382
Epoch [4/30], Loss: 0.1428
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.3595
Epoch [5/30], Batch [100/6000], Loss: 0.0292
Epoch [5/30], Batch [200/6000], Loss: 0.3283
Epoch [5/30], Batch [300/6000], Loss: 0.0600
Epoch [5/30], Batch [400/6000], Loss: 0.0292
Epoch [5/30], Batch [500/6000], Loss: 0.0868
Epoch [5/30], Batch [600/6000], Loss: 0.0368
Epoch [5/30], Batch [700/6000], Loss: 0.3430
Epoch [5/30], Batch [800/6000], Loss: 0.0475
Epoch [5/30], Batch [900/6000], Loss: 0.0307
Epoch [5/30], Batch [1000/6000], Loss: 0.0484
Epoch [5/30], Batch [1100/6000], Loss: 0.0448
Epoch [5/30], Batch [1200/6000], Loss: 0.0610
Epoch [5/30], Batch [1300/6000], Loss: 0.0761
Epoch [5/30], Batch [1400/6000], Loss: 0.0420
Epoch [5/30], Batch [1500/6000], Loss: 0.0963
Epoch [5/30], Batch [1600/6000], Loss: 0.0723
Epoch [5/30], Batch [1700/6000], Loss: 0.0447
Epoch [5/30], Batch [1800/6000], Loss: 0.0834
Epoch [5/30], Batch [1900/6000], Loss: 0.0308
Epoch [5/30], Batch [2000/6000], Loss: 0.0397
Epoch [5/30], Batch [2100/6000], Loss: 0.1447
Epoch [5/30], Batch [2200/6000], Loss: 0.0441
Epoch [5/30], Batch [2300/6000], Loss: 0.0954
Epoch [5/30], Batch [2400/6000], Loss: 0.0861
Epoch [5/30], Batch [2500/6000], Loss: 0.0590
Epoch [5/30], Batch [2600/6000], Loss: 0.0679
Epoch [5/30], Batch [2700/6000], Loss: 0.0321
Epoch [5/30], Batch [2800/6000], Loss: 0.5291
Epoch [5/30], Batch [2900/6000], Loss: 0.3388
Epoch [5/30], Batch [3000/6000], Loss: 0.0273
Epoch [5/30], Batch [3100/6000], Loss: 0.1793
Epoch [5/30], Batch [3200/6000], Loss: 0.4089
Epoch [5/30], Batch [3300/6000], Loss: 0.0578
Epoch [5/30], Batch [3400/6000], Loss: 0.0564
Epoch [5/30], Batch [3500/6000], Loss: 0.0345
Epoch [5/30], Batch [3600/6000], Loss: 0.0589
Epoch [5/30], Batch [3700/6000], Loss: 0.0542
Epoch [5/30], Batch [3800/6000], Loss: 0.0399
Epoch [5/30], Batch [3900/6000], Loss: 0.2003
Epoch [5/30], Batch [4000/6000], Loss: 0.0341
Epoch [5/30], Batch [4100/6000], Loss: 0.4583
Epoch [5/30], Batch [4200/6000], Loss: 0.2248
Epoch [5/30], Batch [4300/6000], Loss: 0.1970
Epoch [5/30], Batch [4400/6000], Loss: 0.2713
Epoch [5/30], Batch [4500/6000], Loss: 0.0390
Epoch [5/30], Batch [4600/6000], Loss: 0.0713
Epoch [5/30], Batch [4700/6000], Loss: 0.0388
Epoch [5/30], Batch [4800/6000], Loss: 0.1696
Epoch [5/30], Batch [4900/6000], Loss: 0.2966
Epoch [5/30], Batch [5000/6000], Loss: 0.0287
Epoch [5/30], Batch [5100/6000], Loss: 0.1173
Epoch [5/30], Batch [5200/6000], Loss: 0.0426
Epoch [5/30], Batch [5300/6000], Loss: 0.0392
Epoch [5/30], Batch [5400/6000], Loss: 0.0550
Epoch [5/30], Batch [5500/6000], Loss: 0.1560
Epoch [5/30], Batch [5600/6000], Loss: 0.0295
Epoch [5/30], Batch [5700/6000], Loss: 0.0645
Epoch [5/30], Batch [5800/6000], Loss: 0.0452
Epoch [5/30], Batch [5900/6000], Loss: 0.0332
Epoch [5/30], Loss: 0.1222
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.0308
Epoch [6/30], Batch [100/6000], Loss: 0.1223
Epoch [6/30], Batch [200/6000], Loss: 0.2490
Epoch [6/30], Batch [300/6000], Loss: 0.4319
Epoch [6/30], Batch [400/6000], Loss: 0.0314
Epoch [6/30], Batch [500/6000], Loss: 0.5151
Epoch [6/30], Batch [600/6000], Loss: 0.0361
Epoch [6/30], Batch [700/6000], Loss: 0.1708
Epoch [6/30], Batch [800/6000], Loss: 0.0203
Epoch [6/30], Batch [900/6000], Loss: 0.0331
Epoch [6/30], Batch [1000/6000], Loss: 0.0415
Epoch [6/30], Batch [1100/6000], Loss: 0.0258
Epoch [6/30], Batch [1200/6000], Loss: 0.0278
Epoch [6/30], Batch [1300/6000], Loss: 0.0587
Epoch [6/30], Batch [1400/6000], Loss: 0.0625
Epoch [6/30], Batch [1500/6000], Loss: 0.0302
Epoch [6/30], Batch [1600/6000], Loss: 0.0486
Epoch [6/30], Batch [1700/6000], Loss: 0.0739
Epoch [6/30], Batch [1800/6000], Loss: 0.5883
Epoch [6/30], Batch [1900/6000], Loss: 0.0305
Epoch [6/30], Batch [2000/6000], Loss: 0.0351
Epoch [6/30], Batch [2100/6000], Loss: 0.0315
Epoch [6/30], Batch [2200/6000], Loss: 0.0696
Epoch [6/30], Batch [2300/6000], Loss: 0.0273
Epoch [6/30], Batch [2400/6000], Loss: 0.0889
Epoch [6/30], Batch [2500/6000], Loss: 0.4556
Epoch [6/30], Batch [2600/6000], Loss: 0.2764
Epoch [6/30], Batch [2700/6000], Loss: 0.0219
Epoch [6/30], Batch [2800/6000], Loss: 0.2055
Epoch [6/30], Batch [2900/6000], Loss: 0.0597
Epoch [6/30], Batch [3000/6000], Loss: 0.0571
Epoch [6/30], Batch [3100/6000], Loss: 0.1209
Epoch [6/30], Batch [3200/6000], Loss: 0.0281
Epoch [6/30], Batch [3300/6000], Loss: 0.1046
Epoch [6/30], Batch [3400/6000], Loss: 0.0597
Epoch [6/30], Batch [3500/6000], Loss: 0.0940
Epoch [6/30], Batch [3600/6000], Loss: 0.0270
Epoch [6/30], Batch [3700/6000], Loss: 0.0327
Epoch [6/30], Batch [3800/6000], Loss: 0.0921
Epoch [6/30], Batch [3900/6000], Loss: 0.0825
Epoch [6/30], Batch [4000/6000], Loss: 0.0243
Epoch [6/30], Batch [4100/6000], Loss: 0.0252
Epoch [6/30], Batch [4200/6000], Loss: 0.0320
Epoch [6/30], Batch [4300/6000], Loss: 0.0464
Epoch [6/30], Batch [4400/6000], Loss: 0.0563
Epoch [6/30], Batch [4500/6000], Loss: 0.0337
Epoch [6/30], Batch [4600/6000], Loss: 0.0716
Epoch [6/30], Batch [4700/6000], Loss: 0.0252
Epoch [6/30], Batch [4800/6000], Loss: 0.2165
Epoch [6/30], Batch [4900/6000], Loss: 0.1056
Epoch [6/30], Batch [5000/6000], Loss: 0.4708
Epoch [6/30], Batch [5100/6000], Loss: 0.0334
Epoch [6/30], Batch [5200/6000], Loss: 0.0380
Epoch [6/30], Batch [5300/6000], Loss: 0.4629
Epoch [6/30], Batch [5400/6000], Loss: 0.0794
Epoch [6/30], Batch [5500/6000], Loss: 0.0265
Epoch [6/30], Batch [5600/6000], Loss: 0.4726
Epoch [6/30], Batch [5700/6000], Loss: 0.0276
Epoch [6/30], Batch [5800/6000], Loss: 0.0277
Epoch [6/30], Batch [5900/6000], Loss: 0.1086
Epoch [6/30], Loss: 0.1069
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.0259
Epoch [7/30], Batch [100/6000], Loss: 0.0337
Epoch [7/30], Batch [200/6000], Loss: 0.1532
Epoch [7/30], Batch [300/6000], Loss: 0.0317
Epoch [7/30], Batch [400/6000], Loss: 0.0501
Epoch [7/30], Batch [500/6000], Loss: 0.0280
Epoch [7/30], Batch [600/6000], Loss: 0.0279
Epoch [7/30], Batch [700/6000], Loss: 0.0900
Epoch [7/30], Batch [800/6000], Loss: 0.0247
Epoch [7/30], Batch [900/6000], Loss: 0.0865
Epoch [7/30], Batch [1000/6000], Loss: 0.0355
Epoch [7/30], Batch [1100/6000], Loss: 0.1701
Epoch [7/30], Batch [1200/6000], Loss: 0.0330
Epoch [7/30], Batch [1300/6000], Loss: 0.0497
Epoch [7/30], Batch [1400/6000], Loss: 0.0353
Epoch [7/30], Batch [1500/6000], Loss: 0.0515
Epoch [7/30], Batch [1600/6000], Loss: 0.0314
Epoch [7/30], Batch [1700/6000], Loss: 0.0289
Epoch [7/30], Batch [1800/6000], Loss: 0.0452
Epoch [7/30], Batch [1900/6000], Loss: 0.0967
Epoch [7/30], Batch [2000/6000], Loss: 0.1109
Epoch [7/30], Batch [2100/6000], Loss: 0.1515
Epoch [7/30], Batch [2200/6000], Loss: 0.1731
Epoch [7/30], Batch [2300/6000], Loss: 0.2106
Epoch [7/30], Batch [2400/6000], Loss: 0.0501
Epoch [7/30], Batch [2500/6000], Loss: 0.3302
Epoch [7/30], Batch [2600/6000], Loss: 0.0380
Epoch [7/30], Batch [2700/6000], Loss: 0.0434
Epoch [7/30], Batch [2800/6000], Loss: 0.4309
Epoch [7/30], Batch [2900/6000], Loss: 0.0316
Epoch [7/30], Batch [3000/6000], Loss: 0.0414
Epoch [7/30], Batch [3100/6000], Loss: 0.0250
Epoch [7/30], Batch [3200/6000], Loss: 0.0298
Epoch [7/30], Batch [3300/6000], Loss: 0.0919
Epoch [7/30], Batch [3400/6000], Loss: 0.0574
Epoch [7/30], Batch [3500/6000], Loss: 0.0298
Epoch [7/30], Batch [3600/6000], Loss: 0.0459
Epoch [7/30], Batch [3700/6000], Loss: 0.1695
Epoch [7/30], Batch [3800/6000], Loss: 0.0347
Epoch [7/30], Batch [3900/6000], Loss: 0.1742
Epoch [7/30], Batch [4000/6000], Loss: 0.0483
Epoch [7/30], Batch [4100/6000], Loss: 0.0797
Epoch [7/30], Batch [4200/6000], Loss: 0.0368
Epoch [7/30], Batch [4300/6000], Loss: 0.0496
Epoch [7/30], Batch [4400/6000], Loss: 0.5317
Epoch [7/30], Batch [4500/6000], Loss: 0.0398
Epoch [7/30], Batch [4600/6000], Loss: 0.0291
Epoch [7/30], Batch [4700/6000], Loss: 0.0283
Epoch [7/30], Batch [4800/6000], Loss: 0.0333
Epoch [7/30], Batch [4900/6000], Loss: 0.0656
Epoch [7/30], Batch [5000/6000], Loss: 0.1329
Epoch [7/30], Batch [5100/6000], Loss: 0.0320
Epoch [7/30], Batch [5200/6000], Loss: 0.0356
Epoch [7/30], Batch [5300/6000], Loss: 0.4006
Epoch [7/30], Batch [5400/6000], Loss: 0.0400
Epoch [7/30], Batch [5500/6000], Loss: 0.0347
Epoch [7/30], Batch [5600/6000], Loss: 0.0490
Epoch [7/30], Batch [5700/6000], Loss: 0.2527
Epoch [7/30], Batch [5800/6000], Loss: 0.5096
Epoch [7/30], Batch [5900/6000], Loss: 0.6380
Epoch [7/30], Loss: 0.0946
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.0251
Epoch [8/30], Batch [100/6000], Loss: 0.0230
Epoch [8/30], Batch [200/6000], Loss: 0.0712
Epoch [8/30], Batch [300/6000], Loss: 0.2036
Epoch [8/30], Batch [400/6000], Loss: 0.0346
Epoch [8/30], Batch [500/6000], Loss: 0.0381
Epoch [8/30], Batch [600/6000], Loss: 0.0374
Epoch [8/30], Batch [700/6000], Loss: 0.0226
Epoch [8/30], Batch [800/6000], Loss: 0.0468
Epoch [8/30], Batch [900/6000], Loss: 0.3253
Epoch [8/30], Batch [1000/6000], Loss: 0.1967
Epoch [8/30], Batch [1100/6000], Loss: 0.0255
Epoch [8/30], Batch [1200/6000], Loss: 0.0705
Epoch [8/30], Batch [1300/6000], Loss: 0.1191
Epoch [8/30], Batch [1400/6000], Loss: 0.0304
Epoch [8/30], Batch [1500/6000], Loss: 0.0712
Epoch [8/30], Batch [1600/6000], Loss: 0.0312
Epoch [8/30], Batch [1700/6000], Loss: 0.0263
Epoch [8/30], Batch [1800/6000], Loss: 0.0441
Epoch [8/30], Batch [1900/6000], Loss: 0.0523
Epoch [8/30], Batch [2000/6000], Loss: 0.1671
Epoch [8/30], Batch [2100/6000], Loss: 0.0275
Epoch [8/30], Batch [2200/6000], Loss: 0.0236
Epoch [8/30], Batch [2300/6000], Loss: 0.1272
Epoch [8/30], Batch [2400/6000], Loss: 0.0264
Epoch [8/30], Batch [2500/6000], Loss: 0.0451
Epoch [8/30], Batch [2600/6000], Loss: 0.0728
Epoch [8/30], Batch [2700/6000], Loss: 0.0336
Epoch [8/30], Batch [2800/6000], Loss: 0.0235
Epoch [8/30], Batch [2900/6000], Loss: 0.0276
Epoch [8/30], Batch [3000/6000], Loss: 0.0248
Epoch [8/30], Batch [3100/6000], Loss: 0.2883
Epoch [8/30], Batch [3200/6000], Loss: 0.0307
Epoch [8/30], Batch [3300/6000], Loss: 0.0423
Epoch [8/30], Batch [3400/6000], Loss: 0.0311
Epoch [8/30], Batch [3500/6000], Loss: 0.0288
Epoch [8/30], Batch [3600/6000], Loss: 0.0432
Epoch [8/30], Batch [3700/6000], Loss: 0.0957
Epoch [8/30], Batch [3800/6000], Loss: 0.0873
Epoch [8/30], Batch [3900/6000], Loss: 0.0276
Epoch [8/30], Batch [4000/6000], Loss: 0.1164
Epoch [8/30], Batch [4100/6000], Loss: 0.0253
Epoch [8/30], Batch [4200/6000], Loss: 0.0298
Epoch [8/30], Batch [4300/6000], Loss: 0.0710
Epoch [8/30], Batch [4400/6000], Loss: 0.0324
Epoch [8/30], Batch [4500/6000], Loss: 0.0995
Epoch [8/30], Batch [4600/6000], Loss: 0.1209
Epoch [8/30], Batch [4700/6000], Loss: 0.0411
Epoch [8/30], Batch [4800/6000], Loss: 0.0504
Epoch [8/30], Batch [4900/6000], Loss: 0.0592
Epoch [8/30], Batch [5000/6000], Loss: 0.0243
Epoch [8/30], Batch [5100/6000], Loss: 0.0484
Epoch [8/30], Batch [5200/6000], Loss: 0.0273
Epoch [8/30], Batch [5300/6000], Loss: 0.0295
Epoch [8/30], Batch [5400/6000], Loss: 0.0307
Epoch [8/30], Batch [5500/6000], Loss: 0.0285
Epoch [8/30], Batch [5600/6000], Loss: 0.0352
Epoch [8/30], Batch [5700/6000], Loss: 0.0263
Epoch [8/30], Batch [5800/6000], Loss: 0.0619
Epoch [8/30], Batch [5900/6000], Loss: 0.0304
Epoch [8/30], Loss: 0.0840
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.2202
Epoch [9/30], Batch [100/6000], Loss: 0.0334
Epoch [9/30], Batch [200/6000], Loss: 0.0382
Epoch [9/30], Batch [300/6000], Loss: 0.1863
Epoch [9/30], Batch [400/6000], Loss: 0.0261
Epoch [9/30], Batch [500/6000], Loss: 0.0292
Epoch [9/30], Batch [600/6000], Loss: 0.5040
Epoch [9/30], Batch [700/6000], Loss: 0.2530
Epoch [9/30], Batch [800/6000], Loss: 0.0269
Epoch [9/30], Batch [900/6000], Loss: 0.1100
Epoch [9/30], Batch [1000/6000], Loss: 0.0621
Epoch [9/30], Batch [1100/6000], Loss: 0.0522
Epoch [9/30], Batch [1200/6000], Loss: 0.6226
Epoch [9/30], Batch [1300/6000], Loss: 0.0598
Epoch [9/30], Batch [1400/6000], Loss: 0.0249
Epoch [9/30], Batch [1500/6000], Loss: 0.1207
Epoch [9/30], Batch [1600/6000], Loss: 0.0351
Epoch [9/30], Batch [1700/6000], Loss: 0.0387
Epoch [9/30], Batch [1800/6000], Loss: 0.2152
Epoch [9/30], Batch [1900/6000], Loss: 0.0230
Epoch [9/30], Batch [2000/6000], Loss: 0.0198
Epoch [9/30], Batch [2100/6000], Loss: 0.0248
Epoch [9/30], Batch [2200/6000], Loss: 0.3441
Epoch [9/30], Batch [2300/6000], Loss: 0.0282
Epoch [9/30], Batch [2400/6000], Loss: 0.0351
Epoch [9/30], Batch [2500/6000], Loss: 0.0348
Epoch [9/30], Batch [2600/6000], Loss: 0.0568
Epoch [9/30], Batch [2700/6000], Loss: 0.0307
Epoch [9/30], Batch [2800/6000], Loss: 0.0299
Epoch [9/30], Batch [2900/6000], Loss: 0.0238
Epoch [9/30], Batch [3000/6000], Loss: 0.0486
Epoch [9/30], Batch [3100/6000], Loss: 0.0450
Epoch [9/30], Batch [3200/6000], Loss: 0.0231
Epoch [9/30], Batch [3300/6000], Loss: 0.1854
Epoch [9/30], Batch [3400/6000], Loss: 0.1240
Epoch [9/30], Batch [3500/6000], Loss: 0.0296
Epoch [9/30], Batch [3600/6000], Loss: 0.0208
Epoch [9/30], Batch [3700/6000], Loss: 0.2381
Epoch [9/30], Batch [3800/6000], Loss: 0.1129
Epoch [9/30], Batch [3900/6000], Loss: 0.0911
Epoch [9/30], Batch [4000/6000], Loss: 0.3088
Epoch [9/30], Batch [4100/6000], Loss: 0.0279
Epoch [9/30], Batch [4200/6000], Loss: 0.0201
Epoch [9/30], Batch [4300/6000], Loss: 0.1529
Epoch [9/30], Batch [4400/6000], Loss: 0.0253
Epoch [9/30], Batch [4500/6000], Loss: 0.0291
Epoch [9/30], Batch [4600/6000], Loss: 0.0378
Epoch [9/30], Batch [4700/6000], Loss: 0.0228
Epoch [9/30], Batch [4800/6000], Loss: 0.0567
Epoch [9/30], Batch [4900/6000], Loss: 0.0181
Epoch [9/30], Batch [5000/6000], Loss: 0.0292
Epoch [9/30], Batch [5100/6000], Loss: 0.0311
Epoch [9/30], Batch [5200/6000], Loss: 0.0198
Epoch [9/30], Batch [5300/6000], Loss: 0.0294
Epoch [9/30], Batch [5400/6000], Loss: 0.0586
Epoch [9/30], Batch [5500/6000], Loss: 0.0460
Epoch [9/30], Batch [5600/6000], Loss: 0.6256
Epoch [9/30], Batch [5700/6000], Loss: 0.0785
Epoch [9/30], Batch [5800/6000], Loss: 0.0283
Epoch [9/30], Batch [5900/6000], Loss: 0.0380
Epoch [9/30], Loss: 0.0762
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.0265
Epoch [10/30], Batch [100/6000], Loss: 0.0430
Epoch [10/30], Batch [200/6000], Loss: 0.0370
Epoch [10/30], Batch [300/6000], Loss: 0.0847
Epoch [10/30], Batch [400/6000], Loss: 0.0319
Epoch [10/30], Batch [500/6000], Loss: 0.1144
Epoch [10/30], Batch [600/6000], Loss: 0.0296
Epoch [10/30], Batch [700/6000], Loss: 0.0664
Epoch [10/30], Batch [800/6000], Loss: 0.0227
Epoch [10/30], Batch [900/6000], Loss: 0.0392
Epoch [10/30], Batch [1000/6000], Loss: 0.0269
Epoch [10/30], Batch [1100/6000], Loss: 0.0348
Epoch [10/30], Batch [1200/6000], Loss: 0.0281
Epoch [10/30], Batch [1300/6000], Loss: 0.0219
Epoch [10/30], Batch [1400/6000], Loss: 0.0306
Epoch [10/30], Batch [1500/6000], Loss: 0.2380
Epoch [10/30], Batch [1600/6000], Loss: 0.0337
Epoch [10/30], Batch [1700/6000], Loss: 0.0230
Epoch [10/30], Batch [1800/6000], Loss: 0.0263
Epoch [10/30], Batch [1900/6000], Loss: 0.0247
Epoch [10/30], Batch [2000/6000], Loss: 0.0236
Epoch [10/30], Batch [2100/6000], Loss: 0.0997
Epoch [10/30], Batch [2200/6000], Loss: 0.4626
Epoch [10/30], Batch [2300/6000], Loss: 0.0271
Epoch [10/30], Batch [2400/6000], Loss: 0.0679
Epoch [10/30], Batch [2500/6000], Loss: 0.0290
Epoch [10/30], Batch [2600/6000], Loss: 0.0218
Epoch [10/30], Batch [2700/6000], Loss: 0.2706
Epoch [10/30], Batch [2800/6000], Loss: 0.0385
Epoch [10/30], Batch [2900/6000], Loss: 0.0215
Epoch [10/30], Batch [3000/6000], Loss: 0.0245
Epoch [10/30], Batch [3100/6000], Loss: 0.0350
Epoch [10/30], Batch [3200/6000], Loss: 0.0271
Epoch [10/30], Batch [3300/6000], Loss: 0.2305
Epoch [10/30], Batch [3400/6000], Loss: 0.1524
Epoch [10/30], Batch [3500/6000], Loss: 0.0236
Epoch [10/30], Batch [3600/6000], Loss: 0.0312
Epoch [10/30], Batch [3700/6000], Loss: 0.0542
Epoch [10/30], Batch [3800/6000], Loss: 0.0245
Epoch [10/30], Batch [3900/6000], Loss: 0.0234
Epoch [10/30], Batch [4000/6000], Loss: 0.0436
Epoch [10/30], Batch [4100/6000], Loss: 0.0276
Epoch [10/30], Batch [4200/6000], Loss: 0.1199
Epoch [10/30], Batch [4300/6000], Loss: 0.0203
Epoch [10/30], Batch [4400/6000], Loss: 0.0220
Epoch [10/30], Batch [4500/6000], Loss: 0.1446
Epoch [10/30], Batch [4600/6000], Loss: 0.0553
Epoch [10/30], Batch [4700/6000], Loss: 0.0279
Epoch [10/30], Batch [4800/6000], Loss: 0.6760
Epoch [10/30], Batch [4900/6000], Loss: 0.0314
Epoch [10/30], Batch [5000/6000], Loss: 0.0299
Epoch [10/30], Batch [5100/6000], Loss: 0.0469
Epoch [10/30], Batch [5200/6000], Loss: 0.0339
Epoch [10/30], Batch [5300/6000], Loss: 0.0291
Epoch [10/30], Batch [5400/6000], Loss: 0.1443
Epoch [10/30], Batch [5500/6000], Loss: 0.2083
Epoch [10/30], Batch [5600/6000], Loss: 0.0703
Epoch [10/30], Batch [5700/6000], Loss: 0.0509
Epoch [10/30], Batch [5800/6000], Loss: 0.0944
Epoch [10/30], Batch [5900/6000], Loss: 0.0220
Epoch [10/30], Loss: 0.0683
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.0332
Epoch [11/30], Batch [100/6000], Loss: 0.0320
Epoch [11/30], Batch [200/6000], Loss: 0.0493
Epoch [11/30], Batch [300/6000], Loss: 0.0237
Epoch [11/30], Batch [400/6000], Loss: 0.0282
Epoch [11/30], Batch [500/6000], Loss: 0.0647
Epoch [11/30], Batch [600/6000], Loss: 0.0248
Epoch [11/30], Batch [700/6000], Loss: 0.0364
Epoch [11/30], Batch [800/6000], Loss: 0.0286
Epoch [11/30], Batch [900/6000], Loss: 0.0294
Epoch [11/30], Batch [1000/6000], Loss: 0.0226
Epoch [11/30], Batch [1100/6000], Loss: 0.0371
Epoch [11/30], Batch [1200/6000], Loss: 0.0302
Epoch [11/30], Batch [1300/6000], Loss: 0.0227
Epoch [11/30], Batch [1400/6000], Loss: 0.0395
Epoch [11/30], Batch [1500/6000], Loss: 0.0267
Epoch [11/30], Batch [1600/6000], Loss: 0.0596
Epoch [11/30], Batch [1700/6000], Loss: 0.0234
Epoch [11/30], Batch [1800/6000], Loss: 0.0302
Epoch [11/30], Batch [1900/6000], Loss: 0.3340
Epoch [11/30], Batch [2000/6000], Loss: 0.0293
Epoch [11/30], Batch [2100/6000], Loss: 0.0870
Epoch [11/30], Batch [2200/6000], Loss: 0.0249
Epoch [11/30], Batch [2300/6000], Loss: 0.1153
Epoch [11/30], Batch [2400/6000], Loss: 0.0602
Epoch [11/30], Batch [2500/6000], Loss: 0.0464
Epoch [11/30], Batch [2600/6000], Loss: 0.3651
Epoch [11/30], Batch [2700/6000], Loss: 0.0284
Epoch [11/30], Batch [2800/6000], Loss: 0.0324
Epoch [11/30], Batch [2900/6000], Loss: 0.0235
Epoch [11/30], Batch [3000/6000], Loss: 0.0919
Epoch [11/30], Batch [3100/6000], Loss: 0.0336
Epoch [11/30], Batch [3200/6000], Loss: 0.0282
Epoch [11/30], Batch [3300/6000], Loss: 0.6830
Epoch [11/30], Batch [3400/6000], Loss: 0.0258
Epoch [11/30], Batch [3500/6000], Loss: 0.0188
Epoch [11/30], Batch [3600/6000], Loss: 0.1151
Epoch [11/30], Batch [3700/6000], Loss: 0.0186
Epoch [11/30], Batch [3800/6000], Loss: 0.0326
Epoch [11/30], Batch [3900/6000], Loss: 0.0359
Epoch [11/30], Batch [4000/6000], Loss: 0.0250
Epoch [11/30], Batch [4100/6000], Loss: 0.0220
Epoch [11/30], Batch [4200/6000], Loss: 0.0313
Epoch [11/30], Batch [4300/6000], Loss: 0.0726
Epoch [11/30], Batch [4400/6000], Loss: 0.0215
Epoch [11/30], Batch [4500/6000], Loss: 0.0696
Epoch [11/30], Batch [4600/6000], Loss: 0.0208
Epoch [11/30], Batch [4700/6000], Loss: 0.0252
Epoch [11/30], Batch [4800/6000], Loss: 0.0326
Epoch [11/30], Batch [4900/6000], Loss: 0.0219
Epoch [11/30], Batch [5000/6000], Loss: 0.0384
Epoch [11/30], Batch [5100/6000], Loss: 0.0182
Epoch [11/30], Batch [5200/6000], Loss: 0.0289
Epoch [11/30], Batch [5300/6000], Loss: 0.0270
Epoch [11/30], Batch [5400/6000], Loss: 0.0264
Epoch [11/30], Batch [5500/6000], Loss: 0.0305
Epoch [11/30], Batch [5600/6000], Loss: 0.0234
Epoch [11/30], Batch [5700/6000], Loss: 0.0324
Epoch [11/30], Batch [5800/6000], Loss: 0.3209
Epoch [11/30], Batch [5900/6000], Loss: 0.0359
Epoch [11/30], Loss: 0.0633
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.0251
Epoch [12/30], Batch [100/6000], Loss: 0.0168
Epoch [12/30], Batch [200/6000], Loss: 0.0215
Epoch [12/30], Batch [300/6000], Loss: 0.0235
Epoch [12/30], Batch [400/6000], Loss: 0.6194
Epoch [12/30], Batch [500/6000], Loss: 0.0590
Epoch [12/30], Batch [600/6000], Loss: 0.0627
Epoch [12/30], Batch [700/6000], Loss: 0.0186
Epoch [12/30], Batch [800/6000], Loss: 0.0284
Epoch [12/30], Batch [900/6000], Loss: 0.0213
Epoch [12/30], Batch [1000/6000], Loss: 0.0190
Epoch [12/30], Batch [1100/6000], Loss: 0.0230
Epoch [12/30], Batch [1200/6000], Loss: 0.0832
Epoch [12/30], Batch [1300/6000], Loss: 0.0242
Epoch [12/30], Batch [1400/6000], Loss: 0.0244
Epoch [12/30], Batch [1500/6000], Loss: 0.0264
Epoch [12/30], Batch [1600/6000], Loss: 0.0210
Epoch [12/30], Batch [1700/6000], Loss: 0.0288
Epoch [12/30], Batch [1800/6000], Loss: 0.0228
Epoch [12/30], Batch [1900/6000], Loss: 0.0246
Epoch [12/30], Batch [2000/6000], Loss: 0.0654
Epoch [12/30], Batch [2100/6000], Loss: 0.0222
Epoch [12/30], Batch [2200/6000], Loss: 0.0465
Epoch [12/30], Batch [2300/6000], Loss: 0.0314
Epoch [12/30], Batch [2400/6000], Loss: 0.0174
Epoch [12/30], Batch [2500/6000], Loss: 0.0236
Epoch [12/30], Batch [2600/6000], Loss: 0.0186
Epoch [12/30], Batch [2700/6000], Loss: 0.0292
Epoch [12/30], Batch [2800/6000], Loss: 0.0332
Epoch [12/30], Batch [2900/6000], Loss: 0.0279
Epoch [12/30], Batch [3000/6000], Loss: 0.2715
Epoch [12/30], Batch [3100/6000], Loss: 0.0212
Epoch [12/30], Batch [3200/6000], Loss: 0.0272
Epoch [12/30], Batch [3300/6000], Loss: 0.0410
Epoch [12/30], Batch [3400/6000], Loss: 0.0173
Epoch [12/30], Batch [3500/6000], Loss: 0.0278
Epoch [12/30], Batch [3600/6000], Loss: 0.0523
Epoch [12/30], Batch [3700/6000], Loss: 0.0252
Epoch [12/30], Batch [3800/6000], Loss: 0.0247
Epoch [12/30], Batch [3900/6000], Loss: 0.5394
Epoch [12/30], Batch [4000/6000], Loss: 0.0376
Epoch [12/30], Batch [4100/6000], Loss: 0.0543
Epoch [12/30], Batch [4200/6000], Loss: 0.0260
Epoch [12/30], Batch [4300/6000], Loss: 0.0246
Epoch [12/30], Batch [4400/6000], Loss: 0.0270
Epoch [12/30], Batch [4500/6000], Loss: 0.0315
Epoch [12/30], Batch [4600/6000], Loss: 0.0238
Epoch [12/30], Batch [4700/6000], Loss: 0.1451
Epoch [12/30], Batch [4800/6000], Loss: 0.0212
Epoch [12/30], Batch [4900/6000], Loss: 0.0328
Epoch [12/30], Batch [5000/6000], Loss: 0.0267
Epoch [12/30], Batch [5100/6000], Loss: 0.0310
Epoch [12/30], Batch [5200/6000], Loss: 0.0212
Epoch [12/30], Batch [5300/6000], Loss: 0.0258
Epoch [12/30], Batch [5400/6000], Loss: 0.1190
Epoch [12/30], Batch [5500/6000], Loss: 0.0381
Epoch [12/30], Batch [5600/6000], Loss: 0.0318
Epoch [12/30], Batch [5700/6000], Loss: 0.0213
Epoch [12/30], Batch [5800/6000], Loss: 0.0484
Epoch [12/30], Batch [5900/6000], Loss: 0.3227
Epoch [12/30], Loss: 0.0577
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.0387
Epoch [13/30], Batch [100/6000], Loss: 0.0336
Epoch [13/30], Batch [200/6000], Loss: 0.0970
Epoch [13/30], Batch [300/6000], Loss: 0.0169
Epoch [13/30], Batch [400/6000], Loss: 0.0166
Epoch [13/30], Batch [500/6000], Loss: 0.0228
Epoch [13/30], Batch [600/6000], Loss: 0.0190
Epoch [13/30], Batch [700/6000], Loss: 0.0235
Epoch [13/30], Batch [800/6000], Loss: 0.0264
Epoch [13/30], Batch [900/6000], Loss: 0.0240
Epoch [13/30], Batch [1000/6000], Loss: 0.0297
Epoch [13/30], Batch [1100/6000], Loss: 0.0753
Epoch [13/30], Batch [1200/6000], Loss: 0.0193
Epoch [13/30], Batch [1300/6000], Loss: 0.0220
Epoch [13/30], Batch [1400/6000], Loss: 0.0882
Epoch [13/30], Batch [1500/6000], Loss: 0.0340
Epoch [13/30], Batch [1600/6000], Loss: 0.0602
Epoch [13/30], Batch [1700/6000], Loss: 0.0280
Epoch [13/30], Batch [1800/6000], Loss: 0.5476
Epoch [13/30], Batch [1900/6000], Loss: 0.0226
Epoch [13/30], Batch [2000/6000], Loss: 0.1628
Epoch [13/30], Batch [2100/6000], Loss: 0.0596
Epoch [13/30], Batch [2200/6000], Loss: 0.0244
Epoch [13/30], Batch [2300/6000], Loss: 0.0763
Epoch [13/30], Batch [2400/6000], Loss: 0.0265
Epoch [13/30], Batch [2500/6000], Loss: 0.0272
Epoch [13/30], Batch [2600/6000], Loss: 0.0206
Epoch [13/30], Batch [2700/6000], Loss: 0.0305
Epoch [13/30], Batch [2800/6000], Loss: 0.0149
Epoch [13/30], Batch [2900/6000], Loss: 0.0296
Epoch [13/30], Batch [3000/6000], Loss: 0.0386
Epoch [13/30], Batch [3100/6000], Loss: 0.0220
Epoch [13/30], Batch [3200/6000], Loss: 0.0264
Epoch [13/30], Batch [3300/6000], Loss: 0.0209
Epoch [13/30], Batch [3400/6000], Loss: 0.0217
Epoch [13/30], Batch [3500/6000], Loss: 0.0185
Epoch [13/30], Batch [3600/6000], Loss: 0.0211
Epoch [13/30], Batch [3700/6000], Loss: 0.0176
Epoch [13/30], Batch [3800/6000], Loss: 0.0211
Epoch [13/30], Batch [3900/6000], Loss: 0.0239
Epoch [13/30], Batch [4000/6000], Loss: 0.0781
Epoch [13/30], Batch [4100/6000], Loss: 0.0232
Epoch [13/30], Batch [4200/6000], Loss: 0.0288
Epoch [13/30], Batch [4300/6000], Loss: 0.0925
Epoch [13/30], Batch [4400/6000], Loss: 0.0230
Epoch [13/30], Batch [4500/6000], Loss: 0.0482
Epoch [13/30], Batch [4600/6000], Loss: 0.0221
Epoch [13/30], Batch [4700/6000], Loss: 0.0858
Epoch [13/30], Batch [4800/6000], Loss: 0.0223
Epoch [13/30], Batch [4900/6000], Loss: 0.0782
Epoch [13/30], Batch [5000/6000], Loss: 0.0305
Epoch [13/30], Batch [5100/6000], Loss: 0.0441
Epoch [13/30], Batch [5200/6000], Loss: 0.0173
Epoch [13/30], Batch [5300/6000], Loss: 0.0182
Epoch [13/30], Batch [5400/6000], Loss: 0.1988
Epoch [13/30], Batch [5500/6000], Loss: 0.0370
Epoch [13/30], Batch [5600/6000], Loss: 0.0214
Epoch [13/30], Batch [5700/6000], Loss: 0.0175
Epoch [13/30], Batch [5800/6000], Loss: 0.0228
Epoch [13/30], Batch [5900/6000], Loss: 0.0244
Epoch [13/30], Loss: 0.0536
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.0267
Epoch [14/30], Batch [100/6000], Loss: 0.0210
Epoch [14/30], Batch [200/6000], Loss: 0.0162
Epoch [14/30], Batch [300/6000], Loss: 0.0184
Epoch [14/30], Batch [400/6000], Loss: 0.0203
Epoch [14/30], Batch [500/6000], Loss: 0.0239
Epoch [14/30], Batch [600/6000], Loss: 0.0252
Epoch [14/30], Batch [700/6000], Loss: 0.0301
Epoch [14/30], Batch [800/6000], Loss: 0.0206
Epoch [14/30], Batch [900/6000], Loss: 0.0257
Epoch [14/30], Batch [1000/6000], Loss: 0.0271
Epoch [14/30], Batch [1100/6000], Loss: 0.0213
Epoch [14/30], Batch [1200/6000], Loss: 0.0589
Epoch [14/30], Batch [1300/6000], Loss: 0.0386
Epoch [14/30], Batch [1400/6000], Loss: 0.0343
Epoch [14/30], Batch [1500/6000], Loss: 0.0214
Epoch [14/30], Batch [1600/6000], Loss: 0.0208
Epoch [14/30], Batch [1700/6000], Loss: 0.0637
Epoch [14/30], Batch [1800/6000], Loss: 0.0478
Epoch [14/30], Batch [1900/6000], Loss: 0.0490
Epoch [14/30], Batch [2000/6000], Loss: 0.0238
Epoch [14/30], Batch [2100/6000], Loss: 0.0188
Epoch [14/30], Batch [2200/6000], Loss: 0.0235
Epoch [14/30], Batch [2300/6000], Loss: 0.0312
Epoch [14/30], Batch [2400/6000], Loss: 0.0533
Epoch [14/30], Batch [2500/6000], Loss: 0.0223
Epoch [14/30], Batch [2600/6000], Loss: 0.0198
Epoch [14/30], Batch [2700/6000], Loss: 0.0580
Epoch [14/30], Batch [2800/6000], Loss: 0.0257
Epoch [14/30], Batch [2900/6000], Loss: 0.0529
Epoch [14/30], Batch [3000/6000], Loss: 0.0180
Epoch [14/30], Batch [3100/6000], Loss: 0.0226
Epoch [14/30], Batch [3200/6000], Loss: 0.0201
Epoch [14/30], Batch [3300/6000], Loss: 0.0209
Epoch [14/30], Batch [3400/6000], Loss: 0.1500
Epoch [14/30], Batch [3500/6000], Loss: 0.0172
Epoch [14/30], Batch [3600/6000], Loss: 0.0425
Epoch [14/30], Batch [3700/6000], Loss: 0.0230
Epoch [14/30], Batch [3800/6000], Loss: 0.0159
Epoch [14/30], Batch [3900/6000], Loss: 0.0229
Epoch [14/30], Batch [4000/6000], Loss: 0.0250
Epoch [14/30], Batch [4100/6000], Loss: 0.0462
Epoch [14/30], Batch [4200/6000], Loss: 0.0489
Epoch [14/30], Batch [4300/6000], Loss: 0.0219
Epoch [14/30], Batch [4400/6000], Loss: 0.0226
Epoch [14/30], Batch [4500/6000], Loss: 0.0194
Epoch [14/30], Batch [4600/6000], Loss: 0.0203
Epoch [14/30], Batch [4700/6000], Loss: 0.0250
Epoch [14/30], Batch [4800/6000], Loss: 0.0335
Epoch [14/30], Batch [4900/6000], Loss: 0.0214
Epoch [14/30], Batch [5000/6000], Loss: 0.0212
Epoch [14/30], Batch [5100/6000], Loss: 0.0229
Epoch [14/30], Batch [5200/6000], Loss: 0.0595
Epoch [14/30], Batch [5300/6000], Loss: 0.0248
Epoch [14/30], Batch [5400/6000], Loss: 0.2720
Epoch [14/30], Batch [5500/6000], Loss: 0.0249
Epoch [14/30], Batch [5600/6000], Loss: 0.0604
Epoch [14/30], Batch [5700/6000], Loss: 0.0256
Epoch [14/30], Batch [5800/6000], Loss: 0.0198
Epoch [14/30], Batch [5900/6000], Loss: 0.0247
Epoch [14/30], Loss: 0.0493
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.0384
Epoch [15/30], Batch [100/6000], Loss: 0.0253
Epoch [15/30], Batch [200/6000], Loss: 0.0219
Epoch [15/30], Batch [300/6000], Loss: 0.0504
Epoch [15/30], Batch [400/6000], Loss: 0.0264
Epoch [15/30], Batch [500/6000], Loss: 0.0222
Epoch [15/30], Batch [600/6000], Loss: 0.0479
Epoch [15/30], Batch [700/6000], Loss: 0.0351
Epoch [15/30], Batch [800/6000], Loss: 0.0745
Epoch [15/30], Batch [900/6000], Loss: 0.0630
Epoch [15/30], Batch [1000/6000], Loss: 0.0277
Epoch [15/30], Batch [1100/6000], Loss: 0.0244
Epoch [15/30], Batch [1200/6000], Loss: 0.7221
Epoch [15/30], Batch [1300/6000], Loss: 0.0552
Epoch [15/30], Batch [1400/6000], Loss: 0.0186
Epoch [15/30], Batch [1500/6000], Loss: 0.0218
Epoch [15/30], Batch [1600/6000], Loss: 0.0185
Epoch [15/30], Batch [1700/6000], Loss: 0.0262
Epoch [15/30], Batch [1800/6000], Loss: 0.0260
Epoch [15/30], Batch [1900/6000], Loss: 0.0373
Epoch [15/30], Batch [2000/6000], Loss: 0.0314
Epoch [15/30], Batch [2100/6000], Loss: 0.0233
Epoch [15/30], Batch [2200/6000], Loss: 0.0277
Epoch [15/30], Batch [2300/6000], Loss: 0.2059
Epoch [15/30], Batch [2400/6000], Loss: 0.0231
Epoch [15/30], Batch [2500/6000], Loss: 0.0174
Epoch [15/30], Batch [2600/6000], Loss: 0.0914
Epoch [15/30], Batch [2700/6000], Loss: 0.0217
Epoch [15/30], Batch [2800/6000], Loss: 0.0200
Epoch [15/30], Batch [2900/6000], Loss: 0.0230
Epoch [15/30], Batch [3000/6000], Loss: 0.0198
Epoch [15/30], Batch [3100/6000], Loss: 0.0371
Epoch [15/30], Batch [3200/6000], Loss: 0.0201
Epoch [15/30], Batch [3300/6000], Loss: 0.1822
Epoch [15/30], Batch [3400/6000], Loss: 0.0225
Epoch [15/30], Batch [3500/6000], Loss: 0.0223
Epoch [15/30], Batch [3600/6000], Loss: 0.0190
Epoch [15/30], Batch [3700/6000], Loss: 0.0333
Epoch [15/30], Batch [3800/6000], Loss: 0.0172
Epoch [15/30], Batch [3900/6000], Loss: 0.0235
Epoch [15/30], Batch [4000/6000], Loss: 0.0218
Epoch [15/30], Batch [4100/6000], Loss: 0.0229
Epoch [15/30], Batch [4200/6000], Loss: 0.0335
Epoch [15/30], Batch [4300/6000], Loss: 0.0232
Epoch [15/30], Batch [4400/6000], Loss: 0.0266
Epoch [15/30], Batch [4500/6000], Loss: 0.0241
Epoch [15/30], Batch [4600/6000], Loss: 0.0236
Epoch [15/30], Batch [4700/6000], Loss: 0.0191
Epoch [15/30], Batch [4800/6000], Loss: 0.0229
Epoch [15/30], Batch [4900/6000], Loss: 0.0212
Epoch [15/30], Batch [5000/6000], Loss: 0.0226
Epoch [15/30], Batch [5100/6000], Loss: 0.2465
Epoch [15/30], Batch [5200/6000], Loss: 0.0214
Epoch [15/30], Batch [5300/6000], Loss: 0.0190
Epoch [15/30], Batch [5400/6000], Loss: 0.0272
Epoch [15/30], Batch [5500/6000], Loss: 0.0229
Epoch [15/30], Batch [5600/6000], Loss: 0.7052
Epoch [15/30], Batch [5700/6000], Loss: 0.0256
Epoch [15/30], Batch [5800/6000], Loss: 0.0196
Epoch [15/30], Batch [5900/6000], Loss: 0.0175
Epoch [15/30], Loss: 0.0471
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.0179
Epoch [16/30], Batch [100/6000], Loss: 0.0209
Epoch [16/30], Batch [200/6000], Loss: 0.0210
Epoch [16/30], Batch [300/6000], Loss: 0.0236
Epoch [16/30], Batch [400/6000], Loss: 0.0196
Epoch [16/30], Batch [500/6000], Loss: 0.0282
Epoch [16/30], Batch [600/6000], Loss: 0.0196
Epoch [16/30], Batch [700/6000], Loss: 0.0165
Epoch [16/30], Batch [800/6000], Loss: 0.0273
Epoch [16/30], Batch [900/6000], Loss: 0.0241
Epoch [16/30], Batch [1000/6000], Loss: 0.0219
Epoch [16/30], Batch [1100/6000], Loss: 0.0241
Epoch [16/30], Batch [1200/6000], Loss: 0.5360
Epoch [16/30], Batch [1300/6000], Loss: 0.0287
Epoch [16/30], Batch [1400/6000], Loss: 0.0269
Epoch [16/30], Batch [1500/6000], Loss: 0.0218
Epoch [16/30], Batch [1600/6000], Loss: 0.0278
Epoch [16/30], Batch [1700/6000], Loss: 0.0246
Epoch [16/30], Batch [1800/6000], Loss: 0.0227
Epoch [16/30], Batch [1900/6000], Loss: 0.1893
Epoch [16/30], Batch [2000/6000], Loss: 0.0174
Epoch [16/30], Batch [2100/6000], Loss: 0.0189
Epoch [16/30], Batch [2200/6000], Loss: 0.0168
Epoch [16/30], Batch [2300/6000], Loss: 0.0487
Epoch [16/30], Batch [2400/6000], Loss: 0.0194
Epoch [16/30], Batch [2500/6000], Loss: 0.0292
Epoch [16/30], Batch [2600/6000], Loss: 0.0228
Epoch [16/30], Batch [2700/6000], Loss: 0.0206
Epoch [16/30], Batch [2800/6000], Loss: 0.0180
Epoch [16/30], Batch [2900/6000], Loss: 0.0334
Epoch [16/30], Batch [3000/6000], Loss: 0.0183
Epoch [16/30], Batch [3100/6000], Loss: 0.1092
Epoch [16/30], Batch [3200/6000], Loss: 0.0207
Epoch [16/30], Batch [3300/6000], Loss: 0.0680
Epoch [16/30], Batch [3400/6000], Loss: 0.0250
Epoch [16/30], Batch [3500/6000], Loss: 0.0169
Epoch [16/30], Batch [3600/6000], Loss: 0.0216
Epoch [16/30], Batch [3700/6000], Loss: 0.0188
Epoch [16/30], Batch [3800/6000], Loss: 0.0267
Epoch [16/30], Batch [3900/6000], Loss: 0.0358
Epoch [16/30], Batch [4000/6000], Loss: 0.0270
Epoch [16/30], Batch [4100/6000], Loss: 0.0227
Epoch [16/30], Batch [4200/6000], Loss: 0.0384
Epoch [16/30], Batch [4300/6000], Loss: 0.0222
Epoch [16/30], Batch [4400/6000], Loss: 0.0269
Epoch [16/30], Batch [4500/6000], Loss: 0.0277
Epoch [16/30], Batch [4600/6000], Loss: 0.1077
Epoch [16/30], Batch [4700/6000], Loss: 0.0260
Epoch [16/30], Batch [4800/6000], Loss: 0.1770
Epoch [16/30], Batch [4900/6000], Loss: 0.0217
Epoch [16/30], Batch [5000/6000], Loss: 0.0229
Epoch [16/30], Batch [5100/6000], Loss: 0.0181
Epoch [16/30], Batch [5200/6000], Loss: 0.0182
Epoch [16/30], Batch [5300/6000], Loss: 0.0217
Epoch [16/30], Batch [5400/6000], Loss: 0.0172
Epoch [16/30], Batch [5500/6000], Loss: 0.0378
Epoch [16/30], Batch [5600/6000], Loss: 0.0303
Epoch [16/30], Batch [5700/6000], Loss: 0.0176
Epoch [16/30], Batch [5800/6000], Loss: 0.0201
Epoch [16/30], Batch [5900/6000], Loss: 0.1805
Epoch [16/30], Loss: 0.0436
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.0294
Epoch [17/30], Batch [100/6000], Loss: 0.0206
Epoch [17/30], Batch [200/6000], Loss: 0.0238
Epoch [17/30], Batch [300/6000], Loss: 0.0199
Epoch [17/30], Batch [400/6000], Loss: 0.0234
Epoch [17/30], Batch [500/6000], Loss: 0.1661
Epoch [17/30], Batch [600/6000], Loss: 0.0352
Epoch [17/30], Batch [700/6000], Loss: 0.0262
Epoch [17/30], Batch [800/6000], Loss: 0.0251
Epoch [17/30], Batch [900/6000], Loss: 0.0310
Epoch [17/30], Batch [1000/6000], Loss: 0.0168
Epoch [17/30], Batch [1100/6000], Loss: 0.0216
Epoch [17/30], Batch [1200/6000], Loss: 0.0480
Epoch [17/30], Batch [1300/6000], Loss: 0.0348
Epoch [17/30], Batch [1400/6000], Loss: 0.0222
Epoch [17/30], Batch [1500/6000], Loss: 0.0247
Epoch [17/30], Batch [1600/6000], Loss: 0.0445
Epoch [17/30], Batch [1700/6000], Loss: 0.0248
Epoch [17/30], Batch [1800/6000], Loss: 0.0199
Epoch [17/30], Batch [1900/6000], Loss: 0.0234
Epoch [17/30], Batch [2000/6000], Loss: 0.0204
Epoch [17/30], Batch [2100/6000], Loss: 0.0185
Epoch [17/30], Batch [2200/6000], Loss: 0.0252
Epoch [17/30], Batch [2300/6000], Loss: 0.0192
Epoch [17/30], Batch [2400/6000], Loss: 0.0189
Epoch [17/30], Batch [2500/6000], Loss: 0.0210
Epoch [17/30], Batch [2600/6000], Loss: 0.0203
Epoch [17/30], Batch [2700/6000], Loss: 0.0219
Epoch [17/30], Batch [2800/6000], Loss: 0.0171
Epoch [17/30], Batch [2900/6000], Loss: 0.0194
Epoch [17/30], Batch [3000/6000], Loss: 0.0205
Epoch [17/30], Batch [3100/6000], Loss: 0.0228
Epoch [17/30], Batch [3200/6000], Loss: 0.0201
Epoch [17/30], Batch [3300/6000], Loss: 0.0257
Epoch [17/30], Batch [3400/6000], Loss: 0.0217
Epoch [17/30], Batch [3500/6000], Loss: 0.0244
Epoch [17/30], Batch [3600/6000], Loss: 0.0198
Epoch [17/30], Batch [3700/6000], Loss: 0.0160
Epoch [17/30], Batch [3800/6000], Loss: 0.0197
Epoch [17/30], Batch [3900/6000], Loss: 0.0183
Epoch [17/30], Batch [4000/6000], Loss: 0.0172
Epoch [17/30], Batch [4100/6000], Loss: 0.3803
Epoch [17/30], Batch [4200/6000], Loss: 0.0257
Epoch [17/30], Batch [4300/6000], Loss: 0.0210
Epoch [17/30], Batch [4400/6000], Loss: 0.0300
Epoch [17/30], Batch [4500/6000], Loss: 0.0180
Epoch [17/30], Batch [4600/6000], Loss: 0.0240
Epoch [17/30], Batch [4700/6000], Loss: 0.0299
Epoch [17/30], Batch [4800/6000], Loss: 0.0232
Epoch [17/30], Batch [4900/6000], Loss: 0.2766
Epoch [17/30], Batch [5000/6000], Loss: 0.0217
Epoch [17/30], Batch [5100/6000], Loss: 0.0152
Epoch [17/30], Batch [5200/6000], Loss: 0.0212
Epoch [17/30], Batch [5300/6000], Loss: 0.0206
Epoch [17/30], Batch [5400/6000], Loss: 0.0736
Epoch [17/30], Batch [5500/6000], Loss: 0.0221
Epoch [17/30], Batch [5600/6000], Loss: 0.0211
Epoch [17/30], Batch [5700/6000], Loss: 0.0221
Epoch [17/30], Batch [5800/6000], Loss: 0.0281
Epoch [17/30], Batch [5900/6000], Loss: 0.0221
Epoch [17/30], Loss: 0.0405
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.0169
Epoch [18/30], Batch [100/6000], Loss: 0.0193
Epoch [18/30], Batch [200/6000], Loss: 0.0704
Epoch [18/30], Batch [300/6000], Loss: 0.0177
Epoch [18/30], Batch [400/6000], Loss: 0.0183
Epoch [18/30], Batch [500/6000], Loss: 0.0217
Epoch [18/30], Batch [600/6000], Loss: 0.0160
Epoch [18/30], Batch [700/6000], Loss: 0.0291
Epoch [18/30], Batch [800/6000], Loss: 0.0239
Epoch [18/30], Batch [900/6000], Loss: 0.0268
Epoch [18/30], Batch [1000/6000], Loss: 0.0179
Epoch [18/30], Batch [1100/6000], Loss: 0.0242
Epoch [18/30], Batch [1200/6000], Loss: 0.3654
Epoch [18/30], Batch [1300/6000], Loss: 0.0208
Epoch [18/30], Batch [1400/6000], Loss: 0.6271
Epoch [18/30], Batch [1500/6000], Loss: 0.0212
Epoch [18/30], Batch [1600/6000], Loss: 0.0219
Epoch [18/30], Batch [1700/6000], Loss: 0.0812
Epoch [18/30], Batch [1800/6000], Loss: 0.0221
Epoch [18/30], Batch [1900/6000], Loss: 0.0150
Epoch [18/30], Batch [2000/6000], Loss: 0.1917
Epoch [18/30], Batch [2100/6000], Loss: 0.0299
Epoch [18/30], Batch [2200/6000], Loss: 0.0132
Epoch [18/30], Batch [2300/6000], Loss: 0.0253
Epoch [18/30], Batch [2400/6000], Loss: 0.0191
Epoch [18/30], Batch [2500/6000], Loss: 0.0221
Epoch [18/30], Batch [2600/6000], Loss: 0.0253
Epoch [18/30], Batch [2700/6000], Loss: 0.0245
Epoch [18/30], Batch [2800/6000], Loss: 0.0181
Epoch [18/30], Batch [2900/6000], Loss: 0.0243
Epoch [18/30], Batch [3000/6000], Loss: 0.0197
Epoch [18/30], Batch [3100/6000], Loss: 0.0224
Epoch [18/30], Batch [3200/6000], Loss: 0.1533
Epoch [18/30], Batch [3300/6000], Loss: 0.0202
Epoch [18/30], Batch [3400/6000], Loss: 0.0163
Epoch [18/30], Batch [3500/6000], Loss: 0.0683
Epoch [18/30], Batch [3600/6000], Loss: 0.1099
Epoch [18/30], Batch [3700/6000], Loss: 0.0164
Epoch [18/30], Batch [3800/6000], Loss: 0.0173
Epoch [18/30], Batch [3900/6000], Loss: 0.3892
Epoch [18/30], Batch [4000/6000], Loss: 0.0325
Epoch [18/30], Batch [4100/6000], Loss: 0.0241
Epoch [18/30], Batch [4200/6000], Loss: 0.3157
Epoch [18/30], Batch [4300/6000], Loss: 0.0197
Epoch [18/30], Batch [4400/6000], Loss: 0.0238
Epoch [18/30], Batch [4500/6000], Loss: 0.0234
Epoch [18/30], Batch [4600/6000], Loss: 0.0210
Epoch [18/30], Batch [4700/6000], Loss: 0.0211
Epoch [18/30], Batch [4800/6000], Loss: 0.0236
Epoch [18/30], Batch [4900/6000], Loss: 0.0285
Epoch [18/30], Batch [5000/6000], Loss: 0.0227
Epoch [18/30], Batch [5100/6000], Loss: 0.1047
Epoch [18/30], Batch [5200/6000], Loss: 0.0214
Epoch [18/30], Batch [5300/6000], Loss: 0.0264
Epoch [18/30], Batch [5400/6000], Loss: 0.0209
Epoch [18/30], Batch [5500/6000], Loss: 0.0207
Epoch [18/30], Batch [5600/6000], Loss: 0.0223
Epoch [18/30], Batch [5700/6000], Loss: 0.0243
Epoch [18/30], Batch [5800/6000], Loss: 0.0276
Epoch [18/30], Batch [5900/6000], Loss: 0.0184
Epoch [18/30], Loss: 0.0401
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.0519
Epoch [19/30], Batch [100/6000], Loss: 0.0187
Epoch [19/30], Batch [200/6000], Loss: 0.0159
Epoch [19/30], Batch [300/6000], Loss: 0.0316
Epoch [19/30], Batch [400/6000], Loss: 0.0209
Epoch [19/30], Batch [500/6000], Loss: 0.0192
Epoch [19/30], Batch [600/6000], Loss: 0.0201
Epoch [19/30], Batch [700/6000], Loss: 0.0175
Epoch [19/30], Batch [800/6000], Loss: 0.0152
Epoch [19/30], Batch [900/6000], Loss: 0.0335
Epoch [19/30], Batch [1000/6000], Loss: 0.0259
Epoch [19/30], Batch [1100/6000], Loss: 0.0170
Epoch [19/30], Batch [1200/6000], Loss: 0.0199
Epoch [19/30], Batch [1300/6000], Loss: 0.0178
Epoch [19/30], Batch [1400/6000], Loss: 0.0192
Epoch [19/30], Batch [1500/6000], Loss: 0.4463
Epoch [19/30], Batch [1600/6000], Loss: 0.0192
Epoch [19/30], Batch [1700/6000], Loss: 0.0501
Epoch [19/30], Batch [1800/6000], Loss: 0.0195
Epoch [19/30], Batch [1900/6000], Loss: 0.0158
Epoch [19/30], Batch [2000/6000], Loss: 0.0273
Epoch [19/30], Batch [2100/6000], Loss: 0.0362
Epoch [19/30], Batch [2200/6000], Loss: 0.0213
Epoch [19/30], Batch [2300/6000], Loss: 0.0328
Epoch [19/30], Batch [2400/6000], Loss: 0.0249
Epoch [19/30], Batch [2500/6000], Loss: 0.0168
Epoch [19/30], Batch [2600/6000], Loss: 0.0212
Epoch [19/30], Batch [2700/6000], Loss: 0.0161
Epoch [19/30], Batch [2800/6000], Loss: 0.0191
Epoch [19/30], Batch [2900/6000], Loss: 0.0233
Epoch [19/30], Batch [3000/6000], Loss: 0.0144
Epoch [19/30], Batch [3100/6000], Loss: 0.0172
Epoch [19/30], Batch [3200/6000], Loss: 0.0217
Epoch [19/30], Batch [3300/6000], Loss: 0.0218
Epoch [19/30], Batch [3400/6000], Loss: 0.0216
Epoch [19/30], Batch [3500/6000], Loss: 0.0194
Epoch [19/30], Batch [3600/6000], Loss: 0.0172
Epoch [19/30], Batch [3700/6000], Loss: 0.0198
Epoch [19/30], Batch [3800/6000], Loss: 0.0240
Epoch [19/30], Batch [3900/6000], Loss: 0.0204
Epoch [19/30], Batch [4000/6000], Loss: 0.0209
Epoch [19/30], Batch [4100/6000], Loss: 0.0502
Epoch [19/30], Batch [4200/6000], Loss: 0.0235
Epoch [19/30], Batch [4300/6000], Loss: 0.0200
Epoch [19/30], Batch [4400/6000], Loss: 0.0381
Epoch [19/30], Batch [4500/6000], Loss: 0.0224
Epoch [19/30], Batch [4600/6000], Loss: 0.0268
Epoch [19/30], Batch [4700/6000], Loss: 0.1830
Epoch [19/30], Batch [4800/6000], Loss: 0.0170
Epoch [19/30], Batch [4900/6000], Loss: 0.0188
Epoch [19/30], Batch [5000/6000], Loss: 0.0147
Epoch [19/30], Batch [5100/6000], Loss: 0.0233
Epoch [19/30], Batch [5200/6000], Loss: 0.0200
Epoch [19/30], Batch [5300/6000], Loss: 0.0270
Epoch [19/30], Batch [5400/6000], Loss: 0.0224
Epoch [19/30], Batch [5500/6000], Loss: 0.0157
Epoch [19/30], Batch [5600/6000], Loss: 0.0222
Epoch [19/30], Batch [5700/6000], Loss: 0.0232
Epoch [19/30], Batch [5800/6000], Loss: 0.0169
Epoch [19/30], Batch [5900/6000], Loss: 0.0456
Epoch [19/30], Loss: 0.0371
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.3320
Epoch [20/30], Batch [100/6000], Loss: 0.0163
Epoch [20/30], Batch [200/6000], Loss: 0.0223
Epoch [20/30], Batch [300/6000], Loss: 0.0604
Epoch [20/30], Batch [400/6000], Loss: 0.0177
Epoch [20/30], Batch [500/6000], Loss: 0.0168
Epoch [20/30], Batch [600/6000], Loss: 0.0178
Epoch [20/30], Batch [700/6000], Loss: 0.0651
Epoch [20/30], Batch [800/6000], Loss: 0.0159
Epoch [20/30], Batch [900/6000], Loss: 0.3664
Epoch [20/30], Batch [1000/6000], Loss: 0.0258
Epoch [20/30], Batch [1100/6000], Loss: 0.0223
Epoch [20/30], Batch [1200/6000], Loss: 0.0218
Epoch [20/30], Batch [1300/6000], Loss: 0.0233
Epoch [20/30], Batch [1400/6000], Loss: 0.1794
Epoch [20/30], Batch [1500/6000], Loss: 0.0167
Epoch [20/30], Batch [1600/6000], Loss: 0.0214
Epoch [20/30], Batch [1700/6000], Loss: 0.0172
Epoch [20/30], Batch [1800/6000], Loss: 0.0193
Epoch [20/30], Batch [1900/6000], Loss: 0.0168
Epoch [20/30], Batch [2000/6000], Loss: 0.0202
Epoch [20/30], Batch [2100/6000], Loss: 0.0328
Epoch [20/30], Batch [2200/6000], Loss: 0.0224
Epoch [20/30], Batch [2300/6000], Loss: 0.0192
Epoch [20/30], Batch [2400/6000], Loss: 0.0171
Epoch [20/30], Batch [2500/6000], Loss: 0.0204
Epoch [20/30], Batch [2600/6000], Loss: 0.0431
Epoch [20/30], Batch [2700/6000], Loss: 0.0221
Epoch [20/30], Batch [2800/6000], Loss: 0.0214
Epoch [20/30], Batch [2900/6000], Loss: 0.0147
Epoch [20/30], Batch [3000/6000], Loss: 0.0178
Epoch [20/30], Batch [3100/6000], Loss: 0.0196
Epoch [20/30], Batch [3200/6000], Loss: 0.0298
Epoch [20/30], Batch [3300/6000], Loss: 0.0452
Epoch [20/30], Batch [3400/6000], Loss: 0.0173
Epoch [20/30], Batch [3500/6000], Loss: 0.0185
Epoch [20/30], Batch [3600/6000], Loss: 0.4761
Epoch [20/30], Batch [3700/6000], Loss: 0.0179
Epoch [20/30], Batch [3800/6000], Loss: 0.0198
Epoch [20/30], Batch [3900/6000], Loss: 0.0174
Epoch [20/30], Batch [4000/6000], Loss: 0.0174
Epoch [20/30], Batch [4100/6000], Loss: 0.0184
Epoch [20/30], Batch [4200/6000], Loss: 0.0164
Epoch [20/30], Batch [4300/6000], Loss: 0.0195
Epoch [20/30], Batch [4400/6000], Loss: 0.0229
Epoch [20/30], Batch [4500/6000], Loss: 0.0214
Epoch [20/30], Batch [4600/6000], Loss: 0.0485
Epoch [20/30], Batch [4700/6000], Loss: 0.0185
Epoch [20/30], Batch [4800/6000], Loss: 0.0251
Epoch [20/30], Batch [4900/6000], Loss: 0.0215
Epoch [20/30], Batch [5000/6000], Loss: 0.0192
Epoch [20/30], Batch [5100/6000], Loss: 0.0356
Epoch [20/30], Batch [5200/6000], Loss: 0.0219
Epoch [20/30], Batch [5300/6000], Loss: 0.0311
Epoch [20/30], Batch [5400/6000], Loss: 0.0311
Epoch [20/30], Batch [5500/6000], Loss: 0.0187
Epoch [20/30], Batch [5600/6000], Loss: 0.0215
Epoch [20/30], Batch [5700/6000], Loss: 0.0128
Epoch [20/30], Batch [5800/6000], Loss: 0.0210
Epoch [20/30], Batch [5900/6000], Loss: 0.0172
Epoch [20/30], Loss: 0.0357
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.0189
Epoch [21/30], Batch [100/6000], Loss: 0.0209
Epoch [21/30], Batch [200/6000], Loss: 0.0659
Epoch [21/30], Batch [300/6000], Loss: 0.0243
Epoch [21/30], Batch [400/6000], Loss: 0.0209
Epoch [21/30], Batch [500/6000], Loss: 0.0246
Epoch [21/30], Batch [600/6000], Loss: 0.0170
Epoch [21/30], Batch [700/6000], Loss: 0.0185
Epoch [21/30], Batch [800/6000], Loss: 0.0265
Epoch [21/30], Batch [900/6000], Loss: 0.0163
Epoch [21/30], Batch [1000/6000], Loss: 0.0158
Epoch [21/30], Batch [1100/6000], Loss: 0.0205
Epoch [21/30], Batch [1200/6000], Loss: 0.0185
Epoch [21/30], Batch [1300/6000], Loss: 0.0242
Epoch [21/30], Batch [1400/6000], Loss: 0.0167
Epoch [21/30], Batch [1500/6000], Loss: 0.0224
Epoch [21/30], Batch [1600/6000], Loss: 0.0183
Epoch [21/30], Batch [1700/6000], Loss: 0.1475
Epoch [21/30], Batch [1800/6000], Loss: 0.0272
Epoch [21/30], Batch [1900/6000], Loss: 0.0203
Epoch [21/30], Batch [2000/6000], Loss: 0.0212
Epoch [21/30], Batch [2100/6000], Loss: 0.0160
Epoch [21/30], Batch [2200/6000], Loss: 0.0201
Epoch [21/30], Batch [2300/6000], Loss: 0.0191
Epoch [21/30], Batch [2400/6000], Loss: 0.0265
Epoch [21/30], Batch [2500/6000], Loss: 0.0192
Epoch [21/30], Batch [2600/6000], Loss: 0.0430
Epoch [21/30], Batch [2700/6000], Loss: 0.0615
Epoch [21/30], Batch [2800/6000], Loss: 0.0198
Epoch [21/30], Batch [2900/6000], Loss: 0.0207
Epoch [21/30], Batch [3000/6000], Loss: 0.0229
Epoch [21/30], Batch [3100/6000], Loss: 0.0196
Epoch [21/30], Batch [3200/6000], Loss: 0.0247
Epoch [21/30], Batch [3300/6000], Loss: 0.0208
Epoch [21/30], Batch [3400/6000], Loss: 0.0170
Epoch [21/30], Batch [3500/6000], Loss: 0.0171
Epoch [21/30], Batch [3600/6000], Loss: 0.0434
Epoch [21/30], Batch [3700/6000], Loss: 0.0251
Epoch [21/30], Batch [3800/6000], Loss: 0.0178
Epoch [21/30], Batch [3900/6000], Loss: 0.0225
Epoch [21/30], Batch [4000/6000], Loss: 0.0187
Epoch [21/30], Batch [4100/6000], Loss: 0.0264
Epoch [21/30], Batch [4200/6000], Loss: 0.0215
Epoch [21/30], Batch [4300/6000], Loss: 0.0229
Epoch [21/30], Batch [4400/6000], Loss: 0.0157
Epoch [21/30], Batch [4500/6000], Loss: 0.0196
Epoch [21/30], Batch [4600/6000], Loss: 0.0237
Epoch [21/30], Batch [4700/6000], Loss: 0.0226
Epoch [21/30], Batch [4800/6000], Loss: 0.0233
Epoch [21/30], Batch [4900/6000], Loss: 0.0259
Epoch [21/30], Batch [5000/6000], Loss: 0.0278
Epoch [21/30], Batch [5100/6000], Loss: 0.0198
Epoch [21/30], Batch [5200/6000], Loss: 0.0172
Epoch [21/30], Batch [5300/6000], Loss: 0.0163
Epoch [21/30], Batch [5400/6000], Loss: 0.0184
Epoch [21/30], Batch [5500/6000], Loss: 0.0188
Epoch [21/30], Batch [5600/6000], Loss: 0.0285
Epoch [21/30], Batch [5700/6000], Loss: 0.0199
Epoch [21/30], Batch [5800/6000], Loss: 0.0234
Epoch [21/30], Batch [5900/6000], Loss: 0.0738
Epoch [21/30], Loss: 0.0345
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.0681
Epoch [22/30], Batch [100/6000], Loss: 0.0151
Epoch [22/30], Batch [200/6000], Loss: 0.0177
Epoch [22/30], Batch [300/6000], Loss: 0.0168
Epoch [22/30], Batch [400/6000], Loss: 0.1705
Epoch [22/30], Batch [500/6000], Loss: 0.0242
Epoch [22/30], Batch [600/6000], Loss: 0.0195
Epoch [22/30], Batch [700/6000], Loss: 0.0172
Epoch [22/30], Batch [800/6000], Loss: 0.0179
Epoch [22/30], Batch [900/6000], Loss: 0.0202
Epoch [22/30], Batch [1000/6000], Loss: 0.0187
Epoch [22/30], Batch [1100/6000], Loss: 0.0198
Epoch [22/30], Batch [1200/6000], Loss: 0.0231
Epoch [22/30], Batch [1300/6000], Loss: 0.0177
Epoch [22/30], Batch [1400/6000], Loss: 0.0175
Epoch [22/30], Batch [1500/6000], Loss: 0.0230
Epoch [22/30], Batch [1600/6000], Loss: 0.0243
Epoch [22/30], Batch [1700/6000], Loss: 0.0179
Epoch [22/30], Batch [1800/6000], Loss: 0.0207
Epoch [22/30], Batch [1900/6000], Loss: 0.0146
Epoch [22/30], Batch [2000/6000], Loss: 0.0250
Epoch [22/30], Batch [2100/6000], Loss: 0.0305
Epoch [22/30], Batch [2200/6000], Loss: 0.0268
Epoch [22/30], Batch [2300/6000], Loss: 0.0186
Epoch [22/30], Batch [2400/6000], Loss: 0.0193
Epoch [22/30], Batch [2500/6000], Loss: 0.0186
Epoch [22/30], Batch [2600/6000], Loss: 0.0200
Epoch [22/30], Batch [2700/6000], Loss: 0.0170
Epoch [22/30], Batch [2800/6000], Loss: 0.0219
Epoch [22/30], Batch [2900/6000], Loss: 0.0197
Epoch [22/30], Batch [3000/6000], Loss: 0.0179
Epoch [22/30], Batch [3100/6000], Loss: 0.0238
Epoch [22/30], Batch [3200/6000], Loss: 0.0175
Epoch [22/30], Batch [3300/6000], Loss: 0.0200
Epoch [22/30], Batch [3400/6000], Loss: 0.0217
Epoch [22/30], Batch [3500/6000], Loss: 0.0350
Epoch [22/30], Batch [3600/6000], Loss: 0.0184
Epoch [22/30], Batch [3700/6000], Loss: 0.0220
Epoch [22/30], Batch [3800/6000], Loss: 0.0185
Epoch [22/30], Batch [3900/6000], Loss: 0.0168
Epoch [22/30], Batch [4000/6000], Loss: 0.0178
Epoch [22/30], Batch [4100/6000], Loss: 0.0157
Epoch [22/30], Batch [4200/6000], Loss: 0.0254
Epoch [22/30], Batch [4300/6000], Loss: 0.0180
Epoch [22/30], Batch [4400/6000], Loss: 0.0264
Epoch [22/30], Batch [4500/6000], Loss: 0.0210
Epoch [22/30], Batch [4600/6000], Loss: 0.0183
Epoch [22/30], Batch [4700/6000], Loss: 0.1577
Epoch [22/30], Batch [4800/6000], Loss: 0.0428
Epoch [22/30], Batch [4900/6000], Loss: 0.0164
Epoch [22/30], Batch [5000/6000], Loss: 0.0244
Epoch [22/30], Batch [5100/6000], Loss: 0.0326
Epoch [22/30], Batch [5200/6000], Loss: 0.0230
Epoch [22/30], Batch [5300/6000], Loss: 0.0385
Epoch [22/30], Batch [5400/6000], Loss: 0.0173
Epoch [22/30], Batch [5500/6000], Loss: 0.0171
Epoch [22/30], Batch [5600/6000], Loss: 0.0226
Epoch [22/30], Batch [5700/6000], Loss: 0.0184
Epoch [22/30], Batch [5800/6000], Loss: 0.0198
Epoch [22/30], Batch [5900/6000], Loss: 0.0261
Epoch [22/30], Loss: 0.0320
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.0174
Epoch [23/30], Batch [100/6000], Loss: 0.0186
Epoch [23/30], Batch [200/6000], Loss: 0.0208
Epoch [23/30], Batch [300/6000], Loss: 0.0255
Epoch [23/30], Batch [400/6000], Loss: 0.0190
Epoch [23/30], Batch [500/6000], Loss: 0.0198
Epoch [23/30], Batch [600/6000], Loss: 0.0162
Epoch [23/30], Batch [700/6000], Loss: 0.0187
Epoch [23/30], Batch [800/6000], Loss: 0.2535
Epoch [23/30], Batch [900/6000], Loss: 0.0201
Epoch [23/30], Batch [1000/6000], Loss: 0.0318
Epoch [23/30], Batch [1100/6000], Loss: 0.0199
Epoch [23/30], Batch [1200/6000], Loss: 0.0195
Epoch [23/30], Batch [1300/6000], Loss: 0.0230
Epoch [23/30], Batch [1400/6000], Loss: 0.0205
Epoch [23/30], Batch [1500/6000], Loss: 0.0830
Epoch [23/30], Batch [1600/6000], Loss: 0.0178
Epoch [23/30], Batch [1700/6000], Loss: 0.0182
Epoch [23/30], Batch [1800/6000], Loss: 0.0265
Epoch [23/30], Batch [1900/6000], Loss: 0.0463
Epoch [23/30], Batch [2000/6000], Loss: 0.0177
Epoch [23/30], Batch [2100/6000], Loss: 0.0228
Epoch [23/30], Batch [2200/6000], Loss: 0.0873
Epoch [23/30], Batch [2300/6000], Loss: 0.0223
Epoch [23/30], Batch [2400/6000], Loss: 0.0186
Epoch [23/30], Batch [2500/6000], Loss: 0.0449
Epoch [23/30], Batch [2600/6000], Loss: 0.0151
Epoch [23/30], Batch [2700/6000], Loss: 0.0175
Epoch [23/30], Batch [2800/6000], Loss: 0.0176
Epoch [23/30], Batch [2900/6000], Loss: 0.0225
Epoch [23/30], Batch [3000/6000], Loss: 0.0186
Epoch [23/30], Batch [3100/6000], Loss: 0.0238
Epoch [23/30], Batch [3200/6000], Loss: 0.0223
Epoch [23/30], Batch [3300/6000], Loss: 0.0167
Epoch [23/30], Batch [3400/6000], Loss: 0.0174
Epoch [23/30], Batch [3500/6000], Loss: 0.0186
Epoch [23/30], Batch [3600/6000], Loss: 0.0645
Epoch [23/30], Batch [3700/6000], Loss: 0.0201
Epoch [23/30], Batch [3800/6000], Loss: 0.0188
Epoch [23/30], Batch [3900/6000], Loss: 0.0192
Epoch [23/30], Batch [4000/6000], Loss: 0.0184
Epoch [23/30], Batch [4100/6000], Loss: 0.0164
Epoch [23/30], Batch [4200/6000], Loss: 0.0189
Epoch [23/30], Batch [4300/6000], Loss: 0.0251
Epoch [23/30], Batch [4400/6000], Loss: 0.0312
Epoch [23/30], Batch [4500/6000], Loss: 0.0166
Epoch [23/30], Batch [4600/6000], Loss: 0.0182
Epoch [23/30], Batch [4700/6000], Loss: 0.0168
Epoch [23/30], Batch [4800/6000], Loss: 0.0923
Epoch [23/30], Batch [4900/6000], Loss: 0.0211
Epoch [23/30], Batch [5000/6000], Loss: 0.5309
Epoch [23/30], Batch [5100/6000], Loss: 0.0198
Epoch [23/30], Batch [5200/6000], Loss: 0.0213
Epoch [23/30], Batch [5300/6000], Loss: 0.0449
Epoch [23/30], Batch [5400/6000], Loss: 0.0156
Epoch [23/30], Batch [5500/6000], Loss: 0.0188
Epoch [23/30], Batch [5600/6000], Loss: 0.0155
Epoch [23/30], Batch [5700/6000], Loss: 0.0154
Epoch [23/30], Batch [5800/6000], Loss: 0.0214
Epoch [23/30], Batch [5900/6000], Loss: 0.0178
Epoch [23/30], Loss: 0.0325
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.0173
Epoch [24/30], Batch [100/6000], Loss: 0.0206
Epoch [24/30], Batch [200/6000], Loss: 0.0233
Epoch [24/30], Batch [300/6000], Loss: 0.0180
Epoch [24/30], Batch [400/6000], Loss: 0.0185
Epoch [24/30], Batch [500/6000], Loss: 0.0192
Epoch [24/30], Batch [600/6000], Loss: 0.0200
Epoch [24/30], Batch [700/6000], Loss: 0.0216
Epoch [24/30], Batch [800/6000], Loss: 0.0209
Epoch [24/30], Batch [900/6000], Loss: 0.0189
Epoch [24/30], Batch [1000/6000], Loss: 0.0225
Epoch [24/30], Batch [1100/6000], Loss: 0.0202
Epoch [24/30], Batch [1200/6000], Loss: 0.0164
Epoch [24/30], Batch [1300/6000], Loss: 0.0192
Epoch [24/30], Batch [1400/6000], Loss: 0.0181
Epoch [24/30], Batch [1500/6000], Loss: 0.0171
Epoch [24/30], Batch [1600/6000], Loss: 0.0174
Epoch [24/30], Batch [1700/6000], Loss: 0.0204
Epoch [24/30], Batch [1800/6000], Loss: 0.0230
Epoch [24/30], Batch [1900/6000], Loss: 0.0204
Epoch [24/30], Batch [2000/6000], Loss: 0.0209
Epoch [24/30], Batch [2100/6000], Loss: 0.3032
Epoch [24/30], Batch [2200/6000], Loss: 0.0165
Epoch [24/30], Batch [2300/6000], Loss: 0.0286
Epoch [24/30], Batch [2400/6000], Loss: 0.0202
Epoch [24/30], Batch [2500/6000], Loss: 0.0168
Epoch [24/30], Batch [2600/6000], Loss: 0.0187
Epoch [24/30], Batch [2700/6000], Loss: 0.0781
Epoch [24/30], Batch [2800/6000], Loss: 0.0184
Epoch [24/30], Batch [2900/6000], Loss: 0.0239
Epoch [24/30], Batch [3000/6000], Loss: 0.0228
Epoch [24/30], Batch [3100/6000], Loss: 0.0144
Epoch [24/30], Batch [3200/6000], Loss: 0.0158
Epoch [24/30], Batch [3300/6000], Loss: 0.0204
Epoch [24/30], Batch [3400/6000], Loss: 0.0154
Epoch [24/30], Batch [3500/6000], Loss: 0.0204
Epoch [24/30], Batch [3600/6000], Loss: 0.0544
Epoch [24/30], Batch [3700/6000], Loss: 0.0203
Epoch [24/30], Batch [3800/6000], Loss: 0.0184
Epoch [24/30], Batch [3900/6000], Loss: 0.0216
Epoch [24/30], Batch [4000/6000], Loss: 0.0207
Epoch [24/30], Batch [4100/6000], Loss: 0.0167
Epoch [24/30], Batch [4200/6000], Loss: 0.0231
Epoch [24/30], Batch [4300/6000], Loss: 0.0260
Epoch [24/30], Batch [4400/6000], Loss: 0.0212
Epoch [24/30], Batch [4500/6000], Loss: 0.0220
Epoch [24/30], Batch [4600/6000], Loss: 0.0205
Epoch [24/30], Batch [4700/6000], Loss: 0.0219
Epoch [24/30], Batch [4800/6000], Loss: 0.0192
Epoch [24/30], Batch [4900/6000], Loss: 0.0199
Epoch [24/30], Batch [5000/6000], Loss: 0.0153
Epoch [24/30], Batch [5100/6000], Loss: 0.0187
Epoch [24/30], Batch [5200/6000], Loss: 0.0514
Epoch [24/30], Batch [5300/6000], Loss: 0.0791
Epoch [24/30], Batch [5400/6000], Loss: 0.0195
Epoch [24/30], Batch [5500/6000], Loss: 0.0174
Epoch [24/30], Batch [5600/6000], Loss: 0.0221
Epoch [24/30], Batch [5700/6000], Loss: 0.0229
Epoch [24/30], Batch [5800/6000], Loss: 0.0207
Epoch [24/30], Batch [5900/6000], Loss: 0.0214
Epoch [24/30], Loss: 0.0298
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.0184
Epoch [25/30], Batch [100/6000], Loss: 0.0214
Epoch [25/30], Batch [200/6000], Loss: 0.0224
Epoch [25/30], Batch [300/6000], Loss: 0.0210
Epoch [25/30], Batch [400/6000], Loss: 0.0210
Epoch [25/30], Batch [500/6000], Loss: 0.0249
Epoch [25/30], Batch [600/6000], Loss: 0.0184
Epoch [25/30], Batch [700/6000], Loss: 0.0199
Epoch [25/30], Batch [800/6000], Loss: 0.0177
Epoch [25/30], Batch [900/6000], Loss: 0.0197
Epoch [25/30], Batch [1000/6000], Loss: 0.0202
Epoch [25/30], Batch [1100/6000], Loss: 0.0158
Epoch [25/30], Batch [1200/6000], Loss: 0.0221
Epoch [25/30], Batch [1300/6000], Loss: 0.0179
Epoch [25/30], Batch [1400/6000], Loss: 0.0190
Epoch [25/30], Batch [1500/6000], Loss: 0.0163
Epoch [25/30], Batch [1600/6000], Loss: 0.0253
Epoch [25/30], Batch [1700/6000], Loss: 0.0309
Epoch [25/30], Batch [1800/6000], Loss: 0.0204
Epoch [25/30], Batch [1900/6000], Loss: 0.0239
Epoch [25/30], Batch [2000/6000], Loss: 0.0235
Epoch [25/30], Batch [2100/6000], Loss: 0.0173
Epoch [25/30], Batch [2200/6000], Loss: 0.0192
Epoch [25/30], Batch [2300/6000], Loss: 0.0192
Epoch [25/30], Batch [2400/6000], Loss: 0.0172
Epoch [25/30], Batch [2500/6000], Loss: 0.0202
Epoch [25/30], Batch [2600/6000], Loss: 0.0199
Epoch [25/30], Batch [2700/6000], Loss: 0.0157
Epoch [25/30], Batch [2800/6000], Loss: 0.0212
Epoch [25/30], Batch [2900/6000], Loss: 0.0335
Epoch [25/30], Batch [3000/6000], Loss: 0.0153
Epoch [25/30], Batch [3100/6000], Loss: 0.1180
Epoch [25/30], Batch [3200/6000], Loss: 0.0167
Epoch [25/30], Batch [3300/6000], Loss: 0.0169
Epoch [25/30], Batch [3400/6000], Loss: 0.0347
Epoch [25/30], Batch [3500/6000], Loss: 0.0169
Epoch [25/30], Batch [3600/6000], Loss: 0.0161
Epoch [25/30], Batch [3700/6000], Loss: 0.0191
Epoch [25/30], Batch [3800/6000], Loss: 0.0153
Epoch [25/30], Batch [3900/6000], Loss: 0.0195
Epoch [25/30], Batch [4000/6000], Loss: 0.0163
Epoch [25/30], Batch [4100/6000], Loss: 0.0148
Epoch [25/30], Batch [4200/6000], Loss: 0.0143
Epoch [25/30], Batch [4300/6000], Loss: 0.0200
Epoch [25/30], Batch [4400/6000], Loss: 0.0187
Epoch [25/30], Batch [4500/6000], Loss: 0.0204
Epoch [25/30], Batch [4600/6000], Loss: 0.0201
Epoch [25/30], Batch [4700/6000], Loss: 0.0152
Epoch [25/30], Batch [4800/6000], Loss: 0.0172
Epoch [25/30], Batch [4900/6000], Loss: 0.0176
Epoch [25/30], Batch [5000/6000], Loss: 0.0158
Epoch [25/30], Batch [5100/6000], Loss: 0.0170
Epoch [25/30], Batch [5200/6000], Loss: 0.0167
Epoch [25/30], Batch [5300/6000], Loss: 0.0139
Epoch [25/30], Batch [5400/6000], Loss: 0.4326
Epoch [25/30], Batch [5500/6000], Loss: 0.0205
Epoch [25/30], Batch [5600/6000], Loss: 0.0237
Epoch [25/30], Batch [5700/6000], Loss: 0.0231
Epoch [25/30], Batch [5800/6000], Loss: 0.0225
Epoch [25/30], Batch [5900/6000], Loss: 0.0189
Epoch [25/30], Loss: 0.0304
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.0209
Epoch [26/30], Batch [100/6000], Loss: 0.0207
Epoch [26/30], Batch [200/6000], Loss: 0.0165
Epoch [26/30], Batch [300/6000], Loss: 0.0194
Epoch [26/30], Batch [400/6000], Loss: 0.0175
Epoch [26/30], Batch [500/6000], Loss: 0.0159
Epoch [26/30], Batch [600/6000], Loss: 0.0209
Epoch [26/30], Batch [700/6000], Loss: 0.0196
Epoch [26/30], Batch [800/6000], Loss: 0.0171
Epoch [26/30], Batch [900/6000], Loss: 0.0173
Epoch [26/30], Batch [1000/6000], Loss: 0.0162
Epoch [26/30], Batch [1100/6000], Loss: 0.0224
Epoch [26/30], Batch [1200/6000], Loss: 0.0208
Epoch [26/30], Batch [1300/6000], Loss: 0.0166
Epoch [26/30], Batch [1400/6000], Loss: 0.0177
Epoch [26/30], Batch [1500/6000], Loss: 0.0161
Epoch [26/30], Batch [1600/6000], Loss: 0.0256
Epoch [26/30], Batch [1700/6000], Loss: 0.0180
Epoch [26/30], Batch [1800/6000], Loss: 0.0224
Epoch [26/30], Batch [1900/6000], Loss: 0.0218
Epoch [26/30], Batch [2000/6000], Loss: 0.1566
Epoch [26/30], Batch [2100/6000], Loss: 0.0357
Epoch [26/30], Batch [2200/6000], Loss: 0.0214
Epoch [26/30], Batch [2300/6000], Loss: 0.0175
Epoch [26/30], Batch [2400/6000], Loss: 0.0853
Epoch [26/30], Batch [2500/6000], Loss: 0.0178
Epoch [26/30], Batch [2600/6000], Loss: 0.0184
Epoch [26/30], Batch [2700/6000], Loss: 0.0187
Epoch [26/30], Batch [2800/6000], Loss: 0.0178
Epoch [26/30], Batch [2900/6000], Loss: 0.0294
Epoch [26/30], Batch [3000/6000], Loss: 0.0170
Epoch [26/30], Batch [3100/6000], Loss: 0.0234
Epoch [26/30], Batch [3200/6000], Loss: 0.0242
Epoch [26/30], Batch [3300/6000], Loss: 0.0254
Epoch [26/30], Batch [3400/6000], Loss: 0.0182
Epoch [26/30], Batch [3500/6000], Loss: 0.0235
Epoch [26/30], Batch [3600/6000], Loss: 0.0126
Epoch [26/30], Batch [3700/6000], Loss: 0.0240
Epoch [26/30], Batch [3800/6000], Loss: 0.0156
Epoch [26/30], Batch [3900/6000], Loss: 0.0165
Epoch [26/30], Batch [4000/6000], Loss: 0.0200
Epoch [26/30], Batch [4100/6000], Loss: 0.0200
Epoch [26/30], Batch [4200/6000], Loss: 0.0267
Epoch [26/30], Batch [4300/6000], Loss: 0.1860
Epoch [26/30], Batch [4400/6000], Loss: 0.0171
Epoch [26/30], Batch [4500/6000], Loss: 0.0187
Epoch [26/30], Batch [4600/6000], Loss: 0.0177
Epoch [26/30], Batch [4700/6000], Loss: 0.0213
Epoch [26/30], Batch [4800/6000], Loss: 0.0342
Epoch [26/30], Batch [4900/6000], Loss: 0.3121
Epoch [26/30], Batch [5000/6000], Loss: 0.0164
Epoch [26/30], Batch [5100/6000], Loss: 0.0819
Epoch [26/30], Batch [5200/6000], Loss: 0.0147
Epoch [26/30], Batch [5300/6000], Loss: 0.5126
Epoch [26/30], Batch [5400/6000], Loss: 0.0433
Epoch [26/30], Batch [5500/6000], Loss: 0.0160
Epoch [26/30], Batch [5600/6000], Loss: 0.0241
Epoch [26/30], Batch [5700/6000], Loss: 0.0185
Epoch [26/30], Batch [5800/6000], Loss: 0.0168
Epoch [26/30], Batch [5900/6000], Loss: 0.0365
Epoch [26/30], Loss: 0.0290
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0180
Epoch [27/30], Batch [100/6000], Loss: 0.0217
Epoch [27/30], Batch [200/6000], Loss: 0.0160
Epoch [27/30], Batch [300/6000], Loss: 0.0170
Epoch [27/30], Batch [400/6000], Loss: 0.0171
Epoch [27/30], Batch [500/6000], Loss: 0.0180
Epoch [27/30], Batch [600/6000], Loss: 0.0169
Epoch [27/30], Batch [700/6000], Loss: 0.0154
Epoch [27/30], Batch [800/6000], Loss: 0.0297
Epoch [27/30], Batch [900/6000], Loss: 0.0181
Epoch [27/30], Batch [1000/6000], Loss: 0.0186
Epoch [27/30], Batch [1100/6000], Loss: 0.0207
Epoch [27/30], Batch [1200/6000], Loss: 0.0201
Epoch [27/30], Batch [1300/6000], Loss: 0.0220
Epoch [27/30], Batch [1400/6000], Loss: 0.0214
Epoch [27/30], Batch [1500/6000], Loss: 0.0206
Epoch [27/30], Batch [1600/6000], Loss: 0.0181
Epoch [27/30], Batch [1700/6000], Loss: 0.0196
Epoch [27/30], Batch [1800/6000], Loss: 0.0152
Epoch [27/30], Batch [1900/6000], Loss: 0.0193
Epoch [27/30], Batch [2000/6000], Loss: 0.0195
Epoch [27/30], Batch [2100/6000], Loss: 0.0185
Epoch [27/30], Batch [2200/6000], Loss: 0.0249
Epoch [27/30], Batch [2300/6000], Loss: 0.0189
Epoch [27/30], Batch [2400/6000], Loss: 0.0203
Epoch [27/30], Batch [2500/6000], Loss: 0.0211
Epoch [27/30], Batch [2600/6000], Loss: 0.0220
Epoch [27/30], Batch [2700/6000], Loss: 0.0197
Epoch [27/30], Batch [2800/6000], Loss: 0.0205
Epoch [27/30], Batch [2900/6000], Loss: 0.0194
Epoch [27/30], Batch [3000/6000], Loss: 0.0204
Epoch [27/30], Batch [3100/6000], Loss: 0.0181
Epoch [27/30], Batch [3200/6000], Loss: 0.0171
Epoch [27/30], Batch [3300/6000], Loss: 0.0158
Epoch [27/30], Batch [3400/6000], Loss: 0.0184
Epoch [27/30], Batch [3500/6000], Loss: 0.0302
Epoch [27/30], Batch [3600/6000], Loss: 0.0182
Epoch [27/30], Batch [3700/6000], Loss: 0.0249
Epoch [27/30], Batch [3800/6000], Loss: 0.0141
Epoch [27/30], Batch [3900/6000], Loss: 0.0172
Epoch [27/30], Batch [4000/6000], Loss: 0.0146
Epoch [27/30], Batch [4100/6000], Loss: 0.0177
Epoch [27/30], Batch [4200/6000], Loss: 0.0169
Epoch [27/30], Batch [4300/6000], Loss: 0.0150
Epoch [27/30], Batch [4400/6000], Loss: 0.0493
Epoch [27/30], Batch [4500/6000], Loss: 0.0179
Epoch [27/30], Batch [4600/6000], Loss: 0.0185
Epoch [27/30], Batch [4700/6000], Loss: 0.0207
Epoch [27/30], Batch [4800/6000], Loss: 0.0235
Epoch [27/30], Batch [4900/6000], Loss: 0.0199
Epoch [27/30], Batch [5000/6000], Loss: 0.0224
Epoch [27/30], Batch [5100/6000], Loss: 0.0178
Epoch [27/30], Batch [5200/6000], Loss: 0.0141
Epoch [27/30], Batch [5300/6000], Loss: 0.0200
Epoch [27/30], Batch [5400/6000], Loss: 0.0209
Epoch [27/30], Batch [5500/6000], Loss: 0.0203
Epoch [27/30], Batch [5600/6000], Loss: 0.0193
Epoch [27/30], Batch [5700/6000], Loss: 0.0178
Epoch [27/30], Batch [5800/6000], Loss: 0.0229
Epoch [27/30], Batch [5900/6000], Loss: 0.0159
Epoch [27/30], Loss: 0.0278
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.0163
Epoch [28/30], Batch [100/6000], Loss: 0.0165
Epoch [28/30], Batch [200/6000], Loss: 0.0218
Epoch [28/30], Batch [300/6000], Loss: 0.0223
Epoch [28/30], Batch [400/6000], Loss: 0.0238
Epoch [28/30], Batch [500/6000], Loss: 0.0176
Epoch [28/30], Batch [600/6000], Loss: 0.0156
Epoch [28/30], Batch [700/6000], Loss: 0.0163
Epoch [28/30], Batch [800/6000], Loss: 0.5513
Epoch [28/30], Batch [900/6000], Loss: 0.0173
Epoch [28/30], Batch [1000/6000], Loss: 0.0162
Epoch [28/30], Batch [1100/6000], Loss: 0.0169
Epoch [28/30], Batch [1200/6000], Loss: 0.0191
Epoch [28/30], Batch [1300/6000], Loss: 0.0190
Epoch [28/30], Batch [1400/6000], Loss: 0.0148
Epoch [28/30], Batch [1500/6000], Loss: 0.0215
Epoch [28/30], Batch [1600/6000], Loss: 0.0198
Epoch [28/30], Batch [1700/6000], Loss: 0.0178
Epoch [28/30], Batch [1800/6000], Loss: 0.0154
Epoch [28/30], Batch [1900/6000], Loss: 0.0194
Epoch [28/30], Batch [2000/6000], Loss: 0.0161
Epoch [28/30], Batch [2100/6000], Loss: 0.0152
Epoch [28/30], Batch [2200/6000], Loss: 0.0205
Epoch [28/30], Batch [2300/6000], Loss: 0.0237
Epoch [28/30], Batch [2400/6000], Loss: 0.0175
Epoch [28/30], Batch [2500/6000], Loss: 0.0209
Epoch [28/30], Batch [2600/6000], Loss: 0.0229
Epoch [28/30], Batch [2700/6000], Loss: 0.0183
Epoch [28/30], Batch [2800/6000], Loss: 0.0186
Epoch [28/30], Batch [2900/6000], Loss: 0.0326
Epoch [28/30], Batch [3000/6000], Loss: 0.0185
Epoch [28/30], Batch [3100/6000], Loss: 0.0175
Epoch [28/30], Batch [3200/6000], Loss: 0.0205
Epoch [28/30], Batch [3300/6000], Loss: 0.0227
Epoch [28/30], Batch [3400/6000], Loss: 0.0190
Epoch [28/30], Batch [3500/6000], Loss: 0.0273
Epoch [28/30], Batch [3600/6000], Loss: 0.0197
Epoch [28/30], Batch [3700/6000], Loss: 0.0146
Epoch [28/30], Batch [3800/6000], Loss: 0.1127
Epoch [28/30], Batch [3900/6000], Loss: 0.0189
Epoch [28/30], Batch [4000/6000], Loss: 0.0360
Epoch [28/30], Batch [4100/6000], Loss: 0.0154
Epoch [28/30], Batch [4200/6000], Loss: 0.0126
Epoch [28/30], Batch [4300/6000], Loss: 0.0192
Epoch [28/30], Batch [4400/6000], Loss: 0.0445
Epoch [28/30], Batch [4500/6000], Loss: 0.0180
Epoch [28/30], Batch [4600/6000], Loss: 0.0212
Epoch [28/30], Batch [4700/6000], Loss: 0.0138
Epoch [28/30], Batch [4800/6000], Loss: 0.0253
Epoch [28/30], Batch [4900/6000], Loss: 0.0382
Epoch [28/30], Batch [5000/6000], Loss: 0.0214
Epoch [28/30], Batch [5100/6000], Loss: 0.0209
Epoch [28/30], Batch [5200/6000], Loss: 0.0201
Epoch [28/30], Batch [5300/6000], Loss: 0.0196
Epoch [28/30], Batch [5400/6000], Loss: 0.0157
Epoch [28/30], Batch [5500/6000], Loss: 0.0183
Epoch [28/30], Batch [5600/6000], Loss: 0.0183
Epoch [28/30], Batch [5700/6000], Loss: 0.0187
Epoch [28/30], Batch [5800/6000], Loss: 0.0172
Epoch [28/30], Batch [5900/6000], Loss: 0.0219
Epoch [28/30], Loss: 0.0275
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0174
Epoch [29/30], Batch [100/6000], Loss: 0.0230
Epoch [29/30], Batch [200/6000], Loss: 0.0178
Epoch [29/30], Batch [300/6000], Loss: 0.0168
Epoch [29/30], Batch [400/6000], Loss: 0.0160
Epoch [29/30], Batch [500/6000], Loss: 0.0147
Epoch [29/30], Batch [600/6000], Loss: 0.0182
Epoch [29/30], Batch [700/6000], Loss: 0.0261
Epoch [29/30], Batch [800/6000], Loss: 0.0151
Epoch [29/30], Batch [900/6000], Loss: 0.2812
Epoch [29/30], Batch [1000/6000], Loss: 0.0485
Epoch [29/30], Batch [1100/6000], Loss: 0.0208
Epoch [29/30], Batch [1200/6000], Loss: 0.0185
Epoch [29/30], Batch [1300/6000], Loss: 0.0191
Epoch [29/30], Batch [1400/6000], Loss: 0.0152
Epoch [29/30], Batch [1500/6000], Loss: 0.0137
Epoch [29/30], Batch [1600/6000], Loss: 0.0197
Epoch [29/30], Batch [1700/6000], Loss: 0.0343
Epoch [29/30], Batch [1800/6000], Loss: 0.0204
Epoch [29/30], Batch [1900/6000], Loss: 0.0205
Epoch [29/30], Batch [2000/6000], Loss: 0.0215
Epoch [29/30], Batch [2100/6000], Loss: 0.0186
Epoch [29/30], Batch [2200/6000], Loss: 0.0190
Epoch [29/30], Batch [2300/6000], Loss: 0.0184
Epoch [29/30], Batch [2400/6000], Loss: 0.0182
Epoch [29/30], Batch [2500/6000], Loss: 0.0161
Epoch [29/30], Batch [2600/6000], Loss: 0.0198
Epoch [29/30], Batch [2700/6000], Loss: 0.0209
Epoch [29/30], Batch [2800/6000], Loss: 0.0213
Epoch [29/30], Batch [2900/6000], Loss: 0.0438
Epoch [29/30], Batch [3000/6000], Loss: 0.0119
Epoch [29/30], Batch [3100/6000], Loss: 0.0273
Epoch [29/30], Batch [3200/6000], Loss: 0.0222
Epoch [29/30], Batch [3300/6000], Loss: 0.0181
Epoch [29/30], Batch [3400/6000], Loss: 0.0170
Epoch [29/30], Batch [3500/6000], Loss: 0.0156
Epoch [29/30], Batch [3600/6000], Loss: 0.0225
Epoch [29/30], Batch [3700/6000], Loss: 0.0144
Epoch [29/30], Batch [3800/6000], Loss: 0.0173
Epoch [29/30], Batch [3900/6000], Loss: 0.0878
Epoch [29/30], Batch [4000/6000], Loss: 0.0226
Epoch [29/30], Batch [4100/6000], Loss: 0.0195
Epoch [29/30], Batch [4200/6000], Loss: 0.0159
Epoch [29/30], Batch [4300/6000], Loss: 0.0239
Epoch [29/30], Batch [4400/6000], Loss: 0.0161
Epoch [29/30], Batch [4500/6000], Loss: 0.0158
Epoch [29/30], Batch [4600/6000], Loss: 0.2486
Epoch [29/30], Batch [4700/6000], Loss: 0.0552
Epoch [29/30], Batch [4800/6000], Loss: 0.0187
Epoch [29/30], Batch [4900/6000], Loss: 0.0175
Epoch [29/30], Batch [5000/6000], Loss: 0.0210
Epoch [29/30], Batch [5100/6000], Loss: 0.1351
Epoch [29/30], Batch [5200/6000], Loss: 0.0175
Epoch [29/30], Batch [5300/6000], Loss: 0.0206
Epoch [29/30], Batch [5400/6000], Loss: 0.0203
Epoch [29/30], Batch [5500/6000], Loss: 0.0215
Epoch [29/30], Batch [5600/6000], Loss: 0.0202
Epoch [29/30], Batch [5700/6000], Loss: 0.0169
Epoch [29/30], Batch [5800/6000], Loss: 0.0170
Epoch [29/30], Batch [5900/6000], Loss: 0.0178
Epoch [29/30], Loss: 0.0273
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0182
Epoch [30/30], Batch [100/6000], Loss: 0.0180
Epoch [30/30], Batch [200/6000], Loss: 0.0152
Epoch [30/30], Batch [300/6000], Loss: 0.0162
Epoch [30/30], Batch [400/6000], Loss: 0.0542
Epoch [30/30], Batch [500/6000], Loss: 0.0158
Epoch [30/30], Batch [600/6000], Loss: 0.0182
Epoch [30/30], Batch [700/6000], Loss: 0.0258
Epoch [30/30], Batch [800/6000], Loss: 0.0168
Epoch [30/30], Batch [900/6000], Loss: 0.0240
Epoch [30/30], Batch [1000/6000], Loss: 0.0181
Epoch [30/30], Batch [1100/6000], Loss: 0.0251
Epoch [30/30], Batch [1200/6000], Loss: 0.0191
Epoch [30/30], Batch [1300/6000], Loss: 0.0198
Epoch [30/30], Batch [1400/6000], Loss: 0.0184
Epoch [30/30], Batch [1500/6000], Loss: 0.0404
Epoch [30/30], Batch [1600/6000], Loss: 0.0183
Epoch [30/30], Batch [1700/6000], Loss: 0.0174
Epoch [30/30], Batch [1800/6000], Loss: 0.0298
Epoch [30/30], Batch [1900/6000], Loss: 0.0150
Epoch [30/30], Batch [2000/6000], Loss: 0.0151
Epoch [30/30], Batch [2100/6000], Loss: 0.0186
Epoch [30/30], Batch [2200/6000], Loss: 0.0153
Epoch [30/30], Batch [2300/6000], Loss: 0.0172
Epoch [30/30], Batch [2400/6000], Loss: 0.0172
Epoch [30/30], Batch [2500/6000], Loss: 0.0166
Epoch [30/30], Batch [2600/6000], Loss: 0.0791
Epoch [30/30], Batch [2700/6000], Loss: 0.0192
Epoch [30/30], Batch [2800/6000], Loss: 0.0155
Epoch [30/30], Batch [2900/6000], Loss: 0.0200
Epoch [30/30], Batch [3000/6000], Loss: 0.0291
Epoch [30/30], Batch [3100/6000], Loss: 0.0192
Epoch [30/30], Batch [3200/6000], Loss: 0.0135
Epoch [30/30], Batch [3300/6000], Loss: 0.0156
Epoch [30/30], Batch [3400/6000], Loss: 0.0184
Epoch [30/30], Batch [3500/6000], Loss: 0.0178
Epoch [30/30], Batch [3600/6000], Loss: 0.0235
Epoch [30/30], Batch [3700/6000], Loss: 0.0181
Epoch [30/30], Batch [3800/6000], Loss: 0.0195
Epoch [30/30], Batch [3900/6000], Loss: 0.1213
Epoch [30/30], Batch [4000/6000], Loss: 0.0198
Epoch [30/30], Batch [4100/6000], Loss: 0.0220
Epoch [30/30], Batch [4200/6000], Loss: 0.0171
Epoch [30/30], Batch [4300/6000], Loss: 0.0167
Epoch [30/30], Batch [4400/6000], Loss: 0.1978
Epoch [30/30], Batch [4500/6000], Loss: 0.0187
Epoch [30/30], Batch [4600/6000], Loss: 0.0209
Epoch [30/30], Batch [4700/6000], Loss: 0.0190
Epoch [30/30], Batch [4800/6000], Loss: 0.0236
Epoch [30/30], Batch [4900/6000], Loss: 0.0212
Epoch [30/30], Batch [5000/6000], Loss: 0.0203
Epoch [30/30], Batch [5100/6000], Loss: 0.0163
Epoch [30/30], Batch [5200/6000], Loss: 0.0172
Epoch [30/30], Batch [5300/6000], Loss: 0.0186
Epoch [30/30], Batch [5400/6000], Loss: 0.0143
Epoch [30/30], Batch [5500/6000], Loss: 0.0140
Epoch [30/30], Batch [5600/6000], Loss: 0.0142
Epoch [30/30], Batch [5700/6000], Loss: 0.0194
Epoch [30/30], Batch [5800/6000], Loss: 0.0160
Epoch [30/30], Batch [5900/6000], Loss: 0.0142
Epoch [30/30], Loss: 0.0267
Visualization saved to figures/visualization_0.png
Test Loss: 0.1267, Accuracy: 97.88%
  Output probs: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 1.5715
  Image Loss: 0.0051
  Total Loss: 157.1542
  Image grad max: 29.60727310180664
  Output probs: [[0.    0.    0.    0.002 0.    0.    0.958 0.    0.04  0.   ]]
Adversarial Training Loop 2/300:
  Label Loss: 1.0887
  Image Loss: 0.0112
  Total Loss: 108.8773
  Image grad max: 36.13593292236328
  Output probs: [[0.    0.    0.001 0.999 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 3/300:
  Label Loss: 1.0436
  Image Loss: 0.0175
  Total Loss: 104.3791
  Image grad max: 5.931090831756592
  Output probs: [[0.    0.    0.013 0.987 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 4/300:
  Label Loss: 1.0504
  Image Loss: 0.0258
  Total Loss: 105.0704
  Image grad max: 7.007537841796875
  Output probs: [[0.    0.    0.884 0.116 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 5/300:
  Label Loss: 1.1326
  Image Loss: 0.0347
  Total Loss: 113.2975
  Image grad max: 31.734106063842773
  Output probs: [[0.    0.    0.001 0.999 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 6/300:
  Label Loss: 1.0039
  Image Loss: 0.0409
  Total Loss: 100.4279
  Image grad max: 6.855685710906982
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 7/300:
  Label Loss: 0.9896
  Image Loss: 0.0475
  Total Loss: 99.0121
  Image grad max: 6.247167110443115
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 8/300:
  Label Loss: 0.9217
  Image Loss: 0.0542
  Total Loss: 92.2240
  Image grad max: 6.448795318603516
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 9/300:
  Label Loss: 0.8039
  Image Loss: 0.0612
  Total Loss: 80.4546
  Image grad max: 7.182470321655273
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 10/300:
  Label Loss: 0.6029
  Image Loss: 0.0685
  Total Loss: 60.3562
  Image grad max: 8.272954940795898
  Output probs: [[0.    0.001 0.09  0.909 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 11/300:
  Label Loss: 0.2672
  Image Loss: 0.0762
  Total Loss: 26.7988
  Image grad max: 7.844079494476318
  Output probs: [[0.    0.137 0.859 0.004 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 12/300:
  Label Loss: 0.3077
  Image Loss: 0.0848
  Total Loss: 30.8581
  Image grad max: 29.305580139160156
  Output probs: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 13/300:
  Label Loss: 0.4884
  Image Loss: 0.0930
  Total Loss: 48.9314
  Image grad max: 13.473746299743652
  Output probs: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 14/300:
  Label Loss: 0.4725
  Image Loss: 0.0995
  Total Loss: 47.3481
  Image grad max: 13.248265266418457
  Output probs: [[0.    0.992 0.    0.008 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 15/300:
  Label Loss: 0.1720
  Image Loss: 0.1049
  Total Loss: 17.3032
  Image grad max: 13.304771423339844
  Output probs: [[0.    0.005 0.    0.995 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 16/300:
  Label Loss: 0.2003
  Image Loss: 0.1097
  Total Loss: 20.1374
  Image grad max: 12.45828628540039
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 17/300:
  Label Loss: 0.3721
  Image Loss: 0.1151
  Total Loss: 37.3266
  Image grad max: 11.300151824951172
  Output probs: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 18/300:
  Label Loss: 0.3686
  Image Loss: 0.1206
  Total Loss: 36.9811
  Image grad max: 11.150322914123535
  Output probs: [[0.    0.003 0.    0.997 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 19/300:
  Label Loss: 0.2303
  Image Loss: 0.1260
  Total Loss: 23.1567
  Image grad max: 11.595234870910645
  Output probs: [[0.    0.404 0.    0.596 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0019
  Image Loss: 0.1315
  Total Loss: 0.3190
  Image grad max: 2.4164230823516846
  Output probs: [[0.    0.996 0.    0.004 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 21/300:
  Label Loss: 0.2056
  Image Loss: 0.1368
  Total Loss: 20.6978
  Image grad max: 12.548128128051758
  Output probs: [[0.    0.998 0.    0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 22/300:
  Label Loss: 0.2521
  Image Loss: 0.1412
  Total Loss: 25.3544
  Image grad max: 12.164223670959473
  Output probs: [[0.   0.97 0.   0.03 0.   0.   0.   0.   0.   0.  ]]
Adversarial Training Loop 23/300:
  Label Loss: 0.1073
  Image Loss: 0.1447
  Total Loss: 10.8707
  Image grad max: 11.467596054077148
  Output probs: [[0.    0.124 0.    0.876 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0415
  Image Loss: 0.1477
  Total Loss: 4.2997
  Image grad max: 8.416802406311035
  Output probs: [[0.    0.016 0.    0.984 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 25/300:
  Label Loss: 0.1379
  Image Loss: 0.1509
  Total Loss: 13.9366
  Image grad max: 10.355045318603516
  Output probs: [[0.    0.029 0.    0.971 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 26/300:
  Label Loss: 0.1090
  Image Loss: 0.1540
  Total Loss: 11.0504
  Image grad max: 10.022564888000488
  Output probs: [[0.    0.357 0.    0.643 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0043
  Image Loss: 0.1570
  Total Loss: 0.5865
  Image grad max: 3.0669353008270264
  Output probs: [[0.    0.936 0.    0.064 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0715
  Image Loss: 0.1599
  Total Loss: 7.3111
  Image grad max: 9.27750015258789
  Output probs: [[0.    0.955 0.    0.045 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0882
  Image Loss: 0.1623
  Total Loss: 8.9780
  Image grad max: 9.456425666809082
  Output probs: [[0.    0.725 0.    0.275 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0113
  Image Loss: 0.1642
  Total Loss: 1.2986
  Image grad max: 4.552489757537842
  Output probs: [[0.    0.125 0.    0.875 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0415
  Image Loss: 0.1659
  Total Loss: 4.3121
  Image grad max: 7.154508113861084
  Output probs: [[0.    0.072 0.    0.928 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0662
  Image Loss: 0.1677
  Total Loss: 6.7842
  Image grad max: 8.068964004516602
  Output probs: [[0.    0.246 0.    0.754 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0149
  Image Loss: 0.1695
  Total Loss: 1.6593
  Image grad max: 4.847605228424072
  Output probs: [[0.    0.796 0.    0.204 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0216
  Image Loss: 0.1712
  Total Loss: 2.3301
  Image grad max: 5.844517230987549
  Output probs: [[0.    0.878 0.    0.122 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0425
  Image Loss: 0.1727
  Total Loss: 4.4231
  Image grad max: 7.45709228515625
  Output probs: [[0.    0.661 0.    0.339 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0054
  Image Loss: 0.1739
  Total Loss: 0.7187
  Image grad max: 3.135500192642212
  Output probs: [[0.    0.224 0.    0.776 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0182
  Image Loss: 0.1749
  Total Loss: 1.9913
  Image grad max: 5.213378429412842
  Output probs: [[0.    0.178 0.    0.822 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0268
  Image Loss: 0.1760
  Total Loss: 2.8556
  Image grad max: 6.03325080871582
  Output probs: [[0.    0.432 0.    0.568 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0009
  Image Loss: 0.1772
  Total Loss: 0.2698
  Image grad max: 1.2862284183502197
  Output probs: [[0.    0.752 0.    0.248 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0146
  Image Loss: 0.1782
  Total Loss: 1.6403
  Image grad max: 4.8799214363098145
  Output probs: [[0.    0.743 0.    0.257 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0135
  Image Loss: 0.1791
  Total Loss: 1.5323
  Image grad max: 4.706170558929443
  Output probs: [[0.    0.453 0.    0.547 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0005
  Image Loss: 0.1798
  Total Loss: 0.2252
  Image grad max: 0.8952591419219971
  Output probs: [[0.    0.269 0.    0.731 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0119
  Image Loss: 0.1805
  Total Loss: 1.3754
  Image grad max: 4.2736968994140625
  Output probs: [[0.    0.357 0.    0.643 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0043
  Image Loss: 0.1812
  Total Loss: 0.6092
  Image grad max: 2.662583351135254
  Output probs: [[0.    0.608 0.    0.392 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0024
  Image Loss: 0.1819
  Total Loss: 0.4221
  Image grad max: 2.047149181365967
  Output probs: [[0.    0.684 0.    0.316 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0073
  Image Loss: 0.1825
  Total Loss: 0.9125
  Image grad max: 3.505626916885376
  Output probs: [[0.    0.538 0.    0.462 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0003
  Image Loss: 0.1830
  Total Loss: 0.2123
  Image grad max: 0.7119548320770264
  Output probs: [[0.    0.365 0.    0.635 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0038
  Image Loss: 0.1834
  Total Loss: 0.5626
  Image grad max: 2.4898884296417236
  Output probs: [[0.    0.378 0.    0.622 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0031
  Image Loss: 0.1838
  Total Loss: 0.4907
  Image grad max: 2.2449846267700195
  Output probs: [[0.    0.538 0.    0.462 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0003
  Image Loss: 0.1843
  Total Loss: 0.2135
  Image grad max: 0.706523597240448
  Output probs: [[0.    0.622 0.    0.378 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0031
  Image Loss: 0.1846
  Total Loss: 0.4920
  Image grad max: 2.281254529953003
  Output probs: [[0.    0.547 0.    0.453 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0004
  Image Loss: 0.1849
  Total Loss: 0.2296
  Image grad max: 0.8712712526321411
  Output probs: [[0.    0.422 0.    0.578 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0012
  Image Loss: 0.1851
  Total Loss: 0.3070
  Image grad max: 1.4228743314743042
  Output probs: [[0.    0.415 0.    0.585 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0015
  Image Loss: 0.1854
  Total Loss: 0.3336
  Image grad max: 1.564476490020752
  Output probs: [[0.    0.515 0.    0.485 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0000
  Image Loss: 0.1856
  Total Loss: 0.1901
  Image grad max: 0.27070802450180054
  Output probs: [[0.    0.578 0.    0.422 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0012
  Image Loss: 0.1858
  Total Loss: 0.3095
  Image grad max: 1.4439042806625366
  Output probs: [[0.    0.535 0.    0.465 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0002
  Image Loss: 0.1860
  Total Loss: 0.2104
  Image grad max: 0.6397090554237366
  Output probs: [[0.    0.451 0.    0.549 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0005
  Image Loss: 0.1861
  Total Loss: 0.2341
  Image grad max: 0.8928900361061096
  Output probs: [[0.    0.445 0.    0.555 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0006
  Image Loss: 0.1862
  Total Loss: 0.2482
  Image grad max: 1.0127222537994385
  Output probs: [[0.    0.511 0.    0.489 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0000
  Image Loss: 0.1863
  Total Loss: 0.1891
  Image grad max: 0.20849551260471344
  Output probs: [[0.    0.551 0.    0.449 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0005
  Image Loss: 0.1864
  Total Loss: 0.2390
  Image grad max: 0.9366952776908875
  Output probs: [[0.    0.518 0.    0.482 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0001
  Image Loss: 0.1865
  Total Loss: 0.1934
  Image grad max: 0.33301472663879395
  Output probs: [[0.    0.465 0.    0.535 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0002
  Image Loss: 0.1865
  Total Loss: 0.2112
  Image grad max: 0.6373292803764343
  Output probs: [[0.    0.468 0.    0.532 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0002
  Image Loss: 0.1866
  Total Loss: 0.2077
  Image grad max: 0.5896640419960022
  Output probs: [[0.    0.514 0.    0.486 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0000
  Image Loss: 0.1867
  Total Loss: 0.1909
  Image grad max: 0.2603725492954254
  Output probs: [[0.    0.533 0.    0.467 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0002
  Image Loss: 0.1867
  Total Loss: 0.2084
  Image grad max: 0.5991170406341553
  Output probs: [[0.    0.503 0.    0.497 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0000
  Image Loss: 0.1867
  Total Loss: 0.1871
  Image grad max: 0.05909951031208038
  Output probs: [[0.    0.474 0.    0.526 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0001
  Image Loss: 0.1867
  Total Loss: 0.2004
  Image grad max: 0.47351351380348206
  Output probs: [[0.    0.486 0.    0.514 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0000
  Image Loss: 0.1867
  Total Loss: 0.1909
  Image grad max: 0.25947239995002747
  Output probs: [[0.    0.516 0.    0.484 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0001
  Image Loss: 0.1867
  Total Loss: 0.1921
  Image grad max: 0.29262804985046387
  Output probs: [[0.    0.518 0.    0.482 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0001
  Image Loss: 0.1867
  Total Loss: 0.1936
  Image grad max: 0.3336694836616516
  Output probs: [[0.    0.493 0.    0.507 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0000
  Image Loss: 0.1867
  Total Loss: 0.1877
  Image grad max: 0.1208389550447464
  Output probs: [[0.    0.482 0.    0.518 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0001
  Image Loss: 0.1866
  Total Loss: 0.1931
  Image grad max: 0.3232639729976654
  Output probs: [[0.    0.499 0.    0.501 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0000
  Image Loss: 0.1866
  Total Loss: 0.1868
  Image grad max: 0.011513445526361465
  Output probs: [[0.    0.514 0.    0.486 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0000
  Image Loss: 0.1866
  Total Loss: 0.1907
  Image grad max: 0.25211137533187866
  Output probs: [[0.    0.506 0.    0.494 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0000
  Image Loss: 0.1866
  Total Loss: 0.1874
  Image grad max: 0.10254146158695221
  Output probs: [[0.   0.49 0.   0.51 0.   0.   0.   0.   0.   0.  ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0000
  Image Loss: 0.1865
  Total Loss: 0.1888
  Image grad max: 0.18732836842536926
  Output probs: [[0.    0.492 0.    0.508 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0000
  Image Loss: 0.1865
  Total Loss: 0.1879
  Image grad max: 0.14572235941886902
  Output probs: [[0.    0.506 0.    0.494 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0000
  Image Loss: 0.1864
  Total Loss: 0.1874
  Image grad max: 0.1139075830578804
  Output probs: [[0.    0.508 0.    0.492 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0000
  Image Loss: 0.1864
  Total Loss: 0.1880
  Image grad max: 0.15257753431797028
  Output probs: [[0.    0.497 0.    0.503 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0000
  Image Loss: 0.1863
  Total Loss: 0.1867
  Image grad max: 0.05982252210378647
  Output probs: [[0.    0.492 0.    0.508 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0000
  Image Loss: 0.1862
  Total Loss: 0.1877
  Image grad max: 0.14263488352298737
  Output probs: [[0.    0.501 0.    0.499 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0000
  Image Loss: 0.1862
  Total Loss: 0.1864
  Image grad max: 0.01866859756410122
  Output probs: [[0.    0.506 0.    0.494 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0000
  Image Loss: 0.1861
  Total Loss: 0.1871
  Image grad max: 0.11599534004926682
  Output probs: [[0.    0.501 0.    0.499 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0000
  Image Loss: 0.1860
  Total Loss: 0.1862
  Image grad max: 0.010544422082602978
  Output probs: [[0.    0.495 0.    0.505 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0000
  Image Loss: 0.1859
  Total Loss: 0.1867
  Image grad max: 0.09814618527889252
  Output probs: [[0.    0.499 0.    0.501 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0000
  Image Loss: 0.1859
  Total Loss: 0.1861
  Image grad max: 0.027423840016126633
  Output probs: [[0.    0.504 0.    0.496 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0000
  Image Loss: 0.1858
  Total Loss: 0.1863
  Image grad max: 0.07372435927391052
  Output probs: [[0.    0.502 0.    0.498 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0000
  Image Loss: 0.1857
  Total Loss: 0.1860
  Image grad max: 0.03652898594737053
  Output probs: [[0.    0.497 0.    0.503 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0000
  Image Loss: 0.1856
  Total Loss: 0.1861
  Image grad max: 0.06146181747317314
  Output probs: [[0.    0.498 0.    0.502 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0000
  Image Loss: 0.1855
  Total Loss: 0.1859
  Image grad max: 0.0416557751595974
  Output probs: [[0.    0.502 0.    0.498 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0000
  Image Loss: 0.1855
  Total Loss: 0.1858
  Image grad max: 0.0427669882774353
  Output probs: [[0.    0.502 0.    0.498 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0000
  Image Loss: 0.1854
  Total Loss: 0.1857
  Image grad max: 0.040363527834415436
  Output probs: [[0.    0.498 0.    0.502 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0000
  Image Loss: 0.1853
  Total Loss: 0.1856
  Image grad max: 0.03585626929998398
  Output probs: [[0.    0.498 0.    0.502 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0000
  Image Loss: 0.1852
  Total Loss: 0.1855
  Image grad max: 0.03898153454065323
  Output probs: [[0.    0.501 0.    0.499 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0000
  Image Loss: 0.1851
  Total Loss: 0.1853
  Image grad max: 0.02461472898721695
  Output probs: [[0.    0.502 0.    0.498 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0000
  Image Loss: 0.1850
  Total Loss: 0.1853
  Image grad max: 0.032653458416461945
  Output probs: [[0.    0.499 0.    0.501 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0000
  Image Loss: 0.1849
  Total Loss: 0.1851
  Image grad max: 0.02208375185728073
  Output probs: [[0.    0.498 0.    0.502 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0000
  Image Loss: 0.1848
  Total Loss: 0.1851
  Image grad max: 0.029984744265675545
  Output probs: [[0.    0.501 0.    0.499 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0000
  Image Loss: 0.1847
  Total Loss: 0.1849
  Image grad max: 0.015954280272126198
  Output probs: [[0.    0.501 0.    0.499 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0000
  Image Loss: 0.1846
  Total Loss: 0.1849
  Image grad max: 0.023798087611794472
  Output probs: [[0.    0.499 0.    0.501 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0000
  Image Loss: 0.1845
  Total Loss: 0.1847
  Image grad max: 0.015361271798610687
  Output probs: [[0.    0.499 0.    0.501 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0000
  Image Loss: 0.1844
  Total Loss: 0.1847
  Image grad max: 0.021499672904610634
  Output probs: [[0.    0.501 0.    0.499 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0000
  Image Loss: 0.1843
  Total Loss: 0.1845
  Image grad max: 0.011560056358575821
  Output probs: [[0.    0.501 0.    0.499 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0000
  Image Loss: 0.1842
  Total Loss: 0.1844
  Image grad max: 0.016986342146992683
  Output probs: [[0.    0.499 0.    0.501 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0000
  Image Loss: 0.1841
  Total Loss: 0.1843
  Image grad max: 0.011878390796482563
  Output probs: [[0.    0.499 0.    0.501 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0000
  Image Loss: 0.1840
  Total Loss: 0.1842
  Image grad max: 0.015173003077507019
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0000
  Image Loss: 0.1839
  Total Loss: 0.1841
  Image grad max: 0.009207719936966896
  Output probs: [[0.    0.501 0.    0.499 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0000
  Image Loss: 0.1838
  Total Loss: 0.1840
  Image grad max: 0.012022780254483223
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0000
  Image Loss: 0.1837
  Total Loss: 0.1839
  Image grad max: 0.009758980013430119
  Output probs: [[0.    0.499 0.    0.501 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0000
  Image Loss: 0.1836
  Total Loss: 0.1838
  Image grad max: 0.01071979757398367
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0000
  Image Loss: 0.1835
  Total Loss: 0.1837
  Image grad max: 0.007724508177489042
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0000
  Image Loss: 0.1834
  Total Loss: 0.1836
  Image grad max: 0.008509098552167416
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0000
  Image Loss: 0.1833
  Total Loss: 0.1835
  Image grad max: 0.00837879441678524
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0000
  Image Loss: 0.1832
  Total Loss: 0.1834
  Image grad max: 0.0075338189490139484
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0000
  Image Loss: 0.1831
  Total Loss: 0.1833
  Image grad max: 0.006767838262021542
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0000
  Image Loss: 0.1829
  Total Loss: 0.1832
  Image grad max: 0.00592036172747612
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0000
  Image Loss: 0.1828
  Total Loss: 0.1831
  Image grad max: 0.007371130399405956
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0000
  Image Loss: 0.1827
  Total Loss: 0.1830
  Image grad max: 0.005241934675723314
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0000
  Image Loss: 0.1826
  Total Loss: 0.1828
  Image grad max: 0.006098417565226555
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0000
  Image Loss: 0.1825
  Total Loss: 0.1827
  Image grad max: 0.0039531029760837555
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0000
  Image Loss: 0.1824
  Total Loss: 0.1826
  Image grad max: 0.006589568220078945
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0000
  Image Loss: 0.1823
  Total Loss: 0.1825
  Image grad max: 0.0034968459513038397
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0000
  Image Loss: 0.1822
  Total Loss: 0.1824
  Image grad max: 0.0055969092063605785
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0000
  Image Loss: 0.1821
  Total Loss: 0.1823
  Image grad max: 0.003241942496970296
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0000
  Image Loss: 0.1820
  Total Loss: 0.1822
  Image grad max: 0.005946599878370762
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0000
  Image Loss: 0.1819
  Total Loss: 0.1821
  Image grad max: 0.002919417805969715
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0000
  Image Loss: 0.1817
  Total Loss: 0.1820
  Image grad max: 0.004957943223416805
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0000
  Image Loss: 0.1816
  Total Loss: 0.1818
  Image grad max: 0.0029093201737850904
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0000
  Image Loss: 0.1815
  Total Loss: 0.1817
  Image grad max: 0.005066483281552792
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0000
  Image Loss: 0.1814
  Total Loss: 0.1816
  Image grad max: 0.003125266171991825
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0000
  Image Loss: 0.1813
  Total Loss: 0.1815
  Image grad max: 0.004069448448717594
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0000
  Image Loss: 0.1812
  Total Loss: 0.1814
  Image grad max: 0.003214519936591387
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0000
  Image Loss: 0.1811
  Total Loss: 0.1813
  Image grad max: 0.0038958450313657522
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0000
  Image Loss: 0.1810
  Total Loss: 0.1812
  Image grad max: 0.0033178785815835
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0000
  Image Loss: 0.1808
  Total Loss: 0.1811
  Image grad max: 0.003356311935931444
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0000
  Image Loss: 0.1807
  Total Loss: 0.1809
  Image grad max: 0.0034161615185439587
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0000
  Image Loss: 0.1806
  Total Loss: 0.1808
  Image grad max: 0.0031493958085775375
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0000
  Image Loss: 0.1805
  Total Loss: 0.1807
  Image grad max: 0.003389758290722966
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0000
  Image Loss: 0.1804
  Total Loss: 0.1806
  Image grad max: 0.003076987573876977
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0000
  Image Loss: 0.1803
  Total Loss: 0.1805
  Image grad max: 0.003443300724029541
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0000
  Image Loss: 0.1802
  Total Loss: 0.1804
  Image grad max: 0.002953037153929472
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0000
  Image Loss: 0.1800
  Total Loss: 0.1803
  Image grad max: 0.00333177438005805
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0000
  Image Loss: 0.1799
  Total Loss: 0.1801
  Image grad max: 0.0029285165946930647
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0000
  Image Loss: 0.1798
  Total Loss: 0.1800
  Image grad max: 0.0032183430157601833
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0000
  Image Loss: 0.1797
  Total Loss: 0.1799
  Image grad max: 0.0031327358447015285
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0000
  Image Loss: 0.1796
  Total Loss: 0.1798
  Image grad max: 0.0031562938820570707
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0000
  Image Loss: 0.1795
  Total Loss: 0.1797
  Image grad max: 0.0031183441169559956
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0000
  Image Loss: 0.1793
  Total Loss: 0.1796
  Image grad max: 0.0029130203183740377
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0000
  Image Loss: 0.1792
  Total Loss: 0.1795
  Image grad max: 0.003193493466824293
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0000
  Image Loss: 0.1791
  Total Loss: 0.1793
  Image grad max: 0.002941548591479659
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0000
  Image Loss: 0.1790
  Total Loss: 0.1792
  Image grad max: 0.0030909120105206966
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0000
  Image Loss: 0.1789
  Total Loss: 0.1791
  Image grad max: 0.003012682544067502
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0000
  Image Loss: 0.1788
  Total Loss: 0.1790
  Image grad max: 0.003109017154201865
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0000
  Image Loss: 0.1786
  Total Loss: 0.1789
  Image grad max: 0.00294689042493701
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0000
  Image Loss: 0.1785
  Total Loss: 0.1788
  Image grad max: 0.0029057315550744534
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0000
  Image Loss: 0.1784
  Total Loss: 0.1786
  Image grad max: 0.0030994610860943794
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0000
  Image Loss: 0.1783
  Total Loss: 0.1785
  Image grad max: 0.002957170130684972
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0000
  Image Loss: 0.1782
  Total Loss: 0.1784
  Image grad max: 0.0029754312708973885
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0000
  Image Loss: 0.1781
  Total Loss: 0.1783
  Image grad max: 0.0029669904615730047
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0000
  Image Loss: 0.1779
  Total Loss: 0.1782
  Image grad max: 0.0030558020807802677
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0000
  Image Loss: 0.1778
  Total Loss: 0.1780
  Image grad max: 0.002878335537388921
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0000
  Image Loss: 0.1777
  Total Loss: 0.1779
  Image grad max: 0.0028623559046536684
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0000
  Image Loss: 0.1776
  Total Loss: 0.1778
  Image grad max: 0.0030276651959866285
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0000
  Image Loss: 0.1775
  Total Loss: 0.1777
  Image grad max: 0.0029542206320911646
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0000
  Image Loss: 0.1774
  Total Loss: 0.1776
  Image grad max: 0.002906863810494542
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0000
  Image Loss: 0.1772
  Total Loss: 0.1775
  Image grad max: 0.0029565589502453804
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0000
  Image Loss: 0.1771
  Total Loss: 0.1773
  Image grad max: 0.0030099910218268633
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0000
  Image Loss: 0.1770
  Total Loss: 0.1772
  Image grad max: 0.002859946573153138
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0000
  Image Loss: 0.1769
  Total Loss: 0.1771
  Image grad max: 0.002866695635020733
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0000
  Image Loss: 0.1768
  Total Loss: 0.1770
  Image grad max: 0.0030209787655621767
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0000
  Image Loss: 0.1766
  Total Loss: 0.1769
  Image grad max: 0.00291754980571568
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0000
  Image Loss: 0.1765
  Total Loss: 0.1767
  Image grad max: 0.002873426303267479
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0000
  Image Loss: 0.1764
  Total Loss: 0.1766
  Image grad max: 0.0029815121088176966
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0000
  Image Loss: 0.1763
  Total Loss: 0.1765
  Image grad max: 0.00295837689191103
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0000
  Image Loss: 0.1762
  Total Loss: 0.1764
  Image grad max: 0.0028473841957747936
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0000
  Image Loss: 0.1760
  Total Loss: 0.1763
  Image grad max: 0.0029306968208402395
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0000
  Image Loss: 0.1759
  Total Loss: 0.1761
  Image grad max: 0.0029674433171749115
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0000
  Image Loss: 0.1758
  Total Loss: 0.1760
  Image grad max: 0.0028918448369950056
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0000
  Image Loss: 0.1757
  Total Loss: 0.1759
  Image grad max: 0.0028724060393869877
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0000
  Image Loss: 0.1756
  Total Loss: 0.1758
  Image grad max: 0.0029633333906531334
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0000
  Image Loss: 0.1754
  Total Loss: 0.1757
  Image grad max: 0.0029345403891056776
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0000
  Image Loss: 0.1753
  Total Loss: 0.1755
  Image grad max: 0.0028589502908289433
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0000
  Image Loss: 0.1752
  Total Loss: 0.1754
  Image grad max: 0.002918098121881485
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0000
  Image Loss: 0.1751
  Total Loss: 0.1753
  Image grad max: 0.002969777910038829
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0000
  Image Loss: 0.1749
  Total Loss: 0.1752
  Image grad max: 0.002867933362722397
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0000
  Image Loss: 0.1748
  Total Loss: 0.1750
  Image grad max: 0.002889653667807579
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0000
  Image Loss: 0.1747
  Total Loss: 0.1749
  Image grad max: 0.0029563361313194036
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0000
  Image Loss: 0.1746
  Total Loss: 0.1748
  Image grad max: 0.002905011409893632
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0000
  Image Loss: 0.1745
  Total Loss: 0.1747
  Image grad max: 0.002872405108064413
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0000
  Image Loss: 0.1743
  Total Loss: 0.1746
  Image grad max: 0.002931618830189109
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0000
  Image Loss: 0.1742
  Total Loss: 0.1744
  Image grad max: 0.0029196212999522686
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0000
  Image Loss: 0.1741
  Total Loss: 0.1743
  Image grad max: 0.002883244538679719
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0000
  Image Loss: 0.1740
  Total Loss: 0.1742
  Image grad max: 0.0029162352439016104
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0000
  Image Loss: 0.1738
  Total Loss: 0.1741
  Image grad max: 0.0029117248486727476
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0000
  Image Loss: 0.1737
  Total Loss: 0.1740
  Image grad max: 0.002886576345190406
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0000
  Image Loss: 0.1736
  Total Loss: 0.1738
  Image grad max: 0.002908324124291539
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0000
  Image Loss: 0.1735
  Total Loss: 0.1737
  Image grad max: 0.002915065037086606
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0000
  Image Loss: 0.1734
  Total Loss: 0.1736
  Image grad max: 0.002891774522140622
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0000
  Image Loss: 0.1732
  Total Loss: 0.1735
  Image grad max: 0.002900314750149846
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0000
  Image Loss: 0.1731
  Total Loss: 0.1733
  Image grad max: 0.002908931113779545
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0000
  Image Loss: 0.1730
  Total Loss: 0.1732
  Image grad max: 0.002887498587369919
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0000
  Image Loss: 0.1729
  Total Loss: 0.1731
  Image grad max: 0.002903626300394535
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0000
  Image Loss: 0.1727
  Total Loss: 0.1730
  Image grad max: 0.0029009701684117317
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0000
  Image Loss: 0.1726
  Total Loss: 0.1728
  Image grad max: 0.0028926737140864134
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0000
  Image Loss: 0.1725
  Total Loss: 0.1727
  Image grad max: 0.002895649755373597
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0000
  Image Loss: 0.1724
  Total Loss: 0.1726
  Image grad max: 0.002892985474318266
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0000
  Image Loss: 0.1723
  Total Loss: 0.1725
  Image grad max: 0.0029128838796168566
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0000
  Image Loss: 0.1721
  Total Loss: 0.1724
  Image grad max: 0.0028669596649706364
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0000
  Image Loss: 0.1720
  Total Loss: 0.1722
  Image grad max: 0.002901907078921795
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0000
  Image Loss: 0.1719
  Total Loss: 0.1721
  Image grad max: 0.0029067620635032654
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0000
  Image Loss: 0.1718
  Total Loss: 0.1720
  Image grad max: 0.002864568028599024
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0000
  Image Loss: 0.1716
  Total Loss: 0.1719
  Image grad max: 0.00291835586540401
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0000
  Image Loss: 0.1715
  Total Loss: 0.1717
  Image grad max: 0.00287802517414093
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0000
  Image Loss: 0.1714
  Total Loss: 0.1716
  Image grad max: 0.00287722353823483
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0000
  Image Loss: 0.1713
  Total Loss: 0.1715
  Image grad max: 0.002900906605646014
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0000
  Image Loss: 0.1711
  Total Loss: 0.1714
  Image grad max: 0.002896338002756238
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0000
  Image Loss: 0.1710
  Total Loss: 0.1712
  Image grad max: 0.0028559628408402205
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0000
  Image Loss: 0.1709
  Total Loss: 0.1711
  Image grad max: 0.0029173477087169886
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0000
  Image Loss: 0.1708
  Total Loss: 0.1710
  Image grad max: 0.002873185556381941
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0000
  Image Loss: 0.1706
  Total Loss: 0.1709
  Image grad max: 0.0028667158912867308
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0000
  Image Loss: 0.1705
  Total Loss: 0.1707
  Image grad max: 0.0029111644253134727
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0000
  Image Loss: 0.1704
  Total Loss: 0.1706
  Image grad max: 0.0028763259761035442
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0000
  Image Loss: 0.1703
  Total Loss: 0.1705
  Image grad max: 0.0028547535184770823
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0000
  Image Loss: 0.1701
  Total Loss: 0.1704
  Image grad max: 0.002920058323070407
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0000
  Image Loss: 0.1700
  Total Loss: 0.1703
  Image grad max: 0.0028606546111404896
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0000
  Image Loss: 0.1699
  Total Loss: 0.1701
  Image grad max: 0.002859828295186162
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0000
  Image Loss: 0.1698
  Total Loss: 0.1700
  Image grad max: 0.0029156566597521305
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0000
  Image Loss: 0.1697
  Total Loss: 0.1699
  Image grad max: 0.0028600601945072412
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0000
  Image Loss: 0.1695
  Total Loss: 0.1698
  Image grad max: 0.002861118409782648
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0000
  Image Loss: 0.1694
  Total Loss: 0.1696
  Image grad max: 0.0029094209894537926
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0000
  Image Loss: 0.1693
  Total Loss: 0.1695
  Image grad max: 0.0028405492193996906
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0000
  Image Loss: 0.1692
  Total Loss: 0.1694
  Image grad max: 0.0028907570522278547
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0000
  Image Loss: 0.1690
  Total Loss: 0.1693
  Image grad max: 0.0028766884934157133
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0000
  Image Loss: 0.1689
  Total Loss: 0.1691
  Image grad max: 0.00285693840123713
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0000
  Image Loss: 0.1688
  Total Loss: 0.1690
  Image grad max: 0.0028806922491639853
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0000
  Image Loss: 0.1687
  Total Loss: 0.1689
  Image grad max: 0.002879854990169406
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0000
  Image Loss: 0.1685
  Total Loss: 0.1688
  Image grad max: 0.0028487308882176876
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0000
  Image Loss: 0.1684
  Total Loss: 0.1686
  Image grad max: 0.002878175349906087
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0000
  Image Loss: 0.1683
  Total Loss: 0.1685
  Image grad max: 0.002879226580262184
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0000
  Image Loss: 0.1682
  Total Loss: 0.1684
  Image grad max: 0.0028480766341090202
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0000
  Image Loss: 0.1680
  Total Loss: 0.1683
  Image grad max: 0.002869958756491542
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0000
  Image Loss: 0.1679
  Total Loss: 0.1681
  Image grad max: 0.0028634248301386833
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0000
  Image Loss: 0.1678
  Total Loss: 0.1680
  Image grad max: 0.002875841222703457
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0000
  Image Loss: 0.1677
  Total Loss: 0.1679
  Image grad max: 0.002848447533324361
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0000
  Image Loss: 0.1675
  Total Loss: 0.1678
  Image grad max: 0.0028760340064764023
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0000
  Image Loss: 0.1674
  Total Loss: 0.1676
  Image grad max: 0.0028448314405977726
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0000
  Image Loss: 0.1673
  Total Loss: 0.1675
  Image grad max: 0.0028760740533471107
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0000
  Image Loss: 0.1672
  Total Loss: 0.1674
  Image grad max: 0.002854342805221677
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0000
  Image Loss: 0.1670
  Total Loss: 0.1673
  Image grad max: 0.002851581433787942
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0000
  Image Loss: 0.1669
  Total Loss: 0.1671
  Image grad max: 0.002869701711460948
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0000
  Image Loss: 0.1668
  Total Loss: 0.1670
  Image grad max: 0.0028479492757469416
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0000
  Image Loss: 0.1667
  Total Loss: 0.1669
  Image grad max: 0.0028603761456906796
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0000
  Image Loss: 0.1665
  Total Loss: 0.1668
  Image grad max: 0.002851908328011632
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0000
  Image Loss: 0.1664
  Total Loss: 0.1666
  Image grad max: 0.002862439723685384
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0000
  Image Loss: 0.1663
  Total Loss: 0.1665
  Image grad max: 0.0028292539063841105
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0000
  Image Loss: 0.1662
  Total Loss: 0.1664
  Image grad max: 0.0028873172122985125
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0000
  Image Loss: 0.1660
  Total Loss: 0.1663
  Image grad max: 0.002825590781867504
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0000
  Image Loss: 0.1659
  Total Loss: 0.1661
  Image grad max: 0.0028608511202037334
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0000
  Image Loss: 0.1658
  Total Loss: 0.1660
  Image grad max: 0.002844751812517643
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0000
  Image Loss: 0.1657
  Total Loss: 0.1659
  Image grad max: 0.0028552894946187735
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0000
  Image Loss: 0.1655
  Total Loss: 0.1658
  Image grad max: 0.0028506009839475155
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0000
  Image Loss: 0.1654
  Total Loss: 0.1656
  Image grad max: 0.0028249630704522133
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0000
  Image Loss: 0.1653
  Total Loss: 0.1655
  Image grad max: 0.0028697845991700888
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0000
  Image Loss: 0.1652
  Total Loss: 0.1654
  Image grad max: 0.002844135509803891
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0000
  Image Loss: 0.1650
  Total Loss: 0.1653
  Image grad max: 0.002822283888235688
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0000
  Image Loss: 0.1649
  Total Loss: 0.1651
  Image grad max: 0.0028728535398840904
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0000
  Image Loss: 0.1648
  Total Loss: 0.1650
  Image grad max: 0.0028300222475081682
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0000
  Image Loss: 0.1647
  Total Loss: 0.1649
  Image grad max: 0.0028195895720273256
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0000
  Image Loss: 0.1645
  Total Loss: 0.1648
  Image grad max: 0.002885454799979925
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0000
  Image Loss: 0.1644
  Total Loss: 0.1646
  Image grad max: 0.0028044283390045166
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0000
  Image Loss: 0.1643
  Total Loss: 0.1645
  Image grad max: 0.002849326469004154
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0000
  Image Loss: 0.1642
  Total Loss: 0.1644
  Image grad max: 0.002842700807377696
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0000
  Image Loss: 0.1640
  Total Loss: 0.1643
  Image grad max: 0.0028264489956200123
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0000
  Image Loss: 0.1639
  Total Loss: 0.1641
  Image grad max: 0.0028484633658081293
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0000
  Image Loss: 0.1638
  Total Loss: 0.1640
  Image grad max: 0.0028322760481387377
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0000
  Image Loss: 0.1636
  Total Loss: 0.1639
  Image grad max: 0.0028237234801054
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0000
  Image Loss: 0.1635
  Total Loss: 0.1638
  Image grad max: 0.0028476628940552473
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0000
  Image Loss: 0.1634
  Total Loss: 0.1636
  Image grad max: 0.0028218987863510847
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0000
  Image Loss: 0.1633
  Total Loss: 0.1635
  Image grad max: 0.0028324592858552933
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0000
  Image Loss: 0.1631
  Total Loss: 0.1634
  Image grad max: 0.0028391981031745672
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0000
  Image Loss: 0.1630
  Total Loss: 0.1632
  Image grad max: 0.0027923583984375
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0000
  Image Loss: 0.1629
  Total Loss: 0.1631
  Image grad max: 0.00286799855530262
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0000
  Image Loss: 0.1628
  Total Loss: 0.1630
  Image grad max: 0.002809650031849742
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0000
  Image Loss: 0.1626
  Total Loss: 0.1629
  Image grad max: 0.002814471023157239
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0000
  Image Loss: 0.1625
  Total Loss: 0.1627
  Image grad max: 0.002857608487829566
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0000
  Image Loss: 0.1624
  Total Loss: 0.1626
  Image grad max: 0.002783878706395626
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0000
  Image Loss: 0.1623
  Total Loss: 0.1625
  Image grad max: 0.0028557798359543085
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0000
  Image Loss: 0.1621
  Total Loss: 0.1624
  Image grad max: 0.0028069319669157267
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0000
  Image Loss: 0.1620
  Total Loss: 0.1622
  Image grad max: 0.0028213427867740393
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0000
  Image Loss: 0.1619
  Total Loss: 0.1621
  Image grad max: 0.002826168667525053
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0000
  Image Loss: 0.1618
  Total Loss: 0.1620
  Image grad max: 0.0028252401389181614
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0000
  Image Loss: 0.1616
  Total Loss: 0.1619
  Image grad max: 0.0027936017140746117
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0000
  Image Loss: 0.1615
  Total Loss: 0.1617
  Image grad max: 0.002850254997611046
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0000
  Image Loss: 0.1614
  Total Loss: 0.1616
  Image grad max: 0.002791719976812601
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0000
  Image Loss: 0.1613
  Total Loss: 0.1615
  Image grad max: 0.0028311137575656176
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0000
  Image Loss: 0.1611
  Total Loss: 0.1614
  Image grad max: 0.0028071238193660975
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0000
  Image Loss: 0.1610
  Total Loss: 0.1612
  Image grad max: 0.002811947837471962
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0000
  Image Loss: 0.1609
  Total Loss: 0.1611
  Image grad max: 0.002822540234774351
  Output probs: [[0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0. ]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0000
  Image Loss: 0.1608
  Total Loss: 0.1610
  Image grad max: 0.0028023705817759037
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
