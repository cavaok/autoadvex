running dynamical.py
Epoch [1/30], Batch [0/6000], Loss: 7.2358
Epoch [1/30], Batch [100/6000], Loss: 4.5720
Epoch [1/30], Batch [200/6000], Loss: 3.4558
Epoch [1/30], Batch [300/6000], Loss: 5.6232
Epoch [1/30], Batch [400/6000], Loss: 2.8292
Epoch [1/30], Batch [500/6000], Loss: 2.4297
Epoch [1/30], Batch [600/6000], Loss: 2.0830
Epoch [1/30], Batch [700/6000], Loss: 2.4704
Epoch [1/30], Batch [800/6000], Loss: 1.9225
Epoch [1/30], Batch [900/6000], Loss: 3.6597
Epoch [1/30], Batch [1000/6000], Loss: 1.8698
Epoch [1/30], Batch [1100/6000], Loss: 1.6366
Epoch [1/30], Batch [1200/6000], Loss: 2.3043
Epoch [1/30], Batch [1300/6000], Loss: 1.7641
Epoch [1/30], Batch [1400/6000], Loss: 1.9511
Epoch [1/30], Batch [1500/6000], Loss: 1.8838
Epoch [1/30], Batch [1600/6000], Loss: 1.3866
Epoch [1/30], Batch [1700/6000], Loss: 1.5748
Epoch [1/30], Batch [1800/6000], Loss: 1.9692
Epoch [1/30], Batch [1900/6000], Loss: 0.5611
Epoch [1/30], Batch [2000/6000], Loss: 3.5624
Epoch [1/30], Batch [2100/6000], Loss: 2.2942
Epoch [1/30], Batch [2200/6000], Loss: 1.1916
Epoch [1/30], Batch [2300/6000], Loss: 1.2695
Epoch [1/30], Batch [2400/6000], Loss: 1.2569
Epoch [1/30], Batch [2500/6000], Loss: 2.5911
Epoch [1/30], Batch [2600/6000], Loss: 0.9126
Epoch [1/30], Batch [2700/6000], Loss: 1.7140
Epoch [1/30], Batch [2800/6000], Loss: 0.8843
Epoch [1/30], Batch [2900/6000], Loss: 2.7560
Epoch [1/30], Batch [3000/6000], Loss: 0.4800
Epoch [1/30], Batch [3100/6000], Loss: 2.8266
Epoch [1/30], Batch [3200/6000], Loss: 0.7313
Epoch [1/30], Batch [3300/6000], Loss: 1.0703
Epoch [1/30], Batch [3400/6000], Loss: 1.1067
Epoch [1/30], Batch [3500/6000], Loss: 2.8894
Epoch [1/30], Batch [3600/6000], Loss: 0.7997
Epoch [1/30], Batch [3700/6000], Loss: 0.6089
Epoch [1/30], Batch [3800/6000], Loss: 1.0973
Epoch [1/30], Batch [3900/6000], Loss: 2.2666
Epoch [1/30], Batch [4000/6000], Loss: 0.5021
Epoch [1/30], Batch [4100/6000], Loss: 0.4234
Epoch [1/30], Batch [4200/6000], Loss: 1.1026
Epoch [1/30], Batch [4300/6000], Loss: 1.7081
Epoch [1/30], Batch [4400/6000], Loss: 1.3195
Epoch [1/30], Batch [4500/6000], Loss: 1.2118
Epoch [1/30], Batch [4600/6000], Loss: 1.2525
Epoch [1/30], Batch [4700/6000], Loss: 1.4529
Epoch [1/30], Batch [4800/6000], Loss: 3.1964
Epoch [1/30], Batch [4900/6000], Loss: 1.9218
Epoch [1/30], Batch [5000/6000], Loss: 0.6445
Epoch [1/30], Batch [5100/6000], Loss: 0.8870
Epoch [1/30], Batch [5200/6000], Loss: 1.3661
Epoch [1/30], Batch [5300/6000], Loss: 3.8033
Epoch [1/30], Batch [5400/6000], Loss: 2.2437
Epoch [1/30], Batch [5500/6000], Loss: 0.6241
Epoch [1/30], Batch [5600/6000], Loss: 1.9603
Epoch [1/30], Batch [5700/6000], Loss: 0.9700
Epoch [1/30], Batch [5800/6000], Loss: 1.6447
Epoch [1/30], Batch [5900/6000], Loss: 0.8791
Epoch [1/30], Loss: 1.8375
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.2889
Epoch [2/30], Batch [100/6000], Loss: 1.3669
Epoch [2/30], Batch [200/6000], Loss: 0.9366
Epoch [2/30], Batch [300/6000], Loss: 1.0566
Epoch [2/30], Batch [400/6000], Loss: 0.5360
Epoch [2/30], Batch [500/6000], Loss: 0.6531
Epoch [2/30], Batch [600/6000], Loss: 0.4515
Epoch [2/30], Batch [700/6000], Loss: 1.7172
Epoch [2/30], Batch [800/6000], Loss: 0.5135
Epoch [2/30], Batch [900/6000], Loss: 0.3318
Epoch [2/30], Batch [1000/6000], Loss: 0.4806
Epoch [2/30], Batch [1100/6000], Loss: 0.6451
Epoch [2/30], Batch [1200/6000], Loss: 1.0602
Epoch [2/30], Batch [1300/6000], Loss: 0.3421
Epoch [2/30], Batch [1400/6000], Loss: 2.5306
Epoch [2/30], Batch [1500/6000], Loss: 2.2373
Epoch [2/30], Batch [1600/6000], Loss: 1.2681
Epoch [2/30], Batch [1700/6000], Loss: 0.5853
Epoch [2/30], Batch [1800/6000], Loss: 0.3641
Epoch [2/30], Batch [1900/6000], Loss: 0.9483
Epoch [2/30], Batch [2000/6000], Loss: 1.0129
Epoch [2/30], Batch [2100/6000], Loss: 0.2784
Epoch [2/30], Batch [2200/6000], Loss: 1.3124
Epoch [2/30], Batch [2300/6000], Loss: 1.2684
Epoch [2/30], Batch [2400/6000], Loss: 0.2480
Epoch [2/30], Batch [2500/6000], Loss: 1.1363
Epoch [2/30], Batch [2600/6000], Loss: 1.0753
Epoch [2/30], Batch [2700/6000], Loss: 1.2806
Epoch [2/30], Batch [2800/6000], Loss: 0.3145
Epoch [2/30], Batch [2900/6000], Loss: 0.9225
Epoch [2/30], Batch [3000/6000], Loss: 1.2076
Epoch [2/30], Batch [3100/6000], Loss: 0.8387
Epoch [2/30], Batch [3200/6000], Loss: 2.0202
Epoch [2/30], Batch [3300/6000], Loss: 1.3887
Epoch [2/30], Batch [3400/6000], Loss: 1.0687
Epoch [2/30], Batch [3500/6000], Loss: 1.4808
Epoch [2/30], Batch [3600/6000], Loss: 1.9216
Epoch [2/30], Batch [3700/6000], Loss: 0.7528
Epoch [2/30], Batch [3800/6000], Loss: 1.4303
Epoch [2/30], Batch [3900/6000], Loss: 0.7404
Epoch [2/30], Batch [4000/6000], Loss: 0.3362
Epoch [2/30], Batch [4100/6000], Loss: 0.3678
Epoch [2/30], Batch [4200/6000], Loss: 0.2933
Epoch [2/30], Batch [4300/6000], Loss: 1.1023
Epoch [2/30], Batch [4400/6000], Loss: 0.3820
Epoch [2/30], Batch [4500/6000], Loss: 1.0171
Epoch [2/30], Batch [4600/6000], Loss: 1.3840
Epoch [2/30], Batch [4700/6000], Loss: 0.6292
Epoch [2/30], Batch [4800/6000], Loss: 0.7497
Epoch [2/30], Batch [4900/6000], Loss: 2.4858
Epoch [2/30], Batch [5000/6000], Loss: 0.2294
Epoch [2/30], Batch [5100/6000], Loss: 1.7175
Epoch [2/30], Batch [5200/6000], Loss: 0.2720
Epoch [2/30], Batch [5300/6000], Loss: 1.4581
Epoch [2/30], Batch [5400/6000], Loss: 0.8053
Epoch [2/30], Batch [5500/6000], Loss: 3.1199
Epoch [2/30], Batch [5600/6000], Loss: 1.3976
Epoch [2/30], Batch [5700/6000], Loss: 0.9591
Epoch [2/30], Batch [5800/6000], Loss: 1.9462
Epoch [2/30], Batch [5900/6000], Loss: 1.0492
Epoch [2/30], Loss: 1.0809
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 1.0722
Epoch [3/30], Batch [100/6000], Loss: 1.2215
Epoch [3/30], Batch [200/6000], Loss: 0.9422
Epoch [3/30], Batch [300/6000], Loss: 0.5530
Epoch [3/30], Batch [400/6000], Loss: 0.3398
Epoch [3/30], Batch [500/6000], Loss: 1.9760
Epoch [3/30], Batch [600/6000], Loss: 0.3417
Epoch [3/30], Batch [700/6000], Loss: 0.9492
Epoch [3/30], Batch [800/6000], Loss: 0.2113
Epoch [3/30], Batch [900/6000], Loss: 0.9841
Epoch [3/30], Batch [1000/6000], Loss: 0.9205
Epoch [3/30], Batch [1100/6000], Loss: 1.1759
Epoch [3/30], Batch [1200/6000], Loss: 0.4667
Epoch [3/30], Batch [1300/6000], Loss: 0.4303
Epoch [3/30], Batch [1400/6000], Loss: 0.7981
Epoch [3/30], Batch [1500/6000], Loss: 0.3788
Epoch [3/30], Batch [1600/6000], Loss: 0.2141
Epoch [3/30], Batch [1700/6000], Loss: 0.1946
Epoch [3/30], Batch [1800/6000], Loss: 0.6176
Epoch [3/30], Batch [1900/6000], Loss: 0.6427
Epoch [3/30], Batch [2000/6000], Loss: 0.7637
Epoch [3/30], Batch [2100/6000], Loss: 0.4550
Epoch [3/30], Batch [2200/6000], Loss: 0.3466
Epoch [3/30], Batch [2300/6000], Loss: 0.5378
Epoch [3/30], Batch [2400/6000], Loss: 0.6101
Epoch [3/30], Batch [2500/6000], Loss: 1.7670
Epoch [3/30], Batch [2600/6000], Loss: 0.6121
Epoch [3/30], Batch [2700/6000], Loss: 0.4845
Epoch [3/30], Batch [2800/6000], Loss: 0.3511
Epoch [3/30], Batch [2900/6000], Loss: 0.5018
Epoch [3/30], Batch [3000/6000], Loss: 0.5147
Epoch [3/30], Batch [3100/6000], Loss: 2.1225
Epoch [3/30], Batch [3200/6000], Loss: 0.4182
Epoch [3/30], Batch [3300/6000], Loss: 2.0361
Epoch [3/30], Batch [3400/6000], Loss: 1.2954
Epoch [3/30], Batch [3500/6000], Loss: 0.2936
Epoch [3/30], Batch [3600/6000], Loss: 0.2196
Epoch [3/30], Batch [3700/6000], Loss: 0.4458
Epoch [3/30], Batch [3800/6000], Loss: 1.5912
Epoch [3/30], Batch [3900/6000], Loss: 0.9089
Epoch [3/30], Batch [4000/6000], Loss: 1.1942
Epoch [3/30], Batch [4100/6000], Loss: 0.8630
Epoch [3/30], Batch [4200/6000], Loss: 1.4309
Epoch [3/30], Batch [4300/6000], Loss: 1.6516
Epoch [3/30], Batch [4400/6000], Loss: 0.2097
Epoch [3/30], Batch [4500/6000], Loss: 1.5600
Epoch [3/30], Batch [4600/6000], Loss: 0.3398
Epoch [3/30], Batch [4700/6000], Loss: 0.7739
Epoch [3/30], Batch [4800/6000], Loss: 1.6979
Epoch [3/30], Batch [4900/6000], Loss: 0.3267
Epoch [3/30], Batch [5000/6000], Loss: 0.9155
Epoch [3/30], Batch [5100/6000], Loss: 0.3804
Epoch [3/30], Batch [5200/6000], Loss: 3.6877
Epoch [3/30], Batch [5300/6000], Loss: 0.3673
Epoch [3/30], Batch [5400/6000], Loss: 0.9735
Epoch [3/30], Batch [5500/6000], Loss: 0.7216
Epoch [3/30], Batch [5600/6000], Loss: 1.4538
Epoch [3/30], Batch [5700/6000], Loss: 0.6718
Epoch [3/30], Batch [5800/6000], Loss: 0.5160
Epoch [3/30], Batch [5900/6000], Loss: 0.3061
Epoch [3/30], Loss: 0.8752
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.2413
Epoch [4/30], Batch [100/6000], Loss: 0.6869
Epoch [4/30], Batch [200/6000], Loss: 0.2775
Epoch [4/30], Batch [300/6000], Loss: 4.1422
Epoch [4/30], Batch [400/6000], Loss: 0.9146
Epoch [4/30], Batch [500/6000], Loss: 1.9801
Epoch [4/30], Batch [600/6000], Loss: 0.4056
Epoch [4/30], Batch [700/6000], Loss: 1.9698
Epoch [4/30], Batch [800/6000], Loss: 0.9728
Epoch [4/30], Batch [900/6000], Loss: 0.4361
Epoch [4/30], Batch [1000/6000], Loss: 0.2406
Epoch [4/30], Batch [1100/6000], Loss: 0.3520
Epoch [4/30], Batch [1200/6000], Loss: 0.3764
Epoch [4/30], Batch [1300/6000], Loss: 0.3095
Epoch [4/30], Batch [1400/6000], Loss: 0.1937
Epoch [4/30], Batch [1500/6000], Loss: 0.1870
Epoch [4/30], Batch [1600/6000], Loss: 1.2380
Epoch [4/30], Batch [1700/6000], Loss: 0.2910
Epoch [4/30], Batch [1800/6000], Loss: 0.2205
Epoch [4/30], Batch [1900/6000], Loss: 0.4287
Epoch [4/30], Batch [2000/6000], Loss: 0.2265
Epoch [4/30], Batch [2100/6000], Loss: 1.3926
Epoch [4/30], Batch [2200/6000], Loss: 0.5149
Epoch [4/30], Batch [2300/6000], Loss: 0.5486
Epoch [4/30], Batch [2400/6000], Loss: 1.2631
Epoch [4/30], Batch [2500/6000], Loss: 0.5196
Epoch [4/30], Batch [2600/6000], Loss: 0.4372
Epoch [4/30], Batch [2700/6000], Loss: 0.2310
Epoch [4/30], Batch [2800/6000], Loss: 0.5191
Epoch [4/30], Batch [2900/6000], Loss: 0.2550
Epoch [4/30], Batch [3000/6000], Loss: 0.2413
Epoch [4/30], Batch [3100/6000], Loss: 0.2294
Epoch [4/30], Batch [3200/6000], Loss: 0.8042
Epoch [4/30], Batch [3300/6000], Loss: 1.6142
Epoch [4/30], Batch [3400/6000], Loss: 0.2528
Epoch [4/30], Batch [3500/6000], Loss: 1.1765
Epoch [4/30], Batch [3600/6000], Loss: 2.9165
Epoch [4/30], Batch [3700/6000], Loss: 0.4096
Epoch [4/30], Batch [3800/6000], Loss: 0.3109
Epoch [4/30], Batch [3900/6000], Loss: 0.8068
Epoch [4/30], Batch [4000/6000], Loss: 0.2052
Epoch [4/30], Batch [4100/6000], Loss: 0.3240
Epoch [4/30], Batch [4200/6000], Loss: 0.4715
Epoch [4/30], Batch [4300/6000], Loss: 0.2381
Epoch [4/30], Batch [4400/6000], Loss: 0.2649
Epoch [4/30], Batch [4500/6000], Loss: 0.3940
Epoch [4/30], Batch [4600/6000], Loss: 0.9323
Epoch [4/30], Batch [4700/6000], Loss: 2.6986
Epoch [4/30], Batch [4800/6000], Loss: 0.8568
Epoch [4/30], Batch [4900/6000], Loss: 1.5998
Epoch [4/30], Batch [5000/6000], Loss: 0.2184
Epoch [4/30], Batch [5100/6000], Loss: 0.2050
Epoch [4/30], Batch [5200/6000], Loss: 0.7583
Epoch [4/30], Batch [5300/6000], Loss: 0.2791
Epoch [4/30], Batch [5400/6000], Loss: 1.9208
Epoch [4/30], Batch [5500/6000], Loss: 0.2815
Epoch [4/30], Batch [5600/6000], Loss: 1.6972
Epoch [4/30], Batch [5700/6000], Loss: 0.2643
Epoch [4/30], Batch [5800/6000], Loss: 0.8290
Epoch [4/30], Batch [5900/6000], Loss: 0.2800
Epoch [4/30], Loss: 0.7399
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.6821
Epoch [5/30], Batch [100/6000], Loss: 0.3841
Epoch [5/30], Batch [200/6000], Loss: 0.7543
Epoch [5/30], Batch [300/6000], Loss: 0.2733
Epoch [5/30], Batch [400/6000], Loss: 0.3656
Epoch [5/30], Batch [500/6000], Loss: 0.3474
Epoch [5/30], Batch [600/6000], Loss: 0.2113
Epoch [5/30], Batch [700/6000], Loss: 0.3062
Epoch [5/30], Batch [800/6000], Loss: 0.2750
Epoch [5/30], Batch [900/6000], Loss: 0.5342
Epoch [5/30], Batch [1000/6000], Loss: 0.2822
Epoch [5/30], Batch [1100/6000], Loss: 0.1736
Epoch [5/30], Batch [1200/6000], Loss: 0.3675
Epoch [5/30], Batch [1300/6000], Loss: 1.6776
Epoch [5/30], Batch [1400/6000], Loss: 0.2017
Epoch [5/30], Batch [1500/6000], Loss: 0.5666
Epoch [5/30], Batch [1600/6000], Loss: 0.3058
Epoch [5/30], Batch [1700/6000], Loss: 0.3722
Epoch [5/30], Batch [1800/6000], Loss: 0.3266
Epoch [5/30], Batch [1900/6000], Loss: 0.2290
Epoch [5/30], Batch [2000/6000], Loss: 0.2036
Epoch [5/30], Batch [2100/6000], Loss: 2.8142
Epoch [5/30], Batch [2200/6000], Loss: 0.6141
Epoch [5/30], Batch [2300/6000], Loss: 0.3677
Epoch [5/30], Batch [2400/6000], Loss: 0.7241
Epoch [5/30], Batch [2500/6000], Loss: 1.3990
Epoch [5/30], Batch [2600/6000], Loss: 0.8561
Epoch [5/30], Batch [2700/6000], Loss: 0.2037
Epoch [5/30], Batch [2800/6000], Loss: 0.7654
Epoch [5/30], Batch [2900/6000], Loss: 1.2521
Epoch [5/30], Batch [3000/6000], Loss: 1.2659
Epoch [5/30], Batch [3100/6000], Loss: 0.3739
Epoch [5/30], Batch [3200/6000], Loss: 0.1951
Epoch [5/30], Batch [3300/6000], Loss: 0.1979
Epoch [5/30], Batch [3400/6000], Loss: 2.7231
Epoch [5/30], Batch [3500/6000], Loss: 0.6588
Epoch [5/30], Batch [3600/6000], Loss: 0.2866
Epoch [5/30], Batch [3700/6000], Loss: 0.1588
Epoch [5/30], Batch [3800/6000], Loss: 0.6235
Epoch [5/30], Batch [3900/6000], Loss: 1.2513
Epoch [5/30], Batch [4000/6000], Loss: 1.6289
Epoch [5/30], Batch [4100/6000], Loss: 0.2138
Epoch [5/30], Batch [4200/6000], Loss: 1.0173
Epoch [5/30], Batch [4300/6000], Loss: 1.1182
Epoch [5/30], Batch [4400/6000], Loss: 0.2292
Epoch [5/30], Batch [4500/6000], Loss: 0.4416
Epoch [5/30], Batch [4600/6000], Loss: 0.2780
Epoch [5/30], Batch [4700/6000], Loss: 0.9792
Epoch [5/30], Batch [4800/6000], Loss: 0.2923
Epoch [5/30], Batch [4900/6000], Loss: 0.2201
Epoch [5/30], Batch [5000/6000], Loss: 0.3315
Epoch [5/30], Batch [5100/6000], Loss: 0.2732
Epoch [5/30], Batch [5200/6000], Loss: 0.3355
Epoch [5/30], Batch [5300/6000], Loss: 0.1769
Epoch [5/30], Batch [5400/6000], Loss: 0.7096
Epoch [5/30], Batch [5500/6000], Loss: 0.5340
Epoch [5/30], Batch [5600/6000], Loss: 0.2448
Epoch [5/30], Batch [5700/6000], Loss: 0.1742
Epoch [5/30], Batch [5800/6000], Loss: 0.1722
Epoch [5/30], Batch [5900/6000], Loss: 0.6906
Epoch [5/30], Loss: 0.6475
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.1961
Epoch [6/30], Batch [100/6000], Loss: 0.1595
Epoch [6/30], Batch [200/6000], Loss: 1.9678
Epoch [6/30], Batch [300/6000], Loss: 0.1682
Epoch [6/30], Batch [400/6000], Loss: 1.2795
Epoch [6/30], Batch [500/6000], Loss: 0.1956
Epoch [6/30], Batch [600/6000], Loss: 1.5296
Epoch [6/30], Batch [700/6000], Loss: 0.2659
Epoch [6/30], Batch [800/6000], Loss: 0.2614
Epoch [6/30], Batch [900/6000], Loss: 1.8173
Epoch [6/30], Batch [1000/6000], Loss: 0.3458
Epoch [6/30], Batch [1100/6000], Loss: 0.1674
Epoch [6/30], Batch [1200/6000], Loss: 0.3039
Epoch [6/30], Batch [1300/6000], Loss: 1.7208
Epoch [6/30], Batch [1400/6000], Loss: 1.0560
Epoch [6/30], Batch [1500/6000], Loss: 0.3005
Epoch [6/30], Batch [1600/6000], Loss: 1.1653
Epoch [6/30], Batch [1700/6000], Loss: 1.2692
Epoch [6/30], Batch [1800/6000], Loss: 0.4273
Epoch [6/30], Batch [1900/6000], Loss: 0.2061
Epoch [6/30], Batch [2000/6000], Loss: 0.2910
Epoch [6/30], Batch [2100/6000], Loss: 0.9805
Epoch [6/30], Batch [2200/6000], Loss: 0.2533
Epoch [6/30], Batch [2300/6000], Loss: 0.8119
Epoch [6/30], Batch [2400/6000], Loss: 0.5362
Epoch [6/30], Batch [2500/6000], Loss: 0.2140
Epoch [6/30], Batch [2600/6000], Loss: 0.1455
Epoch [6/30], Batch [2700/6000], Loss: 0.4361
Epoch [6/30], Batch [2800/6000], Loss: 0.1996
Epoch [6/30], Batch [2900/6000], Loss: 0.1664
Epoch [6/30], Batch [3000/6000], Loss: 0.4055
Epoch [6/30], Batch [3100/6000], Loss: 0.7248
Epoch [6/30], Batch [3200/6000], Loss: 0.2002
Epoch [6/30], Batch [3300/6000], Loss: 0.1797
Epoch [6/30], Batch [3400/6000], Loss: 0.2277
Epoch [6/30], Batch [3500/6000], Loss: 0.1864
Epoch [6/30], Batch [3600/6000], Loss: 2.2459
Epoch [6/30], Batch [3700/6000], Loss: 0.2060
Epoch [6/30], Batch [3800/6000], Loss: 0.2450
Epoch [6/30], Batch [3900/6000], Loss: 0.7602
Epoch [6/30], Batch [4000/6000], Loss: 0.1886
Epoch [6/30], Batch [4100/6000], Loss: 1.1078
Epoch [6/30], Batch [4200/6000], Loss: 0.2304
Epoch [6/30], Batch [4300/6000], Loss: 1.1253
Epoch [6/30], Batch [4400/6000], Loss: 0.2597
Epoch [6/30], Batch [4500/6000], Loss: 0.1820
Epoch [6/30], Batch [4600/6000], Loss: 0.4149
Epoch [6/30], Batch [4700/6000], Loss: 0.1581
Epoch [6/30], Batch [4800/6000], Loss: 0.3860
Epoch [6/30], Batch [4900/6000], Loss: 0.3250
Epoch [6/30], Batch [5000/6000], Loss: 2.1568
Epoch [6/30], Batch [5100/6000], Loss: 2.0062
Epoch [6/30], Batch [5200/6000], Loss: 0.2726
Epoch [6/30], Batch [5300/6000], Loss: 0.2520
Epoch [6/30], Batch [5400/6000], Loss: 0.6264
Epoch [6/30], Batch [5500/6000], Loss: 0.2021
Epoch [6/30], Batch [5600/6000], Loss: 0.1857
Epoch [6/30], Batch [5700/6000], Loss: 0.2636
Epoch [6/30], Batch [5800/6000], Loss: 0.1805
Epoch [6/30], Batch [5900/6000], Loss: 0.2114
Epoch [6/30], Loss: 0.5684
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.7100
Epoch [7/30], Batch [100/6000], Loss: 0.3911
Epoch [7/30], Batch [200/6000], Loss: 1.2116
Epoch [7/30], Batch [300/6000], Loss: 0.2447
Epoch [7/30], Batch [400/6000], Loss: 0.5873
Epoch [7/30], Batch [500/6000], Loss: 0.5306
Epoch [7/30], Batch [600/6000], Loss: 0.6645
Epoch [7/30], Batch [700/6000], Loss: 0.3142
Epoch [7/30], Batch [800/6000], Loss: 0.2543
Epoch [7/30], Batch [900/6000], Loss: 2.1054
Epoch [7/30], Batch [1000/6000], Loss: 0.2737
Epoch [7/30], Batch [1100/6000], Loss: 0.2330
Epoch [7/30], Batch [1200/6000], Loss: 0.1799
Epoch [7/30], Batch [1300/6000], Loss: 0.2081
Epoch [7/30], Batch [1400/6000], Loss: 0.2259
Epoch [7/30], Batch [1500/6000], Loss: 0.2170
Epoch [7/30], Batch [1600/6000], Loss: 0.3550
Epoch [7/30], Batch [1700/6000], Loss: 0.2653
Epoch [7/30], Batch [1800/6000], Loss: 0.4393
Epoch [7/30], Batch [1900/6000], Loss: 0.2237
Epoch [7/30], Batch [2000/6000], Loss: 0.2635
Epoch [7/30], Batch [2100/6000], Loss: 0.3675
Epoch [7/30], Batch [2200/6000], Loss: 0.2043
Epoch [7/30], Batch [2300/6000], Loss: 0.1491
Epoch [7/30], Batch [2400/6000], Loss: 0.3457
Epoch [7/30], Batch [2500/6000], Loss: 0.2063
Epoch [7/30], Batch [2600/6000], Loss: 0.1810
Epoch [7/30], Batch [2700/6000], Loss: 1.7976
Epoch [7/30], Batch [2800/6000], Loss: 0.4971
Epoch [7/30], Batch [2900/6000], Loss: 0.5653
Epoch [7/30], Batch [3000/6000], Loss: 0.1945
Epoch [7/30], Batch [3100/6000], Loss: 1.9605
Epoch [7/30], Batch [3200/6000], Loss: 2.3324
Epoch [7/30], Batch [3300/6000], Loss: 0.1583
Epoch [7/30], Batch [3400/6000], Loss: 2.1033
Epoch [7/30], Batch [3500/6000], Loss: 0.2347
Epoch [7/30], Batch [3600/6000], Loss: 0.6997
Epoch [7/30], Batch [3700/6000], Loss: 0.1486
Epoch [7/30], Batch [3800/6000], Loss: 0.6642
Epoch [7/30], Batch [3900/6000], Loss: 0.4670
Epoch [7/30], Batch [4000/6000], Loss: 0.4838
Epoch [7/30], Batch [4100/6000], Loss: 0.1750
Epoch [7/30], Batch [4200/6000], Loss: 0.1792
Epoch [7/30], Batch [4300/6000], Loss: 0.1424
Epoch [7/30], Batch [4400/6000], Loss: 0.1654
Epoch [7/30], Batch [4500/6000], Loss: 0.2594
Epoch [7/30], Batch [4600/6000], Loss: 1.4208
Epoch [7/30], Batch [4700/6000], Loss: 0.1848
Epoch [7/30], Batch [4800/6000], Loss: 0.1782
Epoch [7/30], Batch [4900/6000], Loss: 0.2259
Epoch [7/30], Batch [5000/6000], Loss: 0.1653
Epoch [7/30], Batch [5100/6000], Loss: 0.6024
Epoch [7/30], Batch [5200/6000], Loss: 3.2235
Epoch [7/30], Batch [5300/6000], Loss: 0.3341
Epoch [7/30], Batch [5400/6000], Loss: 0.5414
Epoch [7/30], Batch [5500/6000], Loss: 0.4578
Epoch [7/30], Batch [5600/6000], Loss: 0.1377
Epoch [7/30], Batch [5700/6000], Loss: 0.1825
Epoch [7/30], Batch [5800/6000], Loss: 0.2707
Epoch [7/30], Batch [5900/6000], Loss: 0.4257
Epoch [7/30], Loss: 0.5187
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.1639
Epoch [8/30], Batch [100/6000], Loss: 1.2184
Epoch [8/30], Batch [200/6000], Loss: 1.2595
Epoch [8/30], Batch [300/6000], Loss: 0.1854
Epoch [8/30], Batch [400/6000], Loss: 0.2691
Epoch [8/30], Batch [500/6000], Loss: 1.8463
Epoch [8/30], Batch [600/6000], Loss: 0.2272
Epoch [8/30], Batch [700/6000], Loss: 0.3361
Epoch [8/30], Batch [800/6000], Loss: 0.7965
Epoch [8/30], Batch [900/6000], Loss: 0.1650
Epoch [8/30], Batch [1000/6000], Loss: 0.2194
Epoch [8/30], Batch [1100/6000], Loss: 0.1939
Epoch [8/30], Batch [1200/6000], Loss: 0.1460
Epoch [8/30], Batch [1300/6000], Loss: 0.2295
Epoch [8/30], Batch [1400/6000], Loss: 0.2828
Epoch [8/30], Batch [1500/6000], Loss: 0.6206
Epoch [8/30], Batch [1600/6000], Loss: 0.2240
Epoch [8/30], Batch [1700/6000], Loss: 0.6000
Epoch [8/30], Batch [1800/6000], Loss: 0.2976
Epoch [8/30], Batch [1900/6000], Loss: 0.1490
Epoch [8/30], Batch [2000/6000], Loss: 0.1815
Epoch [8/30], Batch [2100/6000], Loss: 1.7514
Epoch [8/30], Batch [2200/6000], Loss: 0.3406
Epoch [8/30], Batch [2300/6000], Loss: 1.2709
Epoch [8/30], Batch [2400/6000], Loss: 0.2334
Epoch [8/30], Batch [2500/6000], Loss: 0.2031
Epoch [8/30], Batch [2600/6000], Loss: 0.2168
Epoch [8/30], Batch [2700/6000], Loss: 0.5322
Epoch [8/30], Batch [2800/6000], Loss: 0.1924
Epoch [8/30], Batch [2900/6000], Loss: 0.3393
Epoch [8/30], Batch [3000/6000], Loss: 0.2529
Epoch [8/30], Batch [3100/6000], Loss: 0.6737
Epoch [8/30], Batch [3200/6000], Loss: 1.9304
Epoch [8/30], Batch [3300/6000], Loss: 0.2088
Epoch [8/30], Batch [3400/6000], Loss: 1.3114
Epoch [8/30], Batch [3500/6000], Loss: 0.3317
Epoch [8/30], Batch [3600/6000], Loss: 1.3805
Epoch [8/30], Batch [3700/6000], Loss: 2.1709
Epoch [8/30], Batch [3800/6000], Loss: 0.1615
Epoch [8/30], Batch [3900/6000], Loss: 0.2526
Epoch [8/30], Batch [4000/6000], Loss: 0.3236
Epoch [8/30], Batch [4100/6000], Loss: 0.1763
Epoch [8/30], Batch [4200/6000], Loss: 0.8261
Epoch [8/30], Batch [4300/6000], Loss: 0.3024
Epoch [8/30], Batch [4400/6000], Loss: 0.2548
Epoch [8/30], Batch [4500/6000], Loss: 0.1547
Epoch [8/30], Batch [4600/6000], Loss: 0.1386
Epoch [8/30], Batch [4700/6000], Loss: 0.2486
Epoch [8/30], Batch [4800/6000], Loss: 0.2817
Epoch [8/30], Batch [4900/6000], Loss: 0.2697
Epoch [8/30], Batch [5000/6000], Loss: 0.1901
Epoch [8/30], Batch [5100/6000], Loss: 1.4991
Epoch [8/30], Batch [5200/6000], Loss: 0.1342
Epoch [8/30], Batch [5300/6000], Loss: 0.1822
Epoch [8/30], Batch [5400/6000], Loss: 0.1553
Epoch [8/30], Batch [5500/6000], Loss: 1.3775
Epoch [8/30], Batch [5600/6000], Loss: 0.1626
Epoch [8/30], Batch [5700/6000], Loss: 0.1483
Epoch [8/30], Batch [5800/6000], Loss: 0.1884
Epoch [8/30], Batch [5900/6000], Loss: 0.2419
Epoch [8/30], Loss: 0.4662
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.1391
Epoch [9/30], Batch [100/6000], Loss: 0.1872
Epoch [9/30], Batch [200/6000], Loss: 0.1821
Epoch [9/30], Batch [300/6000], Loss: 0.1479
Epoch [9/30], Batch [400/6000], Loss: 0.1875
Epoch [9/30], Batch [500/6000], Loss: 1.1164
Epoch [9/30], Batch [600/6000], Loss: 0.1915
Epoch [9/30], Batch [700/6000], Loss: 0.4754
Epoch [9/30], Batch [800/6000], Loss: 1.7612
Epoch [9/30], Batch [900/6000], Loss: 1.3997
Epoch [9/30], Batch [1000/6000], Loss: 0.1834
Epoch [9/30], Batch [1100/6000], Loss: 0.1370
Epoch [9/30], Batch [1200/6000], Loss: 1.0595
Epoch [9/30], Batch [1300/6000], Loss: 0.1695
Epoch [9/30], Batch [1400/6000], Loss: 0.1929
Epoch [9/30], Batch [1500/6000], Loss: 0.1896
Epoch [9/30], Batch [1600/6000], Loss: 0.1686
Epoch [9/30], Batch [1700/6000], Loss: 0.2278
Epoch [9/30], Batch [1800/6000], Loss: 0.6095
Epoch [9/30], Batch [1900/6000], Loss: 0.1827
Epoch [9/30], Batch [2000/6000], Loss: 0.2276
Epoch [9/30], Batch [2100/6000], Loss: 0.2066
Epoch [9/30], Batch [2200/6000], Loss: 1.9107
Epoch [9/30], Batch [2300/6000], Loss: 0.1271
Epoch [9/30], Batch [2400/6000], Loss: 0.1578
Epoch [9/30], Batch [2500/6000], Loss: 0.1637
Epoch [9/30], Batch [2600/6000], Loss: 0.3255
Epoch [9/30], Batch [2700/6000], Loss: 0.1387
Epoch [9/30], Batch [2800/6000], Loss: 0.1462
Epoch [9/30], Batch [2900/6000], Loss: 0.1587
Epoch [9/30], Batch [3000/6000], Loss: 0.1771
Epoch [9/30], Batch [3100/6000], Loss: 0.1760
Epoch [9/30], Batch [3200/6000], Loss: 0.1865
Epoch [9/30], Batch [3300/6000], Loss: 0.2472
Epoch [9/30], Batch [3400/6000], Loss: 0.1198
Epoch [9/30], Batch [3500/6000], Loss: 0.1600
Epoch [9/30], Batch [3600/6000], Loss: 0.1738
Epoch [9/30], Batch [3700/6000], Loss: 0.2233
Epoch [9/30], Batch [3800/6000], Loss: 1.2912
Epoch [9/30], Batch [3900/6000], Loss: 0.6945
Epoch [9/30], Batch [4000/6000], Loss: 0.5208
Epoch [9/30], Batch [4100/6000], Loss: 0.1824
Epoch [9/30], Batch [4200/6000], Loss: 0.1551
Epoch [9/30], Batch [4300/6000], Loss: 0.3383
Epoch [9/30], Batch [4400/6000], Loss: 0.2237
Epoch [9/30], Batch [4500/6000], Loss: 1.0951
Epoch [9/30], Batch [4600/6000], Loss: 0.8863
Epoch [9/30], Batch [4700/6000], Loss: 0.2582
Epoch [9/30], Batch [4800/6000], Loss: 0.1813
Epoch [9/30], Batch [4900/6000], Loss: 1.8243
Epoch [9/30], Batch [5000/6000], Loss: 1.3348
Epoch [9/30], Batch [5100/6000], Loss: 0.2267
Epoch [9/30], Batch [5200/6000], Loss: 0.1919
Epoch [9/30], Batch [5300/6000], Loss: 0.1525
Epoch [9/30], Batch [5400/6000], Loss: 0.5740
Epoch [9/30], Batch [5500/6000], Loss: 0.2007
Epoch [9/30], Batch [5600/6000], Loss: 0.8819
Epoch [9/30], Batch [5700/6000], Loss: 0.2303
Epoch [9/30], Batch [5800/6000], Loss: 0.1941
Epoch [9/30], Batch [5900/6000], Loss: 0.1453
Epoch [9/30], Loss: 0.4361
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.1984
Epoch [10/30], Batch [100/6000], Loss: 0.2053
Epoch [10/30], Batch [200/6000], Loss: 0.1352
Epoch [10/30], Batch [300/6000], Loss: 0.8161
Epoch [10/30], Batch [400/6000], Loss: 0.1647
Epoch [10/30], Batch [500/6000], Loss: 2.0453
Epoch [10/30], Batch [600/6000], Loss: 0.1425
Epoch [10/30], Batch [700/6000], Loss: 0.1620
Epoch [10/30], Batch [800/6000], Loss: 0.2156
Epoch [10/30], Batch [900/6000], Loss: 0.1675
Epoch [10/30], Batch [1000/6000], Loss: 1.4015
Epoch [10/30], Batch [1100/6000], Loss: 0.9396
Epoch [10/30], Batch [1200/6000], Loss: 0.1661
Epoch [10/30], Batch [1300/6000], Loss: 0.1717
Epoch [10/30], Batch [1400/6000], Loss: 0.8749
Epoch [10/30], Batch [1500/6000], Loss: 0.1941
Epoch [10/30], Batch [1600/6000], Loss: 0.1231
Epoch [10/30], Batch [1700/6000], Loss: 0.2074
Epoch [10/30], Batch [1800/6000], Loss: 0.1808
Epoch [10/30], Batch [1900/6000], Loss: 0.3088
Epoch [10/30], Batch [2000/6000], Loss: 1.0410
Epoch [10/30], Batch [2100/6000], Loss: 0.1969
Epoch [10/30], Batch [2200/6000], Loss: 0.1846
Epoch [10/30], Batch [2300/6000], Loss: 0.5117
Epoch [10/30], Batch [2400/6000], Loss: 1.9595
Epoch [10/30], Batch [2500/6000], Loss: 0.1651
Epoch [10/30], Batch [2600/6000], Loss: 0.1809
Epoch [10/30], Batch [2700/6000], Loss: 0.7832
Epoch [10/30], Batch [2800/6000], Loss: 0.6869
Epoch [10/30], Batch [2900/6000], Loss: 0.1625
Epoch [10/30], Batch [3000/6000], Loss: 0.1961
Epoch [10/30], Batch [3100/6000], Loss: 0.2653
Epoch [10/30], Batch [3200/6000], Loss: 0.1485
Epoch [10/30], Batch [3300/6000], Loss: 0.2071
Epoch [10/30], Batch [3400/6000], Loss: 0.2630
Epoch [10/30], Batch [3500/6000], Loss: 0.1530
Epoch [10/30], Batch [3600/6000], Loss: 0.1801
Epoch [10/30], Batch [3700/6000], Loss: 0.1618
Epoch [10/30], Batch [3800/6000], Loss: 0.1666
Epoch [10/30], Batch [3900/6000], Loss: 1.3072
Epoch [10/30], Batch [4000/6000], Loss: 0.7028
Epoch [10/30], Batch [4100/6000], Loss: 0.2205
Epoch [10/30], Batch [4200/6000], Loss: 0.1926
Epoch [10/30], Batch [4300/6000], Loss: 0.1610
Epoch [10/30], Batch [4400/6000], Loss: 0.3760
Epoch [10/30], Batch [4500/6000], Loss: 0.1888
Epoch [10/30], Batch [4600/6000], Loss: 0.1276
Epoch [10/30], Batch [4700/6000], Loss: 0.5790
Epoch [10/30], Batch [4800/6000], Loss: 0.1483
Epoch [10/30], Batch [4900/6000], Loss: 0.2890
Epoch [10/30], Batch [5000/6000], Loss: 2.5380
Epoch [10/30], Batch [5100/6000], Loss: 0.1399
Epoch [10/30], Batch [5200/6000], Loss: 0.1709
Epoch [10/30], Batch [5300/6000], Loss: 0.2253
Epoch [10/30], Batch [5400/6000], Loss: 0.2899
Epoch [10/30], Batch [5500/6000], Loss: 0.1495
Epoch [10/30], Batch [5600/6000], Loss: 0.1882
Epoch [10/30], Batch [5700/6000], Loss: 0.1676
Epoch [10/30], Batch [5800/6000], Loss: 0.1515
Epoch [10/30], Batch [5900/6000], Loss: 0.2005
Epoch [10/30], Loss: 0.4018
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.1961
Epoch [11/30], Batch [100/6000], Loss: 0.7394
Epoch [11/30], Batch [200/6000], Loss: 0.8388
Epoch [11/30], Batch [300/6000], Loss: 0.2804
Epoch [11/30], Batch [400/6000], Loss: 0.2952
Epoch [11/30], Batch [500/6000], Loss: 0.5342
Epoch [11/30], Batch [600/6000], Loss: 0.5830
Epoch [11/30], Batch [700/6000], Loss: 0.2070
Epoch [11/30], Batch [800/6000], Loss: 0.3928
Epoch [11/30], Batch [900/6000], Loss: 0.1319
Epoch [11/30], Batch [1000/6000], Loss: 0.1468
Epoch [11/30], Batch [1100/6000], Loss: 0.1825
Epoch [11/30], Batch [1200/6000], Loss: 0.2012
Epoch [11/30], Batch [1300/6000], Loss: 0.3290
Epoch [11/30], Batch [1400/6000], Loss: 0.4336
Epoch [11/30], Batch [1500/6000], Loss: 0.4890
Epoch [11/30], Batch [1600/6000], Loss: 0.3962
Epoch [11/30], Batch [1700/6000], Loss: 0.3444
Epoch [11/30], Batch [1800/6000], Loss: 0.1908
Epoch [11/30], Batch [1900/6000], Loss: 0.1485
Epoch [11/30], Batch [2000/6000], Loss: 0.1261
Epoch [11/30], Batch [2100/6000], Loss: 0.4733
Epoch [11/30], Batch [2200/6000], Loss: 0.1548
Epoch [11/30], Batch [2300/6000], Loss: 0.1483
Epoch [11/30], Batch [2400/6000], Loss: 0.1303
Epoch [11/30], Batch [2500/6000], Loss: 0.1688
Epoch [11/30], Batch [2600/6000], Loss: 0.1512
Epoch [11/30], Batch [2700/6000], Loss: 0.1649
Epoch [11/30], Batch [2800/6000], Loss: 0.1643
Epoch [11/30], Batch [2900/6000], Loss: 0.1692
Epoch [11/30], Batch [3000/6000], Loss: 0.5479
Epoch [11/30], Batch [3100/6000], Loss: 0.1921
Epoch [11/30], Batch [3200/6000], Loss: 0.2150
Epoch [11/30], Batch [3300/6000], Loss: 0.1274
Epoch [11/30], Batch [3400/6000], Loss: 0.1924
Epoch [11/30], Batch [3500/6000], Loss: 0.1623
Epoch [11/30], Batch [3600/6000], Loss: 0.1624
Epoch [11/30], Batch [3700/6000], Loss: 0.1427
Epoch [11/30], Batch [3800/6000], Loss: 0.4030
Epoch [11/30], Batch [3900/6000], Loss: 1.4129
Epoch [11/30], Batch [4000/6000], Loss: 0.1687
Epoch [11/30], Batch [4100/6000], Loss: 0.2067
Epoch [11/30], Batch [4200/6000], Loss: 1.8069
Epoch [11/30], Batch [4300/6000], Loss: 0.3642
Epoch [11/30], Batch [4400/6000], Loss: 0.2421
Epoch [11/30], Batch [4500/6000], Loss: 0.5686
Epoch [11/30], Batch [4600/6000], Loss: 0.1508
Epoch [11/30], Batch [4700/6000], Loss: 1.3238
Epoch [11/30], Batch [4800/6000], Loss: 0.1711
Epoch [11/30], Batch [4900/6000], Loss: 0.4174
Epoch [11/30], Batch [5000/6000], Loss: 0.2199
Epoch [11/30], Batch [5100/6000], Loss: 0.5755
Epoch [11/30], Batch [5200/6000], Loss: 0.1590
Epoch [11/30], Batch [5300/6000], Loss: 0.1445
Epoch [11/30], Batch [5400/6000], Loss: 0.1407
Epoch [11/30], Batch [5500/6000], Loss: 0.1895
Epoch [11/30], Batch [5600/6000], Loss: 0.1331
Epoch [11/30], Batch [5700/6000], Loss: 0.1585
Epoch [11/30], Batch [5800/6000], Loss: 0.6053
Epoch [11/30], Batch [5900/6000], Loss: 0.1465
Epoch [11/30], Loss: 0.3740
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.1933
Epoch [12/30], Batch [100/6000], Loss: 0.1845
Epoch [12/30], Batch [200/6000], Loss: 0.1729
Epoch [12/30], Batch [300/6000], Loss: 0.1552
Epoch [12/30], Batch [400/6000], Loss: 0.4867
Epoch [12/30], Batch [500/6000], Loss: 0.1756
Epoch [12/30], Batch [600/6000], Loss: 0.1596
Epoch [12/30], Batch [700/6000], Loss: 0.1491
Epoch [12/30], Batch [800/6000], Loss: 0.2016
Epoch [12/30], Batch [900/6000], Loss: 0.4245
Epoch [12/30], Batch [1000/6000], Loss: 0.1474
Epoch [12/30], Batch [1100/6000], Loss: 0.2291
Epoch [12/30], Batch [1200/6000], Loss: 0.1646
Epoch [12/30], Batch [1300/6000], Loss: 0.2663
Epoch [12/30], Batch [1400/6000], Loss: 0.7518
Epoch [12/30], Batch [1500/6000], Loss: 1.4828
Epoch [12/30], Batch [1600/6000], Loss: 0.1444
Epoch [12/30], Batch [1700/6000], Loss: 0.1548
Epoch [12/30], Batch [1800/6000], Loss: 0.5443
Epoch [12/30], Batch [1900/6000], Loss: 0.3852
Epoch [12/30], Batch [2000/6000], Loss: 0.2811
Epoch [12/30], Batch [2100/6000], Loss: 0.1565
Epoch [12/30], Batch [2200/6000], Loss: 0.2363
Epoch [12/30], Batch [2300/6000], Loss: 0.1950
Epoch [12/30], Batch [2400/6000], Loss: 0.1371
Epoch [12/30], Batch [2500/6000], Loss: 0.5100
Epoch [12/30], Batch [2600/6000], Loss: 0.1389
Epoch [12/30], Batch [2700/6000], Loss: 1.3237
Epoch [12/30], Batch [2800/6000], Loss: 0.1966
Epoch [12/30], Batch [2900/6000], Loss: 0.1498
Epoch [12/30], Batch [3000/6000], Loss: 0.1587
Epoch [12/30], Batch [3100/6000], Loss: 0.7838
Epoch [12/30], Batch [3200/6000], Loss: 0.1424
Epoch [12/30], Batch [3300/6000], Loss: 0.2030
Epoch [12/30], Batch [3400/6000], Loss: 0.2634
Epoch [12/30], Batch [3500/6000], Loss: 0.9488
Epoch [12/30], Batch [3600/6000], Loss: 2.0475
Epoch [12/30], Batch [3700/6000], Loss: 0.2680
Epoch [12/30], Batch [3800/6000], Loss: 0.1503
Epoch [12/30], Batch [3900/6000], Loss: 0.2315
Epoch [12/30], Batch [4000/6000], Loss: 0.1480
Epoch [12/30], Batch [4100/6000], Loss: 0.1359
Epoch [12/30], Batch [4200/6000], Loss: 0.1481
Epoch [12/30], Batch [4300/6000], Loss: 0.8137
Epoch [12/30], Batch [4400/6000], Loss: 0.1468
Epoch [12/30], Batch [4500/6000], Loss: 0.1940
Epoch [12/30], Batch [4600/6000], Loss: 0.2182
Epoch [12/30], Batch [4700/6000], Loss: 0.7890
Epoch [12/30], Batch [4800/6000], Loss: 0.1429
Epoch [12/30], Batch [4900/6000], Loss: 0.2168
Epoch [12/30], Batch [5000/6000], Loss: 0.1620
Epoch [12/30], Batch [5100/6000], Loss: 0.1846
Epoch [12/30], Batch [5200/6000], Loss: 0.1597
Epoch [12/30], Batch [5300/6000], Loss: 0.1362
Epoch [12/30], Batch [5400/6000], Loss: 0.1811
Epoch [12/30], Batch [5500/6000], Loss: 0.6395
Epoch [12/30], Batch [5600/6000], Loss: 0.6469
Epoch [12/30], Batch [5700/6000], Loss: 0.1740
Epoch [12/30], Batch [5800/6000], Loss: 0.1494
Epoch [12/30], Batch [5900/6000], Loss: 0.6366
Epoch [12/30], Loss: 0.3528
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.2198
Epoch [13/30], Batch [100/6000], Loss: 0.1386
Epoch [13/30], Batch [200/6000], Loss: 1.2371
Epoch [13/30], Batch [300/6000], Loss: 1.2904
Epoch [13/30], Batch [400/6000], Loss: 0.1296
Epoch [13/30], Batch [500/6000], Loss: 0.1537
Epoch [13/30], Batch [600/6000], Loss: 0.1677
Epoch [13/30], Batch [700/6000], Loss: 1.2012
Epoch [13/30], Batch [800/6000], Loss: 0.3327
Epoch [13/30], Batch [900/6000], Loss: 0.3471
Epoch [13/30], Batch [1000/6000], Loss: 0.1166
Epoch [13/30], Batch [1100/6000], Loss: 0.1727
Epoch [13/30], Batch [1200/6000], Loss: 0.2707
Epoch [13/30], Batch [1300/6000], Loss: 0.3669
Epoch [13/30], Batch [1400/6000], Loss: 0.1591
Epoch [13/30], Batch [1500/6000], Loss: 0.3818
Epoch [13/30], Batch [1600/6000], Loss: 0.1268
Epoch [13/30], Batch [1700/6000], Loss: 0.2093
Epoch [13/30], Batch [1800/6000], Loss: 0.1613
Epoch [13/30], Batch [1900/6000], Loss: 0.2924
Epoch [13/30], Batch [2000/6000], Loss: 0.2008
Epoch [13/30], Batch [2100/6000], Loss: 0.8423
Epoch [13/30], Batch [2200/6000], Loss: 0.1567
Epoch [13/30], Batch [2300/6000], Loss: 0.1690
Epoch [13/30], Batch [2400/6000], Loss: 0.2143
Epoch [13/30], Batch [2500/6000], Loss: 0.2123
Epoch [13/30], Batch [2600/6000], Loss: 0.1555
Epoch [13/30], Batch [2700/6000], Loss: 0.8381
Epoch [13/30], Batch [2800/6000], Loss: 0.1767
Epoch [13/30], Batch [2900/6000], Loss: 0.1493
Epoch [13/30], Batch [3000/6000], Loss: 0.1616
Epoch [13/30], Batch [3100/6000], Loss: 0.3705
Epoch [13/30], Batch [3200/6000], Loss: 0.4355
Epoch [13/30], Batch [3300/6000], Loss: 0.1570
Epoch [13/30], Batch [3400/6000], Loss: 0.3524
Epoch [13/30], Batch [3500/6000], Loss: 0.1494
Epoch [13/30], Batch [3600/6000], Loss: 0.2386
Epoch [13/30], Batch [3700/6000], Loss: 0.1483
Epoch [13/30], Batch [3800/6000], Loss: 0.5952
Epoch [13/30], Batch [3900/6000], Loss: 0.1249
Epoch [13/30], Batch [4000/6000], Loss: 0.3200
Epoch [13/30], Batch [4100/6000], Loss: 0.1289
Epoch [13/30], Batch [4200/6000], Loss: 0.1610
Epoch [13/30], Batch [4300/6000], Loss: 0.1029
Epoch [13/30], Batch [4400/6000], Loss: 0.1284
Epoch [13/30], Batch [4500/6000], Loss: 0.2761
Epoch [13/30], Batch [4600/6000], Loss: 0.1913
Epoch [13/30], Batch [4700/6000], Loss: 0.6429
Epoch [13/30], Batch [4800/6000], Loss: 0.2273
Epoch [13/30], Batch [4900/6000], Loss: 0.1402
Epoch [13/30], Batch [5000/6000], Loss: 0.1611
Epoch [13/30], Batch [5100/6000], Loss: 0.1325
Epoch [13/30], Batch [5200/6000], Loss: 0.1382
Epoch [13/30], Batch [5300/6000], Loss: 0.1359
Epoch [13/30], Batch [5400/6000], Loss: 0.1550
Epoch [13/30], Batch [5500/6000], Loss: 0.1444
Epoch [13/30], Batch [5600/6000], Loss: 0.1946
Epoch [13/30], Batch [5700/6000], Loss: 0.1476
Epoch [13/30], Batch [5800/6000], Loss: 0.2703
Epoch [13/30], Batch [5900/6000], Loss: 0.1705
Epoch [13/30], Loss: 0.3240
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.1386
Epoch [14/30], Batch [100/6000], Loss: 0.2813
Epoch [14/30], Batch [200/6000], Loss: 0.1555
Epoch [14/30], Batch [300/6000], Loss: 0.1950
Epoch [14/30], Batch [400/6000], Loss: 0.1494
Epoch [14/30], Batch [500/6000], Loss: 0.1160
Epoch [14/30], Batch [600/6000], Loss: 2.0356
Epoch [14/30], Batch [700/6000], Loss: 0.1471
Epoch [14/30], Batch [800/6000], Loss: 0.1370
Epoch [14/30], Batch [900/6000], Loss: 1.3066
Epoch [14/30], Batch [1000/6000], Loss: 0.2201
Epoch [14/30], Batch [1100/6000], Loss: 0.1256
Epoch [14/30], Batch [1200/6000], Loss: 0.1795
Epoch [14/30], Batch [1300/6000], Loss: 0.2395
Epoch [14/30], Batch [1400/6000], Loss: 0.1363
Epoch [14/30], Batch [1500/6000], Loss: 1.0775
Epoch [14/30], Batch [1600/6000], Loss: 0.9262
Epoch [14/30], Batch [1700/6000], Loss: 0.1283
Epoch [14/30], Batch [1800/6000], Loss: 0.1352
Epoch [14/30], Batch [1900/6000], Loss: 1.2156
Epoch [14/30], Batch [2000/6000], Loss: 0.1370
Epoch [14/30], Batch [2100/6000], Loss: 0.3080
Epoch [14/30], Batch [2200/6000], Loss: 0.1416
Epoch [14/30], Batch [2300/6000], Loss: 0.2414
Epoch [14/30], Batch [2400/6000], Loss: 0.1004
Epoch [14/30], Batch [2500/6000], Loss: 0.2125
Epoch [14/30], Batch [2600/6000], Loss: 0.1013
Epoch [14/30], Batch [2700/6000], Loss: 0.1670
Epoch [14/30], Batch [2800/6000], Loss: 0.1687
Epoch [14/30], Batch [2900/6000], Loss: 0.1957
Epoch [14/30], Batch [3000/6000], Loss: 0.3749
Epoch [14/30], Batch [3100/6000], Loss: 0.2425
Epoch [14/30], Batch [3200/6000], Loss: 0.2747
Epoch [14/30], Batch [3300/6000], Loss: 0.1271
Epoch [14/30], Batch [3400/6000], Loss: 0.4794
Epoch [14/30], Batch [3500/6000], Loss: 0.1821
Epoch [14/30], Batch [3600/6000], Loss: 0.1495
Epoch [14/30], Batch [3700/6000], Loss: 0.1294
Epoch [14/30], Batch [3800/6000], Loss: 0.1528
Epoch [14/30], Batch [3900/6000], Loss: 0.5054
Epoch [14/30], Batch [4000/6000], Loss: 0.1329
Epoch [14/30], Batch [4100/6000], Loss: 0.1233
Epoch [14/30], Batch [4200/6000], Loss: 3.3777
Epoch [14/30], Batch [4300/6000], Loss: 0.1469
Epoch [14/30], Batch [4400/6000], Loss: 0.1371
Epoch [14/30], Batch [4500/6000], Loss: 0.1428
Epoch [14/30], Batch [4600/6000], Loss: 0.1390
Epoch [14/30], Batch [4700/6000], Loss: 0.1330
Epoch [14/30], Batch [4800/6000], Loss: 0.1434
Epoch [14/30], Batch [4900/6000], Loss: 0.1638
Epoch [14/30], Batch [5000/6000], Loss: 0.1488
Epoch [14/30], Batch [5100/6000], Loss: 0.1476
Epoch [14/30], Batch [5200/6000], Loss: 0.1308
Epoch [14/30], Batch [5300/6000], Loss: 0.1154
Epoch [14/30], Batch [5400/6000], Loss: 0.1411
Epoch [14/30], Batch [5500/6000], Loss: 0.1459
Epoch [14/30], Batch [5600/6000], Loss: 0.1585
Epoch [14/30], Batch [5700/6000], Loss: 0.1068
Epoch [14/30], Batch [5800/6000], Loss: 0.1637
Epoch [14/30], Batch [5900/6000], Loss: 0.1464
Epoch [14/30], Loss: 0.3142
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.1314
Epoch [15/30], Batch [100/6000], Loss: 0.1708
Epoch [15/30], Batch [200/6000], Loss: 0.1262
Epoch [15/30], Batch [300/6000], Loss: 1.6271
Epoch [15/30], Batch [400/6000], Loss: 0.1648
Epoch [15/30], Batch [500/6000], Loss: 0.4376
Epoch [15/30], Batch [600/6000], Loss: 0.1319
Epoch [15/30], Batch [700/6000], Loss: 0.1166
Epoch [15/30], Batch [800/6000], Loss: 0.6011
Epoch [15/30], Batch [900/6000], Loss: 0.4530
Epoch [15/30], Batch [1000/6000], Loss: 0.1626
Epoch [15/30], Batch [1100/6000], Loss: 0.1513
Epoch [15/30], Batch [1200/6000], Loss: 0.1412
Epoch [15/30], Batch [1300/6000], Loss: 0.4227
Epoch [15/30], Batch [1400/6000], Loss: 0.1370
Epoch [15/30], Batch [1500/6000], Loss: 0.3103
Epoch [15/30], Batch [1600/6000], Loss: 0.1746
Epoch [15/30], Batch [1700/6000], Loss: 0.2729
Epoch [15/30], Batch [1800/6000], Loss: 0.1232
Epoch [15/30], Batch [1900/6000], Loss: 0.2845
Epoch [15/30], Batch [2000/6000], Loss: 0.1968
Epoch [15/30], Batch [2100/6000], Loss: 0.3788
Epoch [15/30], Batch [2200/6000], Loss: 0.1617
Epoch [15/30], Batch [2300/6000], Loss: 0.5544
Epoch [15/30], Batch [2400/6000], Loss: 0.1488
Epoch [15/30], Batch [2500/6000], Loss: 0.1048
Epoch [15/30], Batch [2600/6000], Loss: 0.1094
Epoch [15/30], Batch [2700/6000], Loss: 0.1733
Epoch [15/30], Batch [2800/6000], Loss: 0.1785
Epoch [15/30], Batch [2900/6000], Loss: 0.2612
Epoch [15/30], Batch [3000/6000], Loss: 0.1556
Epoch [15/30], Batch [3100/6000], Loss: 0.1356
Epoch [15/30], Batch [3200/6000], Loss: 0.1621
Epoch [15/30], Batch [3300/6000], Loss: 0.1285
Epoch [15/30], Batch [3400/6000], Loss: 0.0996
Epoch [15/30], Batch [3500/6000], Loss: 0.2509
Epoch [15/30], Batch [3600/6000], Loss: 0.1316
Epoch [15/30], Batch [3700/6000], Loss: 0.1491
Epoch [15/30], Batch [3800/6000], Loss: 0.4770
Epoch [15/30], Batch [3900/6000], Loss: 0.1417
Epoch [15/30], Batch [4000/6000], Loss: 0.1536
Epoch [15/30], Batch [4100/6000], Loss: 0.1400
Epoch [15/30], Batch [4200/6000], Loss: 0.1666
Epoch [15/30], Batch [4300/6000], Loss: 0.1418
Epoch [15/30], Batch [4400/6000], Loss: 1.5558
Epoch [15/30], Batch [4500/6000], Loss: 0.1710
Epoch [15/30], Batch [4600/6000], Loss: 0.1432
Epoch [15/30], Batch [4700/6000], Loss: 0.1459
Epoch [15/30], Batch [4800/6000], Loss: 0.1451
Epoch [15/30], Batch [4900/6000], Loss: 0.1141
Epoch [15/30], Batch [5000/6000], Loss: 0.1225
Epoch [15/30], Batch [5100/6000], Loss: 0.1695
Epoch [15/30], Batch [5200/6000], Loss: 0.1481
Epoch [15/30], Batch [5300/6000], Loss: 0.1568
Epoch [15/30], Batch [5400/6000], Loss: 0.1335
Epoch [15/30], Batch [5500/6000], Loss: 0.4147
Epoch [15/30], Batch [5600/6000], Loss: 0.2099
Epoch [15/30], Batch [5700/6000], Loss: 0.1511
Epoch [15/30], Batch [5800/6000], Loss: 0.1407
Epoch [15/30], Batch [5900/6000], Loss: 0.1720
Epoch [15/30], Loss: 0.2928
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.1601
Epoch [16/30], Batch [100/6000], Loss: 0.1637
Epoch [16/30], Batch [200/6000], Loss: 0.2277
Epoch [16/30], Batch [300/6000], Loss: 0.1251
Epoch [16/30], Batch [400/6000], Loss: 0.1347
Epoch [16/30], Batch [500/6000], Loss: 0.2874
Epoch [16/30], Batch [600/6000], Loss: 0.1556
Epoch [16/30], Batch [700/6000], Loss: 0.1628
Epoch [16/30], Batch [800/6000], Loss: 0.3678
Epoch [16/30], Batch [900/6000], Loss: 0.1158
Epoch [16/30], Batch [1000/6000], Loss: 0.1394
Epoch [16/30], Batch [1100/6000], Loss: 0.1092
Epoch [16/30], Batch [1200/6000], Loss: 0.1334
Epoch [16/30], Batch [1300/6000], Loss: 0.2134
Epoch [16/30], Batch [1400/6000], Loss: 0.1726
Epoch [16/30], Batch [1500/6000], Loss: 0.1416
Epoch [16/30], Batch [1600/6000], Loss: 0.1325
Epoch [16/30], Batch [1700/6000], Loss: 0.1306
Epoch [16/30], Batch [1800/6000], Loss: 0.1436
Epoch [16/30], Batch [1900/6000], Loss: 0.1474
Epoch [16/30], Batch [2000/6000], Loss: 0.1510
Epoch [16/30], Batch [2100/6000], Loss: 0.2725
Epoch [16/30], Batch [2200/6000], Loss: 1.2134
Epoch [16/30], Batch [2300/6000], Loss: 0.1767
Epoch [16/30], Batch [2400/6000], Loss: 0.1507
Epoch [16/30], Batch [2500/6000], Loss: 0.1888
Epoch [16/30], Batch [2600/6000], Loss: 0.1388
Epoch [16/30], Batch [2700/6000], Loss: 0.3931
Epoch [16/30], Batch [2800/6000], Loss: 0.1856
Epoch [16/30], Batch [2900/6000], Loss: 0.2846
Epoch [16/30], Batch [3000/6000], Loss: 0.1401
Epoch [16/30], Batch [3100/6000], Loss: 0.1564
Epoch [16/30], Batch [3200/6000], Loss: 0.5942
Epoch [16/30], Batch [3300/6000], Loss: 0.2237
Epoch [16/30], Batch [3400/6000], Loss: 1.5929
Epoch [16/30], Batch [3500/6000], Loss: 0.1671
Epoch [16/30], Batch [3600/6000], Loss: 0.1972
Epoch [16/30], Batch [3700/6000], Loss: 0.1599
Epoch [16/30], Batch [3800/6000], Loss: 0.1297
Epoch [16/30], Batch [3900/6000], Loss: 0.1540
Epoch [16/30], Batch [4000/6000], Loss: 0.0983
Epoch [16/30], Batch [4100/6000], Loss: 0.1570
Epoch [16/30], Batch [4200/6000], Loss: 0.1253
Epoch [16/30], Batch [4300/6000], Loss: 0.1429
Epoch [16/30], Batch [4400/6000], Loss: 0.1648
Epoch [16/30], Batch [4500/6000], Loss: 0.1747
Epoch [16/30], Batch [4600/6000], Loss: 0.1946
Epoch [16/30], Batch [4700/6000], Loss: 0.1943
Epoch [16/30], Batch [4800/6000], Loss: 0.1263
Epoch [16/30], Batch [4900/6000], Loss: 0.1217
Epoch [16/30], Batch [5000/6000], Loss: 0.1242
Epoch [16/30], Batch [5100/6000], Loss: 0.1447
Epoch [16/30], Batch [5200/6000], Loss: 0.1273
Epoch [16/30], Batch [5300/6000], Loss: 0.0977
Epoch [16/30], Batch [5400/6000], Loss: 0.1532
Epoch [16/30], Batch [5500/6000], Loss: 0.1634
Epoch [16/30], Batch [5600/6000], Loss: 0.2381
Epoch [16/30], Batch [5700/6000], Loss: 0.2118
Epoch [16/30], Batch [5800/6000], Loss: 0.1285
Epoch [16/30], Batch [5900/6000], Loss: 0.1177
Epoch [16/30], Loss: 0.2775
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.1502
Epoch [17/30], Batch [100/6000], Loss: 0.3760
Epoch [17/30], Batch [200/6000], Loss: 0.1249
Epoch [17/30], Batch [300/6000], Loss: 0.1608
Epoch [17/30], Batch [400/6000], Loss: 0.1555
Epoch [17/30], Batch [500/6000], Loss: 0.1450
Epoch [17/30], Batch [600/6000], Loss: 0.1309
Epoch [17/30], Batch [700/6000], Loss: 0.1148
Epoch [17/30], Batch [800/6000], Loss: 0.5819
Epoch [17/30], Batch [900/6000], Loss: 0.8680
Epoch [17/30], Batch [1000/6000], Loss: 0.1574
Epoch [17/30], Batch [1100/6000], Loss: 0.2168
Epoch [17/30], Batch [1200/6000], Loss: 0.1334
Epoch [17/30], Batch [1300/6000], Loss: 1.4567
Epoch [17/30], Batch [1400/6000], Loss: 0.1281
Epoch [17/30], Batch [1500/6000], Loss: 0.1434
Epoch [17/30], Batch [1600/6000], Loss: 0.1693
Epoch [17/30], Batch [1700/6000], Loss: 0.1431
Epoch [17/30], Batch [1800/6000], Loss: 0.4116
Epoch [17/30], Batch [1900/6000], Loss: 0.1153
Epoch [17/30], Batch [2000/6000], Loss: 0.1233
Epoch [17/30], Batch [2100/6000], Loss: 0.1269
Epoch [17/30], Batch [2200/6000], Loss: 0.1416
Epoch [17/30], Batch [2300/6000], Loss: 0.1559
Epoch [17/30], Batch [2400/6000], Loss: 0.1227
Epoch [17/30], Batch [2500/6000], Loss: 0.1391
Epoch [17/30], Batch [2600/6000], Loss: 0.1015
Epoch [17/30], Batch [2700/6000], Loss: 0.3170
Epoch [17/30], Batch [2800/6000], Loss: 0.1596
Epoch [17/30], Batch [2900/6000], Loss: 0.1498
Epoch [17/30], Batch [3000/6000], Loss: 0.8224
Epoch [17/30], Batch [3100/6000], Loss: 0.1220
Epoch [17/30], Batch [3200/6000], Loss: 0.1461
Epoch [17/30], Batch [3300/6000], Loss: 0.1586
Epoch [17/30], Batch [3400/6000], Loss: 0.1365
Epoch [17/30], Batch [3500/6000], Loss: 0.1543
Epoch [17/30], Batch [3600/6000], Loss: 0.2004
Epoch [17/30], Batch [3700/6000], Loss: 0.5328
Epoch [17/30], Batch [3800/6000], Loss: 0.1424
Epoch [17/30], Batch [3900/6000], Loss: 0.1269
Epoch [17/30], Batch [4000/6000], Loss: 0.1385
Epoch [17/30], Batch [4100/6000], Loss: 0.1685
Epoch [17/30], Batch [4200/6000], Loss: 0.1336
Epoch [17/30], Batch [4300/6000], Loss: 0.1236
Epoch [17/30], Batch [4400/6000], Loss: 0.3146
Epoch [17/30], Batch [4500/6000], Loss: 0.1221
Epoch [17/30], Batch [4600/6000], Loss: 0.0933
Epoch [17/30], Batch [4700/6000], Loss: 0.1456
Epoch [17/30], Batch [4800/6000], Loss: 0.1301
Epoch [17/30], Batch [4900/6000], Loss: 0.9167
Epoch [17/30], Batch [5000/6000], Loss: 0.1401
Epoch [17/30], Batch [5100/6000], Loss: 0.4346
Epoch [17/30], Batch [5200/6000], Loss: 0.1290
Epoch [17/30], Batch [5300/6000], Loss: 0.1158
Epoch [17/30], Batch [5400/6000], Loss: 0.1576
Epoch [17/30], Batch [5500/6000], Loss: 0.1442
Epoch [17/30], Batch [5600/6000], Loss: 0.0977
Epoch [17/30], Batch [5700/6000], Loss: 0.1373
Epoch [17/30], Batch [5800/6000], Loss: 0.1415
Epoch [17/30], Batch [5900/6000], Loss: 0.1600
Epoch [17/30], Loss: 0.2623
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.1536
Epoch [18/30], Batch [100/6000], Loss: 0.1372
Epoch [18/30], Batch [200/6000], Loss: 0.2688
Epoch [18/30], Batch [300/6000], Loss: 0.1397
Epoch [18/30], Batch [400/6000], Loss: 0.1202
Epoch [18/30], Batch [500/6000], Loss: 0.1668
Epoch [18/30], Batch [600/6000], Loss: 0.1571
Epoch [18/30], Batch [700/6000], Loss: 0.0919
Epoch [18/30], Batch [800/6000], Loss: 0.0966
Epoch [18/30], Batch [900/6000], Loss: 0.1255
Epoch [18/30], Batch [1000/6000], Loss: 0.1452
Epoch [18/30], Batch [1100/6000], Loss: 0.1455
Epoch [18/30], Batch [1200/6000], Loss: 0.1200
Epoch [18/30], Batch [1300/6000], Loss: 0.1128
Epoch [18/30], Batch [1400/6000], Loss: 0.1766
Epoch [18/30], Batch [1500/6000], Loss: 0.1880
Epoch [18/30], Batch [1600/6000], Loss: 0.1267
Epoch [18/30], Batch [1700/6000], Loss: 0.7867
Epoch [18/30], Batch [1800/6000], Loss: 0.1473
Epoch [18/30], Batch [1900/6000], Loss: 0.1726
Epoch [18/30], Batch [2000/6000], Loss: 0.1623
Epoch [18/30], Batch [2100/6000], Loss: 0.1225
Epoch [18/30], Batch [2200/6000], Loss: 0.1422
Epoch [18/30], Batch [2300/6000], Loss: 0.1393
Epoch [18/30], Batch [2400/6000], Loss: 0.1580
Epoch [18/30], Batch [2500/6000], Loss: 0.1462
Epoch [18/30], Batch [2600/6000], Loss: 1.4908
Epoch [18/30], Batch [2700/6000], Loss: 0.1369
Epoch [18/30], Batch [2800/6000], Loss: 0.1411
Epoch [18/30], Batch [2900/6000], Loss: 0.3793
Epoch [18/30], Batch [3000/6000], Loss: 0.3074
Epoch [18/30], Batch [3100/6000], Loss: 1.0187
Epoch [18/30], Batch [3200/6000], Loss: 0.1370
Epoch [18/30], Batch [3300/6000], Loss: 0.1752
Epoch [18/30], Batch [3400/6000], Loss: 0.1491
Epoch [18/30], Batch [3500/6000], Loss: 0.3969
Epoch [18/30], Batch [3600/6000], Loss: 0.1338
Epoch [18/30], Batch [3700/6000], Loss: 0.1495
Epoch [18/30], Batch [3800/6000], Loss: 0.1401
Epoch [18/30], Batch [3900/6000], Loss: 0.1259
Epoch [18/30], Batch [4000/6000], Loss: 0.1269
Epoch [18/30], Batch [4100/6000], Loss: 0.1367
Epoch [18/30], Batch [4200/6000], Loss: 0.1976
Epoch [18/30], Batch [4300/6000], Loss: 0.5851
Epoch [18/30], Batch [4400/6000], Loss: 0.1327
Epoch [18/30], Batch [4500/6000], Loss: 0.1266
Epoch [18/30], Batch [4600/6000], Loss: 0.2233
Epoch [18/30], Batch [4700/6000], Loss: 0.1877
Epoch [18/30], Batch [4800/6000], Loss: 0.1154
Epoch [18/30], Batch [4900/6000], Loss: 0.1300
Epoch [18/30], Batch [5000/6000], Loss: 0.2669
Epoch [18/30], Batch [5100/6000], Loss: 0.9874
Epoch [18/30], Batch [5200/6000], Loss: 0.1879
Epoch [18/30], Batch [5300/6000], Loss: 0.1680
Epoch [18/30], Batch [5400/6000], Loss: 0.1519
Epoch [18/30], Batch [5500/6000], Loss: 0.1620
Epoch [18/30], Batch [5600/6000], Loss: 0.1084
Epoch [18/30], Batch [5700/6000], Loss: 0.1518
Epoch [18/30], Batch [5800/6000], Loss: 0.1261
Epoch [18/30], Batch [5900/6000], Loss: 0.1319
Epoch [18/30], Loss: 0.2510
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.1194
Epoch [19/30], Batch [100/6000], Loss: 0.1089
Epoch [19/30], Batch [200/6000], Loss: 0.1577
Epoch [19/30], Batch [300/6000], Loss: 0.1180
Epoch [19/30], Batch [400/6000], Loss: 0.1325
Epoch [19/30], Batch [500/6000], Loss: 0.1743
Epoch [19/30], Batch [600/6000], Loss: 0.1142
Epoch [19/30], Batch [700/6000], Loss: 0.1128
Epoch [19/30], Batch [800/6000], Loss: 0.1373
Epoch [19/30], Batch [900/6000], Loss: 0.1229
Epoch [19/30], Batch [1000/6000], Loss: 0.1296
Epoch [19/30], Batch [1100/6000], Loss: 0.1508
Epoch [19/30], Batch [1200/6000], Loss: 0.1385
Epoch [19/30], Batch [1300/6000], Loss: 0.1369
Epoch [19/30], Batch [1400/6000], Loss: 0.2024
Epoch [19/30], Batch [1500/6000], Loss: 0.1133
Epoch [19/30], Batch [1600/6000], Loss: 0.2027
Epoch [19/30], Batch [1700/6000], Loss: 0.1649
Epoch [19/30], Batch [1800/6000], Loss: 0.1478
Epoch [19/30], Batch [1900/6000], Loss: 0.1714
Epoch [19/30], Batch [2000/6000], Loss: 0.1533
Epoch [19/30], Batch [2100/6000], Loss: 0.1373
Epoch [19/30], Batch [2200/6000], Loss: 0.0926
Epoch [19/30], Batch [2300/6000], Loss: 0.1790
Epoch [19/30], Batch [2400/6000], Loss: 0.1098
Epoch [19/30], Batch [2500/6000], Loss: 0.1410
Epoch [19/30], Batch [2600/6000], Loss: 0.1319
Epoch [19/30], Batch [2700/6000], Loss: 0.1162
Epoch [19/30], Batch [2800/6000], Loss: 0.1247
Epoch [19/30], Batch [2900/6000], Loss: 0.1254
Epoch [19/30], Batch [3000/6000], Loss: 0.2688
Epoch [19/30], Batch [3100/6000], Loss: 0.8563
Epoch [19/30], Batch [3200/6000], Loss: 0.1728
Epoch [19/30], Batch [3300/6000], Loss: 0.1374
Epoch [19/30], Batch [3400/6000], Loss: 0.1331
Epoch [19/30], Batch [3500/6000], Loss: 0.1240
Epoch [19/30], Batch [3600/6000], Loss: 0.2475
Epoch [19/30], Batch [3700/6000], Loss: 0.1655
Epoch [19/30], Batch [3800/6000], Loss: 0.1423
Epoch [19/30], Batch [3900/6000], Loss: 0.1378
Epoch [19/30], Batch [4000/6000], Loss: 0.5297
Epoch [19/30], Batch [4100/6000], Loss: 0.1889
Epoch [19/30], Batch [4200/6000], Loss: 0.3626
Epoch [19/30], Batch [4300/6000], Loss: 0.1490
Epoch [19/30], Batch [4400/6000], Loss: 0.1374
Epoch [19/30], Batch [4500/6000], Loss: 0.4198
Epoch [19/30], Batch [4600/6000], Loss: 0.1239
Epoch [19/30], Batch [4700/6000], Loss: 0.1349
Epoch [19/30], Batch [4800/6000], Loss: 0.1268
Epoch [19/30], Batch [4900/6000], Loss: 0.1363
Epoch [19/30], Batch [5000/6000], Loss: 0.1424
Epoch [19/30], Batch [5100/6000], Loss: 0.1300
Epoch [19/30], Batch [5200/6000], Loss: 0.1405
Epoch [19/30], Batch [5300/6000], Loss: 0.1026
Epoch [19/30], Batch [5400/6000], Loss: 0.8571
Epoch [19/30], Batch [5500/6000], Loss: 0.1370
Epoch [19/30], Batch [5600/6000], Loss: 1.9720
Epoch [19/30], Batch [5700/6000], Loss: 0.4757
Epoch [19/30], Batch [5800/6000], Loss: 0.1241
Epoch [19/30], Batch [5900/6000], Loss: 0.1334
Epoch [19/30], Loss: 0.2409
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.1197
Epoch [20/30], Batch [100/6000], Loss: 0.1860
Epoch [20/30], Batch [200/6000], Loss: 0.1218
Epoch [20/30], Batch [300/6000], Loss: 0.1149
Epoch [20/30], Batch [400/6000], Loss: 0.1270
Epoch [20/30], Batch [500/6000], Loss: 0.1325
Epoch [20/30], Batch [600/6000], Loss: 0.2553
Epoch [20/30], Batch [700/6000], Loss: 0.1302
Epoch [20/30], Batch [800/6000], Loss: 0.1239
Epoch [20/30], Batch [900/6000], Loss: 0.1229
Epoch [20/30], Batch [1000/6000], Loss: 0.1464
Epoch [20/30], Batch [1100/6000], Loss: 0.2762
Epoch [20/30], Batch [1200/6000], Loss: 0.1499
Epoch [20/30], Batch [1300/6000], Loss: 0.1235
Epoch [20/30], Batch [1400/6000], Loss: 0.1523
Epoch [20/30], Batch [1500/6000], Loss: 0.1418
Epoch [20/30], Batch [1600/6000], Loss: 0.1794
Epoch [20/30], Batch [1700/6000], Loss: 0.1009
Epoch [20/30], Batch [1800/6000], Loss: 0.1160
Epoch [20/30], Batch [1900/6000], Loss: 0.1772
Epoch [20/30], Batch [2000/6000], Loss: 0.2420
Epoch [20/30], Batch [2100/6000], Loss: 0.2568
Epoch [20/30], Batch [2200/6000], Loss: 0.2829
Epoch [20/30], Batch [2300/6000], Loss: 0.2423
Epoch [20/30], Batch [2400/6000], Loss: 0.1684
Epoch [20/30], Batch [2500/6000], Loss: 0.1063
Epoch [20/30], Batch [2600/6000], Loss: 0.1910
Epoch [20/30], Batch [2700/6000], Loss: 0.1750
Epoch [20/30], Batch [2800/6000], Loss: 0.1303
Epoch [20/30], Batch [2900/6000], Loss: 0.1134
Epoch [20/30], Batch [3000/6000], Loss: 0.1428
Epoch [20/30], Batch [3100/6000], Loss: 0.1442
Epoch [20/30], Batch [3200/6000], Loss: 0.1381
Epoch [20/30], Batch [3300/6000], Loss: 0.1301
Epoch [20/30], Batch [3400/6000], Loss: 2.6434
Epoch [20/30], Batch [3500/6000], Loss: 0.1546
Epoch [20/30], Batch [3600/6000], Loss: 0.1436
Epoch [20/30], Batch [3700/6000], Loss: 0.2483
Epoch [20/30], Batch [3800/6000], Loss: 0.1252
Epoch [20/30], Batch [3900/6000], Loss: 0.1121
Epoch [20/30], Batch [4000/6000], Loss: 0.4433
Epoch [20/30], Batch [4100/6000], Loss: 0.1787
Epoch [20/30], Batch [4200/6000], Loss: 0.1351
Epoch [20/30], Batch [4300/6000], Loss: 0.1199
Epoch [20/30], Batch [4400/6000], Loss: 0.1106
Epoch [20/30], Batch [4500/6000], Loss: 0.1339
Epoch [20/30], Batch [4600/6000], Loss: 0.1421
Epoch [20/30], Batch [4700/6000], Loss: 0.7581
Epoch [20/30], Batch [4800/6000], Loss: 0.1207
Epoch [20/30], Batch [4900/6000], Loss: 0.1074
Epoch [20/30], Batch [5000/6000], Loss: 0.2389
Epoch [20/30], Batch [5100/6000], Loss: 0.1089
Epoch [20/30], Batch [5200/6000], Loss: 0.1171
Epoch [20/30], Batch [5300/6000], Loss: 0.3278
Epoch [20/30], Batch [5400/6000], Loss: 0.1154
Epoch [20/30], Batch [5500/6000], Loss: 0.1976
Epoch [20/30], Batch [5600/6000], Loss: 0.1698
Epoch [20/30], Batch [5700/6000], Loss: 0.1599
Epoch [20/30], Batch [5800/6000], Loss: 0.1317
Epoch [20/30], Batch [5900/6000], Loss: 0.1206
Epoch [20/30], Loss: 0.2325
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.1260
Epoch [21/30], Batch [100/6000], Loss: 0.1548
Epoch [21/30], Batch [200/6000], Loss: 0.1344
Epoch [21/30], Batch [300/6000], Loss: 0.1386
Epoch [21/30], Batch [400/6000], Loss: 0.1446
Epoch [21/30], Batch [500/6000], Loss: 0.1476
Epoch [21/30], Batch [600/6000], Loss: 0.1222
Epoch [21/30], Batch [700/6000], Loss: 0.5923
Epoch [21/30], Batch [800/6000], Loss: 0.1525
Epoch [21/30], Batch [900/6000], Loss: 0.1538
Epoch [21/30], Batch [1000/6000], Loss: 0.2129
Epoch [21/30], Batch [1100/6000], Loss: 0.9480
Epoch [21/30], Batch [1200/6000], Loss: 0.1243
Epoch [21/30], Batch [1300/6000], Loss: 0.1397
Epoch [21/30], Batch [1400/6000], Loss: 0.1451
Epoch [21/30], Batch [1500/6000], Loss: 0.1118
Epoch [21/30], Batch [1600/6000], Loss: 0.1322
Epoch [21/30], Batch [1700/6000], Loss: 0.0944
Epoch [21/30], Batch [1800/6000], Loss: 0.5200
Epoch [21/30], Batch [1900/6000], Loss: 0.1155
Epoch [21/30], Batch [2000/6000], Loss: 1.5817
Epoch [21/30], Batch [2100/6000], Loss: 0.1528
Epoch [21/30], Batch [2200/6000], Loss: 0.1209
Epoch [21/30], Batch [2300/6000], Loss: 0.1282
Epoch [21/30], Batch [2400/6000], Loss: 0.0970
Epoch [21/30], Batch [2500/6000], Loss: 0.1094
Epoch [21/30], Batch [2600/6000], Loss: 0.1318
Epoch [21/30], Batch [2700/6000], Loss: 0.1093
Epoch [21/30], Batch [2800/6000], Loss: 0.1553
Epoch [21/30], Batch [2900/6000], Loss: 0.1094
Epoch [21/30], Batch [3000/6000], Loss: 0.1276
Epoch [21/30], Batch [3100/6000], Loss: 0.1165
Epoch [21/30], Batch [3200/6000], Loss: 0.1612
Epoch [21/30], Batch [3300/6000], Loss: 0.0890
Epoch [21/30], Batch [3400/6000], Loss: 0.1089
Epoch [21/30], Batch [3500/6000], Loss: 0.1173
Epoch [21/30], Batch [3600/6000], Loss: 0.1454
Epoch [21/30], Batch [3700/6000], Loss: 0.0991
Epoch [21/30], Batch [3800/6000], Loss: 0.1128
Epoch [21/30], Batch [3900/6000], Loss: 0.1569
Epoch [21/30], Batch [4000/6000], Loss: 0.1292
Epoch [21/30], Batch [4100/6000], Loss: 0.1115
Epoch [21/30], Batch [4200/6000], Loss: 0.1343
Epoch [21/30], Batch [4300/6000], Loss: 0.5489
Epoch [21/30], Batch [4400/6000], Loss: 0.1305
Epoch [21/30], Batch [4500/6000], Loss: 0.3593
Epoch [21/30], Batch [4600/6000], Loss: 0.1159
Epoch [21/30], Batch [4700/6000], Loss: 0.1217
Epoch [21/30], Batch [4800/6000], Loss: 0.1054
Epoch [21/30], Batch [4900/6000], Loss: 0.2149
Epoch [21/30], Batch [5000/6000], Loss: 0.5507
Epoch [21/30], Batch [5100/6000], Loss: 0.1556
Epoch [21/30], Batch [5200/6000], Loss: 0.4039
Epoch [21/30], Batch [5300/6000], Loss: 0.1602
Epoch [21/30], Batch [5400/6000], Loss: 0.1317
Epoch [21/30], Batch [5500/6000], Loss: 0.1532
Epoch [21/30], Batch [5600/6000], Loss: 0.1289
Epoch [21/30], Batch [5700/6000], Loss: 0.1898
Epoch [21/30], Batch [5800/6000], Loss: 0.1167
Epoch [21/30], Batch [5900/6000], Loss: 0.2118
Epoch [21/30], Loss: 0.2192
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.1493
Epoch [22/30], Batch [100/6000], Loss: 0.1233
Epoch [22/30], Batch [200/6000], Loss: 0.1237
Epoch [22/30], Batch [300/6000], Loss: 0.1213
Epoch [22/30], Batch [400/6000], Loss: 0.1119
Epoch [22/30], Batch [500/6000], Loss: 0.1707
Epoch [22/30], Batch [600/6000], Loss: 0.2983
Epoch [22/30], Batch [700/6000], Loss: 0.0931
Epoch [22/30], Batch [800/6000], Loss: 0.1417
Epoch [22/30], Batch [900/6000], Loss: 0.2017
Epoch [22/30], Batch [1000/6000], Loss: 0.0830
Epoch [22/30], Batch [1100/6000], Loss: 0.1120
Epoch [22/30], Batch [1200/6000], Loss: 0.1318
Epoch [22/30], Batch [1300/6000], Loss: 0.1028
Epoch [22/30], Batch [1400/6000], Loss: 0.1347
Epoch [22/30], Batch [1500/6000], Loss: 0.1181
Epoch [22/30], Batch [1600/6000], Loss: 0.1100
Epoch [22/30], Batch [1700/6000], Loss: 0.1895
Epoch [22/30], Batch [1800/6000], Loss: 0.1418
Epoch [22/30], Batch [1900/6000], Loss: 0.1132
Epoch [22/30], Batch [2000/6000], Loss: 0.2008
Epoch [22/30], Batch [2100/6000], Loss: 0.0933
Epoch [22/30], Batch [2200/6000], Loss: 0.0665
Epoch [22/30], Batch [2300/6000], Loss: 0.0963
Epoch [22/30], Batch [2400/6000], Loss: 0.2587
Epoch [22/30], Batch [2500/6000], Loss: 2.5222
Epoch [22/30], Batch [2600/6000], Loss: 0.1062
Epoch [22/30], Batch [2700/6000], Loss: 0.1368
Epoch [22/30], Batch [2800/6000], Loss: 1.0605
Epoch [22/30], Batch [2900/6000], Loss: 0.5772
Epoch [22/30], Batch [3000/6000], Loss: 0.1266
Epoch [22/30], Batch [3100/6000], Loss: 0.1377
Epoch [22/30], Batch [3200/6000], Loss: 0.1095
Epoch [22/30], Batch [3300/6000], Loss: 0.1175
Epoch [22/30], Batch [3400/6000], Loss: 0.1143
Epoch [22/30], Batch [3500/6000], Loss: 0.1045
Epoch [22/30], Batch [3600/6000], Loss: 0.1271
Epoch [22/30], Batch [3700/6000], Loss: 0.1205
Epoch [22/30], Batch [3800/6000], Loss: 0.1671
Epoch [22/30], Batch [3900/6000], Loss: 0.1450
Epoch [22/30], Batch [4000/6000], Loss: 0.1199
Epoch [22/30], Batch [4100/6000], Loss: 0.1462
Epoch [22/30], Batch [4200/6000], Loss: 0.1005
Epoch [22/30], Batch [4300/6000], Loss: 0.1153
Epoch [22/30], Batch [4400/6000], Loss: 0.1258
Epoch [22/30], Batch [4500/6000], Loss: 0.1097
Epoch [22/30], Batch [4600/6000], Loss: 0.1520
Epoch [22/30], Batch [4700/6000], Loss: 0.1429
Epoch [22/30], Batch [4800/6000], Loss: 0.1772
Epoch [22/30], Batch [4900/6000], Loss: 0.1190
Epoch [22/30], Batch [5000/6000], Loss: 0.1155
Epoch [22/30], Batch [5100/6000], Loss: 0.1449
Epoch [22/30], Batch [5200/6000], Loss: 0.1258
Epoch [22/30], Batch [5300/6000], Loss: 0.2948
Epoch [22/30], Batch [5400/6000], Loss: 0.0989
Epoch [22/30], Batch [5500/6000], Loss: 0.1193
Epoch [22/30], Batch [5600/6000], Loss: 0.1278
Epoch [22/30], Batch [5700/6000], Loss: 0.1043
Epoch [22/30], Batch [5800/6000], Loss: 0.1382
Epoch [22/30], Batch [5900/6000], Loss: 0.1441
Epoch [22/30], Loss: 0.2104
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.1391
Epoch [23/30], Batch [100/6000], Loss: 0.0894
Epoch [23/30], Batch [200/6000], Loss: 0.1410
Epoch [23/30], Batch [300/6000], Loss: 0.1100
Epoch [23/30], Batch [400/6000], Loss: 0.1003
Epoch [23/30], Batch [500/6000], Loss: 0.1480
Epoch [23/30], Batch [600/6000], Loss: 0.2104
Epoch [23/30], Batch [700/6000], Loss: 0.1130
Epoch [23/30], Batch [800/6000], Loss: 0.0877
Epoch [23/30], Batch [900/6000], Loss: 0.1221
Epoch [23/30], Batch [1000/6000], Loss: 0.0963
Epoch [23/30], Batch [1100/6000], Loss: 0.1012
Epoch [23/30], Batch [1200/6000], Loss: 0.4393
Epoch [23/30], Batch [1300/6000], Loss: 0.1215
Epoch [23/30], Batch [1400/6000], Loss: 0.1410
Epoch [23/30], Batch [1500/6000], Loss: 0.1398
Epoch [23/30], Batch [1600/6000], Loss: 0.1569
Epoch [23/30], Batch [1700/6000], Loss: 0.0944
Epoch [23/30], Batch [1800/6000], Loss: 0.1836
Epoch [23/30], Batch [1900/6000], Loss: 0.2171
Epoch [23/30], Batch [2000/6000], Loss: 0.1199
Epoch [23/30], Batch [2100/6000], Loss: 0.1794
Epoch [23/30], Batch [2200/6000], Loss: 0.1219
Epoch [23/30], Batch [2300/6000], Loss: 0.1412
Epoch [23/30], Batch [2400/6000], Loss: 0.1208
Epoch [23/30], Batch [2500/6000], Loss: 0.0957
Epoch [23/30], Batch [2600/6000], Loss: 0.1314
Epoch [23/30], Batch [2700/6000], Loss: 0.1307
Epoch [23/30], Batch [2800/6000], Loss: 0.1239
Epoch [23/30], Batch [2900/6000], Loss: 0.2593
Epoch [23/30], Batch [3000/6000], Loss: 0.1450
Epoch [23/30], Batch [3100/6000], Loss: 0.1243
Epoch [23/30], Batch [3200/6000], Loss: 0.1399
Epoch [23/30], Batch [3300/6000], Loss: 0.1056
Epoch [23/30], Batch [3400/6000], Loss: 0.1180
Epoch [23/30], Batch [3500/6000], Loss: 0.1245
Epoch [23/30], Batch [3600/6000], Loss: 0.1182
Epoch [23/30], Batch [3700/6000], Loss: 0.0905
Epoch [23/30], Batch [3800/6000], Loss: 0.1261
Epoch [23/30], Batch [3900/6000], Loss: 0.1363
Epoch [23/30], Batch [4000/6000], Loss: 0.1432
Epoch [23/30], Batch [4100/6000], Loss: 0.1282
Epoch [23/30], Batch [4200/6000], Loss: 0.2178
Epoch [23/30], Batch [4300/6000], Loss: 0.1394
Epoch [23/30], Batch [4400/6000], Loss: 0.1199
Epoch [23/30], Batch [4500/6000], Loss: 0.1547
Epoch [23/30], Batch [4600/6000], Loss: 0.1365
Epoch [23/30], Batch [4700/6000], Loss: 0.1064
Epoch [23/30], Batch [4800/6000], Loss: 0.1114
Epoch [23/30], Batch [4900/6000], Loss: 0.2716
Epoch [23/30], Batch [5000/6000], Loss: 0.1844
Epoch [23/30], Batch [5100/6000], Loss: 0.1346
Epoch [23/30], Batch [5200/6000], Loss: 0.5881
Epoch [23/30], Batch [5300/6000], Loss: 0.1483
Epoch [23/30], Batch [5400/6000], Loss: 0.1393
Epoch [23/30], Batch [5500/6000], Loss: 0.1215
Epoch [23/30], Batch [5600/6000], Loss: 0.1101
Epoch [23/30], Batch [5700/6000], Loss: 0.0998
Epoch [23/30], Batch [5800/6000], Loss: 0.1628
Epoch [23/30], Batch [5900/6000], Loss: 0.1203
Epoch [23/30], Loss: 0.2018
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.1073
Epoch [24/30], Batch [100/6000], Loss: 0.1089
Epoch [24/30], Batch [200/6000], Loss: 0.1302
Epoch [24/30], Batch [300/6000], Loss: 0.1233
Epoch [24/30], Batch [400/6000], Loss: 0.1629
Epoch [24/30], Batch [500/6000], Loss: 0.1198
Epoch [24/30], Batch [600/6000], Loss: 0.1256
Epoch [24/30], Batch [700/6000], Loss: 0.1033
Epoch [24/30], Batch [800/6000], Loss: 1.9568
Epoch [24/30], Batch [900/6000], Loss: 0.1143
Epoch [24/30], Batch [1000/6000], Loss: 0.1561
Epoch [24/30], Batch [1100/6000], Loss: 0.9544
Epoch [24/30], Batch [1200/6000], Loss: 0.1433
Epoch [24/30], Batch [1300/6000], Loss: 0.2178
Epoch [24/30], Batch [1400/6000], Loss: 0.1178
Epoch [24/30], Batch [1500/6000], Loss: 0.1233
Epoch [24/30], Batch [1600/6000], Loss: 0.1547
Epoch [24/30], Batch [1700/6000], Loss: 0.0974
Epoch [24/30], Batch [1800/6000], Loss: 0.1389
Epoch [24/30], Batch [1900/6000], Loss: 0.1259
Epoch [24/30], Batch [2000/6000], Loss: 0.1446
Epoch [24/30], Batch [2100/6000], Loss: 0.1271
Epoch [24/30], Batch [2200/6000], Loss: 0.1327
Epoch [24/30], Batch [2300/6000], Loss: 0.1959
Epoch [24/30], Batch [2400/6000], Loss: 0.1153
Epoch [24/30], Batch [2500/6000], Loss: 0.2072
Epoch [24/30], Batch [2600/6000], Loss: 0.1237
Epoch [24/30], Batch [2700/6000], Loss: 0.1230
Epoch [24/30], Batch [2800/6000], Loss: 0.3033
Epoch [24/30], Batch [2900/6000], Loss: 0.1193
Epoch [24/30], Batch [3000/6000], Loss: 0.1282
Epoch [24/30], Batch [3100/6000], Loss: 0.1082
Epoch [24/30], Batch [3200/6000], Loss: 0.1149
Epoch [24/30], Batch [3300/6000], Loss: 0.1068
Epoch [24/30], Batch [3400/6000], Loss: 0.1102
Epoch [24/30], Batch [3500/6000], Loss: 0.1114
Epoch [24/30], Batch [3600/6000], Loss: 0.1309
Epoch [24/30], Batch [3700/6000], Loss: 0.1237
Epoch [24/30], Batch [3800/6000], Loss: 0.1288
Epoch [24/30], Batch [3900/6000], Loss: 0.1678
Epoch [24/30], Batch [4000/6000], Loss: 0.1189
Epoch [24/30], Batch [4100/6000], Loss: 0.6129
Epoch [24/30], Batch [4200/6000], Loss: 0.1348
Epoch [24/30], Batch [4300/6000], Loss: 0.1349
Epoch [24/30], Batch [4400/6000], Loss: 0.2324
Epoch [24/30], Batch [4500/6000], Loss: 0.0971
Epoch [24/30], Batch [4600/6000], Loss: 0.1109
Epoch [24/30], Batch [4700/6000], Loss: 0.1489
Epoch [24/30], Batch [4800/6000], Loss: 0.2911
Epoch [24/30], Batch [4900/6000], Loss: 0.2719
Epoch [24/30], Batch [5000/6000], Loss: 0.1363
Epoch [24/30], Batch [5100/6000], Loss: 0.1167
Epoch [24/30], Batch [5200/6000], Loss: 0.1138
Epoch [24/30], Batch [5300/6000], Loss: 0.1260
Epoch [24/30], Batch [5400/6000], Loss: 0.3136
Epoch [24/30], Batch [5500/6000], Loss: 0.1116
Epoch [24/30], Batch [5600/6000], Loss: 0.5506
Epoch [24/30], Batch [5700/6000], Loss: 0.1399
Epoch [24/30], Batch [5800/6000], Loss: 0.0978
Epoch [24/30], Batch [5900/6000], Loss: 1.0624
Epoch [24/30], Loss: 0.1983
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.0956
Epoch [25/30], Batch [100/6000], Loss: 0.2215
Epoch [25/30], Batch [200/6000], Loss: 0.1795
Epoch [25/30], Batch [300/6000], Loss: 0.1209
Epoch [25/30], Batch [400/6000], Loss: 0.1033
Epoch [25/30], Batch [500/6000], Loss: 0.1080
Epoch [25/30], Batch [600/6000], Loss: 0.0885
Epoch [25/30], Batch [700/6000], Loss: 0.1431
Epoch [25/30], Batch [800/6000], Loss: 0.2120
Epoch [25/30], Batch [900/6000], Loss: 0.1177
Epoch [25/30], Batch [1000/6000], Loss: 0.1084
Epoch [25/30], Batch [1100/6000], Loss: 1.6006
Epoch [25/30], Batch [1200/6000], Loss: 0.1220
Epoch [25/30], Batch [1300/6000], Loss: 0.1206
Epoch [25/30], Batch [1400/6000], Loss: 0.0950
Epoch [25/30], Batch [1500/6000], Loss: 0.0894
Epoch [25/30], Batch [1600/6000], Loss: 0.1486
Epoch [25/30], Batch [1700/6000], Loss: 2.1708
Epoch [25/30], Batch [1800/6000], Loss: 0.1281
Epoch [25/30], Batch [1900/6000], Loss: 1.0031
Epoch [25/30], Batch [2000/6000], Loss: 0.2017
Epoch [25/30], Batch [2100/6000], Loss: 0.3360
Epoch [25/30], Batch [2200/6000], Loss: 0.5279
Epoch [25/30], Batch [2300/6000], Loss: 0.1125
Epoch [25/30], Batch [2400/6000], Loss: 0.1129
Epoch [25/30], Batch [2500/6000], Loss: 0.1273
Epoch [25/30], Batch [2600/6000], Loss: 0.1276
Epoch [25/30], Batch [2700/6000], Loss: 0.0902
Epoch [25/30], Batch [2800/6000], Loss: 0.1261
Epoch [25/30], Batch [2900/6000], Loss: 0.1200
Epoch [25/30], Batch [3000/6000], Loss: 0.1061
Epoch [25/30], Batch [3100/6000], Loss: 0.3600
Epoch [25/30], Batch [3200/6000], Loss: 0.1009
Epoch [25/30], Batch [3300/6000], Loss: 0.1331
Epoch [25/30], Batch [3400/6000], Loss: 0.1031
Epoch [25/30], Batch [3500/6000], Loss: 0.5337
Epoch [25/30], Batch [3600/6000], Loss: 0.1180
Epoch [25/30], Batch [3700/6000], Loss: 0.1495
Epoch [25/30], Batch [3800/6000], Loss: 0.1002
Epoch [25/30], Batch [3900/6000], Loss: 0.1923
Epoch [25/30], Batch [4000/6000], Loss: 0.1130
Epoch [25/30], Batch [4100/6000], Loss: 0.3007
Epoch [25/30], Batch [4200/6000], Loss: 0.1390
Epoch [25/30], Batch [4300/6000], Loss: 0.0985
Epoch [25/30], Batch [4400/6000], Loss: 0.0998
Epoch [25/30], Batch [4500/6000], Loss: 0.1062
Epoch [25/30], Batch [4600/6000], Loss: 0.1357
Epoch [25/30], Batch [4700/6000], Loss: 0.1372
Epoch [25/30], Batch [4800/6000], Loss: 0.1483
Epoch [25/30], Batch [4900/6000], Loss: 0.1245
Epoch [25/30], Batch [5000/6000], Loss: 0.1334
Epoch [25/30], Batch [5100/6000], Loss: 0.1447
Epoch [25/30], Batch [5200/6000], Loss: 0.1141
Epoch [25/30], Batch [5300/6000], Loss: 0.1024
Epoch [25/30], Batch [5400/6000], Loss: 0.3819
Epoch [25/30], Batch [5500/6000], Loss: 0.6381
Epoch [25/30], Batch [5600/6000], Loss: 0.1364
Epoch [25/30], Batch [5700/6000], Loss: 0.0961
Epoch [25/30], Batch [5800/6000], Loss: 0.1460
Epoch [25/30], Batch [5900/6000], Loss: 0.1180
Epoch [25/30], Loss: 0.1909
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.1418
Epoch [26/30], Batch [100/6000], Loss: 0.1224
Epoch [26/30], Batch [200/6000], Loss: 0.0958
Epoch [26/30], Batch [300/6000], Loss: 0.1302
Epoch [26/30], Batch [400/6000], Loss: 0.1079
Epoch [26/30], Batch [500/6000], Loss: 0.1110
Epoch [26/30], Batch [600/6000], Loss: 0.1201
Epoch [26/30], Batch [700/6000], Loss: 0.1187
Epoch [26/30], Batch [800/6000], Loss: 0.1086
Epoch [26/30], Batch [900/6000], Loss: 0.0979
Epoch [26/30], Batch [1000/6000], Loss: 0.0935
Epoch [26/30], Batch [1100/6000], Loss: 0.1188
Epoch [26/30], Batch [1200/6000], Loss: 0.1706
Epoch [26/30], Batch [1300/6000], Loss: 0.1060
Epoch [26/30], Batch [1400/6000], Loss: 0.0740
Epoch [26/30], Batch [1500/6000], Loss: 0.1526
Epoch [26/30], Batch [1600/6000], Loss: 0.1074
Epoch [26/30], Batch [1700/6000], Loss: 0.1152
Epoch [26/30], Batch [1800/6000], Loss: 0.7120
Epoch [26/30], Batch [1900/6000], Loss: 0.1218
Epoch [26/30], Batch [2000/6000], Loss: 0.1674
Epoch [26/30], Batch [2100/6000], Loss: 0.1586
Epoch [26/30], Batch [2200/6000], Loss: 0.1264
Epoch [26/30], Batch [2300/6000], Loss: 0.3771
Epoch [26/30], Batch [2400/6000], Loss: 0.1554
Epoch [26/30], Batch [2500/6000], Loss: 0.1340
Epoch [26/30], Batch [2600/6000], Loss: 0.1033
Epoch [26/30], Batch [2700/6000], Loss: 0.1483
Epoch [26/30], Batch [2800/6000], Loss: 0.1315
Epoch [26/30], Batch [2900/6000], Loss: 0.1030
Epoch [26/30], Batch [3000/6000], Loss: 0.1298
Epoch [26/30], Batch [3100/6000], Loss: 0.1089
Epoch [26/30], Batch [3200/6000], Loss: 0.0987
Epoch [26/30], Batch [3300/6000], Loss: 0.1233
Epoch [26/30], Batch [3400/6000], Loss: 0.1298
Epoch [26/30], Batch [3500/6000], Loss: 0.0970
Epoch [26/30], Batch [3600/6000], Loss: 0.0929
Epoch [26/30], Batch [3700/6000], Loss: 0.1200
Epoch [26/30], Batch [3800/6000], Loss: 0.9586
Epoch [26/30], Batch [3900/6000], Loss: 0.1425
Epoch [26/30], Batch [4000/6000], Loss: 0.0973
Epoch [26/30], Batch [4100/6000], Loss: 1.3019
Epoch [26/30], Batch [4200/6000], Loss: 0.1528
Epoch [26/30], Batch [4300/6000], Loss: 0.1233
Epoch [26/30], Batch [4400/6000], Loss: 0.1309
Epoch [26/30], Batch [4500/6000], Loss: 0.0902
Epoch [26/30], Batch [4600/6000], Loss: 1.4981
Epoch [26/30], Batch [4700/6000], Loss: 0.0952
Epoch [26/30], Batch [4800/6000], Loss: 0.1102
Epoch [26/30], Batch [4900/6000], Loss: 0.3618
Epoch [26/30], Batch [5000/6000], Loss: 0.1090
Epoch [26/30], Batch [5100/6000], Loss: 0.1165
Epoch [26/30], Batch [5200/6000], Loss: 0.1133
Epoch [26/30], Batch [5300/6000], Loss: 0.2347
Epoch [26/30], Batch [5400/6000], Loss: 0.2321
Epoch [26/30], Batch [5500/6000], Loss: 0.1224
Epoch [26/30], Batch [5600/6000], Loss: 0.1352
Epoch [26/30], Batch [5700/6000], Loss: 0.1240
Epoch [26/30], Batch [5800/6000], Loss: 0.1136
Epoch [26/30], Batch [5900/6000], Loss: 0.1170
Epoch [26/30], Loss: 0.1839
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.1937
Epoch [27/30], Batch [100/6000], Loss: 0.1182
Epoch [27/30], Batch [200/6000], Loss: 0.1197
Epoch [27/30], Batch [300/6000], Loss: 0.1337
Epoch [27/30], Batch [400/6000], Loss: 0.2535
Epoch [27/30], Batch [500/6000], Loss: 0.0984
Epoch [27/30], Batch [600/6000], Loss: 0.1147
Epoch [27/30], Batch [700/6000], Loss: 0.1312
Epoch [27/30], Batch [800/6000], Loss: 0.0941
Epoch [27/30], Batch [900/6000], Loss: 0.1009
Epoch [27/30], Batch [1000/6000], Loss: 0.0902
Epoch [27/30], Batch [1100/6000], Loss: 0.9533
Epoch [27/30], Batch [1200/6000], Loss: 0.1233
Epoch [27/30], Batch [1300/6000], Loss: 0.1143
Epoch [27/30], Batch [1400/6000], Loss: 0.1196
Epoch [27/30], Batch [1500/6000], Loss: 0.1045
Epoch [27/30], Batch [1600/6000], Loss: 0.1295
Epoch [27/30], Batch [1700/6000], Loss: 0.1055
Epoch [27/30], Batch [1800/6000], Loss: 0.1865
Epoch [27/30], Batch [1900/6000], Loss: 0.1169
Epoch [27/30], Batch [2000/6000], Loss: 0.1309
Epoch [27/30], Batch [2100/6000], Loss: 0.1224
Epoch [27/30], Batch [2200/6000], Loss: 0.1241
Epoch [27/30], Batch [2300/6000], Loss: 0.1020
Epoch [27/30], Batch [2400/6000], Loss: 0.1308
Epoch [27/30], Batch [2500/6000], Loss: 0.1577
Epoch [27/30], Batch [2600/6000], Loss: 0.0906
Epoch [27/30], Batch [2700/6000], Loss: 0.1142
Epoch [27/30], Batch [2800/6000], Loss: 0.1675
Epoch [27/30], Batch [2900/6000], Loss: 0.1095
Epoch [27/30], Batch [3000/6000], Loss: 0.1121
Epoch [27/30], Batch [3100/6000], Loss: 0.1342
Epoch [27/30], Batch [3200/6000], Loss: 0.1287
Epoch [27/30], Batch [3300/6000], Loss: 0.7018
Epoch [27/30], Batch [3400/6000], Loss: 0.1608
Epoch [27/30], Batch [3500/6000], Loss: 0.1208
Epoch [27/30], Batch [3600/6000], Loss: 0.1183
Epoch [27/30], Batch [3700/6000], Loss: 0.6761
Epoch [27/30], Batch [3800/6000], Loss: 0.1886
Epoch [27/30], Batch [3900/6000], Loss: 0.1366
Epoch [27/30], Batch [4000/6000], Loss: 0.2124
Epoch [27/30], Batch [4100/6000], Loss: 0.1216
Epoch [27/30], Batch [4200/6000], Loss: 0.1371
Epoch [27/30], Batch [4300/6000], Loss: 0.1156
Epoch [27/30], Batch [4400/6000], Loss: 0.1273
Epoch [27/30], Batch [4500/6000], Loss: 0.1227
Epoch [27/30], Batch [4600/6000], Loss: 0.1244
Epoch [27/30], Batch [4700/6000], Loss: 0.1435
Epoch [27/30], Batch [4800/6000], Loss: 0.1065
Epoch [27/30], Batch [4900/6000], Loss: 0.2283
Epoch [27/30], Batch [5000/6000], Loss: 0.1127
Epoch [27/30], Batch [5100/6000], Loss: 0.1294
Epoch [27/30], Batch [5200/6000], Loss: 0.0879
Epoch [27/30], Batch [5300/6000], Loss: 0.1019
Epoch [27/30], Batch [5400/6000], Loss: 0.1108
Epoch [27/30], Batch [5500/6000], Loss: 0.1266
Epoch [27/30], Batch [5600/6000], Loss: 0.1190
Epoch [27/30], Batch [5700/6000], Loss: 0.1141
Epoch [27/30], Batch [5800/6000], Loss: 0.0875
Epoch [27/30], Batch [5900/6000], Loss: 0.2974
Epoch [27/30], Loss: 0.1777
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.1187
Epoch [28/30], Batch [100/6000], Loss: 0.1460
Epoch [28/30], Batch [200/6000], Loss: 0.3347
Epoch [28/30], Batch [300/6000], Loss: 0.1496
Epoch [28/30], Batch [400/6000], Loss: 0.1053
Epoch [28/30], Batch [500/6000], Loss: 0.1097
Epoch [28/30], Batch [600/6000], Loss: 0.1183
Epoch [28/30], Batch [700/6000], Loss: 0.1339
Epoch [28/30], Batch [800/6000], Loss: 0.0950
Epoch [28/30], Batch [900/6000], Loss: 0.1154
Epoch [28/30], Batch [1000/6000], Loss: 0.8079
Epoch [28/30], Batch [1100/6000], Loss: 0.0979
Epoch [28/30], Batch [1200/6000], Loss: 0.0945
Epoch [28/30], Batch [1300/6000], Loss: 0.0972
Epoch [28/30], Batch [1400/6000], Loss: 0.2216
Epoch [28/30], Batch [1500/6000], Loss: 0.1314
Epoch [28/30], Batch [1600/6000], Loss: 0.0784
Epoch [28/30], Batch [1700/6000], Loss: 0.1428
Epoch [28/30], Batch [1800/6000], Loss: 0.1387
Epoch [28/30], Batch [1900/6000], Loss: 0.0793
Epoch [28/30], Batch [2000/6000], Loss: 0.1017
Epoch [28/30], Batch [2100/6000], Loss: 0.1090
Epoch [28/30], Batch [2200/6000], Loss: 0.1730
Epoch [28/30], Batch [2300/6000], Loss: 0.1201
Epoch [28/30], Batch [2400/6000], Loss: 0.1096
Epoch [28/30], Batch [2500/6000], Loss: 0.5862
Epoch [28/30], Batch [2600/6000], Loss: 0.1315
Epoch [28/30], Batch [2700/6000], Loss: 0.1455
Epoch [28/30], Batch [2800/6000], Loss: 0.1183
Epoch [28/30], Batch [2900/6000], Loss: 0.1088
Epoch [28/30], Batch [3000/6000], Loss: 0.9716
Epoch [28/30], Batch [3100/6000], Loss: 0.6588
Epoch [28/30], Batch [3200/6000], Loss: 0.1243
Epoch [28/30], Batch [3300/6000], Loss: 0.1394
Epoch [28/30], Batch [3400/6000], Loss: 0.1041
Epoch [28/30], Batch [3500/6000], Loss: 0.0992
Epoch [28/30], Batch [3600/6000], Loss: 0.1133
Epoch [28/30], Batch [3700/6000], Loss: 0.0991
Epoch [28/30], Batch [3800/6000], Loss: 0.1092
Epoch [28/30], Batch [3900/6000], Loss: 0.0996
Epoch [28/30], Batch [4000/6000], Loss: 0.1300
Epoch [28/30], Batch [4100/6000], Loss: 0.1712
Epoch [28/30], Batch [4200/6000], Loss: 0.1205
Epoch [28/30], Batch [4300/6000], Loss: 0.1012
Epoch [28/30], Batch [4400/6000], Loss: 0.1241
Epoch [28/30], Batch [4500/6000], Loss: 0.1059
Epoch [28/30], Batch [4600/6000], Loss: 0.1465
Epoch [28/30], Batch [4700/6000], Loss: 0.1381
Epoch [28/30], Batch [4800/6000], Loss: 0.0957
Epoch [28/30], Batch [4900/6000], Loss: 0.1086
Epoch [28/30], Batch [5000/6000], Loss: 0.1119
Epoch [28/30], Batch [5100/6000], Loss: 0.1096
Epoch [28/30], Batch [5200/6000], Loss: 0.1137
Epoch [28/30], Batch [5300/6000], Loss: 0.1170
Epoch [28/30], Batch [5400/6000], Loss: 0.0950
Epoch [28/30], Batch [5500/6000], Loss: 0.1238
Epoch [28/30], Batch [5600/6000], Loss: 0.1777
Epoch [28/30], Batch [5700/6000], Loss: 0.0910
Epoch [28/30], Batch [5800/6000], Loss: 0.3321
Epoch [28/30], Batch [5900/6000], Loss: 0.1200
Epoch [28/30], Loss: 0.1756
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.1822
Epoch [29/30], Batch [100/6000], Loss: 0.1069
Epoch [29/30], Batch [200/6000], Loss: 0.1048
Epoch [29/30], Batch [300/6000], Loss: 0.1044
Epoch [29/30], Batch [400/6000], Loss: 0.1227
Epoch [29/30], Batch [500/6000], Loss: 0.1038
Epoch [29/30], Batch [600/6000], Loss: 0.1012
Epoch [29/30], Batch [700/6000], Loss: 0.8659
Epoch [29/30], Batch [800/6000], Loss: 0.1171
Epoch [29/30], Batch [900/6000], Loss: 0.1364
Epoch [29/30], Batch [1000/6000], Loss: 0.1104
Epoch [29/30], Batch [1100/6000], Loss: 0.1082
Epoch [29/30], Batch [1200/6000], Loss: 0.1059
Epoch [29/30], Batch [1300/6000], Loss: 0.1147
Epoch [29/30], Batch [1400/6000], Loss: 0.0857
Epoch [29/30], Batch [1500/6000], Loss: 0.3758
Epoch [29/30], Batch [1600/6000], Loss: 0.1317
Epoch [29/30], Batch [1700/6000], Loss: 0.0991
Epoch [29/30], Batch [1800/6000], Loss: 0.1122
Epoch [29/30], Batch [1900/6000], Loss: 0.1215
Epoch [29/30], Batch [2000/6000], Loss: 0.1235
Epoch [29/30], Batch [2100/6000], Loss: 0.0960
Epoch [29/30], Batch [2200/6000], Loss: 0.2560
Epoch [29/30], Batch [2300/6000], Loss: 0.3701
Epoch [29/30], Batch [2400/6000], Loss: 0.1075
Epoch [29/30], Batch [2500/6000], Loss: 0.1102
Epoch [29/30], Batch [2600/6000], Loss: 0.1049
Epoch [29/30], Batch [2700/6000], Loss: 0.0892
Epoch [29/30], Batch [2800/6000], Loss: 0.1093
Epoch [29/30], Batch [2900/6000], Loss: 0.1041
Epoch [29/30], Batch [3000/6000], Loss: 0.1069
Epoch [29/30], Batch [3100/6000], Loss: 0.1115
Epoch [29/30], Batch [3200/6000], Loss: 0.1226
Epoch [29/30], Batch [3300/6000], Loss: 0.0867
Epoch [29/30], Batch [3400/6000], Loss: 1.8286
Epoch [29/30], Batch [3500/6000], Loss: 0.1078
Epoch [29/30], Batch [3600/6000], Loss: 0.1442
Epoch [29/30], Batch [3700/6000], Loss: 0.1465
Epoch [29/30], Batch [3800/6000], Loss: 0.1261
Epoch [29/30], Batch [3900/6000], Loss: 0.0879
Epoch [29/30], Batch [4000/6000], Loss: 0.0800
Epoch [29/30], Batch [4100/6000], Loss: 0.1165
Epoch [29/30], Batch [4200/6000], Loss: 0.0918
Epoch [29/30], Batch [4300/6000], Loss: 0.1522
Epoch [29/30], Batch [4400/6000], Loss: 0.1231
Epoch [29/30], Batch [4500/6000], Loss: 0.1202
Epoch [29/30], Batch [4600/6000], Loss: 0.1021
Epoch [29/30], Batch [4700/6000], Loss: 0.1320
Epoch [29/30], Batch [4800/6000], Loss: 0.1344
Epoch [29/30], Batch [4900/6000], Loss: 0.1111
Epoch [29/30], Batch [5000/6000], Loss: 0.1109
Epoch [29/30], Batch [5100/6000], Loss: 0.1147
Epoch [29/30], Batch [5200/6000], Loss: 0.1442
Epoch [29/30], Batch [5300/6000], Loss: 0.1042
Epoch [29/30], Batch [5400/6000], Loss: 0.1107
Epoch [29/30], Batch [5500/6000], Loss: 0.1775
Epoch [29/30], Batch [5600/6000], Loss: 0.1191
Epoch [29/30], Batch [5700/6000], Loss: 0.1113
Epoch [29/30], Batch [5800/6000], Loss: 0.1185
Epoch [29/30], Batch [5900/6000], Loss: 0.1022
Epoch [29/30], Loss: 0.1672
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.1187
Epoch [30/30], Batch [100/6000], Loss: 0.1012
Epoch [30/30], Batch [200/6000], Loss: 0.1062
Epoch [30/30], Batch [300/6000], Loss: 0.1182
Epoch [30/30], Batch [400/6000], Loss: 0.1119
Epoch [30/30], Batch [500/6000], Loss: 0.1130
Epoch [30/30], Batch [600/6000], Loss: 0.0861
Epoch [30/30], Batch [700/6000], Loss: 0.1352
Epoch [30/30], Batch [800/6000], Loss: 0.1008
Epoch [30/30], Batch [900/6000], Loss: 0.0903
Epoch [30/30], Batch [1000/6000], Loss: 0.1269
Epoch [30/30], Batch [1100/6000], Loss: 0.1157
Epoch [30/30], Batch [1200/6000], Loss: 0.1061
Epoch [30/30], Batch [1300/6000], Loss: 0.1078
Epoch [30/30], Batch [1400/6000], Loss: 0.1030
Epoch [30/30], Batch [1500/6000], Loss: 0.0997
Epoch [30/30], Batch [1600/6000], Loss: 0.0884
Epoch [30/30], Batch [1700/6000], Loss: 0.1619
Epoch [30/30], Batch [1800/6000], Loss: 0.4299
Epoch [30/30], Batch [1900/6000], Loss: 0.0955
Epoch [30/30], Batch [2000/6000], Loss: 0.1091
Epoch [30/30], Batch [2100/6000], Loss: 0.1100
Epoch [30/30], Batch [2200/6000], Loss: 0.0932
Epoch [30/30], Batch [2300/6000], Loss: 0.1242
Epoch [30/30], Batch [2400/6000], Loss: 0.1138
Epoch [30/30], Batch [2500/6000], Loss: 0.0810
Epoch [30/30], Batch [2600/6000], Loss: 0.1077
Epoch [30/30], Batch [2700/6000], Loss: 0.1595
Epoch [30/30], Batch [2800/6000], Loss: 0.1090
Epoch [30/30], Batch [2900/6000], Loss: 0.1364
Epoch [30/30], Batch [3000/6000], Loss: 0.5719
Epoch [30/30], Batch [3100/6000], Loss: 0.1659
Epoch [30/30], Batch [3200/6000], Loss: 0.1079
Epoch [30/30], Batch [3300/6000], Loss: 0.1046
Epoch [30/30], Batch [3400/6000], Loss: 0.0965
Epoch [30/30], Batch [3500/6000], Loss: 0.0844
Epoch [30/30], Batch [3600/6000], Loss: 0.1217
Epoch [30/30], Batch [3700/6000], Loss: 0.1093
Epoch [30/30], Batch [3800/6000], Loss: 0.0995
Epoch [30/30], Batch [3900/6000], Loss: 0.1106
Epoch [30/30], Batch [4000/6000], Loss: 0.0949
Epoch [30/30], Batch [4100/6000], Loss: 0.3187
Epoch [30/30], Batch [4200/6000], Loss: 0.1726
Epoch [30/30], Batch [4300/6000], Loss: 0.1041
Epoch [30/30], Batch [4400/6000], Loss: 0.1218
Epoch [30/30], Batch [4500/6000], Loss: 1.3169
Epoch [30/30], Batch [4600/6000], Loss: 0.1145
Epoch [30/30], Batch [4700/6000], Loss: 0.1098
Epoch [30/30], Batch [4800/6000], Loss: 0.1014
Epoch [30/30], Batch [4900/6000], Loss: 0.4833
Epoch [30/30], Batch [5000/6000], Loss: 0.0949
Epoch [30/30], Batch [5100/6000], Loss: 0.8341
Epoch [30/30], Batch [5200/6000], Loss: 1.2936
Epoch [30/30], Batch [5300/6000], Loss: 0.1175
Epoch [30/30], Batch [5400/6000], Loss: 0.0939
Epoch [30/30], Batch [5500/6000], Loss: 0.1097
Epoch [30/30], Batch [5600/6000], Loss: 0.6585
Epoch [30/30], Batch [5700/6000], Loss: 0.4059
Epoch [30/30], Batch [5800/6000], Loss: 0.1323
Epoch [30/30], Batch [5900/6000], Loss: 0.0921
Epoch [30/30], Loss: 0.1681
Visualization saved to figures/visualization_0.png
Test Loss: 0.1015, Accuracy: 98.17%
Visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
Adversarial Training Loop 1/300:
  Label Loss: 0.6676
  Image Loss: 0.0246
  Total Loss: 6.7010
  Image grad max: 1.0468770265579224
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
Adversarial Training Loop 2/300:
  Label Loss: 0.5921
  Image Loss: 0.0241
  Total Loss: 5.9451
  Image grad max: 1.3262546062469482
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
Adversarial Training Loop 3/300:
  Label Loss: 0.5092
  Image Loss: 0.0237
  Total Loss: 5.1157
  Image grad max: 1.4863924980163574
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
Adversarial Training Loop 4/300:
  Label Loss: 0.4169
  Image Loss: 0.0235
  Total Loss: 4.1930
  Image grad max: 1.6795393228530884
  Output probs: [[0.    0.    0.    0.    0.002 0.    0.    0.    0.    0.997]]
Adversarial Training Loop 5/300:
  Label Loss: 0.3146
  Image Loss: 0.0233
  Total Loss: 3.1692
  Image grad max: 1.819760799407959
  Output probs: [[0.    0.    0.    0.    0.029 0.    0.    0.004 0.    0.967]]
Adversarial Training Loop 6/300:
  Label Loss: 0.2126
  Image Loss: 0.0233
  Total Loss: 2.1491
  Image grad max: 1.626118779182434
  Output probs: [[0.    0.    0.006 0.    0.25  0.    0.    0.021 0.    0.722]]
Adversarial Training Loop 7/300:
  Label Loss: 0.1402
  Image Loss: 0.0233
  Total Loss: 1.4254
  Image grad max: 1.2576135396957397
  Output probs: [[0.    0.    0.069 0.001 0.539 0.    0.    0.057 0.001 0.333]]
Adversarial Training Loop 8/300:
  Label Loss: 0.1290
  Image Loss: 0.0234
  Total Loss: 1.3135
  Image grad max: 2.4038100242614746
  Output probs: [[0.    0.    0.288 0.005 0.389 0.    0.    0.112 0.002 0.204]]
Adversarial Training Loop 9/300:
  Label Loss: 0.1198
  Image Loss: 0.0236
  Total Loss: 1.2213
  Image grad max: 2.350227117538452
  Output probs: [[0.    0.    0.449 0.013 0.176 0.    0.    0.183 0.002 0.178]]
Adversarial Training Loop 10/300:
  Label Loss: 0.1020
  Image Loss: 0.0237
  Total Loss: 1.0442
  Image grad max: 2.3618550300598145
  Output probs: [[0.    0.    0.307 0.023 0.078 0.    0.    0.326 0.001 0.263]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0534
  Image Loss: 0.0238
  Total Loss: 0.5576
  Image grad max: 1.6051815748214722
  Output probs: [[0.    0.    0.106 0.026 0.029 0.    0.    0.462 0.001 0.377]]
Adversarial Training Loop 12/300:
  Label Loss: 0.0181
  Image Loss: 0.0239
  Total Loss: 0.2047
  Image grad max: 0.601123034954071
  Output probs: [[0.    0.    0.028 0.02  0.01  0.    0.    0.506 0.    0.436]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0063
  Image Loss: 0.0240
  Total Loss: 0.0867
  Image grad max: 0.23354892432689667
  Output probs: [[0.    0.    0.008 0.014 0.004 0.    0.    0.507 0.    0.467]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0027
  Image Loss: 0.0241
  Total Loss: 0.0512
  Image grad max: 0.10807090252637863
  Output probs: [[0.    0.    0.003 0.01  0.002 0.    0.    0.499 0.    0.486]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0015
  Image Loss: 0.0242
  Total Loss: 0.0387
  Image grad max: 0.04650155082345009
  Output probs: [[0.    0.    0.001 0.007 0.001 0.    0.    0.494 0.    0.497]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0009
  Image Loss: 0.0243
  Total Loss: 0.0335
  Image grad max: 0.020585469901561737
  Output probs: [[0.    0.    0.    0.006 0.    0.    0.    0.494 0.    0.499]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0007
  Image Loss: 0.0244
  Total Loss: 0.0310
  Image grad max: 0.015430890023708344
  Output probs: [[0.    0.    0.    0.005 0.    0.    0.    0.499 0.    0.496]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0005
  Image Loss: 0.0245
  Total Loss: 0.0295
  Image grad max: 0.014628779143095016
  Output probs: [[0.    0.    0.    0.004 0.    0.    0.    0.506 0.    0.49 ]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0004
  Image Loss: 0.0246
  Total Loss: 0.0288
  Image grad max: 0.04069750756025314
  Output probs: [[0.    0.    0.    0.003 0.    0.    0.    0.513 0.    0.484]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0004
  Image Loss: 0.0247
  Total Loss: 0.0285
  Image grad max: 0.06912795454263687
  Output probs: [[0.    0.    0.    0.003 0.    0.    0.    0.515 0.    0.482]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0003
  Image Loss: 0.0248
  Total Loss: 0.0282
  Image grad max: 0.07641540467739105
  Output probs: [[0.    0.    0.    0.002 0.    0.    0.    0.511 0.    0.486]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0003
  Image Loss: 0.0248
  Total Loss: 0.0276
  Image grad max: 0.05818422883749008
  Output probs: [[0.    0.    0.    0.002 0.    0.    0.    0.504 0.    0.494]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0002
  Image Loss: 0.0249
  Total Loss: 0.0271
  Image grad max: 0.024208664894104004
  Output probs: [[0.    0.    0.    0.002 0.    0.    0.    0.495 0.    0.503]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0002
  Image Loss: 0.0250
  Total Loss: 0.0269
  Image grad max: 0.013740764930844307
  Output probs: [[0.    0.    0.    0.002 0.    0.    0.    0.49  0.    0.508]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0002
  Image Loss: 0.0251
  Total Loss: 0.0270
  Image grad max: 0.03920562565326691
  Output probs: [[0.    0.    0.    0.002 0.    0.    0.    0.489 0.    0.509]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0002
  Image Loss: 0.0251
  Total Loss: 0.0269
  Image grad max: 0.042601294815540314
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.493 0.    0.505]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0002
  Image Loss: 0.0252
  Total Loss: 0.0267
  Image grad max: 0.025140907615423203
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0001
  Image Loss: 0.0252
  Total Loss: 0.0266
  Image grad max: 0.004502583760768175
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.506 0.    0.493]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0001
  Image Loss: 0.0253
  Total Loss: 0.0266
  Image grad max: 0.030750729143619537
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.509 0.    0.49 ]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0001
  Image Loss: 0.0253
  Total Loss: 0.0267
  Image grad max: 0.04447629675269127
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.508 0.    0.491]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0267
  Image grad max: 0.03943062201142311
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.503 0.    0.496]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0265
  Image grad max: 0.01793130487203598
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.497 0.    0.502]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0265
  Image grad max: 0.0092993238940835
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.493 0.    0.506]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0265
  Image grad max: 0.02926252782344818
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.492 0.    0.507]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0266
  Image grad max: 0.03259539604187012
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.495 0.    0.504]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0265
  Image grad max: 0.0185080599039793
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0264
  Image grad max: 0.004554098471999168
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.504 0.    0.495]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0265
  Image grad max: 0.023432694375514984
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.506 0.    0.494]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0265
  Image grad max: 0.029211947694420815
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.504 0.    0.496]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0264
  Image grad max: 0.01989414170384407
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0264
  Image grad max: 0.002576644066721201
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.496 0.    0.504]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0264
  Image grad max: 0.016557298600673676
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.494 0.    0.505]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0264
  Image grad max: 0.022366801276803017
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.496 0.    0.503]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0264
  Image grad max: 0.01503719575703144
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0264
  Image grad max: 0.002056800527498126
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.502 0.    0.497]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0264
  Image grad max: 0.013538887724280357
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.503 0.    0.496]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0001
  Image Loss: 0.0257
  Total Loss: 0.0264
  Image grad max: 0.01891617849469185
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.502 0.    0.497]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0001
  Image Loss: 0.0257
  Total Loss: 0.0264
  Image grad max: 0.013380737043917179
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0263
  Image grad max: 0.0021880101412534714
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.497 0.    0.503]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0263
  Image grad max: 0.01148860715329647
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.496 0.    0.503]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0263
  Image grad max: 0.014433440752327442
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.498 0.    0.502]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0263
  Image grad max: 0.0075147864408791065
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0263
  Image grad max: 0.0034709973260760307
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.502 0.    0.498]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0263
  Image grad max: 0.01101746316999197
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.502 0.    0.498]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0263
  Image grad max: 0.011694129556417465
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0263
  Image grad max: 0.004898312501609325
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.498 0.    0.501]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0262
  Image grad max: 0.004449537489563227
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.497 0.    0.502]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0262
  Image grad max: 0.0093768909573555
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.498 0.    0.502]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0262
  Image grad max: 0.006975317839533091
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0262
  Image grad max: 0.0021926765330135822
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0262
  Image grad max: 0.006888890638947487
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.501 0.    0.498]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0262
  Image grad max: 0.008145762607455254
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0262
  Image grad max: 0.003621458075940609
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0001
  Image Loss: 0.0256
  Total Loss: 0.0261
  Image grad max: 0.0033379755914211273
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.498 0.    0.501]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0261
  Image grad max: 0.006484738551080227
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.498 0.    0.501]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0261
  Image grad max: 0.004196335095912218
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.5  ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0261
  Image grad max: 0.0026463919784873724
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.501 0.    0.499]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0261
  Image grad max: 0.005506922025233507
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0261
  Image grad max: 0.004925889428704977
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0260
  Image grad max: 0.002138386480510235
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0260
  Image grad max: 0.003947162069380283
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0001
  Image Loss: 0.0255
  Total Loss: 0.0260
  Image grad max: 0.004259645938873291
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0260
  Image grad max: 0.0019475524313747883
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0260
  Image grad max: 0.0033811391331255436
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0260
  Image grad max: 0.0040935613214969635
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.5  ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0259
  Image grad max: 0.0026019830256700516
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0259
  Image grad max: 0.002468359423801303
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0259
  Image grad max: 0.0033756832126528025
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0001
  Image Loss: 0.0254
  Total Loss: 0.0259
  Image grad max: 0.0020558987744152546
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.5  ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0001
  Image Loss: 0.0253
  Total Loss: 0.0259
  Image grad max: 0.002757059643045068
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.499]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0001
  Image Loss: 0.0253
  Total Loss: 0.0258
  Image grad max: 0.0032891281880438328
  Output probs: [[0.    0.    0.    0.001 0.    0.    0.    0.5   0.    0.5  ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0001
  Image Loss: 0.0253
  Total Loss: 0.0258
  Image grad max: 0.0025923314969986677
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0001
  Image Loss: 0.0253
  Total Loss: 0.0258
  Image grad max: 0.0020464558620005846
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.501]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0001
  Image Loss: 0.0253
  Total Loss: 0.0258
  Image grad max: 0.002597321290522814
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0001
  Image Loss: 0.0253
  Total Loss: 0.0258
  Image grad max: 0.0020213215611875057
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0000
  Image Loss: 0.0253
  Total Loss: 0.0258
  Image grad max: 0.002430963097140193
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0000
  Image Loss: 0.0252
  Total Loss: 0.0257
  Image grad max: 0.002923168707638979
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0000
  Image Loss: 0.0252
  Total Loss: 0.0257
  Image grad max: 0.002462166827172041
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0000
  Image Loss: 0.0252
  Total Loss: 0.0257
  Image grad max: 0.001906155375763774
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0000
  Image Loss: 0.0252
  Total Loss: 0.0257
  Image grad max: 0.0022135598119348288
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0000
  Image Loss: 0.0252
  Total Loss: 0.0257
  Image grad max: 0.0019266405142843723
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0000
  Image Loss: 0.0252
  Total Loss: 0.0256
  Image grad max: 0.0022763905581086874
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0000
  Image Loss: 0.0252
  Total Loss: 0.0256
  Image grad max: 0.002646872540935874
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0000
  Image Loss: 0.0251
  Total Loss: 0.0256
  Image grad max: 0.002246195450425148
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0000
  Image Loss: 0.0251
  Total Loss: 0.0256
  Image grad max: 0.001879789400845766
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0000
  Image Loss: 0.0251
  Total Loss: 0.0256
  Image grad max: 0.002028722781687975
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0000
  Image Loss: 0.0251
  Total Loss: 0.0256
  Image grad max: 0.0019040291663259268
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0000
  Image Loss: 0.0251
  Total Loss: 0.0255
  Image grad max: 0.0022702489513903856
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0000
  Image Loss: 0.0251
  Total Loss: 0.0255
  Image grad max: 0.00243928749114275
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0000
  Image Loss: 0.0250
  Total Loss: 0.0255
  Image grad max: 0.0020610583014786243
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0000
  Image Loss: 0.0250
  Total Loss: 0.0255
  Image grad max: 0.0018712326418608427
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0000
  Image Loss: 0.0250
  Total Loss: 0.0255
  Image grad max: 0.001900041475892067
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0000
  Image Loss: 0.0250
  Total Loss: 0.0254
  Image grad max: 0.0019272753270342946
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0000
  Image Loss: 0.0250
  Total Loss: 0.0254
  Image grad max: 0.002238479210063815
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0000
  Image Loss: 0.0250
  Total Loss: 0.0254
  Image grad max: 0.0022293056827038527
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0000
  Image Loss: 0.0249
  Total Loss: 0.0254
  Image grad max: 0.0019287362229079008
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0000
  Image Loss: 0.0249
  Total Loss: 0.0254
  Image grad max: 0.001866389298811555
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0000
  Image Loss: 0.0249
  Total Loss: 0.0253
  Image grad max: 0.001880234805867076
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0000
  Image Loss: 0.0249
  Total Loss: 0.0253
  Image grad max: 0.0019715384114533663
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0000
  Image Loss: 0.0249
  Total Loss: 0.0253
  Image grad max: 0.0021725045517086983
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0000
  Image Loss: 0.0249
  Total Loss: 0.0253
  Image grad max: 0.002046252368018031
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0000
  Image Loss: 0.0248
  Total Loss: 0.0253
  Image grad max: 0.0018966705538332462
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0000
  Image Loss: 0.0248
  Total Loss: 0.0252
  Image grad max: 0.0018683659145608544
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0000
  Image Loss: 0.0248
  Total Loss: 0.0252
  Image grad max: 0.0018984096823260188
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0000
  Image Loss: 0.0248
  Total Loss: 0.0252
  Image grad max: 0.002015118720009923
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0000
  Image Loss: 0.0248
  Total Loss: 0.0252
  Image grad max: 0.002070771297439933
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0000
  Image Loss: 0.0248
  Total Loss: 0.0252
  Image grad max: 0.0019058166071772575
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0000
  Image Loss: 0.0247
  Total Loss: 0.0252
  Image grad max: 0.0018666639225557446
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0000
  Image Loss: 0.0247
  Total Loss: 0.0251
  Image grad max: 0.0018802564591169357
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0000
  Image Loss: 0.0247
  Total Loss: 0.0251
  Image grad max: 0.001936101820319891
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0000
  Image Loss: 0.0247
  Total Loss: 0.0251
  Image grad max: 0.002039938233792782
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0000
  Image Loss: 0.0247
  Total Loss: 0.0251
  Image grad max: 0.0019175595371052623
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0000
  Image Loss: 0.0247
  Total Loss: 0.0251
  Image grad max: 0.001877614064142108
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0000
  Image Loss: 0.0246
  Total Loss: 0.0250
  Image grad max: 0.00186869443859905
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0000
  Image Loss: 0.0246
  Total Loss: 0.0250
  Image grad max: 0.001896418398246169
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0000
  Image Loss: 0.0246
  Total Loss: 0.0250
  Image grad max: 0.0019679716788232327
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0000
  Image Loss: 0.0246
  Total Loss: 0.0250
  Image grad max: 0.0019384201150387526
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0000
  Image Loss: 0.0246
  Total Loss: 0.0250
  Image grad max: 0.0018851631321012974
  Output probs: [[0.    0.    0.    0.    0.    0.    0.    0.499 0.    0.5  ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0000
  Image Loss: 0.0246
  Total Loss: 0.0249
  Image grad max: 0.0018665437819436193
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0000
  Image Loss: 0.0245
  Total Loss: 0.0249
  Image grad max: 0.0018787458539009094
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0000
  Image Loss: 0.0245
  Total Loss: 0.0249
  Image grad max: 0.001902537769638002
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0000
  Image Loss: 0.0245
  Total Loss: 0.0249
  Image grad max: 0.0019245315343141556
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0000
  Image Loss: 0.0245
  Total Loss: 0.0249
  Image grad max: 0.0018860562704503536
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0000
  Image Loss: 0.0245
  Total Loss: 0.0248
  Image grad max: 0.0018658868502825499
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0000
  Image Loss: 0.0244
  Total Loss: 0.0248
  Image grad max: 0.0018680249340832233
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0000
  Image Loss: 0.0244
  Total Loss: 0.0248
  Image grad max: 0.001886707148514688
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0000
  Image Loss: 0.0244
  Total Loss: 0.0248
  Image grad max: 0.0018931617960333824
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0000
  Image Loss: 0.0244
  Total Loss: 0.0248
  Image grad max: 0.001874829875305295
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0000
  Image Loss: 0.0244
  Total Loss: 0.0247
  Image grad max: 0.001860397169366479
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0000
  Image Loss: 0.0244
  Total Loss: 0.0247
  Image grad max: 0.001864096731878817
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0000
  Image Loss: 0.0243
  Total Loss: 0.0247
  Image grad max: 0.0018780864775180817
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0000
  Image Loss: 0.0243
  Total Loss: 0.0247
  Image grad max: 0.0018830837216228247
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0000
  Image Loss: 0.0243
  Total Loss: 0.0247
  Image grad max: 0.0018701620865613222
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0000
  Image Loss: 0.0243
  Total Loss: 0.0246
  Image grad max: 0.0018565226346254349
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0000
  Image Loss: 0.0243
  Total Loss: 0.0246
  Image grad max: 0.0018560702446848154
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0000
  Image Loss: 0.0243
  Total Loss: 0.0246
  Image grad max: 0.0018672143341973424
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0000
  Image Loss: 0.0242
  Total Loss: 0.0246
  Image grad max: 0.0018724554684013128
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0000
  Image Loss: 0.0242
  Total Loss: 0.0246
  Image grad max: 0.001864418387413025
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0000
  Image Loss: 0.0242
  Total Loss: 0.0245
  Image grad max: 0.0018507078057155013
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0000
  Image Loss: 0.0242
  Total Loss: 0.0245
  Image grad max: 0.001849338412284851
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0000
  Image Loss: 0.0242
  Total Loss: 0.0245
  Image grad max: 0.0018583422061055899
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0000
  Image Loss: 0.0241
  Total Loss: 0.0245
  Image grad max: 0.0018627747194841504
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0000
  Image Loss: 0.0241
  Total Loss: 0.0245
  Image grad max: 0.0018559484742581844
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0000
  Image Loss: 0.0241
  Total Loss: 0.0244
  Image grad max: 0.0018451865762472153
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0000
  Image Loss: 0.0241
  Total Loss: 0.0244
  Image grad max: 0.001843624166212976
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0000
  Image Loss: 0.0241
  Total Loss: 0.0244
  Image grad max: 0.0018504231702536345
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0000
  Image Loss: 0.0241
  Total Loss: 0.0244
  Image grad max: 0.0018531541572883725
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0000
  Image Loss: 0.0240
  Total Loss: 0.0244
  Image grad max: 0.0018473351374268532
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0000
  Image Loss: 0.0240
  Total Loss: 0.0243
  Image grad max: 0.001839176402427256
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0000
  Image Loss: 0.0240
  Total Loss: 0.0243
  Image grad max: 0.0018383056158199906
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0000
  Image Loss: 0.0240
  Total Loss: 0.0243
  Image grad max: 0.001839666161686182
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0000
  Image Loss: 0.0240
  Total Loss: 0.0243
  Image grad max: 0.0018392211059108377
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0000
  Image Loss: 0.0239
  Total Loss: 0.0243
  Image grad max: 0.0018379356479272246
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0000
  Image Loss: 0.0239
  Total Loss: 0.0242
  Image grad max: 0.0018372861668467522
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0000
  Image Loss: 0.0239
  Total Loss: 0.0242
  Image grad max: 0.0018343081464990973
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0000
  Image Loss: 0.0239
  Total Loss: 0.0242
  Image grad max: 0.001832823734730482
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0000
  Image Loss: 0.0239
  Total Loss: 0.0242
  Image grad max: 0.001831664820201695
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0000
  Image Loss: 0.0238
  Total Loss: 0.0242
  Image grad max: 0.0018299772636964917
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0000
  Image Loss: 0.0238
  Total Loss: 0.0241
  Image grad max: 0.0018262756057083607
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0000
  Image Loss: 0.0238
  Total Loss: 0.0241
  Image grad max: 0.0018259023781865835
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0000
  Image Loss: 0.0238
  Total Loss: 0.0241
  Image grad max: 0.0018278625793755054
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0000
  Image Loss: 0.0238
  Total Loss: 0.0241
  Image grad max: 0.0018269625725224614
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0000
  Image Loss: 0.0238
  Total Loss: 0.0241
  Image grad max: 0.0018255243776366115
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0000
  Image Loss: 0.0237
  Total Loss: 0.0240
  Image grad max: 0.0018212373834103346
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0000
  Image Loss: 0.0237
  Total Loss: 0.0240
  Image grad max: 0.0018184204818680882
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0000
  Image Loss: 0.0237
  Total Loss: 0.0240
  Image grad max: 0.0018132893601432443
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0000
  Image Loss: 0.0237
  Total Loss: 0.0240
  Image grad max: 0.0018134922720491886
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0000
  Image Loss: 0.0237
  Total Loss: 0.0240
  Image grad max: 0.0018187123350799084
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0000
  Image Loss: 0.0236
  Total Loss: 0.0239
  Image grad max: 0.001818599528633058
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0000
  Image Loss: 0.0236
  Total Loss: 0.0239
  Image grad max: 0.0018078465946018696
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0000
  Image Loss: 0.0236
  Total Loss: 0.0239
  Image grad max: 0.0018047564662992954
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0000
  Image Loss: 0.0236
  Total Loss: 0.0239
  Image grad max: 0.0018112711841240525
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0000
  Image Loss: 0.0236
  Total Loss: 0.0239
  Image grad max: 0.0018166120862588286
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0000
  Image Loss: 0.0235
  Total Loss: 0.0238
  Image grad max: 0.001808642758987844
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0000
  Image Loss: 0.0235
  Total Loss: 0.0238
  Image grad max: 0.0018005722668021917
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0000
  Image Loss: 0.0235
  Total Loss: 0.0238
  Image grad max: 0.0018013454973697662
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0000
  Image Loss: 0.0235
  Total Loss: 0.0238
  Image grad max: 0.0018074429826810956
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0000
  Image Loss: 0.0235
  Total Loss: 0.0238
  Image grad max: 0.0018077859422191978
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0000
  Image Loss: 0.0235
  Total Loss: 0.0237
  Image grad max: 0.0017993315123021603
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0000
  Image Loss: 0.0234
  Total Loss: 0.0237
  Image grad max: 0.0017941707046702504
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0000
  Image Loss: 0.0234
  Total Loss: 0.0237
  Image grad max: 0.0017966589657589793
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0000
  Image Loss: 0.0234
  Total Loss: 0.0237
  Image grad max: 0.0018022328149527311
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0000
  Image Loss: 0.0234
  Total Loss: 0.0237
  Image grad max: 0.0017984687583521008
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0000
  Image Loss: 0.0234
  Total Loss: 0.0236
  Image grad max: 0.0017913131741806865
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0000
  Image Loss: 0.0233
  Total Loss: 0.0236
  Image grad max: 0.001789366127923131
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0000
  Image Loss: 0.0233
  Total Loss: 0.0236
  Image grad max: 0.0017925273859873414
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0000
  Image Loss: 0.0233
  Total Loss: 0.0236
  Image grad max: 0.0017948211170732975
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0000
  Image Loss: 0.0233
  Total Loss: 0.0236
  Image grad max: 0.0017898193327710032
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0000
  Image Loss: 0.0233
  Total Loss: 0.0235
  Image grad max: 0.0017847089329734445
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0000
  Image Loss: 0.0232
  Total Loss: 0.0235
  Image grad max: 0.0017846956616267562
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0000
  Image Loss: 0.0232
  Total Loss: 0.0235
  Image grad max: 0.0017875242047011852
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0000
  Image Loss: 0.0232
  Total Loss: 0.0235
  Image grad max: 0.0017871868330985308
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0000
  Image Loss: 0.0232
  Total Loss: 0.0235
  Image grad max: 0.0017813692102208734
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0000
  Image Loss: 0.0232
  Total Loss: 0.0234
  Image grad max: 0.00177861494012177
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0000
  Image Loss: 0.0231
  Total Loss: 0.0234
  Image grad max: 0.0017811296274885535
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0000
  Image Loss: 0.0231
  Total Loss: 0.0234
  Image grad max: 0.0017812319565564394
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0000
  Image Loss: 0.0231
  Total Loss: 0.0234
  Image grad max: 0.0017778604524210095
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0000
  Image Loss: 0.0231
  Total Loss: 0.0234
  Image grad max: 0.0017750115366652608
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0000
  Image Loss: 0.0231
  Total Loss: 0.0233
  Image grad max: 0.0017749087419360876
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0000
  Image Loss: 0.0231
  Total Loss: 0.0233
  Image grad max: 0.0017747898818925023
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0000
  Image Loss: 0.0230
  Total Loss: 0.0233
  Image grad max: 0.0017733193235471845
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0000
  Image Loss: 0.0230
  Total Loss: 0.0233
  Image grad max: 0.0017711317632347345
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0000
  Image Loss: 0.0230
  Total Loss: 0.0233
  Image grad max: 0.0017698766896501184
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0000
  Image Loss: 0.0230
  Total Loss: 0.0232
  Image grad max: 0.0017694557318463922
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0000
  Image Loss: 0.0230
  Total Loss: 0.0232
  Image grad max: 0.0017678866861388087
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0000
  Image Loss: 0.0229
  Total Loss: 0.0232
  Image grad max: 0.0017663206672295928
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0000
  Image Loss: 0.0229
  Total Loss: 0.0232
  Image grad max: 0.0017650799127295613
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0000
  Image Loss: 0.0229
  Total Loss: 0.0232
  Image grad max: 0.0017642619786784053
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0000
  Image Loss: 0.0229
  Total Loss: 0.0231
  Image grad max: 0.0017632365925237536
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0000
  Image Loss: 0.0229
  Total Loss: 0.0231
  Image grad max: 0.001761373016051948
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0000
  Image Loss: 0.0228
  Total Loss: 0.0231
  Image grad max: 0.0017606575274839997
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0000
  Image Loss: 0.0228
  Total Loss: 0.0231
  Image grad max: 0.0017587982583791018
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0000
  Image Loss: 0.0228
  Total Loss: 0.0231
  Image grad max: 0.0017574651865288615
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0000
  Image Loss: 0.0228
  Total Loss: 0.0230
  Image grad max: 0.001757390913553536
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0000
  Image Loss: 0.0228
  Total Loss: 0.0230
  Image grad max: 0.0017549016047269106
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0000
  Image Loss: 0.0227
  Total Loss: 0.0230
  Image grad max: 0.0017548376927152276
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0000
  Image Loss: 0.0227
  Total Loss: 0.0230
  Image grad max: 0.0017413741443306208
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0000
  Image Loss: 0.0227
  Total Loss: 0.0230
  Image grad max: 0.0017469507874920964
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0000
  Image Loss: 0.0227
  Total Loss: 0.0229
  Image grad max: 0.0017584930174052715
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0000
  Image Loss: 0.0227
  Total Loss: 0.0229
  Image grad max: 0.0017540481640025973
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0000
  Image Loss: 0.0226
  Total Loss: 0.0229
  Image grad max: 0.0017412446904927492
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0000
  Image Loss: 0.0226
  Total Loss: 0.0229
  Image grad max: 0.0017402484081685543
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0000
  Image Loss: 0.0226
  Total Loss: 0.0229
  Image grad max: 0.0017482341499999166
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0000
  Image Loss: 0.0226
  Total Loss: 0.0228
  Image grad max: 0.0017491112230345607
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0000
  Image Loss: 0.0226
  Total Loss: 0.0228
  Image grad max: 0.0017280163010582328
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0000
  Image Loss: 0.0225
  Total Loss: 0.0228
  Image grad max: 0.0017308946698904037
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0000
  Image Loss: 0.0225
  Total Loss: 0.0228
  Image grad max: 0.00175020273309201
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0000
  Image Loss: 0.0225
  Total Loss: 0.0228
  Image grad max: 0.0017487957375124097
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0000
  Image Loss: 0.0225
  Total Loss: 0.0227
  Image grad max: 0.0017272469121962786
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0000
  Image Loss: 0.0225
  Total Loss: 0.0227
  Image grad max: 0.0017171569634228945
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0000
  Image Loss: 0.0225
  Total Loss: 0.0227
  Image grad max: 0.001734672812744975
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0000
  Image Loss: 0.0224
  Total Loss: 0.0227
  Image grad max: 0.0017488391604274511
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0000
  Image Loss: 0.0224
  Total Loss: 0.0227
  Image grad max: 0.0017351234564557672
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0000
  Image Loss: 0.0224
  Total Loss: 0.0226
  Image grad max: 0.0017162255244329572
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0000
  Image Loss: 0.0224
  Total Loss: 0.0226
  Image grad max: 0.0017229310469701886
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0000
  Image Loss: 0.0224
  Total Loss: 0.0226
  Image grad max: 0.0017395697068423033
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0000
  Image Loss: 0.0223
  Total Loss: 0.0226
  Image grad max: 0.0017343686195090413
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0000
  Image Loss: 0.0223
  Total Loss: 0.0226
  Image grad max: 0.0017167533515021205
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0000
  Image Loss: 0.0223
  Total Loss: 0.0225
  Image grad max: 0.001716210856102407
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0000
  Image Loss: 0.0223
  Total Loss: 0.0225
  Image grad max: 0.0017290024552494287
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0000
  Image Loss: 0.0223
  Total Loss: 0.0225
  Image grad max: 0.001730928779579699
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0000
  Image Loss: 0.0222
  Total Loss: 0.0225
  Image grad max: 0.001717498293146491
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0000
  Image Loss: 0.0222
  Total Loss: 0.0225
  Image grad max: 0.0017110762419179082
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0000
  Image Loss: 0.0222
  Total Loss: 0.0224
  Image grad max: 0.0017201349837705493
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0000
  Image Loss: 0.0222
  Total Loss: 0.0224
  Image grad max: 0.0017251514364033937
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0000
  Image Loss: 0.0222
  Total Loss: 0.0224
  Image grad max: 0.001716694561764598
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0000
  Image Loss: 0.0221
  Total Loss: 0.0224
  Image grad max: 0.0017079329118132591
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0000
  Image Loss: 0.0221
  Total Loss: 0.0224
  Image grad max: 0.0017111055785790086
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0000
  Image Loss: 0.0221
  Total Loss: 0.0223
  Image grad max: 0.0017133645014837384
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0000
  Image Loss: 0.0221
  Total Loss: 0.0223
  Image grad max: 0.0017143766162917018
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0000
  Image Loss: 0.0221
  Total Loss: 0.0223
  Image grad max: 0.0017091290792450309
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0000
  Image Loss: 0.0220
  Total Loss: 0.0223
  Image grad max: 0.0017062631668522954
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0000
  Image Loss: 0.0220
  Total Loss: 0.0222
  Image grad max: 0.0017067593289539218
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0000
  Image Loss: 0.0220
  Total Loss: 0.0222
  Image grad max: 0.0017074774950742722
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0000
  Image Loss: 0.0220
  Total Loss: 0.0222
  Image grad max: 0.0017060425598174334
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0000
  Image Loss: 0.0220
  Total Loss: 0.0222
  Image grad max: 0.0017032582545652986
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0000
  Image Loss: 0.0219
  Total Loss: 0.0222
  Image grad max: 0.0017022340325638652
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0000
  Image Loss: 0.0219
  Total Loss: 0.0221
  Image grad max: 0.0017025417182594538
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0000
  Image Loss: 0.0219
  Total Loss: 0.0221
  Image grad max: 0.001701613306067884
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0000
  Image Loss: 0.0219
  Total Loss: 0.0221
  Image grad max: 0.001699671964161098
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0000
  Image Loss: 0.0219
  Total Loss: 0.0221
  Image grad max: 0.0016973192105069757
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0000
  Image Loss: 0.0218
  Total Loss: 0.0221
  Image grad max: 0.001697207917459309
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0000
  Image Loss: 0.0218
  Total Loss: 0.0220
  Image grad max: 0.0016972029116004705
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0000
  Image Loss: 0.0218
  Total Loss: 0.0220
  Image grad max: 0.0016946494579315186
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0000
  Image Loss: 0.0218
  Total Loss: 0.0220
  Image grad max: 0.0016963676316663623
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0000
  Image Loss: 0.0218
  Total Loss: 0.0220
  Image grad max: 0.001693204278126359
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0000
  Image Loss: 0.0217
  Total Loss: 0.0220
  Image grad max: 0.0016897449968382716
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0000
  Image Loss: 0.0217
  Total Loss: 0.0219
  Image grad max: 0.0016901363851502538
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0000
  Image Loss: 0.0217
  Total Loss: 0.0219
  Image grad max: 0.0016910381382331252
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0000
  Image Loss: 0.0217
  Total Loss: 0.0219
  Image grad max: 0.0016887981910258532
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0000
  Image Loss: 0.0217
  Total Loss: 0.0219
  Image grad max: 0.0016860492760315537
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0000
  Image Loss: 0.0216
  Total Loss: 0.0219
  Image grad max: 0.0016848245868459344
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0000
  Image Loss: 0.0216
  Total Loss: 0.0218
  Image grad max: 0.0016841065371409059
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0000
  Image Loss: 0.0216
  Total Loss: 0.0218
  Image grad max: 0.001683585112914443
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0000
  Image Loss: 0.0216
  Total Loss: 0.0218
  Image grad max: 0.0016830679960548878
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0000
  Image Loss: 0.0216
  Total Loss: 0.0218
  Image grad max: 0.001681543537415564
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0000
  Image Loss: 0.0216
  Total Loss: 0.0218
  Image grad max: 0.0016792237292975187
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0000
  Image Loss: 0.0215
  Total Loss: 0.0217
  Image grad max: 0.0016790196532383561
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0000
  Image Loss: 0.0215
  Total Loss: 0.0217
  Image grad max: 0.0016787139466032386
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0000
  Image Loss: 0.0215
  Total Loss: 0.0217
  Image grad max: 0.0016770984511822462
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0000
  Image Loss: 0.0215
  Total Loss: 0.0217
  Image grad max: 0.0016684479778632522
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0000
  Image Loss: 0.0215
  Total Loss: 0.0217
  Image grad max: 0.0016666384181007743
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0000
  Image Loss: 0.0214
  Total Loss: 0.0216
  Image grad max: 0.001671352656558156
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0000
  Image Loss: 0.0214
  Total Loss: 0.0216
  Image grad max: 0.0016746630426496267
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0000
  Image Loss: 0.0214
  Total Loss: 0.0216
  Image grad max: 0.0016707569593563676
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0000
  Image Loss: 0.0214
  Total Loss: 0.0216
  Image grad max: 0.0016652451595291495
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0000
  Image Loss: 0.0214
  Total Loss: 0.0216
  Image grad max: 0.001666643307544291
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0000
  Image Loss: 0.0213
  Total Loss: 0.0215
  Image grad max: 0.0016705394955351949
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0000
  Image Loss: 0.0213
  Total Loss: 0.0215
  Image grad max: 0.0016654391074553132
  Output probs: [[0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0000
  Image Loss: 0.0213
  Total Loss: 0.0215
  Image grad max: 0.001660947105847299
Visualization saved to adversarial_figures/adversarial_training.png
Visualization saved to adversarial_figures/adversarial_testing.png
