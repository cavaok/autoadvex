Epoch [1/30], Batch [0/6000], Loss: 2.3965
Epoch [1/30], Batch [100/6000], Loss: 1.2759
Epoch [1/30], Batch [200/6000], Loss: 0.6496
Epoch [1/30], Batch [300/6000], Loss: 0.5349
Epoch [1/30], Batch [400/6000], Loss: 0.4534
Epoch [1/30], Batch [500/6000], Loss: 0.2099
Epoch [1/30], Batch [600/6000], Loss: 0.4558
Epoch [1/30], Batch [700/6000], Loss: 0.3835
Epoch [1/30], Batch [800/6000], Loss: 0.9560
Epoch [1/30], Batch [900/6000], Loss: 0.3904
Epoch [1/30], Batch [1000/6000], Loss: 0.3631
Epoch [1/30], Batch [1100/6000], Loss: 0.1848
Epoch [1/30], Batch [1200/6000], Loss: 0.1068
Epoch [1/30], Batch [1300/6000], Loss: 0.1944
Epoch [1/30], Batch [1400/6000], Loss: 0.1292
Epoch [1/30], Batch [1500/6000], Loss: 0.5568
Epoch [1/30], Batch [1600/6000], Loss: 0.9797
Epoch [1/30], Batch [1700/6000], Loss: 0.1308
Epoch [1/30], Batch [1800/6000], Loss: 0.0716
Epoch [1/30], Batch [1900/6000], Loss: 0.0606
Epoch [1/30], Batch [2000/6000], Loss: 0.5847
Epoch [1/30], Batch [2100/6000], Loss: 0.2891
Epoch [1/30], Batch [2200/6000], Loss: 0.2029
Epoch [1/30], Batch [2300/6000], Loss: 0.1611
Epoch [1/30], Batch [2400/6000], Loss: 0.2641
Epoch [1/30], Batch [2500/6000], Loss: 1.3916
Epoch [1/30], Batch [2600/6000], Loss: 0.3383
Epoch [1/30], Batch [2700/6000], Loss: 0.1423
Epoch [1/30], Batch [2800/6000], Loss: 0.1008
Epoch [1/30], Batch [2900/6000], Loss: 0.4721
Epoch [1/30], Batch [3000/6000], Loss: 0.3096
Epoch [1/30], Batch [3100/6000], Loss: 0.1054
Epoch [1/30], Batch [3200/6000], Loss: 0.0913
Epoch [1/30], Batch [3300/6000], Loss: 0.4232
Epoch [1/30], Batch [3400/6000], Loss: 0.7366
Epoch [1/30], Batch [3500/6000], Loss: 0.5564
Epoch [1/30], Batch [3600/6000], Loss: 0.0892
Epoch [1/30], Batch [3700/6000], Loss: 0.1059
Epoch [1/30], Batch [3800/6000], Loss: 0.1192
Epoch [1/30], Batch [3900/6000], Loss: 0.2312
Epoch [1/30], Batch [4000/6000], Loss: 0.0982
Epoch [1/30], Batch [4100/6000], Loss: 0.1438
Epoch [1/30], Batch [4200/6000], Loss: 0.4681
Epoch [1/30], Batch [4300/6000], Loss: 0.3464
Epoch [1/30], Batch [4400/6000], Loss: 0.4685
Epoch [1/30], Batch [4500/6000], Loss: 0.5846
Epoch [1/30], Batch [4600/6000], Loss: 0.0625
Epoch [1/30], Batch [4700/6000], Loss: 1.0273
Epoch [1/30], Batch [4800/6000], Loss: 0.2999
Epoch [1/30], Batch [4900/6000], Loss: 0.4258
Epoch [1/30], Batch [5000/6000], Loss: 0.1629
Epoch [1/30], Batch [5100/6000], Loss: 0.0643
Epoch [1/30], Batch [5200/6000], Loss: 0.1396
Epoch [1/30], Batch [5300/6000], Loss: 0.1536
Epoch [1/30], Batch [5400/6000], Loss: 0.1389
Epoch [1/30], Batch [5500/6000], Loss: 0.4004
Epoch [1/30], Batch [5600/6000], Loss: 0.0803
Epoch [1/30], Batch [5700/6000], Loss: 0.2129
Epoch [1/30], Batch [5800/6000], Loss: 0.4827
Epoch [1/30], Batch [5900/6000], Loss: 0.2205
Epoch [1/30], Loss: 0.3959
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.4619
Epoch [2/30], Batch [100/6000], Loss: 0.5642
Epoch [2/30], Batch [200/6000], Loss: 0.3370
Epoch [2/30], Batch [300/6000], Loss: 0.1384
Epoch [2/30], Batch [400/6000], Loss: 0.1230
Epoch [2/30], Batch [500/6000], Loss: 0.3297
Epoch [2/30], Batch [600/6000], Loss: 0.2096
Epoch [2/30], Batch [700/6000], Loss: 0.5649
Epoch [2/30], Batch [800/6000], Loss: 0.0818
Epoch [2/30], Batch [900/6000], Loss: 0.1149
Epoch [2/30], Batch [1000/6000], Loss: 0.1213
Epoch [2/30], Batch [1100/6000], Loss: 0.1352
Epoch [2/30], Batch [1200/6000], Loss: 0.4657
Epoch [2/30], Batch [1300/6000], Loss: 0.2941
Epoch [2/30], Batch [1400/6000], Loss: 0.0668
Epoch [2/30], Batch [1500/6000], Loss: 0.3509
Epoch [2/30], Batch [1600/6000], Loss: 0.1412
Epoch [2/30], Batch [1700/6000], Loss: 0.2409
Epoch [2/30], Batch [1800/6000], Loss: 0.2819
Epoch [2/30], Batch [1900/6000], Loss: 0.0895
Epoch [2/30], Batch [2000/6000], Loss: 0.1287
Epoch [2/30], Batch [2100/6000], Loss: 0.0977
Epoch [2/30], Batch [2200/6000], Loss: 0.4618
Epoch [2/30], Batch [2300/6000], Loss: 0.2243
Epoch [2/30], Batch [2400/6000], Loss: 0.1547
Epoch [2/30], Batch [2500/6000], Loss: 0.2149
Epoch [2/30], Batch [2600/6000], Loss: 0.2296
Epoch [2/30], Batch [2700/6000], Loss: 0.3425
Epoch [2/30], Batch [2800/6000], Loss: 0.1796
Epoch [2/30], Batch [2900/6000], Loss: 0.0442
Epoch [2/30], Batch [3000/6000], Loss: 0.0627
Epoch [2/30], Batch [3100/6000], Loss: 0.3376
Epoch [2/30], Batch [3200/6000], Loss: 0.1477
Epoch [2/30], Batch [3300/6000], Loss: 0.0402
Epoch [2/30], Batch [3400/6000], Loss: 0.3337
Epoch [2/30], Batch [3500/6000], Loss: 1.1806
Epoch [2/30], Batch [3600/6000], Loss: 0.0719
Epoch [2/30], Batch [3700/6000], Loss: 0.0800
Epoch [2/30], Batch [3800/6000], Loss: 0.5510
Epoch [2/30], Batch [3900/6000], Loss: 0.3391
Epoch [2/30], Batch [4000/6000], Loss: 0.0599
Epoch [2/30], Batch [4100/6000], Loss: 0.1491
Epoch [2/30], Batch [4200/6000], Loss: 0.0857
Epoch [2/30], Batch [4300/6000], Loss: 0.6851
Epoch [2/30], Batch [4400/6000], Loss: 0.0755
Epoch [2/30], Batch [4500/6000], Loss: 0.0469
Epoch [2/30], Batch [4600/6000], Loss: 0.1038
Epoch [2/30], Batch [4700/6000], Loss: 0.0446
Epoch [2/30], Batch [4800/6000], Loss: 0.0898
Epoch [2/30], Batch [4900/6000], Loss: 0.1207
Epoch [2/30], Batch [5000/6000], Loss: 0.2228
Epoch [2/30], Batch [5100/6000], Loss: 0.0430
Epoch [2/30], Batch [5200/6000], Loss: 0.0684
Epoch [2/30], Batch [5300/6000], Loss: 0.0537
Epoch [2/30], Batch [5400/6000], Loss: 0.3021
Epoch [2/30], Batch [5500/6000], Loss: 0.0783
Epoch [2/30], Batch [5600/6000], Loss: 0.5539
Epoch [2/30], Batch [5700/6000], Loss: 0.0838
Epoch [2/30], Batch [5800/6000], Loss: 0.4670
Epoch [2/30], Batch [5900/6000], Loss: 0.0680
Epoch [2/30], Loss: 0.2225
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.0926
Epoch [3/30], Batch [100/6000], Loss: 0.1202
Epoch [3/30], Batch [200/6000], Loss: 0.1512
Epoch [3/30], Batch [300/6000], Loss: 0.0391
Epoch [3/30], Batch [400/6000], Loss: 0.1827
Epoch [3/30], Batch [500/6000], Loss: 0.8593
Epoch [3/30], Batch [600/6000], Loss: 0.0392
Epoch [3/30], Batch [700/6000], Loss: 0.3933
Epoch [3/30], Batch [800/6000], Loss: 0.0337
Epoch [3/30], Batch [900/6000], Loss: 0.0831
Epoch [3/30], Batch [1000/6000], Loss: 0.2345
Epoch [3/30], Batch [1100/6000], Loss: 0.1306
Epoch [3/30], Batch [1200/6000], Loss: 0.7104
Epoch [3/30], Batch [1300/6000], Loss: 0.0499
Epoch [3/30], Batch [1400/6000], Loss: 0.2313
Epoch [3/30], Batch [1500/6000], Loss: 0.1808
Epoch [3/30], Batch [1600/6000], Loss: 0.0866
Epoch [3/30], Batch [1700/6000], Loss: 0.1043
Epoch [3/30], Batch [1800/6000], Loss: 0.1705
Epoch [3/30], Batch [1900/6000], Loss: 0.1317
Epoch [3/30], Batch [2000/6000], Loss: 0.3519
Epoch [3/30], Batch [2100/6000], Loss: 0.0443
Epoch [3/30], Batch [2200/6000], Loss: 0.1308
Epoch [3/30], Batch [2300/6000], Loss: 0.0430
Epoch [3/30], Batch [2400/6000], Loss: 0.1135
Epoch [3/30], Batch [2500/6000], Loss: 0.1930
Epoch [3/30], Batch [2600/6000], Loss: 0.1020
Epoch [3/30], Batch [2700/6000], Loss: 0.0475
Epoch [3/30], Batch [2800/6000], Loss: 0.0423
Epoch [3/30], Batch [2900/6000], Loss: 0.2298
Epoch [3/30], Batch [3000/6000], Loss: 0.0401
Epoch [3/30], Batch [3100/6000], Loss: 0.0968
Epoch [3/30], Batch [3200/6000], Loss: 0.0357
Epoch [3/30], Batch [3300/6000], Loss: 0.0372
Epoch [3/30], Batch [3400/6000], Loss: 0.1001
Epoch [3/30], Batch [3500/6000], Loss: 0.0443
Epoch [3/30], Batch [3600/6000], Loss: 0.3605
Epoch [3/30], Batch [3700/6000], Loss: 0.0898
Epoch [3/30], Batch [3800/6000], Loss: 0.0368
Epoch [3/30], Batch [3900/6000], Loss: 0.0776
Epoch [3/30], Batch [4000/6000], Loss: 0.0400
Epoch [3/30], Batch [4100/6000], Loss: 0.0491
Epoch [3/30], Batch [4200/6000], Loss: 0.4461
Epoch [3/30], Batch [4300/6000], Loss: 0.2874
Epoch [3/30], Batch [4400/6000], Loss: 0.1116
Epoch [3/30], Batch [4500/6000], Loss: 0.4845
Epoch [3/30], Batch [4600/6000], Loss: 0.0279
Epoch [3/30], Batch [4700/6000], Loss: 0.0370
Epoch [3/30], Batch [4800/6000], Loss: 0.2363
Epoch [3/30], Batch [4900/6000], Loss: 0.0412
Epoch [3/30], Batch [5000/6000], Loss: 0.0617
Epoch [3/30], Batch [5100/6000], Loss: 0.1255
Epoch [3/30], Batch [5200/6000], Loss: 0.0596
Epoch [3/30], Batch [5300/6000], Loss: 0.0404
Epoch [3/30], Batch [5400/6000], Loss: 0.0428
Epoch [3/30], Batch [5500/6000], Loss: 0.0333
Epoch [3/30], Batch [5600/6000], Loss: 0.1121
Epoch [3/30], Batch [5700/6000], Loss: 0.1628
Epoch [3/30], Batch [5800/6000], Loss: 0.0889
Epoch [3/30], Batch [5900/6000], Loss: 0.1173
Epoch [3/30], Loss: 0.1691
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.0329
Epoch [4/30], Batch [100/6000], Loss: 0.1360
Epoch [4/30], Batch [200/6000], Loss: 0.2170
Epoch [4/30], Batch [300/6000], Loss: 0.0782
Epoch [4/30], Batch [400/6000], Loss: 0.1906
Epoch [4/30], Batch [500/6000], Loss: 0.0395
Epoch [4/30], Batch [600/6000], Loss: 0.9893
Epoch [4/30], Batch [700/6000], Loss: 0.3800
Epoch [4/30], Batch [800/6000], Loss: 0.0389
Epoch [4/30], Batch [900/6000], Loss: 0.0396
Epoch [4/30], Batch [1000/6000], Loss: 0.5727
Epoch [4/30], Batch [1100/6000], Loss: 0.1688
Epoch [4/30], Batch [1200/6000], Loss: 0.2642
Epoch [4/30], Batch [1300/6000], Loss: 0.0370
Epoch [4/30], Batch [1400/6000], Loss: 0.4919
Epoch [4/30], Batch [1500/6000], Loss: 0.1071
Epoch [4/30], Batch [1600/6000], Loss: 0.2632
Epoch [4/30], Batch [1700/6000], Loss: 0.0491
Epoch [4/30], Batch [1800/6000], Loss: 0.0289
Epoch [4/30], Batch [1900/6000], Loss: 0.0427
Epoch [4/30], Batch [2000/6000], Loss: 0.0911
Epoch [4/30], Batch [2100/6000], Loss: 0.0728
Epoch [4/30], Batch [2200/6000], Loss: 0.0613
Epoch [4/30], Batch [2300/6000], Loss: 0.0417
Epoch [4/30], Batch [2400/6000], Loss: 0.0418
Epoch [4/30], Batch [2500/6000], Loss: 0.1162
Epoch [4/30], Batch [2600/6000], Loss: 0.1933
Epoch [4/30], Batch [2700/6000], Loss: 0.0361
Epoch [4/30], Batch [2800/6000], Loss: 0.0303
Epoch [4/30], Batch [2900/6000], Loss: 0.0346
Epoch [4/30], Batch [3000/6000], Loss: 0.0339
Epoch [4/30], Batch [3100/6000], Loss: 0.0514
Epoch [4/30], Batch [3200/6000], Loss: 0.0271
Epoch [4/30], Batch [3300/6000], Loss: 0.3132
Epoch [4/30], Batch [3400/6000], Loss: 0.0322
Epoch [4/30], Batch [3500/6000], Loss: 0.3813
Epoch [4/30], Batch [3600/6000], Loss: 0.0325
Epoch [4/30], Batch [3700/6000], Loss: 0.0397
Epoch [4/30], Batch [3800/6000], Loss: 0.0424
Epoch [4/30], Batch [3900/6000], Loss: 0.0435
Epoch [4/30], Batch [4000/6000], Loss: 0.2197
Epoch [4/30], Batch [4100/6000], Loss: 0.5663
Epoch [4/30], Batch [4200/6000], Loss: 0.0384
Epoch [4/30], Batch [4300/6000], Loss: 0.0369
Epoch [4/30], Batch [4400/6000], Loss: 0.0431
Epoch [4/30], Batch [4500/6000], Loss: 0.0356
Epoch [4/30], Batch [4600/6000], Loss: 0.1365
Epoch [4/30], Batch [4700/6000], Loss: 0.0367
Epoch [4/30], Batch [4800/6000], Loss: 0.0780
Epoch [4/30], Batch [4900/6000], Loss: 0.1169
Epoch [4/30], Batch [5000/6000], Loss: 0.0389
Epoch [4/30], Batch [5100/6000], Loss: 0.1078
Epoch [4/30], Batch [5200/6000], Loss: 0.0421
Epoch [4/30], Batch [5300/6000], Loss: 0.0338
Epoch [4/30], Batch [5400/6000], Loss: 0.0342
Epoch [4/30], Batch [5500/6000], Loss: 0.0511
Epoch [4/30], Batch [5600/6000], Loss: 0.0408
Epoch [4/30], Batch [5700/6000], Loss: 0.0457
Epoch [4/30], Batch [5800/6000], Loss: 0.1469
Epoch [4/30], Batch [5900/6000], Loss: 0.1753
Epoch [4/30], Loss: 0.1392
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.0312
Epoch [5/30], Batch [100/6000], Loss: 0.0331
Epoch [5/30], Batch [200/6000], Loss: 0.1093
Epoch [5/30], Batch [300/6000], Loss: 0.0494
Epoch [5/30], Batch [400/6000], Loss: 0.0822
Epoch [5/30], Batch [500/6000], Loss: 0.2333
Epoch [5/30], Batch [600/6000], Loss: 0.0438
Epoch [5/30], Batch [700/6000], Loss: 0.2693
Epoch [5/30], Batch [800/6000], Loss: 0.0393
Epoch [5/30], Batch [900/6000], Loss: 0.0855
Epoch [5/30], Batch [1000/6000], Loss: 0.1545
Epoch [5/30], Batch [1100/6000], Loss: 0.0351
Epoch [5/30], Batch [1200/6000], Loss: 0.0550
Epoch [5/30], Batch [1300/6000], Loss: 0.0709
Epoch [5/30], Batch [1400/6000], Loss: 0.0542
Epoch [5/30], Batch [1500/6000], Loss: 0.0311
Epoch [5/30], Batch [1600/6000], Loss: 0.9839
Epoch [5/30], Batch [1700/6000], Loss: 0.1014
Epoch [5/30], Batch [1800/6000], Loss: 0.0272
Epoch [5/30], Batch [1900/6000], Loss: 0.0330
Epoch [5/30], Batch [2000/6000], Loss: 0.0320
Epoch [5/30], Batch [2100/6000], Loss: 0.0424
Epoch [5/30], Batch [2200/6000], Loss: 0.0370
Epoch [5/30], Batch [2300/6000], Loss: 0.1018
Epoch [5/30], Batch [2400/6000], Loss: 0.1014
Epoch [5/30], Batch [2500/6000], Loss: 0.1367
Epoch [5/30], Batch [2600/6000], Loss: 0.1102
Epoch [5/30], Batch [2700/6000], Loss: 0.0951
Epoch [5/30], Batch [2800/6000], Loss: 0.0335
Epoch [5/30], Batch [2900/6000], Loss: 0.0381
Epoch [5/30], Batch [3000/6000], Loss: 1.0234
Epoch [5/30], Batch [3100/6000], Loss: 0.0277
Epoch [5/30], Batch [3200/6000], Loss: 0.0263
Epoch [5/30], Batch [3300/6000], Loss: 0.0297
Epoch [5/30], Batch [3400/6000], Loss: 0.0364
Epoch [5/30], Batch [3500/6000], Loss: 0.0383
Epoch [5/30], Batch [3600/6000], Loss: 0.3090
Epoch [5/30], Batch [3700/6000], Loss: 0.0262
Epoch [5/30], Batch [3800/6000], Loss: 0.0944
Epoch [5/30], Batch [3900/6000], Loss: 0.0751
Epoch [5/30], Batch [4000/6000], Loss: 0.1304
Epoch [5/30], Batch [4100/6000], Loss: 0.0414
Epoch [5/30], Batch [4200/6000], Loss: 0.0486
Epoch [5/30], Batch [4300/6000], Loss: 0.0331
Epoch [5/30], Batch [4400/6000], Loss: 0.0427
Epoch [5/30], Batch [4500/6000], Loss: 0.1416
Epoch [5/30], Batch [4600/6000], Loss: 0.0436
Epoch [5/30], Batch [4700/6000], Loss: 0.1318
Epoch [5/30], Batch [4800/6000], Loss: 0.1209
Epoch [5/30], Batch [4900/6000], Loss: 0.0323
Epoch [5/30], Batch [5000/6000], Loss: 0.0306
Epoch [5/30], Batch [5100/6000], Loss: 0.0528
Epoch [5/30], Batch [5200/6000], Loss: 0.0753
Epoch [5/30], Batch [5300/6000], Loss: 0.1251
Epoch [5/30], Batch [5400/6000], Loss: 0.1809
Epoch [5/30], Batch [5500/6000], Loss: 0.0330
Epoch [5/30], Batch [5600/6000], Loss: 0.2179
Epoch [5/30], Batch [5700/6000], Loss: 0.0243
Epoch [5/30], Batch [5800/6000], Loss: 0.0800
Epoch [5/30], Batch [5900/6000], Loss: 0.1343
Epoch [5/30], Loss: 0.1193
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.0301
Epoch [6/30], Batch [100/6000], Loss: 0.0448
Epoch [6/30], Batch [200/6000], Loss: 0.0350
Epoch [6/30], Batch [300/6000], Loss: 0.0466
Epoch [6/30], Batch [400/6000], Loss: 0.0412
Epoch [6/30], Batch [500/6000], Loss: 0.0310
Epoch [6/30], Batch [600/6000], Loss: 0.1071
Epoch [6/30], Batch [700/6000], Loss: 0.0320
Epoch [6/30], Batch [800/6000], Loss: 0.0268
Epoch [6/30], Batch [900/6000], Loss: 0.0392
Epoch [6/30], Batch [1000/6000], Loss: 0.2773
Epoch [6/30], Batch [1100/6000], Loss: 0.0295
Epoch [6/30], Batch [1200/6000], Loss: 0.0449
Epoch [6/30], Batch [1300/6000], Loss: 0.0252
Epoch [6/30], Batch [1400/6000], Loss: 0.1449
Epoch [6/30], Batch [1500/6000], Loss: 0.4436
Epoch [6/30], Batch [1600/6000], Loss: 0.0236
Epoch [6/30], Batch [1700/6000], Loss: 0.2355
Epoch [6/30], Batch [1800/6000], Loss: 0.3327
Epoch [6/30], Batch [1900/6000], Loss: 0.0579
Epoch [6/30], Batch [2000/6000], Loss: 0.0327
Epoch [6/30], Batch [2100/6000], Loss: 0.2947
Epoch [6/30], Batch [2200/6000], Loss: 0.0380
Epoch [6/30], Batch [2300/6000], Loss: 0.0794
Epoch [6/30], Batch [2400/6000], Loss: 0.0575
Epoch [6/30], Batch [2500/6000], Loss: 0.0324
Epoch [6/30], Batch [2600/6000], Loss: 0.0317
Epoch [6/30], Batch [2700/6000], Loss: 0.2524
Epoch [6/30], Batch [2800/6000], Loss: 0.0284
Epoch [6/30], Batch [2900/6000], Loss: 0.6042
Epoch [6/30], Batch [3000/6000], Loss: 0.0371
Epoch [6/30], Batch [3100/6000], Loss: 0.0520
Epoch [6/30], Batch [3200/6000], Loss: 0.0755
Epoch [6/30], Batch [3300/6000], Loss: 0.0262
Epoch [6/30], Batch [3400/6000], Loss: 0.0248
Epoch [6/30], Batch [3500/6000], Loss: 0.0884
Epoch [6/30], Batch [3600/6000], Loss: 0.0315
Epoch [6/30], Batch [3700/6000], Loss: 0.0802
Epoch [6/30], Batch [3800/6000], Loss: 0.0200
Epoch [6/30], Batch [3900/6000], Loss: 0.0862
Epoch [6/30], Batch [4000/6000], Loss: 0.0609
Epoch [6/30], Batch [4100/6000], Loss: 0.1602
Epoch [6/30], Batch [4200/6000], Loss: 0.4858
Epoch [6/30], Batch [4300/6000], Loss: 0.0194
Epoch [6/30], Batch [4400/6000], Loss: 0.0792
Epoch [6/30], Batch [4500/6000], Loss: 0.0320
Epoch [6/30], Batch [4600/6000], Loss: 0.3091
Epoch [6/30], Batch [4700/6000], Loss: 0.1992
Epoch [6/30], Batch [4800/6000], Loss: 0.2341
Epoch [6/30], Batch [4900/6000], Loss: 0.0347
Epoch [6/30], Batch [5000/6000], Loss: 0.0687
Epoch [6/30], Batch [5100/6000], Loss: 0.0516
Epoch [6/30], Batch [5200/6000], Loss: 0.2812
Epoch [6/30], Batch [5300/6000], Loss: 0.0798
Epoch [6/30], Batch [5400/6000], Loss: 0.0256
Epoch [6/30], Batch [5500/6000], Loss: 0.0309
Epoch [6/30], Batch [5600/6000], Loss: 0.0500
Epoch [6/30], Batch [5700/6000], Loss: 0.0300
Epoch [6/30], Batch [5800/6000], Loss: 0.1773
Epoch [6/30], Batch [5900/6000], Loss: 0.0332
Epoch [6/30], Loss: 0.1051
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.4612
Epoch [7/30], Batch [100/6000], Loss: 0.0331
Epoch [7/30], Batch [200/6000], Loss: 0.1618
Epoch [7/30], Batch [300/6000], Loss: 0.1889
Epoch [7/30], Batch [400/6000], Loss: 0.2983
Epoch [7/30], Batch [500/6000], Loss: 0.0498
Epoch [7/30], Batch [600/6000], Loss: 0.0768
Epoch [7/30], Batch [700/6000], Loss: 0.0291
Epoch [7/30], Batch [800/6000], Loss: 0.0277
Epoch [7/30], Batch [900/6000], Loss: 0.0444
Epoch [7/30], Batch [1000/6000], Loss: 0.0329
Epoch [7/30], Batch [1100/6000], Loss: 0.0306
Epoch [7/30], Batch [1200/6000], Loss: 0.0797
Epoch [7/30], Batch [1300/6000], Loss: 0.0730
Epoch [7/30], Batch [1400/6000], Loss: 0.0251
Epoch [7/30], Batch [1500/6000], Loss: 0.0306
Epoch [7/30], Batch [1600/6000], Loss: 0.0301
Epoch [7/30], Batch [1700/6000], Loss: 0.0503
Epoch [7/30], Batch [1800/6000], Loss: 0.0243
Epoch [7/30], Batch [1900/6000], Loss: 0.0436
Epoch [7/30], Batch [2000/6000], Loss: 0.0210
Epoch [7/30], Batch [2100/6000], Loss: 0.0306
Epoch [7/30], Batch [2200/6000], Loss: 0.4466
Epoch [7/30], Batch [2300/6000], Loss: 0.0312
Epoch [7/30], Batch [2400/6000], Loss: 0.8673
Epoch [7/30], Batch [2500/6000], Loss: 0.0341
Epoch [7/30], Batch [2600/6000], Loss: 0.0664
Epoch [7/30], Batch [2700/6000], Loss: 0.0304
Epoch [7/30], Batch [2800/6000], Loss: 0.0892
Epoch [7/30], Batch [2900/6000], Loss: 0.0333
Epoch [7/30], Batch [3000/6000], Loss: 0.0635
Epoch [7/30], Batch [3100/6000], Loss: 0.0525
Epoch [7/30], Batch [3200/6000], Loss: 0.0324
Epoch [7/30], Batch [3300/6000], Loss: 0.0369
Epoch [7/30], Batch [3400/6000], Loss: 0.0292
Epoch [7/30], Batch [3500/6000], Loss: 0.1186
Epoch [7/30], Batch [3600/6000], Loss: 0.0763
Epoch [7/30], Batch [3700/6000], Loss: 0.1611
Epoch [7/30], Batch [3800/6000], Loss: 0.0521
Epoch [7/30], Batch [3900/6000], Loss: 0.0356
Epoch [7/30], Batch [4000/6000], Loss: 0.4142
Epoch [7/30], Batch [4100/6000], Loss: 0.0316
Epoch [7/30], Batch [4200/6000], Loss: 0.0392
Epoch [7/30], Batch [4300/6000], Loss: 0.8484
Epoch [7/30], Batch [4400/6000], Loss: 0.5319
Epoch [7/30], Batch [4500/6000], Loss: 0.0566
Epoch [7/30], Batch [4600/6000], Loss: 0.0276
Epoch [7/30], Batch [4700/6000], Loss: 0.0256
Epoch [7/30], Batch [4800/6000], Loss: 0.0281
Epoch [7/30], Batch [4900/6000], Loss: 0.0358
Epoch [7/30], Batch [5000/6000], Loss: 0.2005
Epoch [7/30], Batch [5100/6000], Loss: 0.0334
Epoch [7/30], Batch [5200/6000], Loss: 0.2003
Epoch [7/30], Batch [5300/6000], Loss: 0.0283
Epoch [7/30], Batch [5400/6000], Loss: 0.0758
Epoch [7/30], Batch [5500/6000], Loss: 0.5528
Epoch [7/30], Batch [5600/6000], Loss: 0.0364
Epoch [7/30], Batch [5700/6000], Loss: 0.0323
Epoch [7/30], Batch [5800/6000], Loss: 0.0886
Epoch [7/30], Batch [5900/6000], Loss: 0.0326
Epoch [7/30], Loss: 0.0936
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.0932
Epoch [8/30], Batch [100/6000], Loss: 0.1355
Epoch [8/30], Batch [200/6000], Loss: 0.1869
Epoch [8/30], Batch [300/6000], Loss: 0.1309
Epoch [8/30], Batch [400/6000], Loss: 0.0492
Epoch [8/30], Batch [500/6000], Loss: 0.3660
Epoch [8/30], Batch [600/6000], Loss: 0.0296
Epoch [8/30], Batch [700/6000], Loss: 0.0283
Epoch [8/30], Batch [800/6000], Loss: 0.0331
Epoch [8/30], Batch [900/6000], Loss: 0.2352
Epoch [8/30], Batch [1000/6000], Loss: 0.2203
Epoch [8/30], Batch [1100/6000], Loss: 0.1355
Epoch [8/30], Batch [1200/6000], Loss: 0.1071
Epoch [8/30], Batch [1300/6000], Loss: 0.0347
Epoch [8/30], Batch [1400/6000], Loss: 0.1140
Epoch [8/30], Batch [1500/6000], Loss: 0.0230
Epoch [8/30], Batch [1600/6000], Loss: 0.0440
Epoch [8/30], Batch [1700/6000], Loss: 0.2865
Epoch [8/30], Batch [1800/6000], Loss: 0.0471
Epoch [8/30], Batch [1900/6000], Loss: 0.0489
Epoch [8/30], Batch [2000/6000], Loss: 0.0206
Epoch [8/30], Batch [2100/6000], Loss: 0.0231
Epoch [8/30], Batch [2200/6000], Loss: 0.0850
Epoch [8/30], Batch [2300/6000], Loss: 0.0361
Epoch [8/30], Batch [2400/6000], Loss: 0.0267
Epoch [8/30], Batch [2500/6000], Loss: 0.1482
Epoch [8/30], Batch [2600/6000], Loss: 0.6164
Epoch [8/30], Batch [2700/6000], Loss: 0.0265
Epoch [8/30], Batch [2800/6000], Loss: 0.0314
Epoch [8/30], Batch [2900/6000], Loss: 0.0317
Epoch [8/30], Batch [3000/6000], Loss: 0.0357
Epoch [8/30], Batch [3100/6000], Loss: 0.1662
Epoch [8/30], Batch [3200/6000], Loss: 0.3541
Epoch [8/30], Batch [3300/6000], Loss: 0.0872
Epoch [8/30], Batch [3400/6000], Loss: 0.0209
Epoch [8/30], Batch [3500/6000], Loss: 0.0356
Epoch [8/30], Batch [3600/6000], Loss: 0.0233
Epoch [8/30], Batch [3700/6000], Loss: 0.0281
Epoch [8/30], Batch [3800/6000], Loss: 0.0420
Epoch [8/30], Batch [3900/6000], Loss: 0.0281
Epoch [8/30], Batch [4000/6000], Loss: 0.0336
Epoch [8/30], Batch [4100/6000], Loss: 0.0262
Epoch [8/30], Batch [4200/6000], Loss: 0.0263
Epoch [8/30], Batch [4300/6000], Loss: 0.1515
Epoch [8/30], Batch [4400/6000], Loss: 0.0281
Epoch [8/30], Batch [4500/6000], Loss: 0.0644
Epoch [8/30], Batch [4600/6000], Loss: 0.0332
Epoch [8/30], Batch [4700/6000], Loss: 0.0230
Epoch [8/30], Batch [4800/6000], Loss: 0.0580
Epoch [8/30], Batch [4900/6000], Loss: 0.0627
Epoch [8/30], Batch [5000/6000], Loss: 0.0221
Epoch [8/30], Batch [5100/6000], Loss: 0.0294
Epoch [8/30], Batch [5200/6000], Loss: 0.0374
Epoch [8/30], Batch [5300/6000], Loss: 0.0362
Epoch [8/30], Batch [5400/6000], Loss: 0.0287
Epoch [8/30], Batch [5500/6000], Loss: 0.0268
Epoch [8/30], Batch [5600/6000], Loss: 0.0267
Epoch [8/30], Batch [5700/6000], Loss: 0.3291
Epoch [8/30], Batch [5800/6000], Loss: 0.0308
Epoch [8/30], Batch [5900/6000], Loss: 0.0302
Epoch [8/30], Loss: 0.0822
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.0274
Epoch [9/30], Batch [100/6000], Loss: 0.0285
Epoch [9/30], Batch [200/6000], Loss: 0.0295
Epoch [9/30], Batch [300/6000], Loss: 0.1648
Epoch [9/30], Batch [400/6000], Loss: 0.0372
Epoch [9/30], Batch [500/6000], Loss: 0.0226
Epoch [9/30], Batch [600/6000], Loss: 0.0350
Epoch [9/30], Batch [700/6000], Loss: 0.1244
Epoch [9/30], Batch [800/6000], Loss: 0.1536
Epoch [9/30], Batch [900/6000], Loss: 0.0309
Epoch [9/30], Batch [1000/6000], Loss: 0.0207
Epoch [9/30], Batch [1100/6000], Loss: 0.0301
Epoch [9/30], Batch [1200/6000], Loss: 0.0300
Epoch [9/30], Batch [1300/6000], Loss: 0.0722
Epoch [9/30], Batch [1400/6000], Loss: 0.0260
Epoch [9/30], Batch [1500/6000], Loss: 0.1005
Epoch [9/30], Batch [1600/6000], Loss: 0.1002
Epoch [9/30], Batch [1700/6000], Loss: 0.0234
Epoch [9/30], Batch [1800/6000], Loss: 0.0673
Epoch [9/30], Batch [1900/6000], Loss: 0.0311
Epoch [9/30], Batch [2000/6000], Loss: 0.0365
Epoch [9/30], Batch [2100/6000], Loss: 0.0281
Epoch [9/30], Batch [2200/6000], Loss: 0.0999
Epoch [9/30], Batch [2300/6000], Loss: 0.0219
Epoch [9/30], Batch [2400/6000], Loss: 0.0223
Epoch [9/30], Batch [2500/6000], Loss: 0.0482
Epoch [9/30], Batch [2600/6000], Loss: 0.0205
Epoch [9/30], Batch [2700/6000], Loss: 0.0479
Epoch [9/30], Batch [2800/6000], Loss: 0.0285
Epoch [9/30], Batch [2900/6000], Loss: 0.1041
Epoch [9/30], Batch [3000/6000], Loss: 0.0400
Epoch [9/30], Batch [3100/6000], Loss: 0.0465
Epoch [9/30], Batch [3200/6000], Loss: 0.0260
Epoch [9/30], Batch [3300/6000], Loss: 0.3756
Epoch [9/30], Batch [3400/6000], Loss: 0.0276
Epoch [9/30], Batch [3500/6000], Loss: 0.0857
Epoch [9/30], Batch [3600/6000], Loss: 0.2364
Epoch [9/30], Batch [3700/6000], Loss: 0.0565
Epoch [9/30], Batch [3800/6000], Loss: 0.1730
Epoch [9/30], Batch [3900/6000], Loss: 0.0268
Epoch [9/30], Batch [4000/6000], Loss: 0.0605
Epoch [9/30], Batch [4100/6000], Loss: 0.2803
Epoch [9/30], Batch [4200/6000], Loss: 0.0291
Epoch [9/30], Batch [4300/6000], Loss: 0.0283
Epoch [9/30], Batch [4400/6000], Loss: 0.0230
Epoch [9/30], Batch [4500/6000], Loss: 0.0301
Epoch [9/30], Batch [4600/6000], Loss: 0.0309
Epoch [9/30], Batch [4700/6000], Loss: 0.0237
Epoch [9/30], Batch [4800/6000], Loss: 0.0325
Epoch [9/30], Batch [4900/6000], Loss: 0.0316
Epoch [9/30], Batch [5000/6000], Loss: 0.0235
Epoch [9/30], Batch [5100/6000], Loss: 0.0376
Epoch [9/30], Batch [5200/6000], Loss: 0.1381
Epoch [9/30], Batch [5300/6000], Loss: 0.0239
Epoch [9/30], Batch [5400/6000], Loss: 0.0976
Epoch [9/30], Batch [5500/6000], Loss: 0.0220
Epoch [9/30], Batch [5600/6000], Loss: 0.0358
Epoch [9/30], Batch [5700/6000], Loss: 0.0262
Epoch [9/30], Batch [5800/6000], Loss: 0.0281
Epoch [9/30], Batch [5900/6000], Loss: 0.0241
Epoch [9/30], Loss: 0.0752
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.0232
Epoch [10/30], Batch [100/6000], Loss: 0.0209
Epoch [10/30], Batch [200/6000], Loss: 0.0210
Epoch [10/30], Batch [300/6000], Loss: 0.0251
Epoch [10/30], Batch [400/6000], Loss: 0.0468
Epoch [10/30], Batch [500/6000], Loss: 0.0232
Epoch [10/30], Batch [600/6000], Loss: 0.0239
Epoch [10/30], Batch [700/6000], Loss: 0.0222
Epoch [10/30], Batch [800/6000], Loss: 0.0306
Epoch [10/30], Batch [900/6000], Loss: 0.0869
Epoch [10/30], Batch [1000/6000], Loss: 0.0260
Epoch [10/30], Batch [1100/6000], Loss: 0.1774
Epoch [10/30], Batch [1200/6000], Loss: 0.1290
Epoch [10/30], Batch [1300/6000], Loss: 0.0276
Epoch [10/30], Batch [1400/6000], Loss: 0.0386
Epoch [10/30], Batch [1500/6000], Loss: 0.0414
Epoch [10/30], Batch [1600/6000], Loss: 0.0271
Epoch [10/30], Batch [1700/6000], Loss: 0.0295
Epoch [10/30], Batch [1800/6000], Loss: 0.0344
Epoch [10/30], Batch [1900/6000], Loss: 0.0385
Epoch [10/30], Batch [2000/6000], Loss: 0.0274
Epoch [10/30], Batch [2100/6000], Loss: 0.0788
Epoch [10/30], Batch [2200/6000], Loss: 0.0257
Epoch [10/30], Batch [2300/6000], Loss: 0.0336
Epoch [10/30], Batch [2400/6000], Loss: 1.4447
Epoch [10/30], Batch [2500/6000], Loss: 0.0217
Epoch [10/30], Batch [2600/6000], Loss: 0.0258
Epoch [10/30], Batch [2700/6000], Loss: 0.0425
Epoch [10/30], Batch [2800/6000], Loss: 0.0255
Epoch [10/30], Batch [2900/6000], Loss: 0.0370
Epoch [10/30], Batch [3000/6000], Loss: 0.0287
Epoch [10/30], Batch [3100/6000], Loss: 0.0221
Epoch [10/30], Batch [3200/6000], Loss: 0.0262
Epoch [10/30], Batch [3300/6000], Loss: 0.0555
Epoch [10/30], Batch [3400/6000], Loss: 0.5846
Epoch [10/30], Batch [3500/6000], Loss: 0.0189
Epoch [10/30], Batch [3600/6000], Loss: 0.0697
Epoch [10/30], Batch [3700/6000], Loss: 0.0271
Epoch [10/30], Batch [3800/6000], Loss: 0.0280
Epoch [10/30], Batch [3900/6000], Loss: 0.3959
Epoch [10/30], Batch [4000/6000], Loss: 0.0234
Epoch [10/30], Batch [4100/6000], Loss: 0.0238
Epoch [10/30], Batch [4200/6000], Loss: 0.1180
Epoch [10/30], Batch [4300/6000], Loss: 0.0248
Epoch [10/30], Batch [4400/6000], Loss: 0.0220
Epoch [10/30], Batch [4500/6000], Loss: 0.0319
Epoch [10/30], Batch [4600/6000], Loss: 0.0339
Epoch [10/30], Batch [4700/6000], Loss: 0.0230
Epoch [10/30], Batch [4800/6000], Loss: 0.0276
Epoch [10/30], Batch [4900/6000], Loss: 0.0383
Epoch [10/30], Batch [5000/6000], Loss: 0.0253
Epoch [10/30], Batch [5100/6000], Loss: 0.0279
Epoch [10/30], Batch [5200/6000], Loss: 0.0252
Epoch [10/30], Batch [5300/6000], Loss: 0.0199
Epoch [10/30], Batch [5400/6000], Loss: 0.0241
Epoch [10/30], Batch [5500/6000], Loss: 0.0729
Epoch [10/30], Batch [5600/6000], Loss: 0.1815
Epoch [10/30], Batch [5700/6000], Loss: 0.0576
Epoch [10/30], Batch [5800/6000], Loss: 0.0934
Epoch [10/30], Batch [5900/6000], Loss: 0.0194
Epoch [10/30], Loss: 0.0685
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.0237
Epoch [11/30], Batch [100/6000], Loss: 0.0385
Epoch [11/30], Batch [200/6000], Loss: 0.0744
Epoch [11/30], Batch [300/6000], Loss: 0.0241
Epoch [11/30], Batch [400/6000], Loss: 0.0195
Epoch [11/30], Batch [500/6000], Loss: 0.0336
Epoch [11/30], Batch [600/6000], Loss: 0.0245
Epoch [11/30], Batch [700/6000], Loss: 0.0433
Epoch [11/30], Batch [800/6000], Loss: 0.0708
Epoch [11/30], Batch [900/6000], Loss: 0.0930
Epoch [11/30], Batch [1000/6000], Loss: 0.0593
Epoch [11/30], Batch [1100/6000], Loss: 0.0273
Epoch [11/30], Batch [1200/6000], Loss: 0.0288
Epoch [11/30], Batch [1300/6000], Loss: 0.5191
Epoch [11/30], Batch [1400/6000], Loss: 0.1268
Epoch [11/30], Batch [1500/6000], Loss: 0.1617
Epoch [11/30], Batch [1600/6000], Loss: 0.0674
Epoch [11/30], Batch [1700/6000], Loss: 0.0305
Epoch [11/30], Batch [1800/6000], Loss: 0.3316
Epoch [11/30], Batch [1900/6000], Loss: 0.0224
Epoch [11/30], Batch [2000/6000], Loss: 0.0293
Epoch [11/30], Batch [2100/6000], Loss: 0.0314
Epoch [11/30], Batch [2200/6000], Loss: 0.0258
Epoch [11/30], Batch [2300/6000], Loss: 0.0234
Epoch [11/30], Batch [2400/6000], Loss: 0.2623
Epoch [11/30], Batch [2500/6000], Loss: 0.0720
Epoch [11/30], Batch [2600/6000], Loss: 0.0457
Epoch [11/30], Batch [2700/6000], Loss: 0.0275
Epoch [11/30], Batch [2800/6000], Loss: 0.0225
Epoch [11/30], Batch [2900/6000], Loss: 0.0231
Epoch [11/30], Batch [3000/6000], Loss: 0.0239
Epoch [11/30], Batch [3100/6000], Loss: 0.0222
Epoch [11/30], Batch [3200/6000], Loss: 0.0195
Epoch [11/30], Batch [3300/6000], Loss: 0.0193
Epoch [11/30], Batch [3400/6000], Loss: 0.0747
Epoch [11/30], Batch [3500/6000], Loss: 0.0252
Epoch [11/30], Batch [3600/6000], Loss: 0.0288
Epoch [11/30], Batch [3700/6000], Loss: 0.0254
Epoch [11/30], Batch [3800/6000], Loss: 0.0882
Epoch [11/30], Batch [3900/6000], Loss: 0.0236
Epoch [11/30], Batch [4000/6000], Loss: 0.0204
Epoch [11/30], Batch [4100/6000], Loss: 0.0246
Epoch [11/30], Batch [4200/6000], Loss: 0.0286
Epoch [11/30], Batch [4300/6000], Loss: 0.0238
Epoch [11/30], Batch [4400/6000], Loss: 0.0587
Epoch [11/30], Batch [4500/6000], Loss: 0.0294
Epoch [11/30], Batch [4600/6000], Loss: 0.0182
Epoch [11/30], Batch [4700/6000], Loss: 0.0402
Epoch [11/30], Batch [4800/6000], Loss: 0.0264
Epoch [11/30], Batch [4900/6000], Loss: 0.0313
Epoch [11/30], Batch [5000/6000], Loss: 0.0304
Epoch [11/30], Batch [5100/6000], Loss: 0.0261
Epoch [11/30], Batch [5200/6000], Loss: 0.1116
Epoch [11/30], Batch [5300/6000], Loss: 0.0281
Epoch [11/30], Batch [5400/6000], Loss: 0.0286
Epoch [11/30], Batch [5500/6000], Loss: 0.7127
Epoch [11/30], Batch [5600/6000], Loss: 0.0455
Epoch [11/30], Batch [5700/6000], Loss: 0.0298
Epoch [11/30], Batch [5800/6000], Loss: 0.0221
Epoch [11/30], Batch [5900/6000], Loss: 0.0274
Epoch [11/30], Loss: 0.0631
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.0265
Epoch [12/30], Batch [100/6000], Loss: 0.0331
Epoch [12/30], Batch [200/6000], Loss: 0.0249
Epoch [12/30], Batch [300/6000], Loss: 0.0192
Epoch [12/30], Batch [400/6000], Loss: 0.0275
Epoch [12/30], Batch [500/6000], Loss: 0.0265
Epoch [12/30], Batch [600/6000], Loss: 0.0422
Epoch [12/30], Batch [700/6000], Loss: 0.0211
Epoch [12/30], Batch [800/6000], Loss: 0.0656
Epoch [12/30], Batch [900/6000], Loss: 0.1686
Epoch [12/30], Batch [1000/6000], Loss: 0.0202
Epoch [12/30], Batch [1100/6000], Loss: 0.0221
Epoch [12/30], Batch [1200/6000], Loss: 0.0499
Epoch [12/30], Batch [1300/6000], Loss: 0.0257
Epoch [12/30], Batch [1400/6000], Loss: 0.0184
Epoch [12/30], Batch [1500/6000], Loss: 0.0264
Epoch [12/30], Batch [1600/6000], Loss: 0.0225
Epoch [12/30], Batch [1700/6000], Loss: 0.0237
Epoch [12/30], Batch [1800/6000], Loss: 0.0394
Epoch [12/30], Batch [1900/6000], Loss: 0.0196
Epoch [12/30], Batch [2000/6000], Loss: 0.0233
Epoch [12/30], Batch [2100/6000], Loss: 0.0230
Epoch [12/30], Batch [2200/6000], Loss: 0.0194
Epoch [12/30], Batch [2300/6000], Loss: 0.0275
Epoch [12/30], Batch [2400/6000], Loss: 0.0161
Epoch [12/30], Batch [2500/6000], Loss: 0.0215
Epoch [12/30], Batch [2600/6000], Loss: 0.0193
Epoch [12/30], Batch [2700/6000], Loss: 0.0420
Epoch [12/30], Batch [2800/6000], Loss: 0.0234
Epoch [12/30], Batch [2900/6000], Loss: 0.0271
Epoch [12/30], Batch [3000/6000], Loss: 0.0982
Epoch [12/30], Batch [3100/6000], Loss: 0.0237
Epoch [12/30], Batch [3200/6000], Loss: 0.0210
Epoch [12/30], Batch [3300/6000], Loss: 0.3468
Epoch [12/30], Batch [3400/6000], Loss: 0.0247
Epoch [12/30], Batch [3500/6000], Loss: 0.0270
Epoch [12/30], Batch [3600/6000], Loss: 0.0238
Epoch [12/30], Batch [3700/6000], Loss: 0.0219
Epoch [12/30], Batch [3800/6000], Loss: 0.0194
Epoch [12/30], Batch [3900/6000], Loss: 0.0234
Epoch [12/30], Batch [4000/6000], Loss: 0.0230
Epoch [12/30], Batch [4100/6000], Loss: 0.0190
Epoch [12/30], Batch [4200/6000], Loss: 0.0185
Epoch [12/30], Batch [4300/6000], Loss: 0.6739
Epoch [12/30], Batch [4400/6000], Loss: 0.0373
Epoch [12/30], Batch [4500/6000], Loss: 0.0210
Epoch [12/30], Batch [4600/6000], Loss: 0.0218
Epoch [12/30], Batch [4700/6000], Loss: 0.0520
Epoch [12/30], Batch [4800/6000], Loss: 0.0583
Epoch [12/30], Batch [4900/6000], Loss: 0.0271
Epoch [12/30], Batch [5000/6000], Loss: 0.1261
Epoch [12/30], Batch [5100/6000], Loss: 0.0218
Epoch [12/30], Batch [5200/6000], Loss: 0.0312
Epoch [12/30], Batch [5300/6000], Loss: 0.0458
Epoch [12/30], Batch [5400/6000], Loss: 0.0743
Epoch [12/30], Batch [5500/6000], Loss: 0.0598
Epoch [12/30], Batch [5600/6000], Loss: 0.0664
Epoch [12/30], Batch [5700/6000], Loss: 0.0237
Epoch [12/30], Batch [5800/6000], Loss: 0.0443
Epoch [12/30], Batch [5900/6000], Loss: 0.0310
Epoch [12/30], Loss: 0.0578
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.0271
Epoch [13/30], Batch [100/6000], Loss: 0.0228
Epoch [13/30], Batch [200/6000], Loss: 0.0748
Epoch [13/30], Batch [300/6000], Loss: 0.0438
Epoch [13/30], Batch [400/6000], Loss: 0.0430
Epoch [13/30], Batch [500/6000], Loss: 0.0242
Epoch [13/30], Batch [600/6000], Loss: 0.0177
Epoch [13/30], Batch [700/6000], Loss: 0.0257
Epoch [13/30], Batch [800/6000], Loss: 0.0313
Epoch [13/30], Batch [900/6000], Loss: 0.0232
Epoch [13/30], Batch [1000/6000], Loss: 0.0217
Epoch [13/30], Batch [1100/6000], Loss: 0.0223
Epoch [13/30], Batch [1200/6000], Loss: 0.0226
Epoch [13/30], Batch [1300/6000], Loss: 0.0244
Epoch [13/30], Batch [1400/6000], Loss: 0.0238
Epoch [13/30], Batch [1500/6000], Loss: 0.0247
Epoch [13/30], Batch [1600/6000], Loss: 0.0863
Epoch [13/30], Batch [1700/6000], Loss: 0.0220
Epoch [13/30], Batch [1800/6000], Loss: 0.0273
Epoch [13/30], Batch [1900/6000], Loss: 0.0525
Epoch [13/30], Batch [2000/6000], Loss: 0.0245
Epoch [13/30], Batch [2100/6000], Loss: 0.1169
Epoch [13/30], Batch [2200/6000], Loss: 0.0357
Epoch [13/30], Batch [2300/6000], Loss: 0.0271
Epoch [13/30], Batch [2400/6000], Loss: 0.0193
Epoch [13/30], Batch [2500/6000], Loss: 0.0189
Epoch [13/30], Batch [2600/6000], Loss: 0.0221
Epoch [13/30], Batch [2700/6000], Loss: 0.0279
Epoch [13/30], Batch [2800/6000], Loss: 0.0258
Epoch [13/30], Batch [2900/6000], Loss: 0.0232
Epoch [13/30], Batch [3000/6000], Loss: 0.0255
Epoch [13/30], Batch [3100/6000], Loss: 0.0250
Epoch [13/30], Batch [3200/6000], Loss: 0.0252
Epoch [13/30], Batch [3300/6000], Loss: 0.1276
Epoch [13/30], Batch [3400/6000], Loss: 0.0246
Epoch [13/30], Batch [3500/6000], Loss: 0.0190
Epoch [13/30], Batch [3600/6000], Loss: 0.1271
Epoch [13/30], Batch [3700/6000], Loss: 0.0225
Epoch [13/30], Batch [3800/6000], Loss: 0.0199
Epoch [13/30], Batch [3900/6000], Loss: 0.0238
Epoch [13/30], Batch [4000/6000], Loss: 0.0178
Epoch [13/30], Batch [4100/6000], Loss: 0.0288
Epoch [13/30], Batch [4200/6000], Loss: 0.0808
Epoch [13/30], Batch [4300/6000], Loss: 0.0213
Epoch [13/30], Batch [4400/6000], Loss: 0.0827
Epoch [13/30], Batch [4500/6000], Loss: 0.0185
Epoch [13/30], Batch [4600/6000], Loss: 0.0694
Epoch [13/30], Batch [4700/6000], Loss: 0.0205
Epoch [13/30], Batch [4800/6000], Loss: 0.0264
Epoch [13/30], Batch [4900/6000], Loss: 0.0306
Epoch [13/30], Batch [5000/6000], Loss: 0.0239
Epoch [13/30], Batch [5100/6000], Loss: 0.0857
Epoch [13/30], Batch [5200/6000], Loss: 0.0256
Epoch [13/30], Batch [5300/6000], Loss: 0.0239
Epoch [13/30], Batch [5400/6000], Loss: 0.0208
Epoch [13/30], Batch [5500/6000], Loss: 0.0211
Epoch [13/30], Batch [5600/6000], Loss: 0.5750
Epoch [13/30], Batch [5700/6000], Loss: 0.0202
Epoch [13/30], Batch [5800/6000], Loss: 0.0237
Epoch [13/30], Batch [5900/6000], Loss: 0.0263
Epoch [13/30], Loss: 0.0548
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.0280
Epoch [14/30], Batch [100/6000], Loss: 0.0202
Epoch [14/30], Batch [200/6000], Loss: 0.0270
Epoch [14/30], Batch [300/6000], Loss: 0.0242
Epoch [14/30], Batch [400/6000], Loss: 0.1649
Epoch [14/30], Batch [500/6000], Loss: 0.1644
Epoch [14/30], Batch [600/6000], Loss: 0.0779
Epoch [14/30], Batch [700/6000], Loss: 0.0237
Epoch [14/30], Batch [800/6000], Loss: 0.0625
Epoch [14/30], Batch [900/6000], Loss: 0.0313
Epoch [14/30], Batch [1000/6000], Loss: 0.0203
Epoch [14/30], Batch [1100/6000], Loss: 0.0214
Epoch [14/30], Batch [1200/6000], Loss: 0.0249
Epoch [14/30], Batch [1300/6000], Loss: 0.5705
Epoch [14/30], Batch [1400/6000], Loss: 0.0256
Epoch [14/30], Batch [1500/6000], Loss: 0.0200
Epoch [14/30], Batch [1600/6000], Loss: 0.0198
Epoch [14/30], Batch [1700/6000], Loss: 0.0239
Epoch [14/30], Batch [1800/6000], Loss: 0.0335
Epoch [14/30], Batch [1900/6000], Loss: 0.0183
Epoch [14/30], Batch [2000/6000], Loss: 0.0259
Epoch [14/30], Batch [2100/6000], Loss: 0.0207
Epoch [14/30], Batch [2200/6000], Loss: 0.0405
Epoch [14/30], Batch [2300/6000], Loss: 0.3585
Epoch [14/30], Batch [2400/6000], Loss: 0.0213
Epoch [14/30], Batch [2500/6000], Loss: 0.0399
Epoch [14/30], Batch [2600/6000], Loss: 0.0194
Epoch [14/30], Batch [2700/6000], Loss: 0.0198
Epoch [14/30], Batch [2800/6000], Loss: 0.0210
Epoch [14/30], Batch [2900/6000], Loss: 0.1048
Epoch [14/30], Batch [3000/6000], Loss: 0.0236
Epoch [14/30], Batch [3100/6000], Loss: 0.0239
Epoch [14/30], Batch [3200/6000], Loss: 0.0229
Epoch [14/30], Batch [3300/6000], Loss: 0.1047
Epoch [14/30], Batch [3400/6000], Loss: 0.1189
Epoch [14/30], Batch [3500/6000], Loss: 0.0537
Epoch [14/30], Batch [3600/6000], Loss: 0.0494
Epoch [14/30], Batch [3700/6000], Loss: 0.0242
Epoch [14/30], Batch [3800/6000], Loss: 0.0276
Epoch [14/30], Batch [3900/6000], Loss: 0.0988
Epoch [14/30], Batch [4000/6000], Loss: 0.0233
Epoch [14/30], Batch [4100/6000], Loss: 0.0250
Epoch [14/30], Batch [4200/6000], Loss: 0.0371
Epoch [14/30], Batch [4300/6000], Loss: 0.0304
Epoch [14/30], Batch [4400/6000], Loss: 0.0228
Epoch [14/30], Batch [4500/6000], Loss: 0.0247
Epoch [14/30], Batch [4600/6000], Loss: 0.0157
Epoch [14/30], Batch [4700/6000], Loss: 0.0283
Epoch [14/30], Batch [4800/6000], Loss: 0.0301
Epoch [14/30], Batch [4900/6000], Loss: 0.0251
Epoch [14/30], Batch [5000/6000], Loss: 0.0263
Epoch [14/30], Batch [5100/6000], Loss: 0.0191
Epoch [14/30], Batch [5200/6000], Loss: 0.0292
Epoch [14/30], Batch [5300/6000], Loss: 0.0238
Epoch [14/30], Batch [5400/6000], Loss: 0.0625
Epoch [14/30], Batch [5500/6000], Loss: 0.3081
Epoch [14/30], Batch [5600/6000], Loss: 0.0236
Epoch [14/30], Batch [5700/6000], Loss: 0.0269
Epoch [14/30], Batch [5800/6000], Loss: 0.0236
Epoch [14/30], Batch [5900/6000], Loss: 0.0254
Epoch [14/30], Loss: 0.0502
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.0246
Epoch [15/30], Batch [100/6000], Loss: 0.0227
Epoch [15/30], Batch [200/6000], Loss: 0.0275
Epoch [15/30], Batch [300/6000], Loss: 0.0225
Epoch [15/30], Batch [400/6000], Loss: 0.0230
Epoch [15/30], Batch [500/6000], Loss: 0.0284
Epoch [15/30], Batch [600/6000], Loss: 0.0238
Epoch [15/30], Batch [700/6000], Loss: 0.0284
Epoch [15/30], Batch [800/6000], Loss: 0.0214
Epoch [15/30], Batch [900/6000], Loss: 0.0199
Epoch [15/30], Batch [1000/6000], Loss: 0.0221
Epoch [15/30], Batch [1100/6000], Loss: 0.0285
Epoch [15/30], Batch [1200/6000], Loss: 0.0244
Epoch [15/30], Batch [1300/6000], Loss: 0.0226
Epoch [15/30], Batch [1400/6000], Loss: 0.0220
Epoch [15/30], Batch [1500/6000], Loss: 0.0336
Epoch [15/30], Batch [1600/6000], Loss: 0.0186
Epoch [15/30], Batch [1700/6000], Loss: 0.0195
Epoch [15/30], Batch [1800/6000], Loss: 0.4291
Epoch [15/30], Batch [1900/6000], Loss: 0.0194
Epoch [15/30], Batch [2000/6000], Loss: 0.0218
Epoch [15/30], Batch [2100/6000], Loss: 0.0501
Epoch [15/30], Batch [2200/6000], Loss: 0.0204
Epoch [15/30], Batch [2300/6000], Loss: 0.0321
Epoch [15/30], Batch [2400/6000], Loss: 0.0249
Epoch [15/30], Batch [2500/6000], Loss: 0.0240
Epoch [15/30], Batch [2600/6000], Loss: 0.0365
Epoch [15/30], Batch [2700/6000], Loss: 0.0417
Epoch [15/30], Batch [2800/6000], Loss: 0.0882
Epoch [15/30], Batch [2900/6000], Loss: 0.0198
Epoch [15/30], Batch [3000/6000], Loss: 0.0431
Epoch [15/30], Batch [3100/6000], Loss: 0.0189
Epoch [15/30], Batch [3200/6000], Loss: 0.1118
Epoch [15/30], Batch [3300/6000], Loss: 0.0188
Epoch [15/30], Batch [3400/6000], Loss: 0.0426
Epoch [15/30], Batch [3500/6000], Loss: 0.0275
Epoch [15/30], Batch [3600/6000], Loss: 0.1649
Epoch [15/30], Batch [3700/6000], Loss: 0.0221
Epoch [15/30], Batch [3800/6000], Loss: 0.0186
Epoch [15/30], Batch [3900/6000], Loss: 0.0212
Epoch [15/30], Batch [4000/6000], Loss: 0.0962
Epoch [15/30], Batch [4100/6000], Loss: 0.0254
Epoch [15/30], Batch [4200/6000], Loss: 0.0204
Epoch [15/30], Batch [4300/6000], Loss: 0.0237
Epoch [15/30], Batch [4400/6000], Loss: 0.0199
Epoch [15/30], Batch [4500/6000], Loss: 0.0290
Epoch [15/30], Batch [4600/6000], Loss: 0.0218
Epoch [15/30], Batch [4700/6000], Loss: 0.0226
Epoch [15/30], Batch [4800/6000], Loss: 0.0190
Epoch [15/30], Batch [4900/6000], Loss: 0.0203
Epoch [15/30], Batch [5000/6000], Loss: 0.0222
Epoch [15/30], Batch [5100/6000], Loss: 0.0206
Epoch [15/30], Batch [5200/6000], Loss: 0.0733
Epoch [15/30], Batch [5300/6000], Loss: 0.0215
Epoch [15/30], Batch [5400/6000], Loss: 0.0218
Epoch [15/30], Batch [5500/6000], Loss: 0.0217
Epoch [15/30], Batch [5600/6000], Loss: 0.0150
Epoch [15/30], Batch [5700/6000], Loss: 0.0169
Epoch [15/30], Batch [5800/6000], Loss: 0.0562
Epoch [15/30], Batch [5900/6000], Loss: 0.0254
Epoch [15/30], Loss: 0.0457
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.0222
Epoch [16/30], Batch [100/6000], Loss: 0.0206
Epoch [16/30], Batch [200/6000], Loss: 0.0286
Epoch [16/30], Batch [300/6000], Loss: 0.0260
Epoch [16/30], Batch [400/6000], Loss: 0.0367
Epoch [16/30], Batch [500/6000], Loss: 0.0246
Epoch [16/30], Batch [600/6000], Loss: 0.0219
Epoch [16/30], Batch [700/6000], Loss: 0.0254
Epoch [16/30], Batch [800/6000], Loss: 0.0268
Epoch [16/30], Batch [900/6000], Loss: 0.0214
Epoch [16/30], Batch [1000/6000], Loss: 0.0217
Epoch [16/30], Batch [1100/6000], Loss: 0.0352
Epoch [16/30], Batch [1200/6000], Loss: 0.0269
Epoch [16/30], Batch [1300/6000], Loss: 0.0641
Epoch [16/30], Batch [1400/6000], Loss: 0.0218
Epoch [16/30], Batch [1500/6000], Loss: 0.0243
Epoch [16/30], Batch [1600/6000], Loss: 0.0313
Epoch [16/30], Batch [1700/6000], Loss: 0.0220
Epoch [16/30], Batch [1800/6000], Loss: 0.0249
Epoch [16/30], Batch [1900/6000], Loss: 0.0685
Epoch [16/30], Batch [2000/6000], Loss: 0.0531
Epoch [16/30], Batch [2100/6000], Loss: 0.0744
Epoch [16/30], Batch [2200/6000], Loss: 0.0229
Epoch [16/30], Batch [2300/6000], Loss: 0.0235
Epoch [16/30], Batch [2400/6000], Loss: 0.0229
Epoch [16/30], Batch [2500/6000], Loss: 0.0200
Epoch [16/30], Batch [2600/6000], Loss: 0.0249
Epoch [16/30], Batch [2700/6000], Loss: 0.0190
Epoch [16/30], Batch [2800/6000], Loss: 0.0323
Epoch [16/30], Batch [2900/6000], Loss: 0.0214
Epoch [16/30], Batch [3000/6000], Loss: 0.0189
Epoch [16/30], Batch [3100/6000], Loss: 0.0204
Epoch [16/30], Batch [3200/6000], Loss: 0.0346
Epoch [16/30], Batch [3300/6000], Loss: 0.0248
Epoch [16/30], Batch [3400/6000], Loss: 0.0234
Epoch [16/30], Batch [3500/6000], Loss: 0.0223
Epoch [16/30], Batch [3600/6000], Loss: 0.7823
Epoch [16/30], Batch [3700/6000], Loss: 0.0257
Epoch [16/30], Batch [3800/6000], Loss: 0.0226
Epoch [16/30], Batch [3900/6000], Loss: 0.1291
Epoch [16/30], Batch [4000/6000], Loss: 0.0346
Epoch [16/30], Batch [4100/6000], Loss: 0.0247
Epoch [16/30], Batch [4200/6000], Loss: 0.0177
Epoch [16/30], Batch [4300/6000], Loss: 0.0405
Epoch [16/30], Batch [4400/6000], Loss: 0.0199
Epoch [16/30], Batch [4500/6000], Loss: 0.5374
Epoch [16/30], Batch [4600/6000], Loss: 0.0310
Epoch [16/30], Batch [4700/6000], Loss: 0.9269
Epoch [16/30], Batch [4800/6000], Loss: 0.0656
Epoch [16/30], Batch [4900/6000], Loss: 0.0253
Epoch [16/30], Batch [5000/6000], Loss: 0.0177
Epoch [16/30], Batch [5100/6000], Loss: 0.0243
Epoch [16/30], Batch [5200/6000], Loss: 0.0247
Epoch [16/30], Batch [5300/6000], Loss: 0.0288
Epoch [16/30], Batch [5400/6000], Loss: 0.0279
Epoch [16/30], Batch [5500/6000], Loss: 0.0232
Epoch [16/30], Batch [5600/6000], Loss: 0.0193
Epoch [16/30], Batch [5700/6000], Loss: 0.0153
Epoch [16/30], Batch [5800/6000], Loss: 0.0214
Epoch [16/30], Batch [5900/6000], Loss: 0.0234
Epoch [16/30], Loss: 0.0447
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.0204
Epoch [17/30], Batch [100/6000], Loss: 0.0250
Epoch [17/30], Batch [200/6000], Loss: 0.0183
Epoch [17/30], Batch [300/6000], Loss: 0.0231
Epoch [17/30], Batch [400/6000], Loss: 0.0211
Epoch [17/30], Batch [500/6000], Loss: 0.0496
Epoch [17/30], Batch [600/6000], Loss: 0.0197
Epoch [17/30], Batch [700/6000], Loss: 0.0183
Epoch [17/30], Batch [800/6000], Loss: 0.0221
Epoch [17/30], Batch [900/6000], Loss: 0.0237
Epoch [17/30], Batch [1000/6000], Loss: 0.0213
Epoch [17/30], Batch [1100/6000], Loss: 0.0226
Epoch [17/30], Batch [1200/6000], Loss: 0.0202
Epoch [17/30], Batch [1300/6000], Loss: 0.0180
Epoch [17/30], Batch [1400/6000], Loss: 0.0182
Epoch [17/30], Batch [1500/6000], Loss: 0.1304
Epoch [17/30], Batch [1600/6000], Loss: 0.0386
Epoch [17/30], Batch [1700/6000], Loss: 0.0172
Epoch [17/30], Batch [1800/6000], Loss: 0.0308
Epoch [17/30], Batch [1900/6000], Loss: 0.0290
Epoch [17/30], Batch [2000/6000], Loss: 0.0329
Epoch [17/30], Batch [2100/6000], Loss: 0.0250
Epoch [17/30], Batch [2200/6000], Loss: 0.0237
Epoch [17/30], Batch [2300/6000], Loss: 0.0224
Epoch [17/30], Batch [2400/6000], Loss: 0.0396
Epoch [17/30], Batch [2500/6000], Loss: 0.0193
Epoch [17/30], Batch [2600/6000], Loss: 0.0178
Epoch [17/30], Batch [2700/6000], Loss: 0.0226
Epoch [17/30], Batch [2800/6000], Loss: 0.0231
Epoch [17/30], Batch [2900/6000], Loss: 0.0220
Epoch [17/30], Batch [3000/6000], Loss: 0.0236
Epoch [17/30], Batch [3100/6000], Loss: 0.0257
Epoch [17/30], Batch [3200/6000], Loss: 0.0193
Epoch [17/30], Batch [3300/6000], Loss: 0.0273
Epoch [17/30], Batch [3400/6000], Loss: 0.0257
Epoch [17/30], Batch [3500/6000], Loss: 0.0226
Epoch [17/30], Batch [3600/6000], Loss: 0.0322
Epoch [17/30], Batch [3700/6000], Loss: 0.0402
Epoch [17/30], Batch [3800/6000], Loss: 0.0193
Epoch [17/30], Batch [3900/6000], Loss: 0.0215
Epoch [17/30], Batch [4000/6000], Loss: 0.1907
Epoch [17/30], Batch [4100/6000], Loss: 0.0222
Epoch [17/30], Batch [4200/6000], Loss: 0.0264
Epoch [17/30], Batch [4300/6000], Loss: 0.0257
Epoch [17/30], Batch [4400/6000], Loss: 0.2280
Epoch [17/30], Batch [4500/6000], Loss: 0.0323
Epoch [17/30], Batch [4600/6000], Loss: 0.0188
Epoch [17/30], Batch [4700/6000], Loss: 0.1406
Epoch [17/30], Batch [4800/6000], Loss: 0.0297
Epoch [17/30], Batch [4900/6000], Loss: 0.0845
Epoch [17/30], Batch [5000/6000], Loss: 0.0185
Epoch [17/30], Batch [5100/6000], Loss: 0.0198
Epoch [17/30], Batch [5200/6000], Loss: 0.0178
Epoch [17/30], Batch [5300/6000], Loss: 0.0500
Epoch [17/30], Batch [5400/6000], Loss: 0.0197
Epoch [17/30], Batch [5500/6000], Loss: 0.0171
Epoch [17/30], Batch [5600/6000], Loss: 0.0194
Epoch [17/30], Batch [5700/6000], Loss: 0.0221
Epoch [17/30], Batch [5800/6000], Loss: 0.0234
Epoch [17/30], Batch [5900/6000], Loss: 0.0182
Epoch [17/30], Loss: 0.0405
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.0241
Epoch [18/30], Batch [100/6000], Loss: 0.0214
Epoch [18/30], Batch [200/6000], Loss: 0.0240
Epoch [18/30], Batch [300/6000], Loss: 0.0193
Epoch [18/30], Batch [400/6000], Loss: 0.0198
Epoch [18/30], Batch [500/6000], Loss: 0.0879
Epoch [18/30], Batch [600/6000], Loss: 0.0169
Epoch [18/30], Batch [700/6000], Loss: 0.0236
Epoch [18/30], Batch [800/6000], Loss: 0.0178
Epoch [18/30], Batch [900/6000], Loss: 0.0267
Epoch [18/30], Batch [1000/6000], Loss: 0.0634
Epoch [18/30], Batch [1100/6000], Loss: 0.2688
Epoch [18/30], Batch [1200/6000], Loss: 0.0225
Epoch [18/30], Batch [1300/6000], Loss: 0.0267
Epoch [18/30], Batch [1400/6000], Loss: 0.0251
Epoch [18/30], Batch [1500/6000], Loss: 0.0525
Epoch [18/30], Batch [1600/6000], Loss: 0.0546
Epoch [18/30], Batch [1700/6000], Loss: 0.0250
Epoch [18/30], Batch [1800/6000], Loss: 0.0193
Epoch [18/30], Batch [1900/6000], Loss: 0.0190
Epoch [18/30], Batch [2000/6000], Loss: 0.0173
Epoch [18/30], Batch [2100/6000], Loss: 0.0824
Epoch [18/30], Batch [2200/6000], Loss: 0.0269
Epoch [18/30], Batch [2300/6000], Loss: 0.0163
Epoch [18/30], Batch [2400/6000], Loss: 0.0170
Epoch [18/30], Batch [2500/6000], Loss: 0.0180
Epoch [18/30], Batch [2600/6000], Loss: 0.0173
Epoch [18/30], Batch [2700/6000], Loss: 0.0183
Epoch [18/30], Batch [2800/6000], Loss: 0.0409
Epoch [18/30], Batch [2900/6000], Loss: 0.0212
Epoch [18/30], Batch [3000/6000], Loss: 0.0234
Epoch [18/30], Batch [3100/6000], Loss: 0.0193
Epoch [18/30], Batch [3200/6000], Loss: 0.0217
Epoch [18/30], Batch [3300/6000], Loss: 0.0191
Epoch [18/30], Batch [3400/6000], Loss: 0.0153
Epoch [18/30], Batch [3500/6000], Loss: 0.5952
Epoch [18/30], Batch [3600/6000], Loss: 0.0188
Epoch [18/30], Batch [3700/6000], Loss: 0.0225
Epoch [18/30], Batch [3800/6000], Loss: 0.0270
Epoch [18/30], Batch [3900/6000], Loss: 0.0181
Epoch [18/30], Batch [4000/6000], Loss: 0.1802
Epoch [18/30], Batch [4100/6000], Loss: 0.0798
Epoch [18/30], Batch [4200/6000], Loss: 0.0188
Epoch [18/30], Batch [4300/6000], Loss: 0.0457
Epoch [18/30], Batch [4400/6000], Loss: 0.0207
Epoch [18/30], Batch [4500/6000], Loss: 0.0175
Epoch [18/30], Batch [4600/6000], Loss: 0.0245
Epoch [18/30], Batch [4700/6000], Loss: 0.0175
Epoch [18/30], Batch [4800/6000], Loss: 0.0180
Epoch [18/30], Batch [4900/6000], Loss: 0.0435
Epoch [18/30], Batch [5000/6000], Loss: 0.0196
Epoch [18/30], Batch [5100/6000], Loss: 0.0237
Epoch [18/30], Batch [5200/6000], Loss: 0.0228
Epoch [18/30], Batch [5300/6000], Loss: 0.0323
Epoch [18/30], Batch [5400/6000], Loss: 0.0247
Epoch [18/30], Batch [5500/6000], Loss: 0.0195
Epoch [18/30], Batch [5600/6000], Loss: 0.0289
Epoch [18/30], Batch [5700/6000], Loss: 0.0231
Epoch [18/30], Batch [5800/6000], Loss: 0.0181
Epoch [18/30], Batch [5900/6000], Loss: 0.0148
Epoch [18/30], Loss: 0.0399
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.0251
Epoch [19/30], Batch [100/6000], Loss: 0.0202
Epoch [19/30], Batch [200/6000], Loss: 0.0443
Epoch [19/30], Batch [300/6000], Loss: 0.0276
Epoch [19/30], Batch [400/6000], Loss: 0.0316
Epoch [19/30], Batch [500/6000], Loss: 0.0227
Epoch [19/30], Batch [600/6000], Loss: 0.0214
Epoch [19/30], Batch [700/6000], Loss: 0.0227
Epoch [19/30], Batch [800/6000], Loss: 0.0334
Epoch [19/30], Batch [900/6000], Loss: 0.0185
Epoch [19/30], Batch [1000/6000], Loss: 0.0255
Epoch [19/30], Batch [1100/6000], Loss: 0.0193
Epoch [19/30], Batch [1200/6000], Loss: 0.0182
Epoch [19/30], Batch [1300/6000], Loss: 0.0185
Epoch [19/30], Batch [1400/6000], Loss: 0.0167
Epoch [19/30], Batch [1500/6000], Loss: 0.0183
Epoch [19/30], Batch [1600/6000], Loss: 0.0181
Epoch [19/30], Batch [1700/6000], Loss: 0.0200
Epoch [19/30], Batch [1800/6000], Loss: 0.0197
Epoch [19/30], Batch [1900/6000], Loss: 0.0277
Epoch [19/30], Batch [2000/6000], Loss: 0.0203
Epoch [19/30], Batch [2100/6000], Loss: 0.0202
Epoch [19/30], Batch [2200/6000], Loss: 0.0392
Epoch [19/30], Batch [2300/6000], Loss: 0.0239
Epoch [19/30], Batch [2400/6000], Loss: 0.0199
Epoch [19/30], Batch [2500/6000], Loss: 0.0320
Epoch [19/30], Batch [2600/6000], Loss: 0.1002
Epoch [19/30], Batch [2700/6000], Loss: 0.0406
Epoch [19/30], Batch [2800/6000], Loss: 0.0266
Epoch [19/30], Batch [2900/6000], Loss: 0.0201
Epoch [19/30], Batch [3000/6000], Loss: 0.0200
Epoch [19/30], Batch [3100/6000], Loss: 0.0151
Epoch [19/30], Batch [3200/6000], Loss: 0.0204
Epoch [19/30], Batch [3300/6000], Loss: 0.0207
Epoch [19/30], Batch [3400/6000], Loss: 0.0178
Epoch [19/30], Batch [3500/6000], Loss: 0.0221
Epoch [19/30], Batch [3600/6000], Loss: 0.0435
Epoch [19/30], Batch [3700/6000], Loss: 0.0220
Epoch [19/30], Batch [3800/6000], Loss: 0.0188
Epoch [19/30], Batch [3900/6000], Loss: 0.0169
Epoch [19/30], Batch [4000/6000], Loss: 0.0233
Epoch [19/30], Batch [4100/6000], Loss: 0.0188
Epoch [19/30], Batch [4200/6000], Loss: 0.0220
Epoch [19/30], Batch [4300/6000], Loss: 0.0188
Epoch [19/30], Batch [4400/6000], Loss: 0.0846
Epoch [19/30], Batch [4500/6000], Loss: 0.0173
Epoch [19/30], Batch [4600/6000], Loss: 0.0247
Epoch [19/30], Batch [4700/6000], Loss: 0.0203
Epoch [19/30], Batch [4800/6000], Loss: 0.0233
Epoch [19/30], Batch [4900/6000], Loss: 0.0229
Epoch [19/30], Batch [5000/6000], Loss: 0.0299
Epoch [19/30], Batch [5100/6000], Loss: 0.0200
Epoch [19/30], Batch [5200/6000], Loss: 0.0187
Epoch [19/30], Batch [5300/6000], Loss: 0.0626
Epoch [19/30], Batch [5400/6000], Loss: 0.0151
Epoch [19/30], Batch [5500/6000], Loss: 0.0359
Epoch [19/30], Batch [5600/6000], Loss: 0.0182
Epoch [19/30], Batch [5700/6000], Loss: 0.0454
Epoch [19/30], Batch [5800/6000], Loss: 0.0219
Epoch [19/30], Batch [5900/6000], Loss: 0.1005
Epoch [19/30], Loss: 0.0377
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.0336
Epoch [20/30], Batch [100/6000], Loss: 0.1131
Epoch [20/30], Batch [200/6000], Loss: 0.0244
Epoch [20/30], Batch [300/6000], Loss: 0.0169
Epoch [20/30], Batch [400/6000], Loss: 0.0172
Epoch [20/30], Batch [500/6000], Loss: 0.7055
Epoch [20/30], Batch [600/6000], Loss: 0.0133
Epoch [20/30], Batch [700/6000], Loss: 0.0227
Epoch [20/30], Batch [800/6000], Loss: 0.0229
Epoch [20/30], Batch [900/6000], Loss: 0.0188
Epoch [20/30], Batch [1000/6000], Loss: 0.0203
Epoch [20/30], Batch [1100/6000], Loss: 0.0176
Epoch [20/30], Batch [1200/6000], Loss: 0.0243
Epoch [20/30], Batch [1300/6000], Loss: 0.0292
Epoch [20/30], Batch [1400/6000], Loss: 0.0570
Epoch [20/30], Batch [1500/6000], Loss: 0.0195
Epoch [20/30], Batch [1600/6000], Loss: 0.0280
Epoch [20/30], Batch [1700/6000], Loss: 0.3627
Epoch [20/30], Batch [1800/6000], Loss: 0.0271
Epoch [20/30], Batch [1900/6000], Loss: 0.0921
Epoch [20/30], Batch [2000/6000], Loss: 0.0169
Epoch [20/30], Batch [2100/6000], Loss: 0.0195
Epoch [20/30], Batch [2200/6000], Loss: 0.0173
Epoch [20/30], Batch [2300/6000], Loss: 0.0233
Epoch [20/30], Batch [2400/6000], Loss: 0.0200
Epoch [20/30], Batch [2500/6000], Loss: 0.0159
Epoch [20/30], Batch [2600/6000], Loss: 0.0285
Epoch [20/30], Batch [2700/6000], Loss: 0.0209
Epoch [20/30], Batch [2800/6000], Loss: 0.0283
Epoch [20/30], Batch [2900/6000], Loss: 0.0229
Epoch [20/30], Batch [3000/6000], Loss: 0.0208
Epoch [20/30], Batch [3100/6000], Loss: 0.0197
Epoch [20/30], Batch [3200/6000], Loss: 0.0250
Epoch [20/30], Batch [3300/6000], Loss: 0.0200
Epoch [20/30], Batch [3400/6000], Loss: 0.3290
Epoch [20/30], Batch [3500/6000], Loss: 0.0201
Epoch [20/30], Batch [3600/6000], Loss: 0.0244
Epoch [20/30], Batch [3700/6000], Loss: 0.1084
Epoch [20/30], Batch [3800/6000], Loss: 0.0204
Epoch [20/30], Batch [3900/6000], Loss: 0.0249
Epoch [20/30], Batch [4000/6000], Loss: 0.0234
Epoch [20/30], Batch [4100/6000], Loss: 0.0208
Epoch [20/30], Batch [4200/6000], Loss: 0.0212
Epoch [20/30], Batch [4300/6000], Loss: 0.0224
Epoch [20/30], Batch [4400/6000], Loss: 0.0238
Epoch [20/30], Batch [4500/6000], Loss: 0.0174
Epoch [20/30], Batch [4600/6000], Loss: 0.0174
Epoch [20/30], Batch [4700/6000], Loss: 0.0411
Epoch [20/30], Batch [4800/6000], Loss: 0.0200
Epoch [20/30], Batch [4900/6000], Loss: 0.0182
Epoch [20/30], Batch [5000/6000], Loss: 0.0206
Epoch [20/30], Batch [5100/6000], Loss: 0.0230
Epoch [20/30], Batch [5200/6000], Loss: 0.0177
Epoch [20/30], Batch [5300/6000], Loss: 0.0268
Epoch [20/30], Batch [5400/6000], Loss: 0.0151
Epoch [20/30], Batch [5500/6000], Loss: 0.0191
Epoch [20/30], Batch [5600/6000], Loss: 0.0237
Epoch [20/30], Batch [5700/6000], Loss: 0.0176
Epoch [20/30], Batch [5800/6000], Loss: 0.0184
Epoch [20/30], Batch [5900/6000], Loss: 0.0349
Epoch [20/30], Loss: 0.0351
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.0263
Epoch [21/30], Batch [100/6000], Loss: 0.0188
Epoch [21/30], Batch [200/6000], Loss: 0.0253
Epoch [21/30], Batch [300/6000], Loss: 0.0208
Epoch [21/30], Batch [400/6000], Loss: 0.0213
Epoch [21/30], Batch [500/6000], Loss: 0.0186
Epoch [21/30], Batch [600/6000], Loss: 0.0227
Epoch [21/30], Batch [700/6000], Loss: 0.0154
Epoch [21/30], Batch [800/6000], Loss: 0.0295
Epoch [21/30], Batch [900/6000], Loss: 0.0217
Epoch [21/30], Batch [1000/6000], Loss: 0.0171
Epoch [21/30], Batch [1100/6000], Loss: 0.1608
Epoch [21/30], Batch [1200/6000], Loss: 0.0612
Epoch [21/30], Batch [1300/6000], Loss: 0.0172
Epoch [21/30], Batch [1400/6000], Loss: 0.0206
Epoch [21/30], Batch [1500/6000], Loss: 0.0223
Epoch [21/30], Batch [1600/6000], Loss: 0.1272
Epoch [21/30], Batch [1700/6000], Loss: 0.0225
Epoch [21/30], Batch [1800/6000], Loss: 0.0209
Epoch [21/30], Batch [1900/6000], Loss: 0.0318
Epoch [21/30], Batch [2000/6000], Loss: 0.0245
Epoch [21/30], Batch [2100/6000], Loss: 0.0164
Epoch [21/30], Batch [2200/6000], Loss: 0.0252
Epoch [21/30], Batch [2300/6000], Loss: 0.2704
Epoch [21/30], Batch [2400/6000], Loss: 0.0198
Epoch [21/30], Batch [2500/6000], Loss: 0.0164
Epoch [21/30], Batch [2600/6000], Loss: 0.0180
Epoch [21/30], Batch [2700/6000], Loss: 0.0261
Epoch [21/30], Batch [2800/6000], Loss: 0.0257
Epoch [21/30], Batch [2900/6000], Loss: 0.0202
Epoch [21/30], Batch [3000/6000], Loss: 0.0183
Epoch [21/30], Batch [3100/6000], Loss: 0.0179
Epoch [21/30], Batch [3200/6000], Loss: 0.0235
Epoch [21/30], Batch [3300/6000], Loss: 0.0163
Epoch [21/30], Batch [3400/6000], Loss: 0.0224
Epoch [21/30], Batch [3500/6000], Loss: 0.0151
Epoch [21/30], Batch [3600/6000], Loss: 0.1688
Epoch [21/30], Batch [3700/6000], Loss: 0.0247
Epoch [21/30], Batch [3800/6000], Loss: 0.0197
Epoch [21/30], Batch [3900/6000], Loss: 0.0378
Epoch [21/30], Batch [4000/6000], Loss: 0.0182
Epoch [21/30], Batch [4100/6000], Loss: 0.0245
Epoch [21/30], Batch [4200/6000], Loss: 0.0219
Epoch [21/30], Batch [4300/6000], Loss: 0.0214
Epoch [21/30], Batch [4400/6000], Loss: 0.2796
Epoch [21/30], Batch [4500/6000], Loss: 0.0199
Epoch [21/30], Batch [4600/6000], Loss: 0.0229
Epoch [21/30], Batch [4700/6000], Loss: 0.0218
Epoch [21/30], Batch [4800/6000], Loss: 0.0240
Epoch [21/30], Batch [4900/6000], Loss: 0.0252
Epoch [21/30], Batch [5000/6000], Loss: 0.0190
Epoch [21/30], Batch [5100/6000], Loss: 0.0234
Epoch [21/30], Batch [5200/6000], Loss: 0.0190
Epoch [21/30], Batch [5300/6000], Loss: 0.0216
Epoch [21/30], Batch [5400/6000], Loss: 0.0210
Epoch [21/30], Batch [5500/6000], Loss: 0.0293
Epoch [21/30], Batch [5600/6000], Loss: 0.0284
Epoch [21/30], Batch [5700/6000], Loss: 0.0245
Epoch [21/30], Batch [5800/6000], Loss: 0.0228
Epoch [21/30], Batch [5900/6000], Loss: 0.0172
Epoch [21/30], Loss: 0.0341
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.0171
Epoch [22/30], Batch [100/6000], Loss: 0.0165
Epoch [22/30], Batch [200/6000], Loss: 0.0214
Epoch [22/30], Batch [300/6000], Loss: 0.0192
Epoch [22/30], Batch [400/6000], Loss: 0.0222
Epoch [22/30], Batch [500/6000], Loss: 0.0183
Epoch [22/30], Batch [600/6000], Loss: 0.0203
Epoch [22/30], Batch [700/6000], Loss: 0.0342
Epoch [22/30], Batch [800/6000], Loss: 0.0198
Epoch [22/30], Batch [900/6000], Loss: 0.0180
Epoch [22/30], Batch [1000/6000], Loss: 0.0222
Epoch [22/30], Batch [1100/6000], Loss: 0.0223
Epoch [22/30], Batch [1200/6000], Loss: 0.0196
Epoch [22/30], Batch [1300/6000], Loss: 0.0404
Epoch [22/30], Batch [1400/6000], Loss: 0.0156
Epoch [22/30], Batch [1500/6000], Loss: 0.0164
Epoch [22/30], Batch [1600/6000], Loss: 0.0179
Epoch [22/30], Batch [1700/6000], Loss: 0.0358
Epoch [22/30], Batch [1800/6000], Loss: 0.0189
Epoch [22/30], Batch [1900/6000], Loss: 0.0205
Epoch [22/30], Batch [2000/6000], Loss: 0.0579
Epoch [22/30], Batch [2100/6000], Loss: 0.0222
Epoch [22/30], Batch [2200/6000], Loss: 0.0245
Epoch [22/30], Batch [2300/6000], Loss: 0.0216
Epoch [22/30], Batch [2400/6000], Loss: 0.0189
Epoch [22/30], Batch [2500/6000], Loss: 0.0184
Epoch [22/30], Batch [2600/6000], Loss: 0.0317
Epoch [22/30], Batch [2700/6000], Loss: 0.0236
Epoch [22/30], Batch [2800/6000], Loss: 0.0204
Epoch [22/30], Batch [2900/6000], Loss: 0.0194
Epoch [22/30], Batch [3000/6000], Loss: 0.0143
Epoch [22/30], Batch [3100/6000], Loss: 0.0191
Epoch [22/30], Batch [3200/6000], Loss: 0.0295
Epoch [22/30], Batch [3300/6000], Loss: 0.0209
Epoch [22/30], Batch [3400/6000], Loss: 0.0199
Epoch [22/30], Batch [3500/6000], Loss: 0.0187
Epoch [22/30], Batch [3600/6000], Loss: 0.1406
Epoch [22/30], Batch [3700/6000], Loss: 0.0790
Epoch [22/30], Batch [3800/6000], Loss: 0.0418
Epoch [22/30], Batch [3900/6000], Loss: 0.0196
Epoch [22/30], Batch [4000/6000], Loss: 0.0242
Epoch [22/30], Batch [4100/6000], Loss: 0.0160
Epoch [22/30], Batch [4200/6000], Loss: 0.2229
Epoch [22/30], Batch [4300/6000], Loss: 0.0180
Epoch [22/30], Batch [4400/6000], Loss: 0.0196
Epoch [22/30], Batch [4500/6000], Loss: 0.0472
Epoch [22/30], Batch [4600/6000], Loss: 0.0229
Epoch [22/30], Batch [4700/6000], Loss: 0.0198
Epoch [22/30], Batch [4800/6000], Loss: 0.0762
Epoch [22/30], Batch [4900/6000], Loss: 0.0232
Epoch [22/30], Batch [5000/6000], Loss: 0.0208
Epoch [22/30], Batch [5100/6000], Loss: 0.0983
Epoch [22/30], Batch [5200/6000], Loss: 0.0301
Epoch [22/30], Batch [5300/6000], Loss: 0.0183
Epoch [22/30], Batch [5400/6000], Loss: 0.0208
Epoch [22/30], Batch [5500/6000], Loss: 0.0188
Epoch [22/30], Batch [5600/6000], Loss: 0.0218
Epoch [22/30], Batch [5700/6000], Loss: 0.0250
Epoch [22/30], Batch [5800/6000], Loss: 0.0239
Epoch [22/30], Batch [5900/6000], Loss: 0.0137
Epoch [22/30], Loss: 0.0327
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.0201
Epoch [23/30], Batch [100/6000], Loss: 0.0188
Epoch [23/30], Batch [200/6000], Loss: 0.0223
Epoch [23/30], Batch [300/6000], Loss: 0.0210
Epoch [23/30], Batch [400/6000], Loss: 0.0416
Epoch [23/30], Batch [500/6000], Loss: 0.0154
Epoch [23/30], Batch [600/6000], Loss: 0.0198
Epoch [23/30], Batch [700/6000], Loss: 0.0232
Epoch [23/30], Batch [800/6000], Loss: 0.0160
Epoch [23/30], Batch [900/6000], Loss: 0.0198
Epoch [23/30], Batch [1000/6000], Loss: 0.0163
Epoch [23/30], Batch [1100/6000], Loss: 0.0198
Epoch [23/30], Batch [1200/6000], Loss: 0.0237
Epoch [23/30], Batch [1300/6000], Loss: 0.0207
Epoch [23/30], Batch [1400/6000], Loss: 0.0189
Epoch [23/30], Batch [1500/6000], Loss: 0.0259
Epoch [23/30], Batch [1600/6000], Loss: 0.0215
Epoch [23/30], Batch [1700/6000], Loss: 0.0212
Epoch [23/30], Batch [1800/6000], Loss: 0.0203
Epoch [23/30], Batch [1900/6000], Loss: 0.0174
Epoch [23/30], Batch [2000/6000], Loss: 0.0244
Epoch [23/30], Batch [2100/6000], Loss: 0.0266
Epoch [23/30], Batch [2200/6000], Loss: 0.0187
Epoch [23/30], Batch [2300/6000], Loss: 0.0201
Epoch [23/30], Batch [2400/6000], Loss: 0.0174
Epoch [23/30], Batch [2500/6000], Loss: 0.0182
Epoch [23/30], Batch [2600/6000], Loss: 0.0258
Epoch [23/30], Batch [2700/6000], Loss: 0.0177
Epoch [23/30], Batch [2800/6000], Loss: 0.0163
Epoch [23/30], Batch [2900/6000], Loss: 0.0221
Epoch [23/30], Batch [3000/6000], Loss: 0.0143
Epoch [23/30], Batch [3100/6000], Loss: 0.0233
Epoch [23/30], Batch [3200/6000], Loss: 0.0216
Epoch [23/30], Batch [3300/6000], Loss: 0.0162
Epoch [23/30], Batch [3400/6000], Loss: 0.0168
Epoch [23/30], Batch [3500/6000], Loss: 0.0221
Epoch [23/30], Batch [3600/6000], Loss: 0.0223
Epoch [23/30], Batch [3700/6000], Loss: 0.0647
Epoch [23/30], Batch [3800/6000], Loss: 0.0211
Epoch [23/30], Batch [3900/6000], Loss: 0.0187
Epoch [23/30], Batch [4000/6000], Loss: 0.0171
Epoch [23/30], Batch [4100/6000], Loss: 0.0173
Epoch [23/30], Batch [4200/6000], Loss: 0.0192
Epoch [23/30], Batch [4300/6000], Loss: 0.0206
Epoch [23/30], Batch [4400/6000], Loss: 0.0818
Epoch [23/30], Batch [4500/6000], Loss: 0.0228
Epoch [23/30], Batch [4600/6000], Loss: 0.0788
Epoch [23/30], Batch [4700/6000], Loss: 0.0209
Epoch [23/30], Batch [4800/6000], Loss: 0.0249
Epoch [23/30], Batch [4900/6000], Loss: 0.0249
Epoch [23/30], Batch [5000/6000], Loss: 0.0240
Epoch [23/30], Batch [5100/6000], Loss: 0.0195
Epoch [23/30], Batch [5200/6000], Loss: 0.0375
Epoch [23/30], Batch [5300/6000], Loss: 0.0180
Epoch [23/30], Batch [5400/6000], Loss: 0.0346
Epoch [23/30], Batch [5500/6000], Loss: 0.0208
Epoch [23/30], Batch [5600/6000], Loss: 0.0182
Epoch [23/30], Batch [5700/6000], Loss: 0.0229
Epoch [23/30], Batch [5800/6000], Loss: 0.0186
Epoch [23/30], Batch [5900/6000], Loss: 0.0174
Epoch [23/30], Loss: 0.0322
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.0451
Epoch [24/30], Batch [100/6000], Loss: 0.0232
Epoch [24/30], Batch [200/6000], Loss: 0.0143
Epoch [24/30], Batch [300/6000], Loss: 0.0180
Epoch [24/30], Batch [400/6000], Loss: 0.0180
Epoch [24/30], Batch [500/6000], Loss: 0.4715
Epoch [24/30], Batch [600/6000], Loss: 0.0169
Epoch [24/30], Batch [700/6000], Loss: 0.0177
Epoch [24/30], Batch [800/6000], Loss: 0.0253
Epoch [24/30], Batch [900/6000], Loss: 0.0193
Epoch [24/30], Batch [1000/6000], Loss: 0.0176
Epoch [24/30], Batch [1100/6000], Loss: 0.0132
Epoch [24/30], Batch [1200/6000], Loss: 0.0233
Epoch [24/30], Batch [1300/6000], Loss: 0.0189
Epoch [24/30], Batch [1400/6000], Loss: 0.0216
Epoch [24/30], Batch [1500/6000], Loss: 0.0174
Epoch [24/30], Batch [1600/6000], Loss: 0.0172
Epoch [24/30], Batch [1700/6000], Loss: 0.0185
Epoch [24/30], Batch [1800/6000], Loss: 0.0159
Epoch [24/30], Batch [1900/6000], Loss: 0.0458
Epoch [24/30], Batch [2000/6000], Loss: 0.0240
Epoch [24/30], Batch [2100/6000], Loss: 0.0216
Epoch [24/30], Batch [2200/6000], Loss: 0.0160
Epoch [24/30], Batch [2300/6000], Loss: 0.0208
Epoch [24/30], Batch [2400/6000], Loss: 0.0170
Epoch [24/30], Batch [2500/6000], Loss: 0.0211
Epoch [24/30], Batch [2600/6000], Loss: 0.0252
Epoch [24/30], Batch [2700/6000], Loss: 0.0150
Epoch [24/30], Batch [2800/6000], Loss: 0.0159
Epoch [24/30], Batch [2900/6000], Loss: 0.0147
Epoch [24/30], Batch [3000/6000], Loss: 0.0163
Epoch [24/30], Batch [3100/6000], Loss: 0.0186
Epoch [24/30], Batch [3200/6000], Loss: 0.0226
Epoch [24/30], Batch [3300/6000], Loss: 0.0206
Epoch [24/30], Batch [3400/6000], Loss: 0.0165
Epoch [24/30], Batch [3500/6000], Loss: 0.0172
Epoch [24/30], Batch [3600/6000], Loss: 0.0197
Epoch [24/30], Batch [3700/6000], Loss: 0.0213
Epoch [24/30], Batch [3800/6000], Loss: 0.0191
Epoch [24/30], Batch [3900/6000], Loss: 0.0175
Epoch [24/30], Batch [4000/6000], Loss: 0.0435
Epoch [24/30], Batch [4100/6000], Loss: 0.0186
Epoch [24/30], Batch [4200/6000], Loss: 0.0235
Epoch [24/30], Batch [4300/6000], Loss: 0.0180
Epoch [24/30], Batch [4400/6000], Loss: 0.0200
Epoch [24/30], Batch [4500/6000], Loss: 0.0205
Epoch [24/30], Batch [4600/6000], Loss: 0.0206
Epoch [24/30], Batch [4700/6000], Loss: 0.0185
Epoch [24/30], Batch [4800/6000], Loss: 0.0534
Epoch [24/30], Batch [4900/6000], Loss: 0.0587
Epoch [24/30], Batch [5000/6000], Loss: 0.0186
Epoch [24/30], Batch [5100/6000], Loss: 0.0170
Epoch [24/30], Batch [5200/6000], Loss: 0.0242
Epoch [24/30], Batch [5300/6000], Loss: 0.0184
Epoch [24/30], Batch [5400/6000], Loss: 0.0236
Epoch [24/30], Batch [5500/6000], Loss: 0.0203
Epoch [24/30], Batch [5600/6000], Loss: 0.0203
Epoch [24/30], Batch [5700/6000], Loss: 0.0195
Epoch [24/30], Batch [5800/6000], Loss: 0.7166
Epoch [24/30], Batch [5900/6000], Loss: 0.0170
Epoch [24/30], Loss: 0.0309
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.0200
Epoch [25/30], Batch [100/6000], Loss: 0.0181
Epoch [25/30], Batch [200/6000], Loss: 0.0153
Epoch [25/30], Batch [300/6000], Loss: 0.0150
Epoch [25/30], Batch [400/6000], Loss: 0.0142
Epoch [25/30], Batch [500/6000], Loss: 0.0201
Epoch [25/30], Batch [600/6000], Loss: 0.0188
Epoch [25/30], Batch [700/6000], Loss: 0.0210
Epoch [25/30], Batch [800/6000], Loss: 0.0197
Epoch [25/30], Batch [900/6000], Loss: 0.0209
Epoch [25/30], Batch [1000/6000], Loss: 0.0197
Epoch [25/30], Batch [1100/6000], Loss: 0.0216
Epoch [25/30], Batch [1200/6000], Loss: 0.0183
Epoch [25/30], Batch [1300/6000], Loss: 0.0171
Epoch [25/30], Batch [1400/6000], Loss: 0.0164
Epoch [25/30], Batch [1500/6000], Loss: 0.0257
Epoch [25/30], Batch [1600/6000], Loss: 0.0203
Epoch [25/30], Batch [1700/6000], Loss: 0.0156
Epoch [25/30], Batch [1800/6000], Loss: 0.0244
Epoch [25/30], Batch [1900/6000], Loss: 0.0272
Epoch [25/30], Batch [2000/6000], Loss: 0.0137
Epoch [25/30], Batch [2100/6000], Loss: 0.0188
Epoch [25/30], Batch [2200/6000], Loss: 0.0225
Epoch [25/30], Batch [2300/6000], Loss: 0.0209
Epoch [25/30], Batch [2400/6000], Loss: 0.0222
Epoch [25/30], Batch [2500/6000], Loss: 0.0171
Epoch [25/30], Batch [2600/6000], Loss: 0.0169
Epoch [25/30], Batch [2700/6000], Loss: 0.0175
Epoch [25/30], Batch [2800/6000], Loss: 0.0187
Epoch [25/30], Batch [2900/6000], Loss: 0.0246
Epoch [25/30], Batch [3000/6000], Loss: 0.0175
Epoch [25/30], Batch [3100/6000], Loss: 0.0161
Epoch [25/30], Batch [3200/6000], Loss: 0.0224
Epoch [25/30], Batch [3300/6000], Loss: 0.0190
Epoch [25/30], Batch [3400/6000], Loss: 0.0195
Epoch [25/30], Batch [3500/6000], Loss: 0.0190
Epoch [25/30], Batch [3600/6000], Loss: 0.0234
Epoch [25/30], Batch [3700/6000], Loss: 0.0231
Epoch [25/30], Batch [3800/6000], Loss: 0.0266
Epoch [25/30], Batch [3900/6000], Loss: 0.0151
Epoch [25/30], Batch [4000/6000], Loss: 0.0183
Epoch [25/30], Batch [4100/6000], Loss: 0.0167
Epoch [25/30], Batch [4200/6000], Loss: 0.0229
Epoch [25/30], Batch [4300/6000], Loss: 0.0213
Epoch [25/30], Batch [4400/6000], Loss: 0.1144
Epoch [25/30], Batch [4500/6000], Loss: 0.0206
Epoch [25/30], Batch [4600/6000], Loss: 0.0383
Epoch [25/30], Batch [4700/6000], Loss: 0.0211
Epoch [25/30], Batch [4800/6000], Loss: 0.0183
Epoch [25/30], Batch [4900/6000], Loss: 0.0210
Epoch [25/30], Batch [5000/6000], Loss: 0.0162
Epoch [25/30], Batch [5100/6000], Loss: 0.0137
Epoch [25/30], Batch [5200/6000], Loss: 0.0176
Epoch [25/30], Batch [5300/6000], Loss: 0.0206
Epoch [25/30], Batch [5400/6000], Loss: 0.0210
Epoch [25/30], Batch [5500/6000], Loss: 0.0176
Epoch [25/30], Batch [5600/6000], Loss: 0.0157
Epoch [25/30], Batch [5700/6000], Loss: 0.0151
Epoch [25/30], Batch [5800/6000], Loss: 0.0195
Epoch [25/30], Batch [5900/6000], Loss: 0.0238
Epoch [25/30], Loss: 0.0297
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.0197
Epoch [26/30], Batch [100/6000], Loss: 0.0183
Epoch [26/30], Batch [200/6000], Loss: 0.0218
Epoch [26/30], Batch [300/6000], Loss: 0.0183
Epoch [26/30], Batch [400/6000], Loss: 0.0210
Epoch [26/30], Batch [500/6000], Loss: 0.0179
Epoch [26/30], Batch [600/6000], Loss: 0.0163
Epoch [26/30], Batch [700/6000], Loss: 0.0217
Epoch [26/30], Batch [800/6000], Loss: 0.0182
Epoch [26/30], Batch [900/6000], Loss: 0.0203
Epoch [26/30], Batch [1000/6000], Loss: 0.0180
Epoch [26/30], Batch [1100/6000], Loss: 0.0197
Epoch [26/30], Batch [1200/6000], Loss: 0.0190
Epoch [26/30], Batch [1300/6000], Loss: 0.0754
Epoch [26/30], Batch [1400/6000], Loss: 0.0719
Epoch [26/30], Batch [1500/6000], Loss: 0.0190
Epoch [26/30], Batch [1600/6000], Loss: 0.0183
Epoch [26/30], Batch [1700/6000], Loss: 0.0438
Epoch [26/30], Batch [1800/6000], Loss: 0.0289
Epoch [26/30], Batch [1900/6000], Loss: 0.0170
Epoch [26/30], Batch [2000/6000], Loss: 0.0175
Epoch [26/30], Batch [2100/6000], Loss: 0.0410
Epoch [26/30], Batch [2200/6000], Loss: 0.0198
Epoch [26/30], Batch [2300/6000], Loss: 0.0222
Epoch [26/30], Batch [2400/6000], Loss: 0.3053
Epoch [26/30], Batch [2500/6000], Loss: 0.0200
Epoch [26/30], Batch [2600/6000], Loss: 0.0159
Epoch [26/30], Batch [2700/6000], Loss: 0.1741
Epoch [26/30], Batch [2800/6000], Loss: 0.0197
Epoch [26/30], Batch [2900/6000], Loss: 0.0187
Epoch [26/30], Batch [3000/6000], Loss: 0.0208
Epoch [26/30], Batch [3100/6000], Loss: 0.0149
Epoch [26/30], Batch [3200/6000], Loss: 0.0187
Epoch [26/30], Batch [3300/6000], Loss: 0.0208
Epoch [26/30], Batch [3400/6000], Loss: 0.0184
Epoch [26/30], Batch [3500/6000], Loss: 0.0359
Epoch [26/30], Batch [3600/6000], Loss: 0.0167
Epoch [26/30], Batch [3700/6000], Loss: 0.0207
Epoch [26/30], Batch [3800/6000], Loss: 0.0190
Epoch [26/30], Batch [3900/6000], Loss: 0.0154
Epoch [26/30], Batch [4000/6000], Loss: 0.0220
Epoch [26/30], Batch [4100/6000], Loss: 0.0182
Epoch [26/30], Batch [4200/6000], Loss: 0.0200
Epoch [26/30], Batch [4300/6000], Loss: 0.0212
Epoch [26/30], Batch [4400/6000], Loss: 0.0176
Epoch [26/30], Batch [4500/6000], Loss: 0.0182
Epoch [26/30], Batch [4600/6000], Loss: 0.2101
Epoch [26/30], Batch [4700/6000], Loss: 0.0134
Epoch [26/30], Batch [4800/6000], Loss: 0.0175
Epoch [26/30], Batch [4900/6000], Loss: 0.0171
Epoch [26/30], Batch [5000/6000], Loss: 0.0152
Epoch [26/30], Batch [5100/6000], Loss: 0.0155
Epoch [26/30], Batch [5200/6000], Loss: 0.0224
Epoch [26/30], Batch [5300/6000], Loss: 0.0152
Epoch [26/30], Batch [5400/6000], Loss: 0.0146
Epoch [26/30], Batch [5500/6000], Loss: 0.0193
Epoch [26/30], Batch [5600/6000], Loss: 0.0264
Epoch [26/30], Batch [5700/6000], Loss: 0.0293
Epoch [26/30], Batch [5800/6000], Loss: 0.0306
Epoch [26/30], Batch [5900/6000], Loss: 0.0196
Epoch [26/30], Loss: 0.0297
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0190
Epoch [27/30], Batch [100/6000], Loss: 0.0189
Epoch [27/30], Batch [200/6000], Loss: 0.0174
Epoch [27/30], Batch [300/6000], Loss: 0.0166
Epoch [27/30], Batch [400/6000], Loss: 0.0169
Epoch [27/30], Batch [500/6000], Loss: 0.0178
Epoch [27/30], Batch [600/6000], Loss: 0.0178
Epoch [27/30], Batch [700/6000], Loss: 0.0163
Epoch [27/30], Batch [800/6000], Loss: 0.0156
Epoch [27/30], Batch [900/6000], Loss: 0.0301
Epoch [27/30], Batch [1000/6000], Loss: 0.0153
Epoch [27/30], Batch [1100/6000], Loss: 0.0171
Epoch [27/30], Batch [1200/6000], Loss: 0.0168
Epoch [27/30], Batch [1300/6000], Loss: 0.0255
Epoch [27/30], Batch [1400/6000], Loss: 0.0182
Epoch [27/30], Batch [1500/6000], Loss: 0.0158
Epoch [27/30], Batch [1600/6000], Loss: 0.0194
Epoch [27/30], Batch [1700/6000], Loss: 0.0147
Epoch [27/30], Batch [1800/6000], Loss: 0.0502
Epoch [27/30], Batch [1900/6000], Loss: 0.0208
Epoch [27/30], Batch [2000/6000], Loss: 0.0242
Epoch [27/30], Batch [2100/6000], Loss: 0.0199
Epoch [27/30], Batch [2200/6000], Loss: 0.0292
Epoch [27/30], Batch [2300/6000], Loss: 0.0254
Epoch [27/30], Batch [2400/6000], Loss: 0.0173
Epoch [27/30], Batch [2500/6000], Loss: 0.0180
Epoch [27/30], Batch [2600/6000], Loss: 0.0151
Epoch [27/30], Batch [2700/6000], Loss: 0.0185
Epoch [27/30], Batch [2800/6000], Loss: 0.0176
Epoch [27/30], Batch [2900/6000], Loss: 0.0499
Epoch [27/30], Batch [3000/6000], Loss: 0.0315
Epoch [27/30], Batch [3100/6000], Loss: 0.0184
Epoch [27/30], Batch [3200/6000], Loss: 0.0162
Epoch [27/30], Batch [3300/6000], Loss: 0.0193
Epoch [27/30], Batch [3400/6000], Loss: 0.0211
Epoch [27/30], Batch [3500/6000], Loss: 0.0161
Epoch [27/30], Batch [3600/6000], Loss: 0.0188
Epoch [27/30], Batch [3700/6000], Loss: 0.0169
Epoch [27/30], Batch [3800/6000], Loss: 0.0189
Epoch [27/30], Batch [3900/6000], Loss: 0.2105
Epoch [27/30], Batch [4000/6000], Loss: 0.0185
Epoch [27/30], Batch [4100/6000], Loss: 0.0203
Epoch [27/30], Batch [4200/6000], Loss: 0.0211
Epoch [27/30], Batch [4300/6000], Loss: 0.0194
Epoch [27/30], Batch [4400/6000], Loss: 0.0211
Epoch [27/30], Batch [4500/6000], Loss: 0.0191
Epoch [27/30], Batch [4600/6000], Loss: 0.0222
Epoch [27/30], Batch [4700/6000], Loss: 0.0182
Epoch [27/30], Batch [4800/6000], Loss: 0.0199
Epoch [27/30], Batch [4900/6000], Loss: 0.0138
Epoch [27/30], Batch [5000/6000], Loss: 0.0186
Epoch [27/30], Batch [5100/6000], Loss: 0.0215
Epoch [27/30], Batch [5200/6000], Loss: 0.0196
Epoch [27/30], Batch [5300/6000], Loss: 0.0207
Epoch [27/30], Batch [5400/6000], Loss: 0.0237
Epoch [27/30], Batch [5500/6000], Loss: 0.0259
Epoch [27/30], Batch [5600/6000], Loss: 0.0175
Epoch [27/30], Batch [5700/6000], Loss: 0.0208
Epoch [27/30], Batch [5800/6000], Loss: 0.0168
Epoch [27/30], Batch [5900/6000], Loss: 0.0177
Epoch [27/30], Loss: 0.0282
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.0631
Epoch [28/30], Batch [100/6000], Loss: 0.0233
Epoch [28/30], Batch [200/6000], Loss: 0.0184
Epoch [28/30], Batch [300/6000], Loss: 0.0195
Epoch [28/30], Batch [400/6000], Loss: 0.0157
Epoch [28/30], Batch [500/6000], Loss: 0.0260
Epoch [28/30], Batch [600/6000], Loss: 0.0155
Epoch [28/30], Batch [700/6000], Loss: 0.0233
Epoch [28/30], Batch [800/6000], Loss: 0.0169
Epoch [28/30], Batch [900/6000], Loss: 0.0388
Epoch [28/30], Batch [1000/6000], Loss: 0.0161
Epoch [28/30], Batch [1100/6000], Loss: 0.0210
Epoch [28/30], Batch [1200/6000], Loss: 0.0173
Epoch [28/30], Batch [1300/6000], Loss: 0.0216
Epoch [28/30], Batch [1400/6000], Loss: 0.0211
Epoch [28/30], Batch [1500/6000], Loss: 0.0163
Epoch [28/30], Batch [1600/6000], Loss: 0.0778
Epoch [28/30], Batch [1700/6000], Loss: 0.0178
Epoch [28/30], Batch [1800/6000], Loss: 0.0217
Epoch [28/30], Batch [1900/6000], Loss: 0.4397
Epoch [28/30], Batch [2000/6000], Loss: 0.0190
Epoch [28/30], Batch [2100/6000], Loss: 0.0194
Epoch [28/30], Batch [2200/6000], Loss: 0.0195
Epoch [28/30], Batch [2300/6000], Loss: 0.0200
Epoch [28/30], Batch [2400/6000], Loss: 0.0227
Epoch [28/30], Batch [2500/6000], Loss: 0.3236
Epoch [28/30], Batch [2600/6000], Loss: 0.0309
Epoch [28/30], Batch [2700/6000], Loss: 0.0154
Epoch [28/30], Batch [2800/6000], Loss: 0.0238
Epoch [28/30], Batch [2900/6000], Loss: 0.0186
Epoch [28/30], Batch [3000/6000], Loss: 0.0189
Epoch [28/30], Batch [3100/6000], Loss: 0.0177
Epoch [28/30], Batch [3200/6000], Loss: 0.0191
Epoch [28/30], Batch [3300/6000], Loss: 0.0184
Epoch [28/30], Batch [3400/6000], Loss: 0.0178
Epoch [28/30], Batch [3500/6000], Loss: 0.0209
Epoch [28/30], Batch [3600/6000], Loss: 0.0109
Epoch [28/30], Batch [3700/6000], Loss: 0.0132
Epoch [28/30], Batch [3800/6000], Loss: 0.0164
Epoch [28/30], Batch [3900/6000], Loss: 0.0189
Epoch [28/30], Batch [4000/6000], Loss: 0.0158
Epoch [28/30], Batch [4100/6000], Loss: 0.0167
Epoch [28/30], Batch [4200/6000], Loss: 0.0397
Epoch [28/30], Batch [4300/6000], Loss: 0.0223
Epoch [28/30], Batch [4400/6000], Loss: 0.0257
Epoch [28/30], Batch [4500/6000], Loss: 0.0178
Epoch [28/30], Batch [4600/6000], Loss: 0.0157
Epoch [28/30], Batch [4700/6000], Loss: 0.0158
Epoch [28/30], Batch [4800/6000], Loss: 0.0180
Epoch [28/30], Batch [4900/6000], Loss: 0.0132
Epoch [28/30], Batch [5000/6000], Loss: 0.0197
Epoch [28/30], Batch [5100/6000], Loss: 0.0164
Epoch [28/30], Batch [5200/6000], Loss: 0.0168
Epoch [28/30], Batch [5300/6000], Loss: 0.0183
Epoch [28/30], Batch [5400/6000], Loss: 0.0188
Epoch [28/30], Batch [5500/6000], Loss: 0.0190
Epoch [28/30], Batch [5600/6000], Loss: 0.0190
Epoch [28/30], Batch [5700/6000], Loss: 0.0167
Epoch [28/30], Batch [5800/6000], Loss: 0.0157
Epoch [28/30], Batch [5900/6000], Loss: 0.0163
Epoch [28/30], Loss: 0.0275
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0244
Epoch [29/30], Batch [100/6000], Loss: 0.0185
Epoch [29/30], Batch [200/6000], Loss: 0.0341
Epoch [29/30], Batch [300/6000], Loss: 0.0126
Epoch [29/30], Batch [400/6000], Loss: 0.0254
Epoch [29/30], Batch [500/6000], Loss: 0.0200
Epoch [29/30], Batch [600/6000], Loss: 0.0154
Epoch [29/30], Batch [700/6000], Loss: 0.0206
Epoch [29/30], Batch [800/6000], Loss: 0.2553
Epoch [29/30], Batch [900/6000], Loss: 0.0208
Epoch [29/30], Batch [1000/6000], Loss: 0.0176
Epoch [29/30], Batch [1100/6000], Loss: 0.0197
Epoch [29/30], Batch [1200/6000], Loss: 0.0158
Epoch [29/30], Batch [1300/6000], Loss: 0.0156
Epoch [29/30], Batch [1400/6000], Loss: 0.0176
Epoch [29/30], Batch [1500/6000], Loss: 0.0242
Epoch [29/30], Batch [1600/6000], Loss: 0.0187
Epoch [29/30], Batch [1700/6000], Loss: 0.0159
Epoch [29/30], Batch [1800/6000], Loss: 0.0186
Epoch [29/30], Batch [1900/6000], Loss: 0.0183
Epoch [29/30], Batch [2000/6000], Loss: 0.0161
Epoch [29/30], Batch [2100/6000], Loss: 0.0182
Epoch [29/30], Batch [2200/6000], Loss: 0.0129
Epoch [29/30], Batch [2300/6000], Loss: 0.0188
Epoch [29/30], Batch [2400/6000], Loss: 0.0183
Epoch [29/30], Batch [2500/6000], Loss: 0.0171
Epoch [29/30], Batch [2600/6000], Loss: 0.0188
Epoch [29/30], Batch [2700/6000], Loss: 0.0221
Epoch [29/30], Batch [2800/6000], Loss: 0.1167
Epoch [29/30], Batch [2900/6000], Loss: 0.0177
Epoch [29/30], Batch [3000/6000], Loss: 0.0193
Epoch [29/30], Batch [3100/6000], Loss: 0.0136
Epoch [29/30], Batch [3200/6000], Loss: 0.0177
Epoch [29/30], Batch [3300/6000], Loss: 0.0161
Epoch [29/30], Batch [3400/6000], Loss: 0.0198
Epoch [29/30], Batch [3500/6000], Loss: 0.1075
Epoch [29/30], Batch [3600/6000], Loss: 0.0163
Epoch [29/30], Batch [3700/6000], Loss: 0.0187
Epoch [29/30], Batch [3800/6000], Loss: 0.0219
Epoch [29/30], Batch [3900/6000], Loss: 0.0281
Epoch [29/30], Batch [4000/6000], Loss: 0.0189
Epoch [29/30], Batch [4100/6000], Loss: 0.0183
Epoch [29/30], Batch [4200/6000], Loss: 0.0198
Epoch [29/30], Batch [4300/6000], Loss: 0.0173
Epoch [29/30], Batch [4400/6000], Loss: 0.0183
Epoch [29/30], Batch [4500/6000], Loss: 0.0206
Epoch [29/30], Batch [4600/6000], Loss: 0.0205
Epoch [29/30], Batch [4700/6000], Loss: 0.0186
Epoch [29/30], Batch [4800/6000], Loss: 0.0137
Epoch [29/30], Batch [4900/6000], Loss: 0.0220
Epoch [29/30], Batch [5000/6000], Loss: 0.0176
Epoch [29/30], Batch [5100/6000], Loss: 0.0186
Epoch [29/30], Batch [5200/6000], Loss: 0.0135
Epoch [29/30], Batch [5300/6000], Loss: 0.0148
Epoch [29/30], Batch [5400/6000], Loss: 0.0128
Epoch [29/30], Batch [5500/6000], Loss: 0.0219
Epoch [29/30], Batch [5600/6000], Loss: 0.0177
Epoch [29/30], Batch [5700/6000], Loss: 0.0198
Epoch [29/30], Batch [5800/6000], Loss: 0.0202
Epoch [29/30], Batch [5900/6000], Loss: 0.0176
Epoch [29/30], Loss: 0.0269
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0207
Epoch [30/30], Batch [100/6000], Loss: 0.0426
Epoch [30/30], Batch [200/6000], Loss: 0.0219
Epoch [30/30], Batch [300/6000], Loss: 0.0206
Epoch [30/30], Batch [400/6000], Loss: 0.0184
Epoch [30/30], Batch [500/6000], Loss: 0.0213
Epoch [30/30], Batch [600/6000], Loss: 0.0215
Epoch [30/30], Batch [700/6000], Loss: 0.0157
Epoch [30/30], Batch [800/6000], Loss: 0.0231
Epoch [30/30], Batch [900/6000], Loss: 0.0225
Epoch [30/30], Batch [1000/6000], Loss: 0.0175
Epoch [30/30], Batch [1100/6000], Loss: 0.0207
Epoch [30/30], Batch [1200/6000], Loss: 0.0152
Epoch [30/30], Batch [1300/6000], Loss: 0.0105
Epoch [30/30], Batch [1400/6000], Loss: 0.0245
Epoch [30/30], Batch [1500/6000], Loss: 0.0205
Epoch [30/30], Batch [1600/6000], Loss: 0.0177
Epoch [30/30], Batch [1700/6000], Loss: 0.0163
Epoch [30/30], Batch [1800/6000], Loss: 0.0203
Epoch [30/30], Batch [1900/6000], Loss: 0.0163
Epoch [30/30], Batch [2000/6000], Loss: 0.0180
Epoch [30/30], Batch [2100/6000], Loss: 0.0227
Epoch [30/30], Batch [2200/6000], Loss: 0.0170
Epoch [30/30], Batch [2300/6000], Loss: 0.0173
Epoch [30/30], Batch [2400/6000], Loss: 0.0164
Epoch [30/30], Batch [2500/6000], Loss: 0.0162
Epoch [30/30], Batch [2600/6000], Loss: 0.0145
Epoch [30/30], Batch [2700/6000], Loss: 0.0163
Epoch [30/30], Batch [2800/6000], Loss: 0.0141
Epoch [30/30], Batch [2900/6000], Loss: 0.0163
Epoch [30/30], Batch [3000/6000], Loss: 0.0140
Epoch [30/30], Batch [3100/6000], Loss: 0.1209
Epoch [30/30], Batch [3200/6000], Loss: 0.0197
Epoch [30/30], Batch [3300/6000], Loss: 0.0151
Epoch [30/30], Batch [3400/6000], Loss: 0.0194
Epoch [30/30], Batch [3500/6000], Loss: 0.0207
Epoch [30/30], Batch [3600/6000], Loss: 0.1105
Epoch [30/30], Batch [3700/6000], Loss: 0.0246
Epoch [30/30], Batch [3800/6000], Loss: 0.0175
Epoch [30/30], Batch [3900/6000], Loss: 0.0209
Epoch [30/30], Batch [4000/6000], Loss: 0.0215
Epoch [30/30], Batch [4100/6000], Loss: 0.0292
Epoch [30/30], Batch [4200/6000], Loss: 0.0314
Epoch [30/30], Batch [4300/6000], Loss: 0.0157
Epoch [30/30], Batch [4400/6000], Loss: 0.0214
Epoch [30/30], Batch [4500/6000], Loss: 0.0183
Epoch [30/30], Batch [4600/6000], Loss: 0.0159
Epoch [30/30], Batch [4700/6000], Loss: 0.0188
Epoch [30/30], Batch [4800/6000], Loss: 0.0156
Epoch [30/30], Batch [4900/6000], Loss: 0.1318
Epoch [30/30], Batch [5000/6000], Loss: 0.0195
Epoch [30/30], Batch [5100/6000], Loss: 0.0170
Epoch [30/30], Batch [5200/6000], Loss: 0.0302
Epoch [30/30], Batch [5300/6000], Loss: 0.0366
Epoch [30/30], Batch [5400/6000], Loss: 0.0162
Epoch [30/30], Batch [5500/6000], Loss: 0.0178
Epoch [30/30], Batch [5600/6000], Loss: 0.0224
Epoch [30/30], Batch [5700/6000], Loss: 0.0189
Epoch [30/30], Batch [5800/6000], Loss: 0.0146
Epoch [30/30], Batch [5900/6000], Loss: 0.0210
Epoch [30/30], Loss: 0.0260
Visualization saved to figures/visualization_0.png
Test Loss: 0.1173, Accuracy: 98.19%
  Output probs: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 1.1660
  Image Loss: 0.0111
  Total Loss: 1165.9713
  Image grad max: 227.96743774414062
  Output probs: [[0.017 0.    0.    0.    0.    0.    0.    0.983 0.    0.   ]]
Adversarial Training Loop 2/300:
  Label Loss: 1.1535
  Image Loss: 0.5300
  Total Loss: 1154.0616
  Image grad max: 122.66365814208984
  Output probs: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 3/300:
  Label Loss: 1.1772
  Image Loss: 0.4866
  Total Loss: 1177.7133
  Image grad max: 41.86515426635742
  Output probs: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 4/300:
  Label Loss: 1.1756
  Image Loss: 0.4924
  Total Loss: 1176.1014
  Image grad max: 49.369266510009766
  Output probs: [[0.999 0.    0.    0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 5/300:
  Label Loss: 0.9029
  Image Loss: 0.4939
  Total Loss: 903.3580
  Image grad max: 55.00823974609375
  Output probs: [[0.    0.    0.003 0.996 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 6/300:
  Label Loss: 1.0912
  Image Loss: 0.4932
  Total Loss: 1091.7219
  Image grad max: 128.7378692626953
  Output probs: [[0.72  0.    0.267 0.001 0.011 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 7/300:
  Label Loss: 0.6815
  Image Loss: 0.4937
  Total Loss: 681.9579
  Image grad max: 30.289737701416016
  Output probs: [[0.227 0.    0.531 0.    0.242 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 8/300:
  Label Loss: 0.7302
  Image Loss: 0.5000
  Total Loss: 730.6561
  Image grad max: 43.69624710083008
  Output probs: [[0.671 0.    0.066 0.    0.263 0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 9/300:
  Label Loss: 0.6760
  Image Loss: 0.5043
  Total Loss: 676.5432
  Image grad max: 17.55885887145996
  Output probs: [[0.722 0.    0.019 0.    0.258 0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 10/300:
  Label Loss: 0.6520
  Image Loss: 0.5086
  Total Loss: 652.4620
  Image grad max: 19.39224624633789
  Output probs: [[0.534 0.    0.016 0.    0.446 0.    0.004 0.    0.    0.   ]]
Adversarial Training Loop 11/300:
  Label Loss: 0.6017
  Image Loss: 0.5123
  Total Loss: 602.2305
  Image grad max: 28.7332763671875
  Output probs: [[0.838 0.    0.01  0.    0.132 0.    0.019 0.    0.    0.   ]]
Adversarial Training Loop 12/300:
  Label Loss: 0.5110
  Image Loss: 0.5126
  Total Loss: 511.4706
  Image grad max: 30.32628631591797
  Output probs: [[0.192 0.    0.014 0.    0.327 0.001 0.445 0.013 0.    0.01 ]]
Adversarial Training Loop 13/300:
  Label Loss: 0.4084
  Image Loss: 0.5134
  Total Loss: 408.9013
  Image grad max: 52.81806182861328
  Output probs: [[0.764 0.004 0.01  0.019 0.005 0.055 0.074 0.067 0.    0.001]]
Adversarial Training Loop 14/300:
  Label Loss: 0.2179
  Image Loss: 0.5109
  Total Loss: 218.3867
  Image grad max: 37.250240325927734
  Output probs: [[0.001 0.003 0.    0.066 0.    0.922 0.    0.008 0.    0.   ]]
Adversarial Training Loop 15/300:
  Label Loss: 0.5853
  Image Loss: 0.5085
  Total Loss: 585.8279
  Image grad max: 138.1458282470703
  Output probs: [[0.278 0.023 0.055 0.007 0.    0.    0.015 0.622 0.    0.   ]]
Adversarial Training Loop 16/300:
  Label Loss: 0.1836
  Image Loss: 0.5104
  Total Loss: 184.1254
  Image grad max: 38.37794876098633
  Output probs: [[0.287 0.04  0.389 0.001 0.    0.    0.045 0.238 0.    0.   ]]
Adversarial Training Loop 17/300:
  Label Loss: 0.1540
  Image Loss: 0.5126
  Total Loss: 154.4633
  Image grad max: 33.05253601074219
  Output probs: [[0.313 0.261 0.315 0.001 0.    0.    0.021 0.089 0.    0.   ]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0558
  Image Loss: 0.5175
  Total Loss: 56.3632
  Image grad max: 25.749588012695312
  Output probs: [[0.059 0.91  0.024 0.    0.    0.    0.001 0.006 0.    0.   ]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0771
  Image Loss: 0.5208
  Total Loss: 77.6244
  Image grad max: 51.7755126953125
  Output probs: [[0.993 0.005 0.001 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 20/300:
  Label Loss: 0.1935
  Image Loss: 0.5194
  Total Loss: 194.0198
  Image grad max: 62.412109375
  Output probs: [[0.734 0.257 0.008 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0141
  Image Loss: 0.5259
  Total Loss: 14.6411
  Image grad max: 30.60243034362793
  Output probs: [[0.001 0.998 0.001 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 22/300:
  Label Loss: 0.2671
  Image Loss: 0.5267
  Total Loss: 267.6588
  Image grad max: 69.30372619628906
  Output probs: [[0.231 0.762 0.007 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0176
  Image Loss: 0.5297
  Total Loss: 18.1463
  Image grad max: 35.965240478515625
  Output probs: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 24/300:
  Label Loss: 0.3827
  Image Loss: 0.5201
  Total Loss: 383.1920
  Image grad max: 56.28356170654297
  Output probs: [[0.998 0.001 0.    0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 25/300:
  Label Loss: 0.2643
  Image Loss: 0.5280
  Total Loss: 264.8745
  Image grad max: 57.164024353027344
  Output probs: [[0.066 0.929 0.005 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0706
  Image Loss: 0.5279
  Total Loss: 71.0841
  Image grad max: 54.38910675048828
  Output probs: [[0.237 0.754 0.008 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0167
  Image Loss: 0.5323
  Total Loss: 17.2393
  Image grad max: 31.909975051879883
  Output probs: [[0.988 0.011 0.001 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 28/300:
  Label Loss: 0.1565
  Image Loss: 0.5314
  Total Loss: 157.0749
  Image grad max: 53.89621353149414
  Output probs: [[0.639 0.352 0.008 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0053
  Image Loss: 0.5349
  Total Loss: 5.7916
  Image grad max: 16.16080093383789
  Output probs: [[0.017 0.979 0.003 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 30/300:
  Label Loss: 0.1342
  Image Loss: 0.5320
  Total Loss: 134.7452
  Image grad max: 58.205238342285156
  Output probs: [[0.687 0.305 0.006 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0088
  Image Loss: 0.5338
  Total Loss: 9.2975
  Image grad max: 21.589828491210938
  Output probs: [[0.611 0.38  0.006 0.    0.    0.    0.    0.002 0.    0.   ]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0036
  Image Loss: 0.5363
  Total Loss: 4.1587
  Image grad max: 13.018928527832031
  Output probs: [[0.257 0.734 0.007 0.    0.    0.    0.    0.002 0.    0.   ]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0141
  Image Loss: 0.5373
  Total Loss: 14.6375
  Image grad max: 28.920488357543945
  Output probs: [[0.814 0.18  0.004 0.    0.    0.    0.    0.002 0.    0.   ]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0267
  Image Loss: 0.5371
  Total Loss: 27.2055
  Image grad max: 36.07144546508789
  Output probs: [[0.211 0.781 0.006 0.    0.    0.    0.    0.002 0.    0.   ]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0209
  Image Loss: 0.5377
  Total Loss: 21.4373
  Image grad max: 34.596717834472656
  Output probs: [[0.801 0.193 0.004 0.    0.    0.    0.    0.002 0.    0.   ]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0240
  Image Loss: 0.5382
  Total Loss: 24.5173
  Image grad max: 35.05327606201172
  Output probs: [[0.185 0.808 0.005 0.    0.    0.    0.    0.003 0.    0.   ]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0258
  Image Loss: 0.5388
  Total Loss: 26.3790
  Image grad max: 37.656314849853516
  Output probs: [[0.893 0.103 0.002 0.    0.    0.    0.    0.002 0.    0.   ]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0499
  Image Loss: 0.5377
  Total Loss: 50.4735
  Image grad max: 45.884368896484375
  Output probs: [[0.078 0.916 0.004 0.    0.    0.    0.    0.002 0.    0.   ]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0624
  Image Loss: 0.5377
  Total Loss: 62.9086
  Image grad max: 50.58830261230469
  Output probs: [[0.963 0.035 0.001 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 40/300:
  Label Loss: 0.1005
  Image Loss: 0.5346
  Total Loss: 101.0732
  Image grad max: 53.973487854003906
  Output probs: [[0.075 0.92  0.004 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0641
  Image Loss: 0.5380
  Total Loss: 64.6579
  Image grad max: 49.617408752441406
  Output probs: [[0.944 0.054 0.001 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0796
  Image Loss: 0.5356
  Total Loss: 80.1739
  Image grad max: 51.43930435180664
  Output probs: [[0.069 0.926 0.004 0.    0.    0.    0.    0.001 0.    0.   ]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0685
  Image Loss: 0.5382
  Total Loss: 69.0345
  Image grad max: 48.647178649902344
  Output probs: [[0.936 0.062 0.002 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0734
  Image Loss: 0.5347
  Total Loss: 73.8921
  Image grad max: 49.192420959472656
  Output probs: [[0.08  0.913 0.006 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0616
  Image Loss: 0.5379
  Total Loss: 62.1333
  Image grad max: 45.36254119873047
  Output probs: [[0.924 0.071 0.004 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0668
  Image Loss: 0.5336
  Total Loss: 67.3188
  Image grad max: 45.96744155883789
  Output probs: [[0.089 0.901 0.009 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0567
  Image Loss: 0.5370
  Total Loss: 57.2721
  Image grad max: 42.40382766723633
  Output probs: [[0.912 0.081 0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0611
  Image Loss: 0.5323
  Total Loss: 61.6237
  Image grad max: 43.09968566894531
  Output probs: [[0.094 0.891 0.014 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0545
  Image Loss: 0.5363
  Total Loss: 55.0036
  Image grad max: 40.204524993896484
  Output probs: [[0.898 0.089 0.011 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0567
  Image Loss: 0.5313
  Total Loss: 57.2496
  Image grad max: 39.56775665283203
  Output probs: [[0.106 0.874 0.019 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0498
  Image Loss: 0.5356
  Total Loss: 50.3746
  Image grad max: 37.76338577270508
  Output probs: [[0.888 0.096 0.014 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0537
  Image Loss: 0.5305
  Total Loss: 54.2211
  Image grad max: 36.86305236816406
  Output probs: [[0.11  0.868 0.022 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0483
  Image Loss: 0.5352
  Total Loss: 48.7908
  Image grad max: 36.46143341064453
  Output probs: [[0.89  0.095 0.014 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0541
  Image Loss: 0.5301
  Total Loss: 54.5921
  Image grad max: 36.16434860229492
  Output probs: [[0.107 0.872 0.02  0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0493
  Image Loss: 0.5348
  Total Loss: 49.8448
  Image grad max: 36.27819061279297
  Output probs: [[0.899 0.089 0.011 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0569
  Image Loss: 0.5293
  Total Loss: 57.4760
  Image grad max: 36.78265380859375
  Output probs: [[0.102 0.88  0.017 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0510
  Image Loss: 0.5342
  Total Loss: 51.5211
  Image grad max: 36.62291717529297
  Output probs: [[0.908 0.082 0.009 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0603
  Image Loss: 0.5286
  Total Loss: 60.8544
  Image grad max: 37.71360397338867
  Output probs: [[0.098 0.887 0.014 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0526
  Image Loss: 0.5339
  Total Loss: 53.1473
  Image grad max: 37.078941345214844
  Output probs: [[0.913 0.078 0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0625
  Image Loss: 0.5281
  Total Loss: 63.0557
  Image grad max: 38.14889907836914
  Output probs: [[0.097 0.89  0.013 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0533
  Image Loss: 0.5340
  Total Loss: 53.8781
  Image grad max: 36.91939926147461
  Output probs: [[0.912 0.08  0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0615
  Image Loss: 0.5283
  Total Loss: 61.9808
  Image grad max: 37.59771728515625
  Output probs: [[0.101 0.887 0.012 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0515
  Image Loss: 0.5345
  Total Loss: 52.0747
  Image grad max: 36.173095703125
  Output probs: [[0.906 0.086 0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0583
  Image Loss: 0.5288
  Total Loss: 58.7790
  Image grad max: 36.73756408691406
  Output probs: [[0.104 0.884 0.012 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0499
  Image Loss: 0.5351
  Total Loss: 50.3931
  Image grad max: 35.58695602416992
  Output probs: [[0.904 0.088 0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0573
  Image Loss: 0.5294
  Total Loss: 57.8792
  Image grad max: 36.471351623535156
  Output probs: [[0.105 0.883 0.011 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0494
  Image Loss: 0.5357
  Total Loss: 49.9269
  Image grad max: 35.39810562133789
  Output probs: [[0.905 0.088 0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0574
  Image Loss: 0.5299
  Total Loss: 57.9195
  Image grad max: 36.371368408203125
  Output probs: [[0.104 0.885 0.011 0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0500
  Image Loss: 0.5363
  Total Loss: 50.5067
  Image grad max: 35.49119567871094
  Output probs: [[0.908 0.084 0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0592
  Image Loss: 0.5304
  Total Loss: 59.7139
  Image grad max: 36.44660186767578
  Output probs: [[0.103 0.886 0.01  0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0502
  Image Loss: 0.5368
  Total Loss: 50.7643
  Image grad max: 35.521080017089844
  Output probs: [[0.911 0.082 0.006 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0604
  Image Loss: 0.5308
  Total Loss: 60.9280
  Image grad max: 36.454200744628906
  Output probs: [[0.103 0.886 0.01  0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0502
  Image Loss: 0.5374
  Total Loss: 50.7144
  Image grad max: 35.47251892089844
  Output probs: [[0.912 0.081 0.006 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0611
  Image Loss: 0.5313
  Total Loss: 61.6126
  Image grad max: 36.36919403076172
  Output probs: [[0.105 0.884 0.01  0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0495
  Image Loss: 0.5379
  Total Loss: 50.0421
  Image grad max: 35.21613311767578
  Output probs: [[0.91  0.083 0.006 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0599
  Image Loss: 0.5318
  Total Loss: 60.4259
  Image grad max: 35.96415710449219
  Output probs: [[0.108 0.881 0.01  0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0482
  Image Loss: 0.5384
  Total Loss: 48.6981
  Image grad max: 34.8147087097168
  Output probs: [[0.903 0.089 0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0566
  Image Loss: 0.5322
  Total Loss: 57.1447
  Image grad max: 35.105533599853516
  Output probs: [[0.111 0.878 0.01  0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0469
  Image Loss: 0.5383
  Total Loss: 47.4388
  Image grad max: 34.17010498046875
  Output probs: [[0.898 0.094 0.007 0.    0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0543
  Image Loss: 0.5320
  Total Loss: 54.8139
  Image grad max: 34.576759338378906
  Output probs: [[0.115 0.875 0.009 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0457
  Image Loss: 0.5378
  Total Loss: 46.2415
  Image grad max: 33.51973342895508
  Output probs: [[0.892 0.101 0.007 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0512
  Image Loss: 0.5318
  Total Loss: 51.7236
  Image grad max: 33.918670654296875
  Output probs: [[0.117 0.872 0.009 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0449
  Image Loss: 0.5376
  Total Loss: 45.4070
  Image grad max: 33.08824157714844
  Output probs: [[0.892 0.101 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0511
  Image Loss: 0.5316
  Total Loss: 51.6775
  Image grad max: 33.90753936767578
  Output probs: [[0.115 0.874 0.009 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0456
  Image Loss: 0.5374
  Total Loss: 46.1103
  Image grad max: 33.04165267944336
  Output probs: [[0.892 0.1   0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0513
  Image Loss: 0.5313
  Total Loss: 51.8360
  Image grad max: 33.91969299316406
  Output probs: [[0.113 0.876 0.009 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0462
  Image Loss: 0.5373
  Total Loss: 46.7468
  Image grad max: 33.02123260498047
  Output probs: [[0.893 0.1   0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0515
  Image Loss: 0.5312
  Total Loss: 52.0469
  Image grad max: 33.85493469238281
  Output probs: [[0.112 0.877 0.009 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0468
  Image Loss: 0.5373
  Total Loss: 47.3060
  Image grad max: 33.012481689453125
  Output probs: [[0.893 0.099 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0517
  Image Loss: 0.5314
  Total Loss: 52.2658
  Image grad max: 33.78002166748047
  Output probs: [[0.111 0.878 0.009 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0471
  Image Loss: 0.5374
  Total Loss: 47.6693
  Image grad max: 32.925926208496094
  Output probs: [[0.893 0.1   0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0516
  Image Loss: 0.5315
  Total Loss: 52.1307
  Image grad max: 33.57334899902344
  Output probs: [[0.111 0.878 0.009 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0472
  Image Loss: 0.5377
  Total Loss: 47.7324
  Image grad max: 32.77653884887695
  Output probs: [[0.892 0.1   0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0513
  Image Loss: 0.5317
  Total Loss: 51.8228
  Image grad max: 33.40715789794922
  Output probs: [[0.111 0.878 0.009 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0471
  Image Loss: 0.5380
  Total Loss: 47.6230
  Image grad max: 32.64340591430664
  Output probs: [[0.892 0.1   0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0515
  Image Loss: 0.5321
  Total Loss: 51.9947
  Image grad max: 33.40856170654297
  Output probs: [[0.111 0.878 0.009 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0469
  Image Loss: 0.5384
  Total Loss: 47.4491
  Image grad max: 32.53447723388672
  Output probs: [[0.892 0.1   0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0514
  Image Loss: 0.5326
  Total Loss: 51.8880
  Image grad max: 33.351341247558594
  Output probs: [[0.113 0.877 0.009 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0463
  Image Loss: 0.5389
  Total Loss: 46.8800
  Image grad max: 32.388465881347656
  Output probs: [[0.891 0.102 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0506
  Image Loss: 0.5332
  Total Loss: 51.1356
  Image grad max: 33.182865142822266
  Output probs: [[0.113 0.877 0.008 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0464
  Image Loss: 0.5395
  Total Loss: 46.9061
  Image grad max: 32.3880615234375
  Output probs: [[0.892 0.101 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0509
  Image Loss: 0.5337
  Total Loss: 51.4703
  Image grad max: 33.24552917480469
  Output probs: [[0.112 0.878 0.008 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0467
  Image Loss: 0.5402
  Total Loss: 47.2242
  Image grad max: 32.453857421875
  Output probs: [[0.893 0.1   0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0515
  Image Loss: 0.5343
  Total Loss: 51.9939
  Image grad max: 33.32236099243164
  Output probs: [[0.111 0.879 0.008 0.001 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0470
  Image Loss: 0.5409
  Total Loss: 47.5784
  Image grad max: 32.52601623535156
  Output probs: [[0.894 0.099 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0519
  Image Loss: 0.5350
  Total Loss: 52.4430
  Image grad max: 33.37659454345703
  Output probs: [[0.111 0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0472
  Image Loss: 0.5417
  Total Loss: 47.7388
  Image grad max: 32.543582916259766
  Output probs: [[0.895 0.098 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0522
  Image Loss: 0.5357
  Total Loss: 52.7053
  Image grad max: 33.369049072265625
  Output probs: [[0.111 0.879 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0470
  Image Loss: 0.5424
  Total Loss: 47.5377
  Image grad max: 32.46719741821289
  Output probs: [[0.894 0.099 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0519
  Image Loss: 0.5366
  Total Loss: 52.4195
  Image grad max: 33.230194091796875
  Output probs: [[0.111 0.879 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0468
  Image Loss: 0.5432
  Total Loss: 47.3811
  Image grad max: 32.40240478515625
  Output probs: [[0.894 0.099 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0518
  Image Loss: 0.5373
  Total Loss: 52.2998
  Image grad max: 33.17467498779297
  Output probs: [[0.111 0.879 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0470
  Image Loss: 0.5440
  Total Loss: 47.5683
  Image grad max: 32.41021728515625
  Output probs: [[0.894 0.099 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0520
  Image Loss: 0.5379
  Total Loss: 52.5317
  Image grad max: 33.19867706298828
  Output probs: [[0.11  0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0474
  Image Loss: 0.5446
  Total Loss: 47.9039
  Image grad max: 32.441707611083984
  Output probs: [[0.895 0.098 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0525
  Image Loss: 0.5385
  Total Loss: 52.9968
  Image grad max: 33.25867462158203
  Output probs: [[0.109 0.881 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0476
  Image Loss: 0.5452
  Total Loss: 48.1635
  Image grad max: 32.45076370239258
  Output probs: [[0.896 0.097 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0527
  Image Loss: 0.5390
  Total Loss: 53.2058
  Image grad max: 33.24983215332031
  Output probs: [[0.11  0.881 0.007 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0476
  Image Loss: 0.5456
  Total Loss: 48.1522
  Image grad max: 32.39276123046875
  Output probs: [[0.895 0.098 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0525
  Image Loss: 0.5395
  Total Loss: 53.0869
  Image grad max: 33.16567611694336
  Output probs: [[0.11  0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0474
  Image Loss: 0.5460
  Total Loss: 47.9886
  Image grad max: 32.29689025878906
  Output probs: [[0.895 0.098 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0523
  Image Loss: 0.5398
  Total Loss: 52.8180
  Image grad max: 33.05616760253906
  Output probs: [[0.11  0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0473
  Image Loss: 0.5464
  Total Loss: 47.8723
  Image grad max: 32.219947814941406
  Output probs: [[0.895 0.098 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0522
  Image Loss: 0.5402
  Total Loss: 52.7677
  Image grad max: 32.99728775024414
  Output probs: [[0.111 0.879 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0472
  Image Loss: 0.5467
  Total Loss: 47.7085
  Image grad max: 32.135223388671875
  Output probs: [[0.894 0.099 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0520
  Image Loss: 0.5406
  Total Loss: 52.5419
  Image grad max: 32.90046691894531
  Output probs: [[0.111 0.879 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0469
  Image Loss: 0.5471
  Total Loss: 47.4798
  Image grad max: 32.04405212402344
  Output probs: [[0.893 0.1   0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0517
  Image Loss: 0.5410
  Total Loss: 52.2227
  Image grad max: 32.798851013183594
  Output probs: [[0.112 0.878 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0467
  Image Loss: 0.5474
  Total Loss: 47.2938
  Image grad max: 31.966981887817383
  Output probs: [[0.892 0.1   0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0514
  Image Loss: 0.5412
  Total Loss: 51.9378
  Image grad max: 32.70688247680664
  Output probs: [[0.112 0.878 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0467
  Image Loss: 0.5476
  Total Loss: 47.2760
  Image grad max: 31.932435989379883
  Output probs: [[0.892 0.1   0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0514
  Image Loss: 0.5412
  Total Loss: 51.9640
  Image grad max: 32.67266845703125
  Output probs: [[0.112 0.878 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0468
  Image Loss: 0.5477
  Total Loss: 47.3964
  Image grad max: 31.930437088012695
  Output probs: [[0.893 0.1   0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0517
  Image Loss: 0.5413
  Total Loss: 52.2317
  Image grad max: 32.6966667175293
  Output probs: [[0.111 0.879 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0470
  Image Loss: 0.5477
  Total Loss: 47.5750
  Image grad max: 31.94493293762207
  Output probs: [[0.893 0.099 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0520
  Image Loss: 0.5413
  Total Loss: 52.5061
  Image grad max: 32.68898010253906
  Output probs: [[0.111 0.879 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0472
  Image Loss: 0.5478
  Total Loss: 47.7638
  Image grad max: 31.96575355529785
  Output probs: [[0.894 0.098 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0522
  Image Loss: 0.5413
  Total Loss: 52.7778
  Image grad max: 32.70769119262695
  Output probs: [[0.11  0.879 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0474
  Image Loss: 0.5479
  Total Loss: 47.9056
  Image grad max: 31.970775604248047
  Output probs: [[0.894 0.098 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0524
  Image Loss: 0.5415
  Total Loss: 52.9536
  Image grad max: 32.70489501953125
  Output probs: [[0.11  0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0475
  Image Loss: 0.5480
  Total Loss: 48.0399
  Image grad max: 31.980554580688477
  Output probs: [[0.895 0.098 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0525
  Image Loss: 0.5416
  Total Loss: 53.0466
  Image grad max: 32.703922271728516
  Output probs: [[0.11  0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0476
  Image Loss: 0.5481
  Total Loss: 48.1753
  Image grad max: 31.9981632232666
  Output probs: [[0.895 0.097 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0527
  Image Loss: 0.5417
  Total Loss: 53.2657
  Image grad max: 32.749637603759766
  Output probs: [[0.109 0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0478
  Image Loss: 0.5482
  Total Loss: 48.3581
  Image grad max: 32.03448486328125
  Output probs: [[0.895 0.097 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0529
  Image Loss: 0.5418
  Total Loss: 53.4253
  Image grad max: 32.8017692565918
  Output probs: [[0.109 0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0478
  Image Loss: 0.5483
  Total Loss: 48.3750
  Image grad max: 32.03441619873047
  Output probs: [[0.895 0.097 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0529
  Image Loss: 0.5420
  Total Loss: 53.4321
  Image grad max: 32.81011962890625
  Output probs: [[0.109 0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0477
  Image Loss: 0.5485
  Total Loss: 48.2611
  Image grad max: 31.992324829101562
  Output probs: [[0.895 0.097 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0527
  Image Loss: 0.5422
  Total Loss: 53.2158
  Image grad max: 32.74612045288086
  Output probs: [[0.109 0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0477
  Image Loss: 0.5486
  Total Loss: 48.2380
  Image grad max: 31.968908309936523
  Output probs: [[0.895 0.097 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0526
  Image Loss: 0.5423
  Total Loss: 53.1852
  Image grad max: 32.73311996459961
  Output probs: [[0.109 0.88  0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0478
  Image Loss: 0.5487
  Total Loss: 48.3771
  Image grad max: 31.979570388793945
  Output probs: [[0.895 0.097 0.006 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0529
  Image Loss: 0.5424
  Total Loss: 53.4408
  Image grad max: 32.77022933959961
  Output probs: [[0.109 0.881 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0480
  Image Loss: 0.5488
  Total Loss: 48.5534
  Image grad max: 31.996835708618164
  Output probs: [[0.896 0.097 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0530
  Image Loss: 0.5426
  Total Loss: 53.5805
  Image grad max: 32.777610778808594
  Output probs: [[0.109 0.881 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0481
  Image Loss: 0.5489
  Total Loss: 48.6147
  Image grad max: 31.991464614868164
  Output probs: [[0.896 0.097 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0531
  Image Loss: 0.5427
  Total Loss: 53.5941
  Image grad max: 32.762271881103516
  Output probs: [[0.108 0.881 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0482
  Image Loss: 0.5490
  Total Loss: 48.7014
  Image grad max: 31.99713706970215
  Output probs: [[0.896 0.097 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0530
  Image Loss: 0.5428
  Total Loss: 53.5176
  Image grad max: 32.740299224853516
  Output probs: [[0.108 0.882 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0483
  Image Loss: 0.5491
  Total Loss: 48.8578
  Image grad max: 32.021270751953125
  Output probs: [[0.896 0.096 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0534
  Image Loss: 0.5428
  Total Loss: 53.9100
  Image grad max: 32.81256866455078
  Output probs: [[0.107 0.882 0.008 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0486
  Image Loss: 0.5492
  Total Loss: 49.1103
  Image grad max: 32.062931060791016
  Output probs: [[0.897 0.096 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0536
  Image Loss: 0.5429
  Total Loss: 54.0990
  Image grad max: 32.83938980102539
  Output probs: [[0.107 0.883 0.007 0.002 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0487
  Image Loss: 0.5493
  Total Loss: 49.2060
  Image grad max: 32.069583892822266
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0537
  Image Loss: 0.5430
  Total Loss: 54.2017
  Image grad max: 32.83987045288086
  Output probs: [[0.107 0.883 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0487
  Image Loss: 0.5493
  Total Loss: 49.2081
  Image grad max: 32.05440139770508
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0536
  Image Loss: 0.5431
  Total Loss: 54.1869
  Image grad max: 32.820072174072266
  Output probs: [[0.107 0.882 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0485
  Image Loss: 0.5494
  Total Loss: 49.0687
  Image grad max: 32.02351379394531
  Output probs: [[0.897 0.096 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0535
  Image Loss: 0.5432
  Total Loss: 54.0651
  Image grad max: 32.8079833984375
  Output probs: [[0.107 0.882 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0485
  Image Loss: 0.5495
  Total Loss: 49.0434
  Image grad max: 32.02317428588867
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0536
  Image Loss: 0.5433
  Total Loss: 54.1099
  Image grad max: 32.824851989746094
  Output probs: [[0.107 0.882 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0485
  Image Loss: 0.5496
  Total Loss: 49.0868
  Image grad max: 32.03102111816406
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0536
  Image Loss: 0.5435
  Total Loss: 54.1826
  Image grad max: 32.83017349243164
  Output probs: [[0.108 0.882 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0485
  Image Loss: 0.5497
  Total Loss: 49.0296
  Image grad max: 32.0119743347168
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0536
  Image Loss: 0.5436
  Total Loss: 54.1359
  Image grad max: 32.7990837097168
  Output probs: [[0.108 0.882 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0484
  Image Loss: 0.5498
  Total Loss: 48.9877
  Image grad max: 31.991493225097656
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0536
  Image Loss: 0.5437
  Total Loss: 54.1515
  Image grad max: 32.775978088378906
  Output probs: [[0.108 0.881 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0483
  Image Loss: 0.5499
  Total Loss: 48.8280
  Image grad max: 31.946876525878906
  Output probs: [[0.897 0.096 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0535
  Image Loss: 0.5438
  Total Loss: 54.0554
  Image grad max: 32.73478698730469
  Output probs: [[0.108 0.881 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0481
  Image Loss: 0.5500
  Total Loss: 48.6960
  Image grad max: 31.90814781188965
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0536
  Image Loss: 0.5439
  Total Loss: 54.1131
  Image grad max: 32.724159240722656
  Output probs: [[0.108 0.881 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0481
  Image Loss: 0.5501
  Total Loss: 48.6543
  Image grad max: 31.88645362854004
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0537
  Image Loss: 0.5441
  Total Loss: 54.1942
  Image grad max: 32.71101760864258
  Output probs: [[0.109 0.88  0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0479
  Image Loss: 0.5502
  Total Loss: 48.4612
  Image grad max: 31.830068588256836
  Output probs: [[0.897 0.095 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0536
  Image Loss: 0.5442
  Total Loss: 54.1268
  Image grad max: 32.665279388427734
  Output probs: [[0.109 0.88  0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0477
  Image Loss: 0.5502
  Total Loss: 48.2405
  Image grad max: 31.766067504882812
  Output probs: [[0.896 0.096 0.005 0.001 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0535
  Image Loss: 0.5443
  Total Loss: 54.0043
  Image grad max: 32.6094856262207
  Output probs: [[0.11  0.879 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0474
  Image Loss: 0.5503
  Total Loss: 47.9675
  Image grad max: 31.690217971801758
  Output probs: [[0.896 0.096 0.005 0.002 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0533
  Image Loss: 0.5444
  Total Loss: 53.8904
  Image grad max: 32.54929733276367
  Output probs: [[0.111 0.878 0.007 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0470
  Image Loss: 0.5503
  Total Loss: 47.5476
  Image grad max: 31.58005714416504
  Output probs: [[0.895 0.097 0.005 0.002 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0531
  Image Loss: 0.5444
  Total Loss: 53.6116
  Image grad max: 32.44076919555664
  Output probs: [[0.113 0.876 0.008 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0464
  Image Loss: 0.5502
  Total Loss: 46.9696
  Image grad max: 31.429094314575195
  Output probs: [[0.894 0.098 0.005 0.002 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0526
  Image Loss: 0.5444
  Total Loss: 53.1118
  Image grad max: 32.28814697265625
  Output probs: [[0.115 0.874 0.008 0.003 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0457
  Image Loss: 0.5501
  Total Loss: 46.2957
  Image grad max: 31.24456214904785
  Output probs: [[0.892 0.1   0.006 0.002 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0517
  Image Loss: 0.5444
  Total Loss: 52.2676
  Image grad max: 32.0482177734375
  Output probs: [[0.117 0.871 0.008 0.004 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0450
  Image Loss: 0.5500
  Total Loss: 45.5299
  Image grad max: 31.02906608581543
  Output probs: [[0.891 0.1   0.006 0.002 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0514
  Image Loss: 0.5442
  Total Loss: 51.9064
  Image grad max: 31.89385986328125
  Output probs: [[0.119 0.869 0.008 0.004 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0442
  Image Loss: 0.5499
  Total Loss: 44.7187
  Image grad max: 30.8005313873291
  Output probs: [[0.889 0.102 0.006 0.002 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0505
  Image Loss: 0.5441
  Total Loss: 51.0791
  Image grad max: 31.659048080444336
  Output probs: [[0.123 0.864 0.008 0.004 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0428
  Image Loss: 0.5496
  Total Loss: 43.3275
  Image grad max: 30.396289825439453
  Output probs: [[0.882 0.108 0.006 0.002 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0480
  Image Loss: 0.5441
  Total Loss: 48.5935
  Image grad max: 31.061227798461914
  Output probs: [[0.128 0.859 0.008 0.004 0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0411
  Image Loss: 0.5494
  Total Loss: 41.6942
  Image grad max: 29.905838012695312
  Output probs: [[0.877 0.113 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0462
  Image Loss: 0.5441
  Total Loss: 46.7165
  Image grad max: 30.557092666625977
  Output probs: [[0.13  0.856 0.009 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0403
  Image Loss: 0.5492
  Total Loss: 40.8415
  Image grad max: 29.551992416381836
  Output probs: [[0.874 0.116 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0450
  Image Loss: 0.5440
  Total Loss: 45.5014
  Image grad max: 30.213285446166992
  Output probs: [[0.131 0.855 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0400
  Image Loss: 0.5491
  Total Loss: 40.5212
  Image grad max: 29.337875366210938
  Output probs: [[0.873 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0445
  Image Loss: 0.5440
  Total Loss: 45.0518
  Image grad max: 30.108642578125
  Output probs: [[0.131 0.856 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0401
  Image Loss: 0.5491
  Total Loss: 40.6801
  Image grad max: 29.29743003845215
  Output probs: [[0.873 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0445
  Image Loss: 0.5439
  Total Loss: 45.0728
  Image grad max: 30.132061004638672
  Output probs: [[0.13  0.857 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0403
  Image Loss: 0.5492
  Total Loss: 40.8956
  Image grad max: 29.279497146606445
  Output probs: [[0.873 0.117 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0446
  Image Loss: 0.5440
  Total Loss: 45.1595
  Image grad max: 30.156164169311523
  Output probs: [[0.13  0.857 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0405
  Image Loss: 0.5494
  Total Loss: 41.0676
  Image grad max: 29.248023986816406
  Output probs: [[0.873 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0445
  Image Loss: 0.5442
  Total Loss: 45.0886
  Image grad max: 30.118335723876953
  Output probs: [[0.129 0.858 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0406
  Image Loss: 0.5498
  Total Loss: 41.1889
  Image grad max: 29.20070457458496
  Output probs: [[0.872 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0444
  Image Loss: 0.5445
  Total Loss: 44.9412
  Image grad max: 30.03038787841797
  Output probs: [[0.129 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0408
  Image Loss: 0.5499
  Total Loss: 41.3841
  Image grad max: 29.167261123657227
  Output probs: [[0.872 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0444
  Image Loss: 0.5446
  Total Loss: 44.9669
  Image grad max: 29.961164474487305
  Output probs: [[0.128 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0410
  Image Loss: 0.5502
  Total Loss: 41.5108
  Image grad max: 29.11931800842285
  Output probs: [[0.872 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0445
  Image Loss: 0.5448
  Total Loss: 45.0000
  Image grad max: 29.925312042236328
  Output probs: [[0.128 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0410
  Image Loss: 0.5504
  Total Loss: 41.5955
  Image grad max: 29.08160400390625
  Output probs: [[0.873 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0445
  Image Loss: 0.5449
  Total Loss: 45.0650
  Image grad max: 29.90542984008789
  Output probs: [[0.128 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0411
  Image Loss: 0.5506
  Total Loss: 41.6185
  Image grad max: 29.05058479309082
  Output probs: [[0.872 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0445
  Image Loss: 0.5450
  Total Loss: 45.0223
  Image grad max: 29.858230590820312
  Output probs: [[0.128 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0410
  Image Loss: 0.5507
  Total Loss: 41.5509
  Image grad max: 29.007787704467773
  Output probs: [[0.872 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0443
  Image Loss: 0.5452
  Total Loss: 44.8557
  Image grad max: 29.813547134399414
  Output probs: [[0.129 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0409
  Image Loss: 0.5509
  Total Loss: 41.4250
  Image grad max: 28.963594436645508
  Output probs: [[0.871 0.119 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0440
  Image Loss: 0.5453
  Total Loss: 44.5616
  Image grad max: 29.755020141601562
  Output probs: [[0.129 0.858 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0408
  Image Loss: 0.5510
  Total Loss: 41.3333
  Image grad max: 28.938692092895508
  Output probs: [[0.871 0.119 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0440
  Image Loss: 0.5454
  Total Loss: 44.5954
  Image grad max: 29.779409408569336
  Output probs: [[0.129 0.858 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0409
  Image Loss: 0.5511
  Total Loss: 41.4604
  Image grad max: 28.978971481323242
  Output probs: [[0.871 0.119 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0442
  Image Loss: 0.5455
  Total Loss: 44.6996
  Image grad max: 29.830232620239258
  Output probs: [[0.128 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0411
  Image Loss: 0.5512
  Total Loss: 41.6203
  Image grad max: 29.031278610229492
  Output probs: [[0.871 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0443
  Image Loss: 0.5456
  Total Loss: 44.8206
  Image grad max: 29.88475227355957
  Output probs: [[0.128 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0412
  Image Loss: 0.5513
  Total Loss: 41.7502
  Image grad max: 29.076919555664062
  Output probs: [[0.871 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0443
  Image Loss: 0.5457
  Total Loss: 44.8459
  Image grad max: 29.904708862304688
  Output probs: [[0.127 0.859 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0413
  Image Loss: 0.5514
  Total Loss: 41.8651
  Image grad max: 29.108692169189453
  Output probs: [[0.871 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0443
  Image Loss: 0.5457
  Total Loss: 44.8399
  Image grad max: 29.911083221435547
  Output probs: [[0.127 0.86  0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0415
  Image Loss: 0.5514
  Total Loss: 42.0384
  Image grad max: 29.14691734313965
  Output probs: [[0.872 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0444
  Image Loss: 0.5457
  Total Loss: 44.9721
  Image grad max: 29.949413299560547
  Output probs: [[0.127 0.86  0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0416
  Image Loss: 0.5515
  Total Loss: 42.1370
  Image grad max: 29.16290855407715
  Output probs: [[0.872 0.118 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0446
  Image Loss: 0.5457
  Total Loss: 45.1318
  Image grad max: 29.978878021240234
  Output probs: [[0.126 0.861 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0418
  Image Loss: 0.5515
  Total Loss: 42.3402
  Image grad max: 29.199127197265625
  Output probs: [[0.873 0.117 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0448
  Image Loss: 0.5457
  Total Loss: 45.3293
  Image grad max: 30.007417678833008
  Output probs: [[0.126 0.861 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0419
  Image Loss: 0.5515
  Total Loss: 42.4807
  Image grad max: 29.215620040893555
  Output probs: [[0.873 0.117 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0449
  Image Loss: 0.5457
  Total Loss: 45.4146
  Image grad max: 30.004547119140625
  Output probs: [[0.125 0.861 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0420
  Image Loss: 0.5515
  Total Loss: 42.5941
  Image grad max: 29.225475311279297
  Output probs: [[0.873 0.117 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0449
  Image Loss: 0.5457
  Total Loss: 45.4616
  Image grad max: 29.990686416625977
  Output probs: [[0.125 0.862 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0422
  Image Loss: 0.5515
  Total Loss: 42.7373
  Image grad max: 29.244340896606445
  Output probs: [[0.873 0.116 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0451
  Image Loss: 0.5456
  Total Loss: 45.5984
  Image grad max: 29.996736526489258
  Output probs: [[0.124 0.862 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0424
  Image Loss: 0.5515
  Total Loss: 42.9356
  Image grad max: 29.27897834777832
  Output probs: [[0.873 0.116 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0452
  Image Loss: 0.5455
  Total Loss: 45.7310
  Image grad max: 30.0137882232666
  Output probs: [[0.124 0.863 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0426
  Image Loss: 0.5515
  Total Loss: 43.1721
  Image grad max: 29.325742721557617
  Output probs: [[0.874 0.115 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0454
  Image Loss: 0.5454
  Total Loss: 45.9172
  Image grad max: 30.05636215209961
  Output probs: [[0.123 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0429
  Image Loss: 0.5514
  Total Loss: 43.4655
  Image grad max: 29.38776969909668
  Output probs: [[0.875 0.114 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0458
  Image Loss: 0.5453
  Total Loss: 46.3485
  Image grad max: 30.154266357421875
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0431
  Image Loss: 0.5514
  Total Loss: 43.6909
  Image grad max: 29.429309844970703
  Output probs: [[0.875 0.114 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0459
  Image Loss: 0.5453
  Total Loss: 46.4552
  Image grad max: 30.158950805664062
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0432
  Image Loss: 0.5514
  Total Loss: 43.7391
  Image grad max: 29.428865432739258
  Output probs: [[0.875 0.114 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0459
  Image Loss: 0.5454
  Total Loss: 46.4492
  Image grad max: 30.140846252441406
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0432
  Image Loss: 0.5515
  Total Loss: 43.7381
  Image grad max: 29.424150466918945
  Output probs: [[0.875 0.114 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0459
  Image Loss: 0.5454
  Total Loss: 46.4524
  Image grad max: 30.14346694946289
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0432
  Image Loss: 0.5515
  Total Loss: 43.7439
  Image grad max: 29.426700592041016
  Output probs: [[0.875 0.114 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0458
  Image Loss: 0.5455
  Total Loss: 46.3746
  Image grad max: 30.143367767333984
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0432
  Image Loss: 0.5516
  Total Loss: 43.7056
  Image grad max: 29.42270851135254
  Output probs: [[0.875 0.115 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0457
  Image Loss: 0.5455
  Total Loss: 46.2529
  Image grad max: 30.143457412719727
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0432
  Image Loss: 0.5516
  Total Loss: 43.7473
  Image grad max: 29.437467575073242
  Output probs: [[0.875 0.114 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0458
  Image Loss: 0.5455
  Total Loss: 46.3376
  Image grad max: 30.18075942993164
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0433
  Image Loss: 0.5517
  Total Loss: 43.8662
  Image grad max: 29.468425750732422
  Output probs: [[0.875 0.114 0.006 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0459
  Image Loss: 0.5455
  Total Loss: 46.4382
  Image grad max: 30.205434799194336
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0434
  Image Loss: 0.5517
  Total Loss: 43.9032
  Image grad max: 29.4791202545166
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0459
  Image Loss: 0.5455
  Total Loss: 46.4857
  Image grad max: 30.210105895996094
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0433
  Image Loss: 0.5517
  Total Loss: 43.8777
  Image grad max: 29.475109100341797
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0459
  Image Loss: 0.5456
  Total Loss: 46.4473
  Image grad max: 30.195945739746094
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0433
  Image Loss: 0.5518
  Total Loss: 43.8375
  Image grad max: 29.46792984008789
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0459
  Image Loss: 0.5457
  Total Loss: 46.4269
  Image grad max: 30.192087173461914
  Output probs: [[0.122 0.864 0.008 0.005 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0433
  Image Loss: 0.5519
  Total Loss: 43.8501
  Image grad max: 29.4705867767334
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0459
  Image Loss: 0.5458
  Total Loss: 46.4475
  Image grad max: 30.19070053100586
  Output probs: [[0.122 0.864 0.009 0.005 0.    0.    0.001 0.    0.    0.   ]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0433
  Image Loss: 0.5520
  Total Loss: 43.8375
  Image grad max: 29.469247817993164
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0459
  Image Loss: 0.5459
  Total Loss: 46.4309
  Image grad max: 30.184438705444336
  Output probs: [[0.122 0.864 0.009 0.005 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0432
  Image Loss: 0.5520
  Total Loss: 43.7323
  Image grad max: 29.456411361694336
  Output probs: [[0.874 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0458
  Image Loss: 0.5460
  Total Loss: 46.3340
  Image grad max: 30.18036651611328
  Output probs: [[0.122 0.864 0.008 0.005 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0432
  Image Loss: 0.5521
  Total Loss: 43.7320
  Image grad max: 29.464717864990234
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0458
  Image Loss: 0.5461
  Total Loss: 46.3926
  Image grad max: 30.21079444885254
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0433
  Image Loss: 0.5521
  Total Loss: 43.8025
  Image grad max: 29.485774993896484
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0459
  Image Loss: 0.5461
  Total Loss: 46.4844
  Image grad max: 30.239946365356445
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0433
  Image Loss: 0.5522
  Total Loss: 43.8622
  Image grad max: 29.502403259277344
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0460
  Image Loss: 0.5462
  Total Loss: 46.5427
  Image grad max: 30.25715446472168
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0433
  Image Loss: 0.5522
  Total Loss: 43.8850
  Image grad max: 29.510522842407227
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0460
  Image Loss: 0.5462
  Total Loss: 46.5553
  Image grad max: 30.263593673706055
  Output probs: [[0.122 0.864 0.008 0.004 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0433
  Image Loss: 0.5522
  Total Loss: 43.8384
  Image grad max: 29.50188446044922
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0459
  Image Loss: 0.5462
  Total Loss: 46.4950
  Image grad max: 30.248991012573242
  Output probs: [[0.122 0.864 0.008 0.005 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0432
  Image Loss: 0.5521
  Total Loss: 43.7515
  Image grad max: 29.478515625
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0458
  Image Loss: 0.5462
  Total Loss: 46.3723
  Image grad max: 30.21343994140625
  Output probs: [[0.122 0.864 0.009 0.005 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0431
  Image Loss: 0.5521
  Total Loss: 43.6795
  Image grad max: 29.459360122680664
  Output probs: [[0.874 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0458
  Image Loss: 0.5462
  Total Loss: 46.3462
  Image grad max: 30.208053588867188
  Output probs: [[0.122 0.864 0.009 0.005 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0431
  Image Loss: 0.5521
  Total Loss: 43.6910
  Image grad max: 29.46100425720215
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0459
  Image Loss: 0.5462
  Total Loss: 46.4015
  Image grad max: 30.218992233276367
  Output probs: [[0.122 0.864 0.009 0.005 0.    0.    0.001 0.001 0.    0.   ]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0432
  Image Loss: 0.5521
  Total Loss: 43.7731
  Image grad max: 29.479583740234375
  Output probs: [[0.875 0.114 0.007 0.002 0.    0.    0.002 0.    0.    0.   ]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0460
  Image Loss: 0.5462
  Total Loss: 46.5070
  Image grad max: 30.242395401000977
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
