Epoch [1/30], Batch [0/6000], Loss: 2.4186
Epoch [1/30], Batch [100/6000], Loss: 1.3530
Epoch [1/30], Batch [200/6000], Loss: 0.6469
Epoch [1/30], Batch [300/6000], Loss: 0.6074
Epoch [1/30], Batch [400/6000], Loss: 0.5612
Epoch [1/30], Batch [500/6000], Loss: 1.3864
Epoch [1/30], Batch [600/6000], Loss: 0.2149
Epoch [1/30], Batch [700/6000], Loss: 0.2045
Epoch [1/30], Batch [800/6000], Loss: 0.3858
Epoch [1/30], Batch [900/6000], Loss: 0.2223
Epoch [1/30], Batch [1000/6000], Loss: 0.2243
Epoch [1/30], Batch [1100/6000], Loss: 0.6306
Epoch [1/30], Batch [1200/6000], Loss: 1.1494
Epoch [1/30], Batch [1300/6000], Loss: 0.6217
Epoch [1/30], Batch [1400/6000], Loss: 0.3975
Epoch [1/30], Batch [1500/6000], Loss: 0.1371
Epoch [1/30], Batch [1600/6000], Loss: 0.4575
Epoch [1/30], Batch [1700/6000], Loss: 0.2907
Epoch [1/30], Batch [1800/6000], Loss: 0.4579
Epoch [1/30], Batch [1900/6000], Loss: 0.1222
Epoch [1/30], Batch [2000/6000], Loss: 0.6704
Epoch [1/30], Batch [2100/6000], Loss: 0.2272
Epoch [1/30], Batch [2200/6000], Loss: 0.6455
Epoch [1/30], Batch [2300/6000], Loss: 0.0611
Epoch [1/30], Batch [2400/6000], Loss: 0.0963
Epoch [1/30], Batch [2500/6000], Loss: 0.6918
Epoch [1/30], Batch [2600/6000], Loss: 0.6377
Epoch [1/30], Batch [2700/6000], Loss: 0.1070
Epoch [1/30], Batch [2800/6000], Loss: 0.1412
Epoch [1/30], Batch [2900/6000], Loss: 0.3773
Epoch [1/30], Batch [3000/6000], Loss: 0.9354
Epoch [1/30], Batch [3100/6000], Loss: 0.1410
Epoch [1/30], Batch [3200/6000], Loss: 0.2072
Epoch [1/30], Batch [3300/6000], Loss: 0.1982
Epoch [1/30], Batch [3400/6000], Loss: 0.4366
Epoch [1/30], Batch [3500/6000], Loss: 0.1662
Epoch [1/30], Batch [3600/6000], Loss: 0.2018
Epoch [1/30], Batch [3700/6000], Loss: 0.2750
Epoch [1/30], Batch [3800/6000], Loss: 0.1031
Epoch [1/30], Batch [3900/6000], Loss: 0.2685
Epoch [1/30], Batch [4000/6000], Loss: 0.2112
Epoch [1/30], Batch [4100/6000], Loss: 0.2682
Epoch [1/30], Batch [4200/6000], Loss: 0.0632
Epoch [1/30], Batch [4300/6000], Loss: 0.4404
Epoch [1/30], Batch [4400/6000], Loss: 0.6485
Epoch [1/30], Batch [4500/6000], Loss: 0.2084
Epoch [1/30], Batch [4600/6000], Loss: 0.1207
Epoch [1/30], Batch [4700/6000], Loss: 0.2252
Epoch [1/30], Batch [4800/6000], Loss: 0.2741
Epoch [1/30], Batch [4900/6000], Loss: 0.4465
Epoch [1/30], Batch [5000/6000], Loss: 0.2169
Epoch [1/30], Batch [5100/6000], Loss: 0.0857
Epoch [1/30], Batch [5200/6000], Loss: 0.4605
Epoch [1/30], Batch [5300/6000], Loss: 0.2017
Epoch [1/30], Batch [5400/6000], Loss: 0.0617
Epoch [1/30], Batch [5500/6000], Loss: 0.1164
Epoch [1/30], Batch [5600/6000], Loss: 0.1161
Epoch [1/30], Batch [5700/6000], Loss: 0.8746
Epoch [1/30], Batch [5800/6000], Loss: 0.9069
Epoch [1/30], Batch [5900/6000], Loss: 0.1657
Epoch [1/30], Loss: 0.3998
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.2771
Epoch [2/30], Batch [100/6000], Loss: 0.2439
Epoch [2/30], Batch [200/6000], Loss: 0.1303
Epoch [2/30], Batch [300/6000], Loss: 0.9097
Epoch [2/30], Batch [400/6000], Loss: 0.3273
Epoch [2/30], Batch [500/6000], Loss: 0.2123
Epoch [2/30], Batch [600/6000], Loss: 0.0660
Epoch [2/30], Batch [700/6000], Loss: 0.0829
Epoch [2/30], Batch [800/6000], Loss: 0.1359
Epoch [2/30], Batch [900/6000], Loss: 0.2976
Epoch [2/30], Batch [1000/6000], Loss: 0.1039
Epoch [2/30], Batch [1100/6000], Loss: 0.1960
Epoch [2/30], Batch [1200/6000], Loss: 0.4221
Epoch [2/30], Batch [1300/6000], Loss: 0.1982
Epoch [2/30], Batch [1400/6000], Loss: 0.0952
Epoch [2/30], Batch [1500/6000], Loss: 0.2752
Epoch [2/30], Batch [1600/6000], Loss: 0.1090
Epoch [2/30], Batch [1700/6000], Loss: 0.0690
Epoch [2/30], Batch [1800/6000], Loss: 0.1926
Epoch [2/30], Batch [1900/6000], Loss: 0.4337
Epoch [2/30], Batch [2000/6000], Loss: 0.5800
Epoch [2/30], Batch [2100/6000], Loss: 0.5428
Epoch [2/30], Batch [2200/6000], Loss: 0.0376
Epoch [2/30], Batch [2300/6000], Loss: 0.0852
Epoch [2/30], Batch [2400/6000], Loss: 0.2905
Epoch [2/30], Batch [2500/6000], Loss: 0.3345
Epoch [2/30], Batch [2600/6000], Loss: 0.0877
Epoch [2/30], Batch [2700/6000], Loss: 0.2255
Epoch [2/30], Batch [2800/6000], Loss: 0.0362
Epoch [2/30], Batch [2900/6000], Loss: 0.0572
Epoch [2/30], Batch [3000/6000], Loss: 0.2400
Epoch [2/30], Batch [3100/6000], Loss: 0.1584
Epoch [2/30], Batch [3200/6000], Loss: 0.0788
Epoch [2/30], Batch [3300/6000], Loss: 0.0686
Epoch [2/30], Batch [3400/6000], Loss: 0.2543
Epoch [2/30], Batch [3500/6000], Loss: 0.0553
Epoch [2/30], Batch [3600/6000], Loss: 0.7182
Epoch [2/30], Batch [3700/6000], Loss: 0.1330
Epoch [2/30], Batch [3800/6000], Loss: 0.0600
Epoch [2/30], Batch [3900/6000], Loss: 0.0432
Epoch [2/30], Batch [4000/6000], Loss: 0.0629
Epoch [2/30], Batch [4100/6000], Loss: 0.2528
Epoch [2/30], Batch [4200/6000], Loss: 0.1132
Epoch [2/30], Batch [4300/6000], Loss: 0.2291
Epoch [2/30], Batch [4400/6000], Loss: 0.4856
Epoch [2/30], Batch [4500/6000], Loss: 0.0545
Epoch [2/30], Batch [4600/6000], Loss: 0.0484
Epoch [2/30], Batch [4700/6000], Loss: 0.1046
Epoch [2/30], Batch [4800/6000], Loss: 0.5258
Epoch [2/30], Batch [4900/6000], Loss: 0.7388
Epoch [2/30], Batch [5000/6000], Loss: 0.0561
Epoch [2/30], Batch [5100/6000], Loss: 0.0434
Epoch [2/30], Batch [5200/6000], Loss: 0.2187
Epoch [2/30], Batch [5300/6000], Loss: 0.3424
Epoch [2/30], Batch [5400/6000], Loss: 0.7329
Epoch [2/30], Batch [5500/6000], Loss: 0.1352
Epoch [2/30], Batch [5600/6000], Loss: 0.1184
Epoch [2/30], Batch [5700/6000], Loss: 0.0558
Epoch [2/30], Batch [5800/6000], Loss: 0.0719
Epoch [2/30], Batch [5900/6000], Loss: 0.3842
Epoch [2/30], Loss: 0.2237
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.0405
Epoch [3/30], Batch [100/6000], Loss: 0.8588
Epoch [3/30], Batch [200/6000], Loss: 0.0401
Epoch [3/30], Batch [300/6000], Loss: 0.2065
Epoch [3/30], Batch [400/6000], Loss: 0.0942
Epoch [3/30], Batch [500/6000], Loss: 0.2844
Epoch [3/30], Batch [600/6000], Loss: 0.1149
Epoch [3/30], Batch [700/6000], Loss: 0.1216
Epoch [3/30], Batch [800/6000], Loss: 0.1754
Epoch [3/30], Batch [900/6000], Loss: 0.2202
Epoch [3/30], Batch [1000/6000], Loss: 0.2691
Epoch [3/30], Batch [1100/6000], Loss: 0.0549
Epoch [3/30], Batch [1200/6000], Loss: 0.0720
Epoch [3/30], Batch [1300/6000], Loss: 0.0317
Epoch [3/30], Batch [1400/6000], Loss: 0.0441
Epoch [3/30], Batch [1500/6000], Loss: 0.0408
Epoch [3/30], Batch [1600/6000], Loss: 0.0594
Epoch [3/30], Batch [1700/6000], Loss: 0.1951
Epoch [3/30], Batch [1800/6000], Loss: 0.0330
Epoch [3/30], Batch [1900/6000], Loss: 0.0838
Epoch [3/30], Batch [2000/6000], Loss: 0.0400
Epoch [3/30], Batch [2100/6000], Loss: 0.0455
Epoch [3/30], Batch [2200/6000], Loss: 0.1144
Epoch [3/30], Batch [2300/6000], Loss: 0.1517
Epoch [3/30], Batch [2400/6000], Loss: 0.1611
Epoch [3/30], Batch [2500/6000], Loss: 0.0377
Epoch [3/30], Batch [2600/6000], Loss: 0.0909
Epoch [3/30], Batch [2700/6000], Loss: 0.0763
Epoch [3/30], Batch [2800/6000], Loss: 0.1626
Epoch [3/30], Batch [2900/6000], Loss: 0.0448
Epoch [3/30], Batch [3000/6000], Loss: 0.0357
Epoch [3/30], Batch [3100/6000], Loss: 0.0831
Epoch [3/30], Batch [3200/6000], Loss: 0.2039
Epoch [3/30], Batch [3300/6000], Loss: 0.0439
Epoch [3/30], Batch [3400/6000], Loss: 0.5691
Epoch [3/30], Batch [3500/6000], Loss: 0.0273
Epoch [3/30], Batch [3600/6000], Loss: 0.1710
Epoch [3/30], Batch [3700/6000], Loss: 0.0640
Epoch [3/30], Batch [3800/6000], Loss: 0.0649
Epoch [3/30], Batch [3900/6000], Loss: 0.0992
Epoch [3/30], Batch [4000/6000], Loss: 0.0431
Epoch [3/30], Batch [4100/6000], Loss: 0.8918
Epoch [3/30], Batch [4200/6000], Loss: 0.0472
Epoch [3/30], Batch [4300/6000], Loss: 0.2785
Epoch [3/30], Batch [4400/6000], Loss: 0.0430
Epoch [3/30], Batch [4500/6000], Loss: 0.0548
Epoch [3/30], Batch [4600/6000], Loss: 0.2837
Epoch [3/30], Batch [4700/6000], Loss: 0.0384
Epoch [3/30], Batch [4800/6000], Loss: 0.0805
Epoch [3/30], Batch [4900/6000], Loss: 0.0478
Epoch [3/30], Batch [5000/6000], Loss: 0.0440
Epoch [3/30], Batch [5100/6000], Loss: 0.0650
Epoch [3/30], Batch [5200/6000], Loss: 0.4617
Epoch [3/30], Batch [5300/6000], Loss: 0.3699
Epoch [3/30], Batch [5400/6000], Loss: 0.0911
Epoch [3/30], Batch [5500/6000], Loss: 0.0451
Epoch [3/30], Batch [5600/6000], Loss: 0.0869
Epoch [3/30], Batch [5700/6000], Loss: 0.0388
Epoch [3/30], Batch [5800/6000], Loss: 0.4862
Epoch [3/30], Batch [5900/6000], Loss: 0.1824
Epoch [3/30], Loss: 0.1700
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.1054
Epoch [4/30], Batch [100/6000], Loss: 0.0623
Epoch [4/30], Batch [200/6000], Loss: 0.1114
Epoch [4/30], Batch [300/6000], Loss: 0.1714
Epoch [4/30], Batch [400/6000], Loss: 0.1312
Epoch [4/30], Batch [500/6000], Loss: 0.0578
Epoch [4/30], Batch [600/6000], Loss: 0.0570
Epoch [4/30], Batch [700/6000], Loss: 0.1954
Epoch [4/30], Batch [800/6000], Loss: 0.4716
Epoch [4/30], Batch [900/6000], Loss: 0.0372
Epoch [4/30], Batch [1000/6000], Loss: 0.1671
Epoch [4/30], Batch [1100/6000], Loss: 0.0842
Epoch [4/30], Batch [1200/6000], Loss: 0.0329
Epoch [4/30], Batch [1300/6000], Loss: 0.0995
Epoch [4/30], Batch [1400/6000], Loss: 0.1059
Epoch [4/30], Batch [1500/6000], Loss: 0.2190
Epoch [4/30], Batch [1600/6000], Loss: 0.2436
Epoch [4/30], Batch [1700/6000], Loss: 0.0578
Epoch [4/30], Batch [1800/6000], Loss: 0.0373
Epoch [4/30], Batch [1900/6000], Loss: 0.0331
Epoch [4/30], Batch [2000/6000], Loss: 0.0529
Epoch [4/30], Batch [2100/6000], Loss: 0.1243
Epoch [4/30], Batch [2200/6000], Loss: 0.1428
Epoch [4/30], Batch [2300/6000], Loss: 0.3398
Epoch [4/30], Batch [2400/6000], Loss: 0.0372
Epoch [4/30], Batch [2500/6000], Loss: 0.2661
Epoch [4/30], Batch [2600/6000], Loss: 0.1241
Epoch [4/30], Batch [2700/6000], Loss: 0.0693
Epoch [4/30], Batch [2800/6000], Loss: 0.0331
Epoch [4/30], Batch [2900/6000], Loss: 0.0365
Epoch [4/30], Batch [3000/6000], Loss: 0.0479
Epoch [4/30], Batch [3100/6000], Loss: 0.1245
Epoch [4/30], Batch [3200/6000], Loss: 0.0570
Epoch [4/30], Batch [3300/6000], Loss: 0.0368
Epoch [4/30], Batch [3400/6000], Loss: 0.0375
Epoch [4/30], Batch [3500/6000], Loss: 0.1200
Epoch [4/30], Batch [3600/6000], Loss: 0.0442
Epoch [4/30], Batch [3700/6000], Loss: 0.0356
Epoch [4/30], Batch [3800/6000], Loss: 0.0317
Epoch [4/30], Batch [3900/6000], Loss: 0.1589
Epoch [4/30], Batch [4000/6000], Loss: 0.0669
Epoch [4/30], Batch [4100/6000], Loss: 0.0755
Epoch [4/30], Batch [4200/6000], Loss: 0.0429
Epoch [4/30], Batch [4300/6000], Loss: 0.0562
Epoch [4/30], Batch [4400/6000], Loss: 0.0372
Epoch [4/30], Batch [4500/6000], Loss: 0.0473
Epoch [4/30], Batch [4600/6000], Loss: 0.0442
Epoch [4/30], Batch [4700/6000], Loss: 0.0387
Epoch [4/30], Batch [4800/6000], Loss: 0.0314
Epoch [4/30], Batch [4900/6000], Loss: 0.0361
Epoch [4/30], Batch [5000/6000], Loss: 0.2277
Epoch [4/30], Batch [5100/6000], Loss: 0.1015
Epoch [4/30], Batch [5200/6000], Loss: 0.0495
Epoch [4/30], Batch [5300/6000], Loss: 0.0501
Epoch [4/30], Batch [5400/6000], Loss: 0.0430
Epoch [4/30], Batch [5500/6000], Loss: 0.2056
Epoch [4/30], Batch [5600/6000], Loss: 0.0971
Epoch [4/30], Batch [5700/6000], Loss: 0.4611
Epoch [4/30], Batch [5800/6000], Loss: 0.0662
Epoch [4/30], Batch [5900/6000], Loss: 0.0562
Epoch [4/30], Loss: 0.1407
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.0871
Epoch [5/30], Batch [100/6000], Loss: 0.2767
Epoch [5/30], Batch [200/6000], Loss: 0.4059
Epoch [5/30], Batch [300/6000], Loss: 0.0354
Epoch [5/30], Batch [400/6000], Loss: 0.0322
Epoch [5/30], Batch [500/6000], Loss: 0.0473
Epoch [5/30], Batch [600/6000], Loss: 0.0560
Epoch [5/30], Batch [700/6000], Loss: 0.0919
Epoch [5/30], Batch [800/6000], Loss: 0.1500
Epoch [5/30], Batch [900/6000], Loss: 0.2333
Epoch [5/30], Batch [1000/6000], Loss: 0.0330
Epoch [5/30], Batch [1100/6000], Loss: 0.0380
Epoch [5/30], Batch [1200/6000], Loss: 0.0306
Epoch [5/30], Batch [1300/6000], Loss: 0.0291
Epoch [5/30], Batch [1400/6000], Loss: 0.0794
Epoch [5/30], Batch [1500/6000], Loss: 0.0337
Epoch [5/30], Batch [1600/6000], Loss: 0.0839
Epoch [5/30], Batch [1700/6000], Loss: 0.0349
Epoch [5/30], Batch [1800/6000], Loss: 0.1243
Epoch [5/30], Batch [1900/6000], Loss: 0.0501
Epoch [5/30], Batch [2000/6000], Loss: 0.0468
Epoch [5/30], Batch [2100/6000], Loss: 0.0352
Epoch [5/30], Batch [2200/6000], Loss: 0.0586
Epoch [5/30], Batch [2300/6000], Loss: 0.0910
Epoch [5/30], Batch [2400/6000], Loss: 0.0392
Epoch [5/30], Batch [2500/6000], Loss: 0.0299
Epoch [5/30], Batch [2600/6000], Loss: 0.0695
Epoch [5/30], Batch [2700/6000], Loss: 0.0296
Epoch [5/30], Batch [2800/6000], Loss: 0.0316
Epoch [5/30], Batch [2900/6000], Loss: 0.0299
Epoch [5/30], Batch [3000/6000], Loss: 0.0255
Epoch [5/30], Batch [3100/6000], Loss: 0.0305
Epoch [5/30], Batch [3200/6000], Loss: 0.4748
Epoch [5/30], Batch [3300/6000], Loss: 0.0448
Epoch [5/30], Batch [3400/6000], Loss: 0.0356
Epoch [5/30], Batch [3500/6000], Loss: 0.0298
Epoch [5/30], Batch [3600/6000], Loss: 0.1620
Epoch [5/30], Batch [3700/6000], Loss: 0.4489
Epoch [5/30], Batch [3800/6000], Loss: 0.2174
Epoch [5/30], Batch [3900/6000], Loss: 0.0345
Epoch [5/30], Batch [4000/6000], Loss: 0.3757
Epoch [5/30], Batch [4100/6000], Loss: 0.1961
Epoch [5/30], Batch [4200/6000], Loss: 0.6106
Epoch [5/30], Batch [4300/6000], Loss: 0.1707
Epoch [5/30], Batch [4400/6000], Loss: 0.0286
Epoch [5/30], Batch [4500/6000], Loss: 0.0464
Epoch [5/30], Batch [4600/6000], Loss: 0.1188
Epoch [5/30], Batch [4700/6000], Loss: 0.1794
Epoch [5/30], Batch [4800/6000], Loss: 0.0296
Epoch [5/30], Batch [4900/6000], Loss: 0.0277
Epoch [5/30], Batch [5000/6000], Loss: 0.0574
Epoch [5/30], Batch [5100/6000], Loss: 0.0392
Epoch [5/30], Batch [5200/6000], Loss: 0.0414
Epoch [5/30], Batch [5300/6000], Loss: 0.0357
Epoch [5/30], Batch [5400/6000], Loss: 0.0410
Epoch [5/30], Batch [5500/6000], Loss: 0.2087
Epoch [5/30], Batch [5600/6000], Loss: 0.0463
Epoch [5/30], Batch [5700/6000], Loss: 0.0359
Epoch [5/30], Batch [5800/6000], Loss: 0.0479
Epoch [5/30], Batch [5900/6000], Loss: 0.3615
Epoch [5/30], Loss: 0.1202
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.0560
Epoch [6/30], Batch [100/6000], Loss: 0.0492
Epoch [6/30], Batch [200/6000], Loss: 0.0542
Epoch [6/30], Batch [300/6000], Loss: 0.0416
Epoch [6/30], Batch [400/6000], Loss: 0.0300
Epoch [6/30], Batch [500/6000], Loss: 0.0569
Epoch [6/30], Batch [600/6000], Loss: 0.0294
Epoch [6/30], Batch [700/6000], Loss: 0.0348
Epoch [6/30], Batch [800/6000], Loss: 0.0322
Epoch [6/30], Batch [900/6000], Loss: 0.2436
Epoch [6/30], Batch [1000/6000], Loss: 0.0474
Epoch [6/30], Batch [1100/6000], Loss: 0.2293
Epoch [6/30], Batch [1200/6000], Loss: 0.0423
Epoch [6/30], Batch [1300/6000], Loss: 0.4626
Epoch [6/30], Batch [1400/6000], Loss: 0.0290
Epoch [6/30], Batch [1500/6000], Loss: 0.0687
Epoch [6/30], Batch [1600/6000], Loss: 0.4204
Epoch [6/30], Batch [1700/6000], Loss: 0.0389
Epoch [6/30], Batch [1800/6000], Loss: 0.0824
Epoch [6/30], Batch [1900/6000], Loss: 0.6246
Epoch [6/30], Batch [2000/6000], Loss: 0.2519
Epoch [6/30], Batch [2100/6000], Loss: 0.0392
Epoch [6/30], Batch [2200/6000], Loss: 0.4366
Epoch [6/30], Batch [2300/6000], Loss: 0.2643
Epoch [6/30], Batch [2400/6000], Loss: 0.0429
Epoch [6/30], Batch [2500/6000], Loss: 0.1121
Epoch [6/30], Batch [2600/6000], Loss: 0.1692
Epoch [6/30], Batch [2700/6000], Loss: 0.1688
Epoch [6/30], Batch [2800/6000], Loss: 0.1453
Epoch [6/30], Batch [2900/6000], Loss: 0.0291
Epoch [6/30], Batch [3000/6000], Loss: 0.1484
Epoch [6/30], Batch [3100/6000], Loss: 0.0233
Epoch [6/30], Batch [3200/6000], Loss: 0.0341
Epoch [6/30], Batch [3300/6000], Loss: 0.2715
Epoch [6/30], Batch [3400/6000], Loss: 0.1544
Epoch [6/30], Batch [3500/6000], Loss: 0.0503
Epoch [6/30], Batch [3600/6000], Loss: 0.0898
Epoch [6/30], Batch [3700/6000], Loss: 0.0729
Epoch [6/30], Batch [3800/6000], Loss: 0.4228
Epoch [6/30], Batch [3900/6000], Loss: 0.2149
Epoch [6/30], Batch [4000/6000], Loss: 0.0319
Epoch [6/30], Batch [4100/6000], Loss: 0.2055
Epoch [6/30], Batch [4200/6000], Loss: 0.3484
Epoch [6/30], Batch [4300/6000], Loss: 0.0288
Epoch [6/30], Batch [4400/6000], Loss: 0.0275
Epoch [6/30], Batch [4500/6000], Loss: 0.1368
Epoch [6/30], Batch [4600/6000], Loss: 0.0406
Epoch [6/30], Batch [4700/6000], Loss: 0.0290
Epoch [6/30], Batch [4800/6000], Loss: 0.0880
Epoch [6/30], Batch [4900/6000], Loss: 0.2928
Epoch [6/30], Batch [5000/6000], Loss: 0.0300
Epoch [6/30], Batch [5100/6000], Loss: 0.1678
Epoch [6/30], Batch [5200/6000], Loss: 0.1062
Epoch [6/30], Batch [5300/6000], Loss: 0.1379
Epoch [6/30], Batch [5400/6000], Loss: 0.8550
Epoch [6/30], Batch [5500/6000], Loss: 0.1110
Epoch [6/30], Batch [5600/6000], Loss: 0.0336
Epoch [6/30], Batch [5700/6000], Loss: 0.0856
Epoch [6/30], Batch [5800/6000], Loss: 0.0649
Epoch [6/30], Batch [5900/6000], Loss: 0.0468
Epoch [6/30], Loss: 0.1053
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.3119
Epoch [7/30], Batch [100/6000], Loss: 0.0464
Epoch [7/30], Batch [200/6000], Loss: 0.0285
Epoch [7/30], Batch [300/6000], Loss: 0.0424
Epoch [7/30], Batch [400/6000], Loss: 0.0477
Epoch [7/30], Batch [500/6000], Loss: 0.1864
Epoch [7/30], Batch [600/6000], Loss: 0.0250
Epoch [7/30], Batch [700/6000], Loss: 0.0300
Epoch [7/30], Batch [800/6000], Loss: 0.0404
Epoch [7/30], Batch [900/6000], Loss: 0.1934
Epoch [7/30], Batch [1000/6000], Loss: 0.1294
Epoch [7/30], Batch [1100/6000], Loss: 0.0305
Epoch [7/30], Batch [1200/6000], Loss: 0.0887
Epoch [7/30], Batch [1300/6000], Loss: 0.0436
Epoch [7/30], Batch [1400/6000], Loss: 0.0321
Epoch [7/30], Batch [1500/6000], Loss: 0.0347
Epoch [7/30], Batch [1600/6000], Loss: 0.0359
Epoch [7/30], Batch [1700/6000], Loss: 0.0377
Epoch [7/30], Batch [1800/6000], Loss: 0.0291
Epoch [7/30], Batch [1900/6000], Loss: 0.4954
Epoch [7/30], Batch [2000/6000], Loss: 0.1187
Epoch [7/30], Batch [2100/6000], Loss: 0.0352
Epoch [7/30], Batch [2200/6000], Loss: 0.1187
Epoch [7/30], Batch [2300/6000], Loss: 0.0219
Epoch [7/30], Batch [2400/6000], Loss: 0.3424
Epoch [7/30], Batch [2500/6000], Loss: 0.0261
Epoch [7/30], Batch [2600/6000], Loss: 0.0359
Epoch [7/30], Batch [2700/6000], Loss: 0.0222
Epoch [7/30], Batch [2800/6000], Loss: 0.0800
Epoch [7/30], Batch [2900/6000], Loss: 0.0791
Epoch [7/30], Batch [3000/6000], Loss: 0.1514
Epoch [7/30], Batch [3100/6000], Loss: 0.0257
Epoch [7/30], Batch [3200/6000], Loss: 0.0346
Epoch [7/30], Batch [3300/6000], Loss: 0.0307
Epoch [7/30], Batch [3400/6000], Loss: 0.0651
Epoch [7/30], Batch [3500/6000], Loss: 0.3112
Epoch [7/30], Batch [3600/6000], Loss: 0.0297
Epoch [7/30], Batch [3700/6000], Loss: 0.0712
Epoch [7/30], Batch [3800/6000], Loss: 0.0346
Epoch [7/30], Batch [3900/6000], Loss: 0.1151
Epoch [7/30], Batch [4000/6000], Loss: 0.0675
Epoch [7/30], Batch [4100/6000], Loss: 0.1440
Epoch [7/30], Batch [4200/6000], Loss: 0.6089
Epoch [7/30], Batch [4300/6000], Loss: 0.0348
Epoch [7/30], Batch [4400/6000], Loss: 0.0328
Epoch [7/30], Batch [4500/6000], Loss: 0.0970
Epoch [7/30], Batch [4600/6000], Loss: 0.0346
Epoch [7/30], Batch [4700/6000], Loss: 0.0370
Epoch [7/30], Batch [4800/6000], Loss: 0.1651
Epoch [7/30], Batch [4900/6000], Loss: 0.3278
Epoch [7/30], Batch [5000/6000], Loss: 0.1525
Epoch [7/30], Batch [5100/6000], Loss: 0.0273
Epoch [7/30], Batch [5200/6000], Loss: 0.0395
Epoch [7/30], Batch [5300/6000], Loss: 0.0721
Epoch [7/30], Batch [5400/6000], Loss: 0.0212
Epoch [7/30], Batch [5500/6000], Loss: 0.0366
Epoch [7/30], Batch [5600/6000], Loss: 0.0270
Epoch [7/30], Batch [5700/6000], Loss: 0.0330
Epoch [7/30], Batch [5800/6000], Loss: 0.0721
Epoch [7/30], Batch [5900/6000], Loss: 0.0932
Epoch [7/30], Loss: 0.0930
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.0402
Epoch [8/30], Batch [100/6000], Loss: 0.0495
Epoch [8/30], Batch [200/6000], Loss: 0.0602
Epoch [8/30], Batch [300/6000], Loss: 0.0247
Epoch [8/30], Batch [400/6000], Loss: 0.0295
Epoch [8/30], Batch [500/6000], Loss: 0.1545
Epoch [8/30], Batch [600/6000], Loss: 0.0469
Epoch [8/30], Batch [700/6000], Loss: 0.1925
Epoch [8/30], Batch [800/6000], Loss: 0.0475
Epoch [8/30], Batch [900/6000], Loss: 0.0288
Epoch [8/30], Batch [1000/6000], Loss: 0.0247
Epoch [8/30], Batch [1100/6000], Loss: 0.0243
Epoch [8/30], Batch [1200/6000], Loss: 0.9861
Epoch [8/30], Batch [1300/6000], Loss: 0.0247
Epoch [8/30], Batch [1400/6000], Loss: 0.0257
Epoch [8/30], Batch [1500/6000], Loss: 0.1380
Epoch [8/30], Batch [1600/6000], Loss: 0.0439
Epoch [8/30], Batch [1700/6000], Loss: 0.0239
Epoch [8/30], Batch [1800/6000], Loss: 0.1181
Epoch [8/30], Batch [1900/6000], Loss: 0.0413
Epoch [8/30], Batch [2000/6000], Loss: 0.2008
Epoch [8/30], Batch [2100/6000], Loss: 0.0225
Epoch [8/30], Batch [2200/6000], Loss: 0.0296
Epoch [8/30], Batch [2300/6000], Loss: 0.0269
Epoch [8/30], Batch [2400/6000], Loss: 0.0204
Epoch [8/30], Batch [2500/6000], Loss: 0.3811
Epoch [8/30], Batch [2600/6000], Loss: 0.0307
Epoch [8/30], Batch [2700/6000], Loss: 0.3570
Epoch [8/30], Batch [2800/6000], Loss: 0.0197
Epoch [8/30], Batch [2900/6000], Loss: 0.0237
Epoch [8/30], Batch [3000/6000], Loss: 0.0209
Epoch [8/30], Batch [3100/6000], Loss: 0.4940
Epoch [8/30], Batch [3200/6000], Loss: 0.0369
Epoch [8/30], Batch [3300/6000], Loss: 0.2845
Epoch [8/30], Batch [3400/6000], Loss: 0.0175
Epoch [8/30], Batch [3500/6000], Loss: 0.0231
Epoch [8/30], Batch [3600/6000], Loss: 0.0286
Epoch [8/30], Batch [3700/6000], Loss: 0.0529
Epoch [8/30], Batch [3800/6000], Loss: 0.3382
Epoch [8/30], Batch [3900/6000], Loss: 0.1402
Epoch [8/30], Batch [4000/6000], Loss: 0.0775
Epoch [8/30], Batch [4100/6000], Loss: 0.0284
Epoch [8/30], Batch [4200/6000], Loss: 0.0601
Epoch [8/30], Batch [4300/6000], Loss: 0.0761
Epoch [8/30], Batch [4400/6000], Loss: 0.0716
Epoch [8/30], Batch [4500/6000], Loss: 0.0249
Epoch [8/30], Batch [4600/6000], Loss: 0.0941
Epoch [8/30], Batch [4700/6000], Loss: 0.0251
Epoch [8/30], Batch [4800/6000], Loss: 0.0220
Epoch [8/30], Batch [4900/6000], Loss: 0.0437
Epoch [8/30], Batch [5000/6000], Loss: 0.0290
Epoch [8/30], Batch [5100/6000], Loss: 0.2907
Epoch [8/30], Batch [5200/6000], Loss: 0.3496
Epoch [8/30], Batch [5300/6000], Loss: 0.0358
Epoch [8/30], Batch [5400/6000], Loss: 0.0715
Epoch [8/30], Batch [5500/6000], Loss: 0.0841
Epoch [8/30], Batch [5600/6000], Loss: 0.0484
Epoch [8/30], Batch [5700/6000], Loss: 0.0402
Epoch [8/30], Batch [5800/6000], Loss: 0.0198
Epoch [8/30], Batch [5900/6000], Loss: 0.0251
Epoch [8/30], Loss: 0.0829
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.0432
Epoch [9/30], Batch [100/6000], Loss: 0.2421
Epoch [9/30], Batch [200/6000], Loss: 0.0360
Epoch [9/30], Batch [300/6000], Loss: 0.0313
Epoch [9/30], Batch [400/6000], Loss: 0.4563
Epoch [9/30], Batch [500/6000], Loss: 0.0252
Epoch [9/30], Batch [600/6000], Loss: 0.1651
Epoch [9/30], Batch [700/6000], Loss: 0.3332
Epoch [9/30], Batch [800/6000], Loss: 0.0686
Epoch [9/30], Batch [900/6000], Loss: 0.0352
Epoch [9/30], Batch [1000/6000], Loss: 0.1666
Epoch [9/30], Batch [1100/6000], Loss: 0.0268
Epoch [9/30], Batch [1200/6000], Loss: 0.2984
Epoch [9/30], Batch [1300/6000], Loss: 0.0298
Epoch [9/30], Batch [1400/6000], Loss: 0.0251
Epoch [9/30], Batch [1500/6000], Loss: 0.0510
Epoch [9/30], Batch [1600/6000], Loss: 0.0332
Epoch [9/30], Batch [1700/6000], Loss: 0.0302
Epoch [9/30], Batch [1800/6000], Loss: 0.0258
Epoch [9/30], Batch [1900/6000], Loss: 0.0278
Epoch [9/30], Batch [2000/6000], Loss: 0.0289
Epoch [9/30], Batch [2100/6000], Loss: 0.0274
Epoch [9/30], Batch [2200/6000], Loss: 0.0381
Epoch [9/30], Batch [2300/6000], Loss: 0.0378
Epoch [9/30], Batch [2400/6000], Loss: 0.1652
Epoch [9/30], Batch [2500/6000], Loss: 0.0477
Epoch [9/30], Batch [2600/6000], Loss: 0.0300
Epoch [9/30], Batch [2700/6000], Loss: 0.4532
Epoch [9/30], Batch [2800/6000], Loss: 0.0270
Epoch [9/30], Batch [2900/6000], Loss: 0.0331
Epoch [9/30], Batch [3000/6000], Loss: 0.0225
Epoch [9/30], Batch [3100/6000], Loss: 0.0486
Epoch [9/30], Batch [3200/6000], Loss: 0.0262
Epoch [9/30], Batch [3300/6000], Loss: 0.0256
Epoch [9/30], Batch [3400/6000], Loss: 0.2806
Epoch [9/30], Batch [3500/6000], Loss: 0.0190
Epoch [9/30], Batch [3600/6000], Loss: 0.0243
Epoch [9/30], Batch [3700/6000], Loss: 0.0264
Epoch [9/30], Batch [3800/6000], Loss: 0.0835
Epoch [9/30], Batch [3900/6000], Loss: 0.1749
Epoch [9/30], Batch [4000/6000], Loss: 0.1234
Epoch [9/30], Batch [4100/6000], Loss: 0.0279
Epoch [9/30], Batch [4200/6000], Loss: 0.0158
Epoch [9/30], Batch [4300/6000], Loss: 0.0241
Epoch [9/30], Batch [4400/6000], Loss: 0.0252
Epoch [9/30], Batch [4500/6000], Loss: 0.0249
Epoch [9/30], Batch [4600/6000], Loss: 0.0395
Epoch [9/30], Batch [4700/6000], Loss: 0.0289
Epoch [9/30], Batch [4800/6000], Loss: 0.0587
Epoch [9/30], Batch [4900/6000], Loss: 0.0360
Epoch [9/30], Batch [5000/6000], Loss: 0.0250
Epoch [9/30], Batch [5100/6000], Loss: 0.0287
Epoch [9/30], Batch [5200/6000], Loss: 0.0178
Epoch [9/30], Batch [5300/6000], Loss: 0.0475
Epoch [9/30], Batch [5400/6000], Loss: 0.0214
Epoch [9/30], Batch [5500/6000], Loss: 0.0383
Epoch [9/30], Batch [5600/6000], Loss: 0.0229
Epoch [9/30], Batch [5700/6000], Loss: 0.0216
Epoch [9/30], Batch [5800/6000], Loss: 0.0475
Epoch [9/30], Batch [5900/6000], Loss: 0.0277
Epoch [9/30], Loss: 0.0746
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.0279
Epoch [10/30], Batch [100/6000], Loss: 0.0194
Epoch [10/30], Batch [200/6000], Loss: 0.0246
Epoch [10/30], Batch [300/6000], Loss: 0.0220
Epoch [10/30], Batch [400/6000], Loss: 0.0346
Epoch [10/30], Batch [500/6000], Loss: 0.0213
Epoch [10/30], Batch [600/6000], Loss: 0.0348
Epoch [10/30], Batch [700/6000], Loss: 0.0273
Epoch [10/30], Batch [800/6000], Loss: 0.0505
Epoch [10/30], Batch [900/6000], Loss: 0.0368
Epoch [10/30], Batch [1000/6000], Loss: 0.0214
Epoch [10/30], Batch [1100/6000], Loss: 0.0325
Epoch [10/30], Batch [1200/6000], Loss: 0.0878
Epoch [10/30], Batch [1300/6000], Loss: 0.0310
Epoch [10/30], Batch [1400/6000], Loss: 0.0508
Epoch [10/30], Batch [1500/6000], Loss: 0.0327
Epoch [10/30], Batch [1600/6000], Loss: 0.0382
Epoch [10/30], Batch [1700/6000], Loss: 0.0285
Epoch [10/30], Batch [1800/6000], Loss: 0.0256
Epoch [10/30], Batch [1900/6000], Loss: 0.0252
Epoch [10/30], Batch [2000/6000], Loss: 0.0248
Epoch [10/30], Batch [2100/6000], Loss: 0.0216
Epoch [10/30], Batch [2200/6000], Loss: 0.0188
Epoch [10/30], Batch [2300/6000], Loss: 0.0242
Epoch [10/30], Batch [2400/6000], Loss: 0.0281
Epoch [10/30], Batch [2500/6000], Loss: 0.1237
Epoch [10/30], Batch [2600/6000], Loss: 0.1091
Epoch [10/30], Batch [2700/6000], Loss: 0.0166
Epoch [10/30], Batch [2800/6000], Loss: 0.0275
Epoch [10/30], Batch [2900/6000], Loss: 0.0280
Epoch [10/30], Batch [3000/6000], Loss: 0.0287
Epoch [10/30], Batch [3100/6000], Loss: 0.0254
Epoch [10/30], Batch [3200/6000], Loss: 0.0214
Epoch [10/30], Batch [3300/6000], Loss: 0.0317
Epoch [10/30], Batch [3400/6000], Loss: 0.0237
Epoch [10/30], Batch [3500/6000], Loss: 0.5759
Epoch [10/30], Batch [3600/6000], Loss: 0.0860
Epoch [10/30], Batch [3700/6000], Loss: 0.3312
Epoch [10/30], Batch [3800/6000], Loss: 0.0915
Epoch [10/30], Batch [3900/6000], Loss: 0.0271
Epoch [10/30], Batch [4000/6000], Loss: 0.6686
Epoch [10/30], Batch [4100/6000], Loss: 0.1746
Epoch [10/30], Batch [4200/6000], Loss: 0.2381
Epoch [10/30], Batch [4300/6000], Loss: 0.0209
Epoch [10/30], Batch [4400/6000], Loss: 0.0313
Epoch [10/30], Batch [4500/6000], Loss: 0.0236
Epoch [10/30], Batch [4600/6000], Loss: 0.0387
Epoch [10/30], Batch [4700/6000], Loss: 0.0443
Epoch [10/30], Batch [4800/6000], Loss: 0.0262
Epoch [10/30], Batch [4900/6000], Loss: 0.0225
Epoch [10/30], Batch [5000/6000], Loss: 0.0221
Epoch [10/30], Batch [5100/6000], Loss: 0.6001
Epoch [10/30], Batch [5200/6000], Loss: 0.0641
Epoch [10/30], Batch [5300/6000], Loss: 0.2493
Epoch [10/30], Batch [5400/6000], Loss: 0.0294
Epoch [10/30], Batch [5500/6000], Loss: 0.3466
Epoch [10/30], Batch [5600/6000], Loss: 0.0257
Epoch [10/30], Batch [5700/6000], Loss: 0.0255
Epoch [10/30], Batch [5800/6000], Loss: 0.0221
Epoch [10/30], Batch [5900/6000], Loss: 0.1230
Epoch [10/30], Loss: 0.0690
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.0332
Epoch [11/30], Batch [100/6000], Loss: 0.0346
Epoch [11/30], Batch [200/6000], Loss: 0.0702
Epoch [11/30], Batch [300/6000], Loss: 0.2005
Epoch [11/30], Batch [400/6000], Loss: 0.3492
Epoch [11/30], Batch [500/6000], Loss: 0.0389
Epoch [11/30], Batch [600/6000], Loss: 0.0255
Epoch [11/30], Batch [700/6000], Loss: 0.0210
Epoch [11/30], Batch [800/6000], Loss: 0.0593
Epoch [11/30], Batch [900/6000], Loss: 0.0258
Epoch [11/30], Batch [1000/6000], Loss: 0.3344
Epoch [11/30], Batch [1100/6000], Loss: 0.3640
Epoch [11/30], Batch [1200/6000], Loss: 0.0202
Epoch [11/30], Batch [1300/6000], Loss: 0.0269
Epoch [11/30], Batch [1400/6000], Loss: 0.1249
Epoch [11/30], Batch [1500/6000], Loss: 0.0498
Epoch [11/30], Batch [1600/6000], Loss: 0.0186
Epoch [11/30], Batch [1700/6000], Loss: 0.0235
Epoch [11/30], Batch [1800/6000], Loss: 0.0354
Epoch [11/30], Batch [1900/6000], Loss: 0.0313
Epoch [11/30], Batch [2000/6000], Loss: 0.0935
Epoch [11/30], Batch [2100/6000], Loss: 0.0713
Epoch [11/30], Batch [2200/6000], Loss: 0.0279
Epoch [11/30], Batch [2300/6000], Loss: 0.0275
Epoch [11/30], Batch [2400/6000], Loss: 0.0239
Epoch [11/30], Batch [2500/6000], Loss: 0.0199
Epoch [11/30], Batch [2600/6000], Loss: 0.2022
Epoch [11/30], Batch [2700/6000], Loss: 0.3663
Epoch [11/30], Batch [2800/6000], Loss: 0.0258
Epoch [11/30], Batch [2900/6000], Loss: 0.0234
Epoch [11/30], Batch [3000/6000], Loss: 0.0604
Epoch [11/30], Batch [3100/6000], Loss: 0.0875
Epoch [11/30], Batch [3200/6000], Loss: 0.0207
Epoch [11/30], Batch [3300/6000], Loss: 0.0194
Epoch [11/30], Batch [3400/6000], Loss: 0.0286
Epoch [11/30], Batch [3500/6000], Loss: 0.0464
Epoch [11/30], Batch [3600/6000], Loss: 0.0199
Epoch [11/30], Batch [3700/6000], Loss: 0.0332
Epoch [11/30], Batch [3800/6000], Loss: 0.0859
Epoch [11/30], Batch [3900/6000], Loss: 0.0261
Epoch [11/30], Batch [4000/6000], Loss: 0.0254
Epoch [11/30], Batch [4100/6000], Loss: 0.0514
Epoch [11/30], Batch [4200/6000], Loss: 0.0367
Epoch [11/30], Batch [4300/6000], Loss: 0.0265
Epoch [11/30], Batch [4400/6000], Loss: 0.0274
Epoch [11/30], Batch [4500/6000], Loss: 0.0421
Epoch [11/30], Batch [4600/6000], Loss: 0.0228
Epoch [11/30], Batch [4700/6000], Loss: 0.0268
Epoch [11/30], Batch [4800/6000], Loss: 0.0733
Epoch [11/30], Batch [4900/6000], Loss: 0.1631
Epoch [11/30], Batch [5000/6000], Loss: 0.0234
Epoch [11/30], Batch [5100/6000], Loss: 0.0631
Epoch [11/30], Batch [5200/6000], Loss: 0.1407
Epoch [11/30], Batch [5300/6000], Loss: 0.0269
Epoch [11/30], Batch [5400/6000], Loss: 0.0234
Epoch [11/30], Batch [5500/6000], Loss: 0.1011
Epoch [11/30], Batch [5600/6000], Loss: 0.0305
Epoch [11/30], Batch [5700/6000], Loss: 0.1007
Epoch [11/30], Batch [5800/6000], Loss: 0.0452
Epoch [11/30], Batch [5900/6000], Loss: 0.0232
Epoch [11/30], Loss: 0.0629
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.0361
Epoch [12/30], Batch [100/6000], Loss: 0.0227
Epoch [12/30], Batch [200/6000], Loss: 0.0653
Epoch [12/30], Batch [300/6000], Loss: 0.0261
Epoch [12/30], Batch [400/6000], Loss: 0.0256
Epoch [12/30], Batch [500/6000], Loss: 0.0307
Epoch [12/30], Batch [600/6000], Loss: 0.0256
Epoch [12/30], Batch [700/6000], Loss: 0.0640
Epoch [12/30], Batch [800/6000], Loss: 0.1018
Epoch [12/30], Batch [900/6000], Loss: 0.0432
Epoch [12/30], Batch [1000/6000], Loss: 0.0166
Epoch [12/30], Batch [1100/6000], Loss: 0.0197
Epoch [12/30], Batch [1200/6000], Loss: 0.0632
Epoch [12/30], Batch [1300/6000], Loss: 0.0212
Epoch [12/30], Batch [1400/6000], Loss: 0.0177
Epoch [12/30], Batch [1500/6000], Loss: 0.0252
Epoch [12/30], Batch [1600/6000], Loss: 0.1464
Epoch [12/30], Batch [1700/6000], Loss: 0.0171
Epoch [12/30], Batch [1800/6000], Loss: 0.0286
Epoch [12/30], Batch [1900/6000], Loss: 0.0288
Epoch [12/30], Batch [2000/6000], Loss: 0.0223
Epoch [12/30], Batch [2100/6000], Loss: 0.0236
Epoch [12/30], Batch [2200/6000], Loss: 0.0252
Epoch [12/30], Batch [2300/6000], Loss: 0.0230
Epoch [12/30], Batch [2400/6000], Loss: 0.0331
Epoch [12/30], Batch [2500/6000], Loss: 0.0355
Epoch [12/30], Batch [2600/6000], Loss: 0.0223
Epoch [12/30], Batch [2700/6000], Loss: 0.0200
Epoch [12/30], Batch [2800/6000], Loss: 0.0228
Epoch [12/30], Batch [2900/6000], Loss: 0.0336
Epoch [12/30], Batch [3000/6000], Loss: 0.1377
Epoch [12/30], Batch [3100/6000], Loss: 0.0666
Epoch [12/30], Batch [3200/6000], Loss: 0.3396
Epoch [12/30], Batch [3300/6000], Loss: 0.0298
Epoch [12/30], Batch [3400/6000], Loss: 0.2213
Epoch [12/30], Batch [3500/6000], Loss: 0.0253
Epoch [12/30], Batch [3600/6000], Loss: 0.0247
Epoch [12/30], Batch [3700/6000], Loss: 0.1041
Epoch [12/30], Batch [3800/6000], Loss: 0.0264
Epoch [12/30], Batch [3900/6000], Loss: 0.0257
Epoch [12/30], Batch [4000/6000], Loss: 0.0254
Epoch [12/30], Batch [4100/6000], Loss: 0.0256
Epoch [12/30], Batch [4200/6000], Loss: 0.1854
Epoch [12/30], Batch [4300/6000], Loss: 0.0249
Epoch [12/30], Batch [4400/6000], Loss: 0.0198
Epoch [12/30], Batch [4500/6000], Loss: 0.0218
Epoch [12/30], Batch [4600/6000], Loss: 0.0232
Epoch [12/30], Batch [4700/6000], Loss: 0.0187
Epoch [12/30], Batch [4800/6000], Loss: 0.0184
Epoch [12/30], Batch [4900/6000], Loss: 0.0264
Epoch [12/30], Batch [5000/6000], Loss: 0.0287
Epoch [12/30], Batch [5100/6000], Loss: 0.2253
Epoch [12/30], Batch [5200/6000], Loss: 0.0220
Epoch [12/30], Batch [5300/6000], Loss: 0.0304
Epoch [12/30], Batch [5400/6000], Loss: 0.0487
Epoch [12/30], Batch [5500/6000], Loss: 0.1853
Epoch [12/30], Batch [5600/6000], Loss: 0.0193
Epoch [12/30], Batch [5700/6000], Loss: 0.0270
Epoch [12/30], Batch [5800/6000], Loss: 0.0201
Epoch [12/30], Batch [5900/6000], Loss: 0.0350
Epoch [12/30], Loss: 0.0576
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.0230
Epoch [13/30], Batch [100/6000], Loss: 0.0174
Epoch [13/30], Batch [200/6000], Loss: 0.0185
Epoch [13/30], Batch [300/6000], Loss: 0.0265
Epoch [13/30], Batch [400/6000], Loss: 0.0302
Epoch [13/30], Batch [500/6000], Loss: 0.0231
Epoch [13/30], Batch [600/6000], Loss: 0.0450
Epoch [13/30], Batch [700/6000], Loss: 0.1498
Epoch [13/30], Batch [800/6000], Loss: 0.0212
Epoch [13/30], Batch [900/6000], Loss: 0.0226
Epoch [13/30], Batch [1000/6000], Loss: 0.4826
Epoch [13/30], Batch [1100/6000], Loss: 0.0214
Epoch [13/30], Batch [1200/6000], Loss: 0.0793
Epoch [13/30], Batch [1300/6000], Loss: 0.0239
Epoch [13/30], Batch [1400/6000], Loss: 0.1440
Epoch [13/30], Batch [1500/6000], Loss: 0.0464
Epoch [13/30], Batch [1600/6000], Loss: 0.0545
Epoch [13/30], Batch [1700/6000], Loss: 0.0161
Epoch [13/30], Batch [1800/6000], Loss: 0.0186
Epoch [13/30], Batch [1900/6000], Loss: 0.0233
Epoch [13/30], Batch [2000/6000], Loss: 0.2472
Epoch [13/30], Batch [2100/6000], Loss: 0.0185
Epoch [13/30], Batch [2200/6000], Loss: 0.0202
Epoch [13/30], Batch [2300/6000], Loss: 0.0308
Epoch [13/30], Batch [2400/6000], Loss: 0.0271
Epoch [13/30], Batch [2500/6000], Loss: 0.0480
Epoch [13/30], Batch [2600/6000], Loss: 0.1197
Epoch [13/30], Batch [2700/6000], Loss: 0.0275
Epoch [13/30], Batch [2800/6000], Loss: 0.0280
Epoch [13/30], Batch [2900/6000], Loss: 0.0204
Epoch [13/30], Batch [3000/6000], Loss: 0.0408
Epoch [13/30], Batch [3100/6000], Loss: 0.0300
Epoch [13/30], Batch [3200/6000], Loss: 0.0283
Epoch [13/30], Batch [3300/6000], Loss: 0.0492
Epoch [13/30], Batch [3400/6000], Loss: 0.0835
Epoch [13/30], Batch [3500/6000], Loss: 0.0251
Epoch [13/30], Batch [3600/6000], Loss: 0.0234
Epoch [13/30], Batch [3700/6000], Loss: 0.0228
Epoch [13/30], Batch [3800/6000], Loss: 0.0335
Epoch [13/30], Batch [3900/6000], Loss: 0.0177
Epoch [13/30], Batch [4000/6000], Loss: 0.3346
Epoch [13/30], Batch [4100/6000], Loss: 0.0677
Epoch [13/30], Batch [4200/6000], Loss: 0.0245
Epoch [13/30], Batch [4300/6000], Loss: 0.2904
Epoch [13/30], Batch [4400/6000], Loss: 0.1011
Epoch [13/30], Batch [4500/6000], Loss: 0.0185
Epoch [13/30], Batch [4600/6000], Loss: 0.0663
Epoch [13/30], Batch [4700/6000], Loss: 0.0387
Epoch [13/30], Batch [4800/6000], Loss: 0.0219
Epoch [13/30], Batch [4900/6000], Loss: 0.0186
Epoch [13/30], Batch [5000/6000], Loss: 0.0184
Epoch [13/30], Batch [5100/6000], Loss: 0.0335
Epoch [13/30], Batch [5200/6000], Loss: 0.0173
Epoch [13/30], Batch [5300/6000], Loss: 0.0222
Epoch [13/30], Batch [5400/6000], Loss: 0.0638
Epoch [13/30], Batch [5500/6000], Loss: 0.0235
Epoch [13/30], Batch [5600/6000], Loss: 0.0278
Epoch [13/30], Batch [5700/6000], Loss: 0.0297
Epoch [13/30], Batch [5800/6000], Loss: 0.0228
Epoch [13/30], Batch [5900/6000], Loss: 0.0341
Epoch [13/30], Loss: 0.0538
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.0227
Epoch [14/30], Batch [100/6000], Loss: 0.0261
Epoch [14/30], Batch [200/6000], Loss: 0.0253
Epoch [14/30], Batch [300/6000], Loss: 0.0235
Epoch [14/30], Batch [400/6000], Loss: 0.0193
Epoch [14/30], Batch [500/6000], Loss: 0.0244
Epoch [14/30], Batch [600/6000], Loss: 0.0273
Epoch [14/30], Batch [700/6000], Loss: 0.1470
Epoch [14/30], Batch [800/6000], Loss: 0.0274
Epoch [14/30], Batch [900/6000], Loss: 0.0190
Epoch [14/30], Batch [1000/6000], Loss: 0.0194
Epoch [14/30], Batch [1100/6000], Loss: 0.0204
Epoch [14/30], Batch [1200/6000], Loss: 0.0253
Epoch [14/30], Batch [1300/6000], Loss: 0.0343
Epoch [14/30], Batch [1400/6000], Loss: 0.0253
Epoch [14/30], Batch [1500/6000], Loss: 0.0332
Epoch [14/30], Batch [1600/6000], Loss: 0.2620
Epoch [14/30], Batch [1700/6000], Loss: 0.0205
Epoch [14/30], Batch [1800/6000], Loss: 0.0246
Epoch [14/30], Batch [1900/6000], Loss: 0.0293
Epoch [14/30], Batch [2000/6000], Loss: 0.0242
Epoch [14/30], Batch [2100/6000], Loss: 0.0187
Epoch [14/30], Batch [2200/6000], Loss: 0.0633
Epoch [14/30], Batch [2300/6000], Loss: 0.0224
Epoch [14/30], Batch [2400/6000], Loss: 0.1033
Epoch [14/30], Batch [2500/6000], Loss: 0.0294
Epoch [14/30], Batch [2600/6000], Loss: 0.0216
Epoch [14/30], Batch [2700/6000], Loss: 0.0199
Epoch [14/30], Batch [2800/6000], Loss: 0.0216
Epoch [14/30], Batch [2900/6000], Loss: 0.0308
Epoch [14/30], Batch [3000/6000], Loss: 0.0252
Epoch [14/30], Batch [3100/6000], Loss: 0.0230
Epoch [14/30], Batch [3200/6000], Loss: 0.0212
Epoch [14/30], Batch [3300/6000], Loss: 0.2799
Epoch [14/30], Batch [3400/6000], Loss: 0.0230
Epoch [14/30], Batch [3500/6000], Loss: 0.2545
Epoch [14/30], Batch [3600/6000], Loss: 0.0279
Epoch [14/30], Batch [3700/6000], Loss: 0.0296
Epoch [14/30], Batch [3800/6000], Loss: 0.0300
Epoch [14/30], Batch [3900/6000], Loss: 0.0489
Epoch [14/30], Batch [4000/6000], Loss: 0.0196
Epoch [14/30], Batch [4100/6000], Loss: 0.0198
Epoch [14/30], Batch [4200/6000], Loss: 0.0204
Epoch [14/30], Batch [4300/6000], Loss: 0.0440
Epoch [14/30], Batch [4400/6000], Loss: 0.0641
Epoch [14/30], Batch [4500/6000], Loss: 0.0250
Epoch [14/30], Batch [4600/6000], Loss: 0.0200
Epoch [14/30], Batch [4700/6000], Loss: 0.0986
Epoch [14/30], Batch [4800/6000], Loss: 0.0240
Epoch [14/30], Batch [4900/6000], Loss: 0.0218
Epoch [14/30], Batch [5000/6000], Loss: 0.0276
Epoch [14/30], Batch [5100/6000], Loss: 0.0279
Epoch [14/30], Batch [5200/6000], Loss: 0.0258
Epoch [14/30], Batch [5300/6000], Loss: 0.0232
Epoch [14/30], Batch [5400/6000], Loss: 0.2170
Epoch [14/30], Batch [5500/6000], Loss: 0.0275
Epoch [14/30], Batch [5600/6000], Loss: 1.1867
Epoch [14/30], Batch [5700/6000], Loss: 0.0216
Epoch [14/30], Batch [5800/6000], Loss: 0.0204
Epoch [14/30], Batch [5900/6000], Loss: 0.1757
Epoch [14/30], Loss: 0.0495
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.0297
Epoch [15/30], Batch [100/6000], Loss: 0.0531
Epoch [15/30], Batch [200/6000], Loss: 0.0212
Epoch [15/30], Batch [300/6000], Loss: 0.1181
Epoch [15/30], Batch [400/6000], Loss: 0.0251
Epoch [15/30], Batch [500/6000], Loss: 0.0398
Epoch [15/30], Batch [600/6000], Loss: 0.0229
Epoch [15/30], Batch [700/6000], Loss: 0.0241
Epoch [15/30], Batch [800/6000], Loss: 0.0201
Epoch [15/30], Batch [900/6000], Loss: 0.0275
Epoch [15/30], Batch [1000/6000], Loss: 0.0177
Epoch [15/30], Batch [1100/6000], Loss: 0.0173
Epoch [15/30], Batch [1200/6000], Loss: 0.0238
Epoch [15/30], Batch [1300/6000], Loss: 0.0177
Epoch [15/30], Batch [1400/6000], Loss: 0.5318
Epoch [15/30], Batch [1500/6000], Loss: 0.0236
Epoch [15/30], Batch [1600/6000], Loss: 0.0220
Epoch [15/30], Batch [1700/6000], Loss: 0.1505
Epoch [15/30], Batch [1800/6000], Loss: 0.1321
Epoch [15/30], Batch [1900/6000], Loss: 0.0256
Epoch [15/30], Batch [2000/6000], Loss: 0.0212
Epoch [15/30], Batch [2100/6000], Loss: 0.0319
Epoch [15/30], Batch [2200/6000], Loss: 0.3652
Epoch [15/30], Batch [2300/6000], Loss: 0.0286
Epoch [15/30], Batch [2400/6000], Loss: 0.0263
Epoch [15/30], Batch [2500/6000], Loss: 0.0236
Epoch [15/30], Batch [2600/6000], Loss: 0.0959
Epoch [15/30], Batch [2700/6000], Loss: 0.0270
Epoch [15/30], Batch [2800/6000], Loss: 0.0343
Epoch [15/30], Batch [2900/6000], Loss: 0.0250
Epoch [15/30], Batch [3000/6000], Loss: 0.0171
Epoch [15/30], Batch [3100/6000], Loss: 0.0202
Epoch [15/30], Batch [3200/6000], Loss: 0.0336
Epoch [15/30], Batch [3300/6000], Loss: 0.0343
Epoch [15/30], Batch [3400/6000], Loss: 0.0220
Epoch [15/30], Batch [3500/6000], Loss: 0.0229
Epoch [15/30], Batch [3600/6000], Loss: 0.0576
Epoch [15/30], Batch [3700/6000], Loss: 0.0173
Epoch [15/30], Batch [3800/6000], Loss: 0.0265
Epoch [15/30], Batch [3900/6000], Loss: 0.0230
Epoch [15/30], Batch [4000/6000], Loss: 0.0186
Epoch [15/30], Batch [4100/6000], Loss: 0.0429
Epoch [15/30], Batch [4200/6000], Loss: 0.0221
Epoch [15/30], Batch [4300/6000], Loss: 0.0265
Epoch [15/30], Batch [4400/6000], Loss: 0.0187
Epoch [15/30], Batch [4500/6000], Loss: 0.0178
Epoch [15/30], Batch [4600/6000], Loss: 0.0189
Epoch [15/30], Batch [4700/6000], Loss: 0.0239
Epoch [15/30], Batch [4800/6000], Loss: 0.0350
Epoch [15/30], Batch [4900/6000], Loss: 0.0191
Epoch [15/30], Batch [5000/6000], Loss: 0.0694
Epoch [15/30], Batch [5100/6000], Loss: 0.0322
Epoch [15/30], Batch [5200/6000], Loss: 0.0201
Epoch [15/30], Batch [5300/6000], Loss: 0.7874
Epoch [15/30], Batch [5400/6000], Loss: 0.0209
Epoch [15/30], Batch [5500/6000], Loss: 0.0225
Epoch [15/30], Batch [5600/6000], Loss: 0.0214
Epoch [15/30], Batch [5700/6000], Loss: 0.1476
Epoch [15/30], Batch [5800/6000], Loss: 0.0322
Epoch [15/30], Batch [5900/6000], Loss: 0.0424
Epoch [15/30], Loss: 0.0462
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.0182
Epoch [16/30], Batch [100/6000], Loss: 0.0210
Epoch [16/30], Batch [200/6000], Loss: 0.0189
Epoch [16/30], Batch [300/6000], Loss: 0.0248
Epoch [16/30], Batch [400/6000], Loss: 0.0453
Epoch [16/30], Batch [500/6000], Loss: 0.0244
Epoch [16/30], Batch [600/6000], Loss: 0.0225
Epoch [16/30], Batch [700/6000], Loss: 0.0169
Epoch [16/30], Batch [800/6000], Loss: 0.0895
Epoch [16/30], Batch [900/6000], Loss: 0.0218
Epoch [16/30], Batch [1000/6000], Loss: 0.0345
Epoch [16/30], Batch [1100/6000], Loss: 0.0336
Epoch [16/30], Batch [1200/6000], Loss: 0.0205
Epoch [16/30], Batch [1300/6000], Loss: 0.0148
Epoch [16/30], Batch [1400/6000], Loss: 0.0330
Epoch [16/30], Batch [1500/6000], Loss: 0.1149
Epoch [16/30], Batch [1600/6000], Loss: 0.0223
Epoch [16/30], Batch [1700/6000], Loss: 0.0209
Epoch [16/30], Batch [1800/6000], Loss: 0.2121
Epoch [16/30], Batch [1900/6000], Loss: 0.0232
Epoch [16/30], Batch [2000/6000], Loss: 0.0251
Epoch [16/30], Batch [2100/6000], Loss: 0.0213
Epoch [16/30], Batch [2200/6000], Loss: 0.0215
Epoch [16/30], Batch [2300/6000], Loss: 0.0212
Epoch [16/30], Batch [2400/6000], Loss: 0.0233
Epoch [16/30], Batch [2500/6000], Loss: 0.0263
Epoch [16/30], Batch [2600/6000], Loss: 0.0302
Epoch [16/30], Batch [2700/6000], Loss: 0.0219
Epoch [16/30], Batch [2800/6000], Loss: 0.0189
Epoch [16/30], Batch [2900/6000], Loss: 0.0194
Epoch [16/30], Batch [3000/6000], Loss: 0.0153
Epoch [16/30], Batch [3100/6000], Loss: 0.0193
Epoch [16/30], Batch [3200/6000], Loss: 0.0169
Epoch [16/30], Batch [3300/6000], Loss: 0.0223
Epoch [16/30], Batch [3400/6000], Loss: 0.6091
Epoch [16/30], Batch [3500/6000], Loss: 0.0203
Epoch [16/30], Batch [3600/6000], Loss: 0.0262
Epoch [16/30], Batch [3700/6000], Loss: 0.0319
Epoch [16/30], Batch [3800/6000], Loss: 0.0228
Epoch [16/30], Batch [3900/6000], Loss: 0.0226
Epoch [16/30], Batch [4000/6000], Loss: 0.1146
Epoch [16/30], Batch [4100/6000], Loss: 0.0201
Epoch [16/30], Batch [4200/6000], Loss: 0.0251
Epoch [16/30], Batch [4300/6000], Loss: 0.0181
Epoch [16/30], Batch [4400/6000], Loss: 0.1324
Epoch [16/30], Batch [4500/6000], Loss: 0.0604
Epoch [16/30], Batch [4600/6000], Loss: 0.0169
Epoch [16/30], Batch [4700/6000], Loss: 0.0222
Epoch [16/30], Batch [4800/6000], Loss: 0.0230
Epoch [16/30], Batch [4900/6000], Loss: 0.4043
Epoch [16/30], Batch [5000/6000], Loss: 0.0217
Epoch [16/30], Batch [5100/6000], Loss: 0.0203
Epoch [16/30], Batch [5200/6000], Loss: 0.0200
Epoch [16/30], Batch [5300/6000], Loss: 0.0303
Epoch [16/30], Batch [5400/6000], Loss: 0.0205
Epoch [16/30], Batch [5500/6000], Loss: 0.1230
Epoch [16/30], Batch [5600/6000], Loss: 0.0226
Epoch [16/30], Batch [5700/6000], Loss: 0.0266
Epoch [16/30], Batch [5800/6000], Loss: 0.0277
Epoch [16/30], Batch [5900/6000], Loss: 0.0267
Epoch [16/30], Loss: 0.0430
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.0272
Epoch [17/30], Batch [100/6000], Loss: 0.0239
Epoch [17/30], Batch [200/6000], Loss: 0.0873
Epoch [17/30], Batch [300/6000], Loss: 0.0169
Epoch [17/30], Batch [400/6000], Loss: 0.0229
Epoch [17/30], Batch [500/6000], Loss: 0.0224
Epoch [17/30], Batch [600/6000], Loss: 0.0907
Epoch [17/30], Batch [700/6000], Loss: 0.1003
Epoch [17/30], Batch [800/6000], Loss: 0.0199
Epoch [17/30], Batch [900/6000], Loss: 0.0228
Epoch [17/30], Batch [1000/6000], Loss: 0.0200
Epoch [17/30], Batch [1100/6000], Loss: 0.0253
Epoch [17/30], Batch [1200/6000], Loss: 0.0185
Epoch [17/30], Batch [1300/6000], Loss: 0.0323
Epoch [17/30], Batch [1400/6000], Loss: 0.0261
Epoch [17/30], Batch [1500/6000], Loss: 0.0208
Epoch [17/30], Batch [1600/6000], Loss: 0.0294
Epoch [17/30], Batch [1700/6000], Loss: 0.0269
Epoch [17/30], Batch [1800/6000], Loss: 0.0218
Epoch [17/30], Batch [1900/6000], Loss: 0.0231
Epoch [17/30], Batch [2000/6000], Loss: 0.0507
Epoch [17/30], Batch [2100/6000], Loss: 0.0206
Epoch [17/30], Batch [2200/6000], Loss: 0.0243
Epoch [17/30], Batch [2300/6000], Loss: 0.0192
Epoch [17/30], Batch [2400/6000], Loss: 0.0307
Epoch [17/30], Batch [2500/6000], Loss: 0.0191
Epoch [17/30], Batch [2600/6000], Loss: 0.4069
Epoch [17/30], Batch [2700/6000], Loss: 0.0225
Epoch [17/30], Batch [2800/6000], Loss: 0.0198
Epoch [17/30], Batch [2900/6000], Loss: 0.0226
Epoch [17/30], Batch [3000/6000], Loss: 0.0152
Epoch [17/30], Batch [3100/6000], Loss: 0.0206
Epoch [17/30], Batch [3200/6000], Loss: 0.0160
Epoch [17/30], Batch [3300/6000], Loss: 0.0216
Epoch [17/30], Batch [3400/6000], Loss: 0.0572
Epoch [17/30], Batch [3500/6000], Loss: 0.0231
Epoch [17/30], Batch [3600/6000], Loss: 0.0204
Epoch [17/30], Batch [3700/6000], Loss: 0.0227
Epoch [17/30], Batch [3800/6000], Loss: 0.0195
Epoch [17/30], Batch [3900/6000], Loss: 0.0262
Epoch [17/30], Batch [4000/6000], Loss: 0.0190
Epoch [17/30], Batch [4100/6000], Loss: 0.0228
Epoch [17/30], Batch [4200/6000], Loss: 0.0341
Epoch [17/30], Batch [4300/6000], Loss: 0.0239
Epoch [17/30], Batch [4400/6000], Loss: 0.0215
Epoch [17/30], Batch [4500/6000], Loss: 0.0358
Epoch [17/30], Batch [4600/6000], Loss: 0.0160
Epoch [17/30], Batch [4700/6000], Loss: 0.0266
Epoch [17/30], Batch [4800/6000], Loss: 0.0422
Epoch [17/30], Batch [4900/6000], Loss: 0.0221
Epoch [17/30], Batch [5000/6000], Loss: 0.0224
Epoch [17/30], Batch [5100/6000], Loss: 0.0785
Epoch [17/30], Batch [5200/6000], Loss: 0.0225
Epoch [17/30], Batch [5300/6000], Loss: 0.0183
Epoch [17/30], Batch [5400/6000], Loss: 0.0328
Epoch [17/30], Batch [5500/6000], Loss: 0.0308
Epoch [17/30], Batch [5600/6000], Loss: 0.0194
Epoch [17/30], Batch [5700/6000], Loss: 0.1368
Epoch [17/30], Batch [5800/6000], Loss: 0.0179
Epoch [17/30], Batch [5900/6000], Loss: 0.0229
Epoch [17/30], Loss: 0.0418
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.0223
Epoch [18/30], Batch [100/6000], Loss: 0.0181
Epoch [18/30], Batch [200/6000], Loss: 0.0190
Epoch [18/30], Batch [300/6000], Loss: 0.0212
Epoch [18/30], Batch [400/6000], Loss: 0.0216
Epoch [18/30], Batch [500/6000], Loss: 0.0171
Epoch [18/30], Batch [600/6000], Loss: 0.0230
Epoch [18/30], Batch [700/6000], Loss: 0.0201
Epoch [18/30], Batch [800/6000], Loss: 0.0349
Epoch [18/30], Batch [900/6000], Loss: 0.0207
Epoch [18/30], Batch [1000/6000], Loss: 0.0240
Epoch [18/30], Batch [1100/6000], Loss: 0.0216
Epoch [18/30], Batch [1200/6000], Loss: 0.0230
Epoch [18/30], Batch [1300/6000], Loss: 0.0163
Epoch [18/30], Batch [1400/6000], Loss: 0.0261
Epoch [18/30], Batch [1500/6000], Loss: 0.0451
Epoch [18/30], Batch [1600/6000], Loss: 0.0200
Epoch [18/30], Batch [1700/6000], Loss: 0.0163
Epoch [18/30], Batch [1800/6000], Loss: 0.0207
Epoch [18/30], Batch [1900/6000], Loss: 0.1067
Epoch [18/30], Batch [2000/6000], Loss: 0.0679
Epoch [18/30], Batch [2100/6000], Loss: 0.0201
Epoch [18/30], Batch [2200/6000], Loss: 0.0277
Epoch [18/30], Batch [2300/6000], Loss: 0.0380
Epoch [18/30], Batch [2400/6000], Loss: 0.0204
Epoch [18/30], Batch [2500/6000], Loss: 0.0296
Epoch [18/30], Batch [2600/6000], Loss: 0.0551
Epoch [18/30], Batch [2700/6000], Loss: 0.0190
Epoch [18/30], Batch [2800/6000], Loss: 0.0255
Epoch [18/30], Batch [2900/6000], Loss: 0.0985
Epoch [18/30], Batch [3000/6000], Loss: 0.0455
Epoch [18/30], Batch [3100/6000], Loss: 0.0178
Epoch [18/30], Batch [3200/6000], Loss: 0.0187
Epoch [18/30], Batch [3300/6000], Loss: 0.0181
Epoch [18/30], Batch [3400/6000], Loss: 0.0675
Epoch [18/30], Batch [3500/6000], Loss: 0.0178
Epoch [18/30], Batch [3600/6000], Loss: 0.0354
Epoch [18/30], Batch [3700/6000], Loss: 0.0190
Epoch [18/30], Batch [3800/6000], Loss: 0.0296
Epoch [18/30], Batch [3900/6000], Loss: 0.0217
Epoch [18/30], Batch [4000/6000], Loss: 0.0213
Epoch [18/30], Batch [4100/6000], Loss: 0.0182
Epoch [18/30], Batch [4200/6000], Loss: 0.0232
Epoch [18/30], Batch [4300/6000], Loss: 0.0247
Epoch [18/30], Batch [4400/6000], Loss: 0.0210
Epoch [18/30], Batch [4500/6000], Loss: 0.0264
Epoch [18/30], Batch [4600/6000], Loss: 0.0303
Epoch [18/30], Batch [4700/6000], Loss: 0.0196
Epoch [18/30], Batch [4800/6000], Loss: 0.0176
Epoch [18/30], Batch [4900/6000], Loss: 0.0193
Epoch [18/30], Batch [5000/6000], Loss: 0.0203
Epoch [18/30], Batch [5100/6000], Loss: 0.0194
Epoch [18/30], Batch [5200/6000], Loss: 0.0172
Epoch [18/30], Batch [5300/6000], Loss: 0.0209
Epoch [18/30], Batch [5400/6000], Loss: 0.0163
Epoch [18/30], Batch [5500/6000], Loss: 0.0166
Epoch [18/30], Batch [5600/6000], Loss: 0.0191
Epoch [18/30], Batch [5700/6000], Loss: 0.0467
Epoch [18/30], Batch [5800/6000], Loss: 0.0225
Epoch [18/30], Batch [5900/6000], Loss: 0.0223
Epoch [18/30], Loss: 0.0377
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.0165
Epoch [19/30], Batch [100/6000], Loss: 0.0228
Epoch [19/30], Batch [200/6000], Loss: 0.0183
Epoch [19/30], Batch [300/6000], Loss: 0.0256
Epoch [19/30], Batch [400/6000], Loss: 0.0286
Epoch [19/30], Batch [500/6000], Loss: 0.0210
Epoch [19/30], Batch [600/6000], Loss: 0.0185
Epoch [19/30], Batch [700/6000], Loss: 0.0199
Epoch [19/30], Batch [800/6000], Loss: 0.0194
Epoch [19/30], Batch [900/6000], Loss: 0.0269
Epoch [19/30], Batch [1000/6000], Loss: 0.0245
Epoch [19/30], Batch [1100/6000], Loss: 0.0257
Epoch [19/30], Batch [1200/6000], Loss: 0.0346
Epoch [19/30], Batch [1300/6000], Loss: 0.0173
Epoch [19/30], Batch [1400/6000], Loss: 0.0232
Epoch [19/30], Batch [1500/6000], Loss: 0.0166
Epoch [19/30], Batch [1600/6000], Loss: 0.0307
Epoch [19/30], Batch [1700/6000], Loss: 0.0201
Epoch [19/30], Batch [1800/6000], Loss: 0.0191
Epoch [19/30], Batch [1900/6000], Loss: 0.0420
Epoch [19/30], Batch [2000/6000], Loss: 0.0232
Epoch [19/30], Batch [2100/6000], Loss: 0.0166
Epoch [19/30], Batch [2200/6000], Loss: 0.0220
Epoch [19/30], Batch [2300/6000], Loss: 0.0239
Epoch [19/30], Batch [2400/6000], Loss: 0.0666
Epoch [19/30], Batch [2500/6000], Loss: 0.0188
Epoch [19/30], Batch [2600/6000], Loss: 0.1204
Epoch [19/30], Batch [2700/6000], Loss: 0.0138
Epoch [19/30], Batch [2800/6000], Loss: 0.0137
Epoch [19/30], Batch [2900/6000], Loss: 0.0242
Epoch [19/30], Batch [3000/6000], Loss: 0.0202
Epoch [19/30], Batch [3100/6000], Loss: 0.0325
Epoch [19/30], Batch [3200/6000], Loss: 0.0228
Epoch [19/30], Batch [3300/6000], Loss: 0.0187
Epoch [19/30], Batch [3400/6000], Loss: 0.0475
Epoch [19/30], Batch [3500/6000], Loss: 0.0244
Epoch [19/30], Batch [3600/6000], Loss: 0.0662
Epoch [19/30], Batch [3700/6000], Loss: 0.0181
Epoch [19/30], Batch [3800/6000], Loss: 0.0375
Epoch [19/30], Batch [3900/6000], Loss: 0.0202
Epoch [19/30], Batch [4000/6000], Loss: 0.0187
Epoch [19/30], Batch [4100/6000], Loss: 0.0222
Epoch [19/30], Batch [4200/6000], Loss: 0.0220
Epoch [19/30], Batch [4300/6000], Loss: 0.0211
Epoch [19/30], Batch [4400/6000], Loss: 0.0193
Epoch [19/30], Batch [4500/6000], Loss: 0.0242
Epoch [19/30], Batch [4600/6000], Loss: 0.0219
Epoch [19/30], Batch [4700/6000], Loss: 0.0227
Epoch [19/30], Batch [4800/6000], Loss: 0.0211
Epoch [19/30], Batch [4900/6000], Loss: 0.0805
Epoch [19/30], Batch [5000/6000], Loss: 0.0194
Epoch [19/30], Batch [5100/6000], Loss: 0.0201
Epoch [19/30], Batch [5200/6000], Loss: 0.0230
Epoch [19/30], Batch [5300/6000], Loss: 0.0209
Epoch [19/30], Batch [5400/6000], Loss: 0.0216
Epoch [19/30], Batch [5500/6000], Loss: 0.0222
Epoch [19/30], Batch [5600/6000], Loss: 0.1320
Epoch [19/30], Batch [5700/6000], Loss: 0.0456
Epoch [19/30], Batch [5800/6000], Loss: 0.0212
Epoch [19/30], Batch [5900/6000], Loss: 0.0166
Epoch [19/30], Loss: 0.0372
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.0218
Epoch [20/30], Batch [100/6000], Loss: 0.0218
Epoch [20/30], Batch [200/6000], Loss: 0.0492
Epoch [20/30], Batch [300/6000], Loss: 0.0372
Epoch [20/30], Batch [400/6000], Loss: 0.0210
Epoch [20/30], Batch [500/6000], Loss: 0.0196
Epoch [20/30], Batch [600/6000], Loss: 0.0256
Epoch [20/30], Batch [700/6000], Loss: 0.0209
Epoch [20/30], Batch [800/6000], Loss: 0.0209
Epoch [20/30], Batch [900/6000], Loss: 0.0171
Epoch [20/30], Batch [1000/6000], Loss: 0.0173
Epoch [20/30], Batch [1100/6000], Loss: 0.0182
Epoch [20/30], Batch [1200/6000], Loss: 0.0206
Epoch [20/30], Batch [1300/6000], Loss: 0.0259
Epoch [20/30], Batch [1400/6000], Loss: 0.0190
Epoch [20/30], Batch [1500/6000], Loss: 0.1800
Epoch [20/30], Batch [1600/6000], Loss: 0.0190
Epoch [20/30], Batch [1700/6000], Loss: 0.0244
Epoch [20/30], Batch [1800/6000], Loss: 0.0210
Epoch [20/30], Batch [1900/6000], Loss: 0.0217
Epoch [20/30], Batch [2000/6000], Loss: 0.0250
Epoch [20/30], Batch [2100/6000], Loss: 0.0181
Epoch [20/30], Batch [2200/6000], Loss: 0.0230
Epoch [20/30], Batch [2300/6000], Loss: 0.0298
Epoch [20/30], Batch [2400/6000], Loss: 0.0189
Epoch [20/30], Batch [2500/6000], Loss: 0.0237
Epoch [20/30], Batch [2600/6000], Loss: 0.0220
Epoch [20/30], Batch [2700/6000], Loss: 0.0197
Epoch [20/30], Batch [2800/6000], Loss: 0.0288
Epoch [20/30], Batch [2900/6000], Loss: 0.0169
Epoch [20/30], Batch [3000/6000], Loss: 0.0710
Epoch [20/30], Batch [3100/6000], Loss: 0.0155
Epoch [20/30], Batch [3200/6000], Loss: 0.0190
Epoch [20/30], Batch [3300/6000], Loss: 0.0204
Epoch [20/30], Batch [3400/6000], Loss: 0.0222
Epoch [20/30], Batch [3500/6000], Loss: 0.0232
Epoch [20/30], Batch [3600/6000], Loss: 0.0191
Epoch [20/30], Batch [3700/6000], Loss: 0.0211
Epoch [20/30], Batch [3800/6000], Loss: 0.0202
Epoch [20/30], Batch [3900/6000], Loss: 0.0176
Epoch [20/30], Batch [4000/6000], Loss: 0.0389
Epoch [20/30], Batch [4100/6000], Loss: 0.0235
Epoch [20/30], Batch [4200/6000], Loss: 0.0270
Epoch [20/30], Batch [4300/6000], Loss: 0.1616
Epoch [20/30], Batch [4400/6000], Loss: 0.0255
Epoch [20/30], Batch [4500/6000], Loss: 0.0178
Epoch [20/30], Batch [4600/6000], Loss: 0.0229
Epoch [20/30], Batch [4700/6000], Loss: 0.0183
Epoch [20/30], Batch [4800/6000], Loss: 0.0237
Epoch [20/30], Batch [4900/6000], Loss: 0.0190
Epoch [20/30], Batch [5000/6000], Loss: 0.0694
Epoch [20/30], Batch [5100/6000], Loss: 0.0258
Epoch [20/30], Batch [5200/6000], Loss: 0.0167
Epoch [20/30], Batch [5300/6000], Loss: 0.0281
Epoch [20/30], Batch [5400/6000], Loss: 0.0226
Epoch [20/30], Batch [5500/6000], Loss: 0.0183
Epoch [20/30], Batch [5600/6000], Loss: 0.0199
Epoch [20/30], Batch [5700/6000], Loss: 0.0208
Epoch [20/30], Batch [5800/6000], Loss: 0.0207
Epoch [20/30], Batch [5900/6000], Loss: 0.0166
Epoch [20/30], Loss: 0.0358
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.0243
Epoch [21/30], Batch [100/6000], Loss: 0.0841
Epoch [21/30], Batch [200/6000], Loss: 0.0516
Epoch [21/30], Batch [300/6000], Loss: 0.0202
Epoch [21/30], Batch [400/6000], Loss: 0.0204
Epoch [21/30], Batch [500/6000], Loss: 0.0221
Epoch [21/30], Batch [600/6000], Loss: 0.0244
Epoch [21/30], Batch [700/6000], Loss: 0.0231
Epoch [21/30], Batch [800/6000], Loss: 0.0213
Epoch [21/30], Batch [900/6000], Loss: 0.0200
Epoch [21/30], Batch [1000/6000], Loss: 0.0216
Epoch [21/30], Batch [1100/6000], Loss: 0.0210
Epoch [21/30], Batch [1200/6000], Loss: 0.0234
Epoch [21/30], Batch [1300/6000], Loss: 0.0183
Epoch [21/30], Batch [1400/6000], Loss: 0.0166
Epoch [21/30], Batch [1500/6000], Loss: 0.0212
Epoch [21/30], Batch [1600/6000], Loss: 0.0222
Epoch [21/30], Batch [1700/6000], Loss: 0.0222
Epoch [21/30], Batch [1800/6000], Loss: 0.0179
Epoch [21/30], Batch [1900/6000], Loss: 0.0214
Epoch [21/30], Batch [2000/6000], Loss: 0.0314
Epoch [21/30], Batch [2100/6000], Loss: 0.0204
Epoch [21/30], Batch [2200/6000], Loss: 0.0155
Epoch [21/30], Batch [2300/6000], Loss: 0.0239
Epoch [21/30], Batch [2400/6000], Loss: 0.0166
Epoch [21/30], Batch [2500/6000], Loss: 0.0176
Epoch [21/30], Batch [2600/6000], Loss: 0.0331
Epoch [21/30], Batch [2700/6000], Loss: 0.0197
Epoch [21/30], Batch [2800/6000], Loss: 0.0204
Epoch [21/30], Batch [2900/6000], Loss: 0.0233
Epoch [21/30], Batch [3000/6000], Loss: 0.0208
Epoch [21/30], Batch [3100/6000], Loss: 0.0189
Epoch [21/30], Batch [3200/6000], Loss: 0.0350
Epoch [21/30], Batch [3300/6000], Loss: 0.0157
Epoch [21/30], Batch [3400/6000], Loss: 0.0243
Epoch [21/30], Batch [3500/6000], Loss: 0.0160
Epoch [21/30], Batch [3600/6000], Loss: 0.0356
Epoch [21/30], Batch [3700/6000], Loss: 0.0199
Epoch [21/30], Batch [3800/6000], Loss: 0.0226
Epoch [21/30], Batch [3900/6000], Loss: 0.0196
Epoch [21/30], Batch [4000/6000], Loss: 0.0239
Epoch [21/30], Batch [4100/6000], Loss: 0.0181
Epoch [21/30], Batch [4200/6000], Loss: 0.0190
Epoch [21/30], Batch [4300/6000], Loss: 0.0182
Epoch [21/30], Batch [4400/6000], Loss: 0.0192
Epoch [21/30], Batch [4500/6000], Loss: 0.0314
Epoch [21/30], Batch [4600/6000], Loss: 0.1240
Epoch [21/30], Batch [4700/6000], Loss: 0.0189
Epoch [21/30], Batch [4800/6000], Loss: 0.0195
Epoch [21/30], Batch [4900/6000], Loss: 0.0195
Epoch [21/30], Batch [5000/6000], Loss: 0.2103
Epoch [21/30], Batch [5100/6000], Loss: 0.0304
Epoch [21/30], Batch [5200/6000], Loss: 0.1266
Epoch [21/30], Batch [5300/6000], Loss: 0.0304
Epoch [21/30], Batch [5400/6000], Loss: 0.0194
Epoch [21/30], Batch [5500/6000], Loss: 0.0183
Epoch [21/30], Batch [5600/6000], Loss: 0.0497
Epoch [21/30], Batch [5700/6000], Loss: 0.0190
Epoch [21/30], Batch [5800/6000], Loss: 0.0226
Epoch [21/30], Batch [5900/6000], Loss: 0.0303
Epoch [21/30], Loss: 0.0340
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.0157
Epoch [22/30], Batch [100/6000], Loss: 0.0232
Epoch [22/30], Batch [200/6000], Loss: 0.0163
Epoch [22/30], Batch [300/6000], Loss: 0.0162
Epoch [22/30], Batch [400/6000], Loss: 0.0224
Epoch [22/30], Batch [500/6000], Loss: 0.0233
Epoch [22/30], Batch [600/6000], Loss: 0.0163
Epoch [22/30], Batch [700/6000], Loss: 0.0218
Epoch [22/30], Batch [800/6000], Loss: 0.0183
Epoch [22/30], Batch [900/6000], Loss: 0.0181
Epoch [22/30], Batch [1000/6000], Loss: 0.0189
Epoch [22/30], Batch [1100/6000], Loss: 0.0178
Epoch [22/30], Batch [1200/6000], Loss: 0.0204
Epoch [22/30], Batch [1300/6000], Loss: 0.0241
Epoch [22/30], Batch [1400/6000], Loss: 0.0161
Epoch [22/30], Batch [1500/6000], Loss: 0.0221
Epoch [22/30], Batch [1600/6000], Loss: 0.0228
Epoch [22/30], Batch [1700/6000], Loss: 0.0208
Epoch [22/30], Batch [1800/6000], Loss: 0.0273
Epoch [22/30], Batch [1900/6000], Loss: 0.0203
Epoch [22/30], Batch [2000/6000], Loss: 0.0222
Epoch [22/30], Batch [2100/6000], Loss: 0.0897
Epoch [22/30], Batch [2200/6000], Loss: 0.0168
Epoch [22/30], Batch [2300/6000], Loss: 0.0242
Epoch [22/30], Batch [2400/6000], Loss: 0.0240
Epoch [22/30], Batch [2500/6000], Loss: 0.0301
Epoch [22/30], Batch [2600/6000], Loss: 0.0202
Epoch [22/30], Batch [2700/6000], Loss: 0.0219
Epoch [22/30], Batch [2800/6000], Loss: 0.0188
Epoch [22/30], Batch [2900/6000], Loss: 0.0610
Epoch [22/30], Batch [3000/6000], Loss: 0.0178
Epoch [22/30], Batch [3100/6000], Loss: 0.0200
Epoch [22/30], Batch [3200/6000], Loss: 0.0188
Epoch [22/30], Batch [3300/6000], Loss: 0.0202
Epoch [22/30], Batch [3400/6000], Loss: 0.0148
Epoch [22/30], Batch [3500/6000], Loss: 0.0206
Epoch [22/30], Batch [3600/6000], Loss: 0.0219
Epoch [22/30], Batch [3700/6000], Loss: 0.0188
Epoch [22/30], Batch [3800/6000], Loss: 0.0164
Epoch [22/30], Batch [3900/6000], Loss: 0.0266
Epoch [22/30], Batch [4000/6000], Loss: 0.0226
Epoch [22/30], Batch [4100/6000], Loss: 0.0177
Epoch [22/30], Batch [4200/6000], Loss: 0.0177
Epoch [22/30], Batch [4300/6000], Loss: 0.0205
Epoch [22/30], Batch [4400/6000], Loss: 0.1987
Epoch [22/30], Batch [4500/6000], Loss: 0.1083
Epoch [22/30], Batch [4600/6000], Loss: 0.0275
Epoch [22/30], Batch [4700/6000], Loss: 0.0368
Epoch [22/30], Batch [4800/6000], Loss: 0.0169
Epoch [22/30], Batch [4900/6000], Loss: 0.0195
Epoch [22/30], Batch [5000/6000], Loss: 0.0267
Epoch [22/30], Batch [5100/6000], Loss: 0.0238
Epoch [22/30], Batch [5200/6000], Loss: 0.0179
Epoch [22/30], Batch [5300/6000], Loss: 0.0162
Epoch [22/30], Batch [5400/6000], Loss: 0.0482
Epoch [22/30], Batch [5500/6000], Loss: 0.0219
Epoch [22/30], Batch [5600/6000], Loss: 0.0161
Epoch [22/30], Batch [5700/6000], Loss: 0.0170
Epoch [22/30], Batch [5800/6000], Loss: 0.0179
Epoch [22/30], Batch [5900/6000], Loss: 0.0157
Epoch [22/30], Loss: 0.0325
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.0264
Epoch [23/30], Batch [100/6000], Loss: 0.0228
Epoch [23/30], Batch [200/6000], Loss: 0.2286
Epoch [23/30], Batch [300/6000], Loss: 0.0177
Epoch [23/30], Batch [400/6000], Loss: 0.0212
Epoch [23/30], Batch [500/6000], Loss: 0.0180
Epoch [23/30], Batch [600/6000], Loss: 0.0235
Epoch [23/30], Batch [700/6000], Loss: 0.0193
Epoch [23/30], Batch [800/6000], Loss: 0.0196
Epoch [23/30], Batch [900/6000], Loss: 0.0221
Epoch [23/30], Batch [1000/6000], Loss: 0.0218
Epoch [23/30], Batch [1100/6000], Loss: 0.0171
Epoch [23/30], Batch [1200/6000], Loss: 0.0228
Epoch [23/30], Batch [1300/6000], Loss: 0.0208
Epoch [23/30], Batch [1400/6000], Loss: 0.0209
Epoch [23/30], Batch [1500/6000], Loss: 0.0179
Epoch [23/30], Batch [1600/6000], Loss: 0.0162
Epoch [23/30], Batch [1700/6000], Loss: 0.0781
Epoch [23/30], Batch [1800/6000], Loss: 0.0183
Epoch [23/30], Batch [1900/6000], Loss: 0.0173
Epoch [23/30], Batch [2000/6000], Loss: 0.0207
Epoch [23/30], Batch [2100/6000], Loss: 0.0210
Epoch [23/30], Batch [2200/6000], Loss: 0.0177
Epoch [23/30], Batch [2300/6000], Loss: 0.3203
Epoch [23/30], Batch [2400/6000], Loss: 0.0174
Epoch [23/30], Batch [2500/6000], Loss: 0.0209
Epoch [23/30], Batch [2600/6000], Loss: 0.0209
Epoch [23/30], Batch [2700/6000], Loss: 0.0177
Epoch [23/30], Batch [2800/6000], Loss: 0.0193
Epoch [23/30], Batch [2900/6000], Loss: 0.0211
Epoch [23/30], Batch [3000/6000], Loss: 0.0200
Epoch [23/30], Batch [3100/6000], Loss: 0.0191
Epoch [23/30], Batch [3200/6000], Loss: 0.0200
Epoch [23/30], Batch [3300/6000], Loss: 0.0197
Epoch [23/30], Batch [3400/6000], Loss: 0.0200
Epoch [23/30], Batch [3500/6000], Loss: 0.0153
Epoch [23/30], Batch [3600/6000], Loss: 0.0176
Epoch [23/30], Batch [3700/6000], Loss: 0.0155
Epoch [23/30], Batch [3800/6000], Loss: 0.0185
Epoch [23/30], Batch [3900/6000], Loss: 0.0115
Epoch [23/30], Batch [4000/6000], Loss: 0.0179
Epoch [23/30], Batch [4100/6000], Loss: 0.0184
Epoch [23/30], Batch [4200/6000], Loss: 0.0402
Epoch [23/30], Batch [4300/6000], Loss: 0.0196
Epoch [23/30], Batch [4400/6000], Loss: 0.0223
Epoch [23/30], Batch [4500/6000], Loss: 0.0462
Epoch [23/30], Batch [4600/6000], Loss: 0.0211
Epoch [23/30], Batch [4700/6000], Loss: 0.0156
Epoch [23/30], Batch [4800/6000], Loss: 0.0214
Epoch [23/30], Batch [4900/6000], Loss: 0.0143
Epoch [23/30], Batch [5000/6000], Loss: 0.0222
Epoch [23/30], Batch [5100/6000], Loss: 0.0219
Epoch [23/30], Batch [5200/6000], Loss: 0.0198
Epoch [23/30], Batch [5300/6000], Loss: 0.0193
Epoch [23/30], Batch [5400/6000], Loss: 0.0186
Epoch [23/30], Batch [5500/6000], Loss: 0.0200
Epoch [23/30], Batch [5600/6000], Loss: 0.0618
Epoch [23/30], Batch [5700/6000], Loss: 0.0220
Epoch [23/30], Batch [5800/6000], Loss: 0.0171
Epoch [23/30], Batch [5900/6000], Loss: 0.0215
Epoch [23/30], Loss: 0.0324
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.0167
Epoch [24/30], Batch [100/6000], Loss: 0.0170
Epoch [24/30], Batch [200/6000], Loss: 0.0212
Epoch [24/30], Batch [300/6000], Loss: 0.0166
Epoch [24/30], Batch [400/6000], Loss: 0.0148
Epoch [24/30], Batch [500/6000], Loss: 0.0174
Epoch [24/30], Batch [600/6000], Loss: 0.0217
Epoch [24/30], Batch [700/6000], Loss: 0.0471
Epoch [24/30], Batch [800/6000], Loss: 0.0357
Epoch [24/30], Batch [900/6000], Loss: 0.0223
Epoch [24/30], Batch [1000/6000], Loss: 0.0329
Epoch [24/30], Batch [1100/6000], Loss: 0.0662
Epoch [24/30], Batch [1200/6000], Loss: 0.0190
Epoch [24/30], Batch [1300/6000], Loss: 0.0165
Epoch [24/30], Batch [1400/6000], Loss: 0.0250
Epoch [24/30], Batch [1500/6000], Loss: 0.0185
Epoch [24/30], Batch [1600/6000], Loss: 0.0173
Epoch [24/30], Batch [1700/6000], Loss: 0.0184
Epoch [24/30], Batch [1800/6000], Loss: 0.0226
Epoch [24/30], Batch [1900/6000], Loss: 0.0162
Epoch [24/30], Batch [2000/6000], Loss: 0.1172
Epoch [24/30], Batch [2100/6000], Loss: 0.0167
Epoch [24/30], Batch [2200/6000], Loss: 0.0244
Epoch [24/30], Batch [2300/6000], Loss: 0.0214
Epoch [24/30], Batch [2400/6000], Loss: 0.0204
Epoch [24/30], Batch [2500/6000], Loss: 0.0201
Epoch [24/30], Batch [2600/6000], Loss: 0.1030
Epoch [24/30], Batch [2700/6000], Loss: 0.0227
Epoch [24/30], Batch [2800/6000], Loss: 0.0203
Epoch [24/30], Batch [2900/6000], Loss: 0.0198
Epoch [24/30], Batch [3000/6000], Loss: 0.0192
Epoch [24/30], Batch [3100/6000], Loss: 0.0197
Epoch [24/30], Batch [3200/6000], Loss: 0.0194
Epoch [24/30], Batch [3300/6000], Loss: 0.0170
Epoch [24/30], Batch [3400/6000], Loss: 0.0246
Epoch [24/30], Batch [3500/6000], Loss: 0.0203
Epoch [24/30], Batch [3600/6000], Loss: 0.0191
Epoch [24/30], Batch [3700/6000], Loss: 0.0750
Epoch [24/30], Batch [3800/6000], Loss: 0.0240
Epoch [24/30], Batch [3900/6000], Loss: 0.0222
Epoch [24/30], Batch [4000/6000], Loss: 0.0143
Epoch [24/30], Batch [4100/6000], Loss: 0.0183
Epoch [24/30], Batch [4200/6000], Loss: 0.0159
Epoch [24/30], Batch [4300/6000], Loss: 0.0202
Epoch [24/30], Batch [4400/6000], Loss: 0.0230
Epoch [24/30], Batch [4500/6000], Loss: 0.0208
Epoch [24/30], Batch [4600/6000], Loss: 0.0131
Epoch [24/30], Batch [4700/6000], Loss: 0.0185
Epoch [24/30], Batch [4800/6000], Loss: 0.2987
Epoch [24/30], Batch [4900/6000], Loss: 0.0248
Epoch [24/30], Batch [5000/6000], Loss: 0.0223
Epoch [24/30], Batch [5100/6000], Loss: 0.0234
Epoch [24/30], Batch [5200/6000], Loss: 0.0762
Epoch [24/30], Batch [5300/6000], Loss: 0.0216
Epoch [24/30], Batch [5400/6000], Loss: 0.0240
Epoch [24/30], Batch [5500/6000], Loss: 0.0184
Epoch [24/30], Batch [5600/6000], Loss: 0.0170
Epoch [24/30], Batch [5700/6000], Loss: 0.0226
Epoch [24/30], Batch [5800/6000], Loss: 0.0167
Epoch [24/30], Batch [5900/6000], Loss: 0.0211
Epoch [24/30], Loss: 0.0307
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.0194
Epoch [25/30], Batch [100/6000], Loss: 0.0186
Epoch [25/30], Batch [200/6000], Loss: 0.0166
Epoch [25/30], Batch [300/6000], Loss: 0.0172
Epoch [25/30], Batch [400/6000], Loss: 0.0252
Epoch [25/30], Batch [500/6000], Loss: 0.0168
Epoch [25/30], Batch [600/6000], Loss: 0.0160
Epoch [25/30], Batch [700/6000], Loss: 0.0199
Epoch [25/30], Batch [800/6000], Loss: 0.0232
Epoch [25/30], Batch [900/6000], Loss: 0.0182
Epoch [25/30], Batch [1000/6000], Loss: 0.0167
Epoch [25/30], Batch [1100/6000], Loss: 0.0176
Epoch [25/30], Batch [1200/6000], Loss: 0.0233
Epoch [25/30], Batch [1300/6000], Loss: 0.0209
Epoch [25/30], Batch [1400/6000], Loss: 0.0161
Epoch [25/30], Batch [1500/6000], Loss: 0.0189
Epoch [25/30], Batch [1600/6000], Loss: 0.0183
Epoch [25/30], Batch [1700/6000], Loss: 0.0204
Epoch [25/30], Batch [1800/6000], Loss: 0.0188
Epoch [25/30], Batch [1900/6000], Loss: 0.0174
Epoch [25/30], Batch [2000/6000], Loss: 0.0171
Epoch [25/30], Batch [2100/6000], Loss: 0.0853
Epoch [25/30], Batch [2200/6000], Loss: 0.0190
Epoch [25/30], Batch [2300/6000], Loss: 0.0194
Epoch [25/30], Batch [2400/6000], Loss: 0.0145
Epoch [25/30], Batch [2500/6000], Loss: 0.0214
Epoch [25/30], Batch [2600/6000], Loss: 0.0209
Epoch [25/30], Batch [2700/6000], Loss: 0.0188
Epoch [25/30], Batch [2800/6000], Loss: 0.0320
Epoch [25/30], Batch [2900/6000], Loss: 0.0153
Epoch [25/30], Batch [3000/6000], Loss: 0.0192
Epoch [25/30], Batch [3100/6000], Loss: 0.0194
Epoch [25/30], Batch [3200/6000], Loss: 0.0860
Epoch [25/30], Batch [3300/6000], Loss: 0.0168
Epoch [25/30], Batch [3400/6000], Loss: 0.0233
Epoch [25/30], Batch [3500/6000], Loss: 0.0138
Epoch [25/30], Batch [3600/6000], Loss: 0.0168
Epoch [25/30], Batch [3700/6000], Loss: 0.0190
Epoch [25/30], Batch [3800/6000], Loss: 0.0169
Epoch [25/30], Batch [3900/6000], Loss: 0.0201
Epoch [25/30], Batch [4000/6000], Loss: 0.0224
Epoch [25/30], Batch [4100/6000], Loss: 0.0193
Epoch [25/30], Batch [4200/6000], Loss: 0.0199
Epoch [25/30], Batch [4300/6000], Loss: 0.0195
Epoch [25/30], Batch [4400/6000], Loss: 0.0195
Epoch [25/30], Batch [4500/6000], Loss: 0.0219
Epoch [25/30], Batch [4600/6000], Loss: 0.0221
Epoch [25/30], Batch [4700/6000], Loss: 0.0169
Epoch [25/30], Batch [4800/6000], Loss: 0.0147
Epoch [25/30], Batch [4900/6000], Loss: 0.0167
Epoch [25/30], Batch [5000/6000], Loss: 0.0241
Epoch [25/30], Batch [5100/6000], Loss: 0.0372
Epoch [25/30], Batch [5200/6000], Loss: 0.0331
Epoch [25/30], Batch [5300/6000], Loss: 0.0872
Epoch [25/30], Batch [5400/6000], Loss: 0.0210
Epoch [25/30], Batch [5500/6000], Loss: 0.0196
Epoch [25/30], Batch [5600/6000], Loss: 0.0171
Epoch [25/30], Batch [5700/6000], Loss: 0.0191
Epoch [25/30], Batch [5800/6000], Loss: 0.0200
Epoch [25/30], Batch [5900/6000], Loss: 0.0169
Epoch [25/30], Loss: 0.0293
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.0424
Epoch [26/30], Batch [100/6000], Loss: 0.0227
Epoch [26/30], Batch [200/6000], Loss: 0.0185
Epoch [26/30], Batch [300/6000], Loss: 0.0226
Epoch [26/30], Batch [400/6000], Loss: 0.0161
Epoch [26/30], Batch [500/6000], Loss: 0.0179
Epoch [26/30], Batch [600/6000], Loss: 0.0204
Epoch [26/30], Batch [700/6000], Loss: 0.0125
Epoch [26/30], Batch [800/6000], Loss: 0.0206
Epoch [26/30], Batch [900/6000], Loss: 0.0157
Epoch [26/30], Batch [1000/6000], Loss: 0.0199
Epoch [26/30], Batch [1100/6000], Loss: 0.0222
Epoch [26/30], Batch [1200/6000], Loss: 0.0199
Epoch [26/30], Batch [1300/6000], Loss: 0.0182
Epoch [26/30], Batch [1400/6000], Loss: 0.0216
Epoch [26/30], Batch [1500/6000], Loss: 0.0144
Epoch [26/30], Batch [1600/6000], Loss: 0.0202
Epoch [26/30], Batch [1700/6000], Loss: 0.0200
Epoch [26/30], Batch [1800/6000], Loss: 0.0190
Epoch [26/30], Batch [1900/6000], Loss: 0.0186
Epoch [26/30], Batch [2000/6000], Loss: 0.0183
Epoch [26/30], Batch [2100/6000], Loss: 0.0275
Epoch [26/30], Batch [2200/6000], Loss: 0.0170
Epoch [26/30], Batch [2300/6000], Loss: 0.0263
Epoch [26/30], Batch [2400/6000], Loss: 0.0176
Epoch [26/30], Batch [2500/6000], Loss: 0.0148
Epoch [26/30], Batch [2600/6000], Loss: 0.0179
Epoch [26/30], Batch [2700/6000], Loss: 0.0206
Epoch [26/30], Batch [2800/6000], Loss: 0.0203
Epoch [26/30], Batch [2900/6000], Loss: 0.0163
Epoch [26/30], Batch [3000/6000], Loss: 0.0182
Epoch [26/30], Batch [3100/6000], Loss: 0.1953
Epoch [26/30], Batch [3200/6000], Loss: 0.0195
Epoch [26/30], Batch [3300/6000], Loss: 0.0186
Epoch [26/30], Batch [3400/6000], Loss: 0.0195
Epoch [26/30], Batch [3500/6000], Loss: 0.2267
Epoch [26/30], Batch [3600/6000], Loss: 0.0175
Epoch [26/30], Batch [3700/6000], Loss: 0.0226
Epoch [26/30], Batch [3800/6000], Loss: 0.0225
Epoch [26/30], Batch [3900/6000], Loss: 0.0218
Epoch [26/30], Batch [4000/6000], Loss: 0.0270
Epoch [26/30], Batch [4100/6000], Loss: 0.0366
Epoch [26/30], Batch [4200/6000], Loss: 0.0167
Epoch [26/30], Batch [4300/6000], Loss: 0.0178
Epoch [26/30], Batch [4400/6000], Loss: 0.0189
Epoch [26/30], Batch [4500/6000], Loss: 0.0189
Epoch [26/30], Batch [4600/6000], Loss: 0.0229
Epoch [26/30], Batch [4700/6000], Loss: 0.0162
Epoch [26/30], Batch [4800/6000], Loss: 0.0206
Epoch [26/30], Batch [4900/6000], Loss: 0.0187
Epoch [26/30], Batch [5000/6000], Loss: 0.0200
Epoch [26/30], Batch [5100/6000], Loss: 0.0165
Epoch [26/30], Batch [5200/6000], Loss: 0.0151
Epoch [26/30], Batch [5300/6000], Loss: 0.0211
Epoch [26/30], Batch [5400/6000], Loss: 0.0184
Epoch [26/30], Batch [5500/6000], Loss: 0.0281
Epoch [26/30], Batch [5600/6000], Loss: 0.0209
Epoch [26/30], Batch [5700/6000], Loss: 0.0234
Epoch [26/30], Batch [5800/6000], Loss: 0.0234
Epoch [26/30], Batch [5900/6000], Loss: 0.0179
Epoch [26/30], Loss: 0.0285
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0167
Epoch [27/30], Batch [100/6000], Loss: 0.0201
Epoch [27/30], Batch [200/6000], Loss: 0.0155
Epoch [27/30], Batch [300/6000], Loss: 0.0107
Epoch [27/30], Batch [400/6000], Loss: 0.0199
Epoch [27/30], Batch [500/6000], Loss: 0.0182
Epoch [27/30], Batch [600/6000], Loss: 0.0356
Epoch [27/30], Batch [700/6000], Loss: 0.0288
Epoch [27/30], Batch [800/6000], Loss: 0.0181
Epoch [27/30], Batch [900/6000], Loss: 0.0219
Epoch [27/30], Batch [1000/6000], Loss: 0.0179
Epoch [27/30], Batch [1100/6000], Loss: 0.0220
Epoch [27/30], Batch [1200/6000], Loss: 0.1012
Epoch [27/30], Batch [1300/6000], Loss: 0.0155
Epoch [27/30], Batch [1400/6000], Loss: 0.0151
Epoch [27/30], Batch [1500/6000], Loss: 0.0130
Epoch [27/30], Batch [1600/6000], Loss: 0.0193
Epoch [27/30], Batch [1700/6000], Loss: 0.0171
Epoch [27/30], Batch [1800/6000], Loss: 0.0186
Epoch [27/30], Batch [1900/6000], Loss: 0.1500
Epoch [27/30], Batch [2000/6000], Loss: 0.0182
Epoch [27/30], Batch [2100/6000], Loss: 0.0172
Epoch [27/30], Batch [2200/6000], Loss: 0.0198
Epoch [27/30], Batch [2300/6000], Loss: 0.0187
Epoch [27/30], Batch [2400/6000], Loss: 0.0169
Epoch [27/30], Batch [2500/6000], Loss: 0.0198
Epoch [27/30], Batch [2600/6000], Loss: 0.0212
Epoch [27/30], Batch [2700/6000], Loss: 0.0182
Epoch [27/30], Batch [2800/6000], Loss: 0.0167
Epoch [27/30], Batch [2900/6000], Loss: 0.0202
Epoch [27/30], Batch [3000/6000], Loss: 0.0367
Epoch [27/30], Batch [3100/6000], Loss: 0.0258
Epoch [27/30], Batch [3200/6000], Loss: 0.0177
Epoch [27/30], Batch [3300/6000], Loss: 0.0190
Epoch [27/30], Batch [3400/6000], Loss: 0.0191
Epoch [27/30], Batch [3500/6000], Loss: 0.0200
Epoch [27/30], Batch [3600/6000], Loss: 0.0188
Epoch [27/30], Batch [3700/6000], Loss: 0.0156
Epoch [27/30], Batch [3800/6000], Loss: 0.0130
Epoch [27/30], Batch [3900/6000], Loss: 0.0198
Epoch [27/30], Batch [4000/6000], Loss: 0.0333
Epoch [27/30], Batch [4100/6000], Loss: 0.0199
Epoch [27/30], Batch [4200/6000], Loss: 0.0262
Epoch [27/30], Batch [4300/6000], Loss: 0.2337
Epoch [27/30], Batch [4400/6000], Loss: 0.0165
Epoch [27/30], Batch [4500/6000], Loss: 0.0205
Epoch [27/30], Batch [4600/6000], Loss: 0.0246
Epoch [27/30], Batch [4700/6000], Loss: 0.0169
Epoch [27/30], Batch [4800/6000], Loss: 0.0221
Epoch [27/30], Batch [4900/6000], Loss: 0.0200
Epoch [27/30], Batch [5000/6000], Loss: 0.0179
Epoch [27/30], Batch [5100/6000], Loss: 0.0163
Epoch [27/30], Batch [5200/6000], Loss: 0.0177
Epoch [27/30], Batch [5300/6000], Loss: 0.0192
Epoch [27/30], Batch [5400/6000], Loss: 0.0441
Epoch [27/30], Batch [5500/6000], Loss: 0.0159
Epoch [27/30], Batch [5600/6000], Loss: 0.0206
Epoch [27/30], Batch [5700/6000], Loss: 0.0138
Epoch [27/30], Batch [5800/6000], Loss: 0.0215
Epoch [27/30], Batch [5900/6000], Loss: 0.0207
Epoch [27/30], Loss: 0.0281
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.0173
Epoch [28/30], Batch [100/6000], Loss: 0.0174
Epoch [28/30], Batch [200/6000], Loss: 0.0210
Epoch [28/30], Batch [300/6000], Loss: 0.0175
Epoch [28/30], Batch [400/6000], Loss: 0.0150
Epoch [28/30], Batch [500/6000], Loss: 0.0171
Epoch [28/30], Batch [600/6000], Loss: 0.0189
Epoch [28/30], Batch [700/6000], Loss: 0.0191
Epoch [28/30], Batch [800/6000], Loss: 0.0226
Epoch [28/30], Batch [900/6000], Loss: 0.0159
Epoch [28/30], Batch [1000/6000], Loss: 0.0206
Epoch [28/30], Batch [1100/6000], Loss: 0.0372
Epoch [28/30], Batch [1200/6000], Loss: 0.0219
Epoch [28/30], Batch [1300/6000], Loss: 0.0190
Epoch [28/30], Batch [1400/6000], Loss: 0.0230
Epoch [28/30], Batch [1500/6000], Loss: 0.0203
Epoch [28/30], Batch [1600/6000], Loss: 0.0169
Epoch [28/30], Batch [1700/6000], Loss: 0.0208
Epoch [28/30], Batch [1800/6000], Loss: 0.0179
Epoch [28/30], Batch [1900/6000], Loss: 0.0186
Epoch [28/30], Batch [2000/6000], Loss: 0.0192
Epoch [28/30], Batch [2100/6000], Loss: 0.0190
Epoch [28/30], Batch [2200/6000], Loss: 0.0218
Epoch [28/30], Batch [2300/6000], Loss: 0.0177
Epoch [28/30], Batch [2400/6000], Loss: 0.0182
Epoch [28/30], Batch [2500/6000], Loss: 0.0181
Epoch [28/30], Batch [2600/6000], Loss: 0.0149
Epoch [28/30], Batch [2700/6000], Loss: 0.0170
Epoch [28/30], Batch [2800/6000], Loss: 0.0263
Epoch [28/30], Batch [2900/6000], Loss: 0.0160
Epoch [28/30], Batch [3000/6000], Loss: 0.0186
Epoch [28/30], Batch [3100/6000], Loss: 0.0205
Epoch [28/30], Batch [3200/6000], Loss: 0.3394
Epoch [28/30], Batch [3300/6000], Loss: 0.0211
Epoch [28/30], Batch [3400/6000], Loss: 0.0200
Epoch [28/30], Batch [3500/6000], Loss: 0.0336
Epoch [28/30], Batch [3600/6000], Loss: 0.0155
Epoch [28/30], Batch [3700/6000], Loss: 0.0152
Epoch [28/30], Batch [3800/6000], Loss: 0.0242
Epoch [28/30], Batch [3900/6000], Loss: 0.0198
Epoch [28/30], Batch [4000/6000], Loss: 0.0196
Epoch [28/30], Batch [4100/6000], Loss: 0.0143
Epoch [28/30], Batch [4200/6000], Loss: 0.0250
Epoch [28/30], Batch [4300/6000], Loss: 0.0164
Epoch [28/30], Batch [4400/6000], Loss: 0.0153
Epoch [28/30], Batch [4500/6000], Loss: 0.0313
Epoch [28/30], Batch [4600/6000], Loss: 0.0207
Epoch [28/30], Batch [4700/6000], Loss: 0.0187
Epoch [28/30], Batch [4800/6000], Loss: 0.0213
Epoch [28/30], Batch [4900/6000], Loss: 0.0211
Epoch [28/30], Batch [5000/6000], Loss: 0.0143
Epoch [28/30], Batch [5100/6000], Loss: 0.0131
Epoch [28/30], Batch [5200/6000], Loss: 0.0179
Epoch [28/30], Batch [5300/6000], Loss: 0.0192
Epoch [28/30], Batch [5400/6000], Loss: 0.0170
Epoch [28/30], Batch [5500/6000], Loss: 0.0209
Epoch [28/30], Batch [5600/6000], Loss: 0.0169
Epoch [28/30], Batch [5700/6000], Loss: 0.0164
Epoch [28/30], Batch [5800/6000], Loss: 0.0171
Epoch [28/30], Batch [5900/6000], Loss: 0.0177
Epoch [28/30], Loss: 0.0276
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0107
Epoch [29/30], Batch [100/6000], Loss: 0.0776
Epoch [29/30], Batch [200/6000], Loss: 0.0154
Epoch [29/30], Batch [300/6000], Loss: 0.0216
Epoch [29/30], Batch [400/6000], Loss: 0.0154
Epoch [29/30], Batch [500/6000], Loss: 0.0165
Epoch [29/30], Batch [600/6000], Loss: 0.0175
Epoch [29/30], Batch [700/6000], Loss: 0.0191
Epoch [29/30], Batch [800/6000], Loss: 0.0179
Epoch [29/30], Batch [900/6000], Loss: 0.0169
Epoch [29/30], Batch [1000/6000], Loss: 0.0226
Epoch [29/30], Batch [1100/6000], Loss: 0.0110
Epoch [29/30], Batch [1200/6000], Loss: 0.0145
Epoch [29/30], Batch [1300/6000], Loss: 0.0225
Epoch [29/30], Batch [1400/6000], Loss: 0.0150
Epoch [29/30], Batch [1500/6000], Loss: 0.0183
Epoch [29/30], Batch [1600/6000], Loss: 0.0172
Epoch [29/30], Batch [1700/6000], Loss: 0.0120
Epoch [29/30], Batch [1800/6000], Loss: 0.0153
Epoch [29/30], Batch [1900/6000], Loss: 0.0184
Epoch [29/30], Batch [2000/6000], Loss: 0.0146
Epoch [29/30], Batch [2100/6000], Loss: 0.0172
Epoch [29/30], Batch [2200/6000], Loss: 0.0213
Epoch [29/30], Batch [2300/6000], Loss: 0.0269
Epoch [29/30], Batch [2400/6000], Loss: 0.0205
Epoch [29/30], Batch [2500/6000], Loss: 0.0161
Epoch [29/30], Batch [2600/6000], Loss: 0.0198
Epoch [29/30], Batch [2700/6000], Loss: 0.0177
Epoch [29/30], Batch [2800/6000], Loss: 0.0256
Epoch [29/30], Batch [2900/6000], Loss: 0.0211
Epoch [29/30], Batch [3000/6000], Loss: 0.0378
Epoch [29/30], Batch [3100/6000], Loss: 0.0179
Epoch [29/30], Batch [3200/6000], Loss: 0.0178
Epoch [29/30], Batch [3300/6000], Loss: 0.0664
Epoch [29/30], Batch [3400/6000], Loss: 0.0216
Epoch [29/30], Batch [3500/6000], Loss: 0.0195
Epoch [29/30], Batch [3600/6000], Loss: 0.0146
Epoch [29/30], Batch [3700/6000], Loss: 0.0172
Epoch [29/30], Batch [3800/6000], Loss: 0.0194
Epoch [29/30], Batch [3900/6000], Loss: 0.0168
Epoch [29/30], Batch [4000/6000], Loss: 0.0157
Epoch [29/30], Batch [4100/6000], Loss: 0.0179
Epoch [29/30], Batch [4200/6000], Loss: 0.0210
Epoch [29/30], Batch [4300/6000], Loss: 0.0178
Epoch [29/30], Batch [4400/6000], Loss: 0.0143
Epoch [29/30], Batch [4500/6000], Loss: 0.0161
Epoch [29/30], Batch [4600/6000], Loss: 0.0203
Epoch [29/30], Batch [4700/6000], Loss: 0.0202
Epoch [29/30], Batch [4800/6000], Loss: 0.0196
Epoch [29/30], Batch [4900/6000], Loss: 0.0346
Epoch [29/30], Batch [5000/6000], Loss: 0.0162
Epoch [29/30], Batch [5100/6000], Loss: 0.0151
Epoch [29/30], Batch [5200/6000], Loss: 0.0161
Epoch [29/30], Batch [5300/6000], Loss: 0.0173
Epoch [29/30], Batch [5400/6000], Loss: 0.0165
Epoch [29/30], Batch [5500/6000], Loss: 0.0175
Epoch [29/30], Batch [5600/6000], Loss: 0.0167
Epoch [29/30], Batch [5700/6000], Loss: 0.0834
Epoch [29/30], Batch [5800/6000], Loss: 0.0171
Epoch [29/30], Batch [5900/6000], Loss: 0.0215
Epoch [29/30], Loss: 0.0271
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0160
Epoch [30/30], Batch [100/6000], Loss: 0.0143
Epoch [30/30], Batch [200/6000], Loss: 0.0191
Epoch [30/30], Batch [300/6000], Loss: 0.0181
Epoch [30/30], Batch [400/6000], Loss: 0.0307
Epoch [30/30], Batch [500/6000], Loss: 0.0180
Epoch [30/30], Batch [600/6000], Loss: 0.0212
Epoch [30/30], Batch [700/6000], Loss: 0.0168
Epoch [30/30], Batch [800/6000], Loss: 0.0144
Epoch [30/30], Batch [900/6000], Loss: 0.0165
Epoch [30/30], Batch [1000/6000], Loss: 0.0168
Epoch [30/30], Batch [1100/6000], Loss: 0.0198
Epoch [30/30], Batch [1200/6000], Loss: 0.0203
Epoch [30/30], Batch [1300/6000], Loss: 0.0241
Epoch [30/30], Batch [1400/6000], Loss: 0.0375
Epoch [30/30], Batch [1500/6000], Loss: 0.0211
Epoch [30/30], Batch [1600/6000], Loss: 0.0196
Epoch [30/30], Batch [1700/6000], Loss: 0.0192
Epoch [30/30], Batch [1800/6000], Loss: 0.0186
Epoch [30/30], Batch [1900/6000], Loss: 0.0189
Epoch [30/30], Batch [2000/6000], Loss: 0.0185
Epoch [30/30], Batch [2100/6000], Loss: 0.0231
Epoch [30/30], Batch [2200/6000], Loss: 0.0182
Epoch [30/30], Batch [2300/6000], Loss: 0.0183
Epoch [30/30], Batch [2400/6000], Loss: 0.0217
Epoch [30/30], Batch [2500/6000], Loss: 0.0173
Epoch [30/30], Batch [2600/6000], Loss: 0.0226
Epoch [30/30], Batch [2700/6000], Loss: 0.0165
Epoch [30/30], Batch [2800/6000], Loss: 0.0167
Epoch [30/30], Batch [2900/6000], Loss: 0.0178
Epoch [30/30], Batch [3000/6000], Loss: 0.0173
Epoch [30/30], Batch [3100/6000], Loss: 0.0179
Epoch [30/30], Batch [3200/6000], Loss: 0.0193
Epoch [30/30], Batch [3300/6000], Loss: 0.0200
Epoch [30/30], Batch [3400/6000], Loss: 0.0182
Epoch [30/30], Batch [3500/6000], Loss: 0.0186
Epoch [30/30], Batch [3600/6000], Loss: 0.0132
Epoch [30/30], Batch [3700/6000], Loss: 0.1970
Epoch [30/30], Batch [3800/6000], Loss: 0.0187
Epoch [30/30], Batch [3900/6000], Loss: 0.0196
Epoch [30/30], Batch [4000/6000], Loss: 0.0135
Epoch [30/30], Batch [4100/6000], Loss: 0.0151
Epoch [30/30], Batch [4200/6000], Loss: 0.0180
Epoch [30/30], Batch [4300/6000], Loss: 0.0165
Epoch [30/30], Batch [4400/6000], Loss: 0.0172
Epoch [30/30], Batch [4500/6000], Loss: 0.0207
Epoch [30/30], Batch [4600/6000], Loss: 0.0164
Epoch [30/30], Batch [4700/6000], Loss: 0.0193
Epoch [30/30], Batch [4800/6000], Loss: 0.0225
Epoch [30/30], Batch [4900/6000], Loss: 0.0184
Epoch [30/30], Batch [5000/6000], Loss: 0.0144
Epoch [30/30], Batch [5100/6000], Loss: 0.0197
Epoch [30/30], Batch [5200/6000], Loss: 0.0678
Epoch [30/30], Batch [5300/6000], Loss: 0.0197
Epoch [30/30], Batch [5400/6000], Loss: 0.0181
Epoch [30/30], Batch [5500/6000], Loss: 0.0215
Epoch [30/30], Batch [5600/6000], Loss: 0.0177
Epoch [30/30], Batch [5700/6000], Loss: 0.0159
Epoch [30/30], Batch [5800/6000], Loss: 0.0166
Epoch [30/30], Batch [5900/6000], Loss: 0.0401
Epoch [30/30], Loss: 0.0259
Visualization saved to figures/visualization_0.png
Test Loss: 0.1067, Accuracy: 98.32%
Visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 0.9207
  Image Loss: 0.0151
  Total Loss: 9.2223
  Image grad max: 1.2484451532363892
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]
Adversarial Training Loop 2/300:
  Label Loss: 0.7633
  Image Loss: 0.0151
  Total Loss: 7.6483
  Image grad max: 1.6392178535461426
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]
Adversarial Training Loop 3/300:
  Label Loss: 0.5803
  Image Loss: 0.0151
  Total Loss: 5.8177
  Image grad max: 2.2304158210754395
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]
Adversarial Training Loop 4/300:
  Label Loss: 0.3484
  Image Loss: 0.0152
  Total Loss: 3.4993
  Image grad max: 2.988459348678589
  Output probs: [[0.    0.    0.053 0.    0.    0.    0.    0.    0.947 0.   ]]
Adversarial Training Loop 5/300:
  Label Loss: 0.0801
  Image Loss: 0.0155
  Total Loss: 0.8161
  Image grad max: 3.176542043685913
  Output probs: [[0.    0.    0.954 0.    0.    0.    0.    0.    0.046 0.   ]]
Adversarial Training Loop 6/300:
  Label Loss: 0.0873
  Image Loss: 0.0158
  Total Loss: 0.8885
  Image grad max: 3.3731203079223633
  Output probs: [[0.    0.    0.996 0.    0.    0.    0.    0.    0.004 0.   ]]
Adversarial Training Loop 7/300:
  Label Loss: 0.2121
  Image Loss: 0.0160
  Total Loss: 2.1369
  Image grad max: 3.5393643379211426
  Output probs: [[0.    0.    0.998 0.    0.    0.    0.    0.    0.002 0.   ]]
Adversarial Training Loop 8/300:
  Label Loss: 0.2363
  Image Loss: 0.0160
  Total Loss: 2.3789
  Image grad max: 3.4708962440490723
  Output probs: [[0.    0.    0.992 0.    0.    0.    0.    0.    0.008 0.   ]]
Adversarial Training Loop 9/300:
  Label Loss: 0.1702
  Image Loss: 0.0160
  Total Loss: 1.7184
  Image grad max: 3.4329311847686768
  Output probs: [[0.    0.    0.872 0.    0.    0.    0.    0.    0.128 0.   ]]
Adversarial Training Loop 10/300:
  Label Loss: 0.0402
  Image Loss: 0.0159
  Total Loss: 0.4182
  Image grad max: 2.4974594116210938
  Output probs: [[0.    0.    0.166 0.    0.    0.    0.    0.    0.833 0.   ]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0294
  Image Loss: 0.0157
  Total Loss: 0.3101
  Image grad max: 2.074878692626953
  Output probs: [[0.   0.   0.03 0.   0.   0.   0.   0.   0.97 0.  ]]
Adversarial Training Loop 12/300:
  Label Loss: 0.1069
  Image Loss: 0.0157
  Total Loss: 1.0845
  Image grad max: 2.725257396697998
  Output probs: [[0.    0.    0.022 0.    0.    0.    0.    0.    0.978 0.   ]]
Adversarial Training Loop 13/300:
  Label Loss: 0.1236
  Image Loss: 0.0157
  Total Loss: 1.2519
  Image grad max: 2.688401460647583
  Output probs: [[0.    0.    0.045 0.    0.    0.    0.    0.    0.955 0.   ]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0877
  Image Loss: 0.0158
  Total Loss: 0.8933
  Image grad max: 2.5667099952697754
  Output probs: [[0.    0.    0.198 0.    0.    0.    0.    0.    0.802 0.   ]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0226
  Image Loss: 0.0159
  Total Loss: 0.2422
  Image grad max: 1.7310785055160522
  Output probs: [[0.    0.    0.668 0.    0.    0.    0.    0.    0.332 0.   ]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0060
  Image Loss: 0.0161
  Total Loss: 0.0763
  Image grad max: 0.9927877187728882
  Output probs: [[0.    0.    0.896 0.    0.    0.    0.    0.    0.104 0.   ]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0494
  Image Loss: 0.0162
  Total Loss: 0.5103
  Image grad max: 2.359408378601074
  Output probs: [[0.    0.    0.927 0.    0.    0.    0.    0.    0.073 0.   ]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0650
  Image Loss: 0.0163
  Total Loss: 0.6667
  Image grad max: 2.522000312805176
  Output probs: [[0.    0.    0.877 0.    0.    0.    0.    0.    0.123 0.   ]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0419
  Image Loss: 0.0163
  Total Loss: 0.4351
  Image grad max: 2.193265914916992
  Output probs: [[0.    0.    0.653 0.    0.    0.    0.    0.    0.347 0.   ]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0049
  Image Loss: 0.0163
  Total Loss: 0.0655
  Image grad max: 0.866255521774292
  Output probs: [[0.    0.    0.299 0.    0.    0.    0.    0.    0.701 0.   ]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0088
  Image Loss: 0.0162
  Total Loss: 0.1042
  Image grad max: 1.0978988409042358
  Output probs: [[0.    0.    0.149 0.    0.    0.    0.    0.    0.851 0.   ]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0339
  Image Loss: 0.0162
  Total Loss: 0.3550
  Image grad max: 1.87300443649292
  Output probs: [[0.    0.    0.137 0.    0.    0.    0.    0.    0.863 0.   ]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0374
  Image Loss: 0.0162
  Total Loss: 0.3903
  Image grad max: 1.920292615890503
  Output probs: [[0.    0.    0.224 0.    0.    0.    0.    0.    0.776 0.   ]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0181
  Image Loss: 0.0163
  Total Loss: 0.1974
  Image grad max: 1.4621363878250122
  Output probs: [[0.    0.    0.455 0.    0.    0.    0.    0.    0.545 0.   ]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0004
  Image Loss: 0.0164
  Total Loss: 0.0204
  Image grad max: 0.24004624783992767
  Output probs: [[0.    0.    0.698 0.    0.    0.    0.    0.    0.302 0.   ]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0086
  Image Loss: 0.0165
  Total Loss: 0.1022
  Image grad max: 1.0810998678207397
  Output probs: [[0.    0.    0.793 0.    0.    0.    0.    0.    0.207 0.   ]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0211
  Image Loss: 0.0166
  Total Loss: 0.2273
  Image grad max: 1.6041607856750488
  Output probs: [[0.    0.    0.775 0.    0.    0.    0.    0.    0.225 0.   ]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0180
  Image Loss: 0.0166
  Total Loss: 0.1965
  Image grad max: 1.4936076402664185
  Output probs: [[0.    0.    0.642 0.    0.    0.    0.    0.    0.358 0.   ]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0042
  Image Loss: 0.0166
  Total Loss: 0.0585
  Image grad max: 0.7586953639984131
  Output probs: [[0.    0.    0.428 0.    0.    0.    0.    0.    0.572 0.   ]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0010
  Image Loss: 0.0165
  Total Loss: 0.0270
  Image grad max: 0.3768158257007599
  Output probs: [[0.    0.    0.285 0.    0.    0.    0.    0.    0.715 0.   ]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0103
  Image Loss: 0.0165
  Total Loss: 0.1193
  Image grad max: 1.1123178005218506
  Output probs: [[0.    0.    0.256 0.    0.    0.    0.    0.    0.744 0.   ]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0135
  Image Loss: 0.0165
  Total Loss: 0.1520
  Image grad max: 1.2491031885147095
  Output probs: [[0.    0.    0.328 0.    0.    0.    0.    0.    0.672 0.   ]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0063
  Image Loss: 0.0166
  Total Loss: 0.0795
  Image grad max: 0.8836762309074402
  Output probs: [[0.    0.    0.483 0.    0.    0.    0.    0.    0.517 0.   ]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0001
  Image Loss: 0.0166
  Total Loss: 0.0172
  Image grad max: 0.08616171777248383
  Output probs: [[0.    0.    0.632 0.    0.    0.    0.    0.    0.368 0.   ]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0036
  Image Loss: 0.0167
  Total Loss: 0.0529
  Image grad max: 0.6922639012336731
  Output probs: [[0.    0.    0.691 0.    0.    0.    0.    0.    0.308 0.   ]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0079
  Image Loss: 0.0167
  Total Loss: 0.0961
  Image grad max: 1.0057532787322998
  Output probs: [[0.    0.    0.662 0.    0.    0.    0.    0.    0.338 0.   ]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0055
  Image Loss: 0.0167
  Total Loss: 0.0719
  Image grad max: 0.844761848449707
  Output probs: [[0.    0.    0.552 0.    0.    0.    0.    0.    0.448 0.   ]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0005
  Image Loss: 0.0167
  Total Loss: 0.0221
  Image grad max: 0.2677454650402069
  Output probs: [[0.    0.    0.421 0.    0.    0.    0.    0.    0.579 0.   ]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0013
  Image Loss: 0.0167
  Total Loss: 0.0292
  Image grad max: 0.4006824195384979
  Output probs: [[0.    0.    0.349 0.    0.    0.    0.    0.    0.651 0.   ]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0048
  Image Loss: 0.0167
  Total Loss: 0.0646
  Image grad max: 0.7652696967124939
  Output probs: [[0.    0.    0.358 0.    0.    0.    0.    0.    0.642 0.   ]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0042
  Image Loss: 0.0167
  Total Loss: 0.0590
  Image grad max: 0.7199625372886658
  Output probs: [[0.    0.    0.438 0.    0.    0.    0.    0.    0.562 0.   ]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0008
  Image Loss: 0.0167
  Total Loss: 0.0246
  Image grad max: 0.316984087228775
  Output probs: [[0.    0.    0.545 0.    0.    0.    0.    0.    0.455 0.   ]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0004
  Image Loss: 0.0167
  Total Loss: 0.0209
  Image grad max: 0.2305188924074173
  Output probs: [[0.    0.    0.614 0.    0.    0.    0.    0.    0.386 0.   ]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0027
  Image Loss: 0.0168
  Total Loss: 0.0433
  Image grad max: 0.584122359752655
  Output probs: [[0.    0.    0.616 0.    0.    0.    0.    0.    0.384 0.   ]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0028
  Image Loss: 0.0168
  Total Loss: 0.0447
  Image grad max: 0.5980424284934998
  Output probs: [[0.    0.    0.557 0.    0.    0.    0.    0.    0.443 0.   ]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0007
  Image Loss: 0.0168
  Total Loss: 0.0233
  Image grad max: 0.2903309166431427
  Output probs: [[0.   0.   0.47 0.   0.   0.   0.   0.   0.53 0.  ]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0002
  Image Loss: 0.0167
  Total Loss: 0.0186
  Image grad max: 0.15087464451789856
  Output probs: [[0.   0.   0.41 0.   0.   0.   0.   0.   0.59 0.  ]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0017
  Image Loss: 0.0167
  Total Loss: 0.0334
  Image grad max: 0.4552846848964691
  Output probs: [[0.    0.    0.405 0.    0.    0.    0.    0.    0.595 0.   ]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0019
  Image Loss: 0.0167
  Total Loss: 0.0352
  Image grad max: 0.4786945581436157
  Output probs: [[0.    0.    0.453 0.    0.    0.    0.    0.    0.547 0.   ]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0004
  Image Loss: 0.0167
  Total Loss: 0.0212
  Image grad max: 0.23570574820041656
  Output probs: [[0.    0.    0.524 0.    0.    0.    0.    0.    0.476 0.   ]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0001
  Image Loss: 0.0168
  Total Loss: 0.0180
  Image grad max: 0.12401831150054932
  Output probs: [[0.    0.    0.573 0.    0.    0.    0.    0.    0.427 0.   ]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0011
  Image Loss: 0.0168
  Total Loss: 0.0276
  Image grad max: 0.370681494474411
  Output probs: [[0.    0.    0.575 0.    0.    0.    0.    0.    0.425 0.   ]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0011
  Image Loss: 0.0168
  Total Loss: 0.0281
  Image grad max: 0.38006484508514404
  Output probs: [[0.    0.    0.533 0.    0.    0.    0.    0.    0.467 0.   ]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0002
  Image Loss: 0.0168
  Total Loss: 0.0190
  Image grad max: 0.16599662601947784
  Output probs: [[0.    0.    0.475 0.    0.    0.    0.    0.    0.525 0.   ]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0001
  Image Loss: 0.0168
  Total Loss: 0.0180
  Image grad max: 0.12558257579803467
  Output probs: [[0.    0.    0.438 0.    0.    0.    0.    0.    0.562 0.   ]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0008
  Image Loss: 0.0167
  Total Loss: 0.0245
  Image grad max: 0.30988070368766785
  Output probs: [[0.    0.    0.442 0.    0.    0.    0.    0.    0.558 0.   ]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0007
  Image Loss: 0.0167
  Total Loss: 0.0236
  Image grad max: 0.2921634018421173
  Output probs: [[0.   0.   0.48 0.   0.   0.   0.   0.   0.52 0.  ]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0001
  Image Loss: 0.0167
  Total Loss: 0.0176
  Image grad max: 0.0986327975988388
  Output probs: [[0.    0.    0.527 0.    0.    0.    0.    0.    0.473 0.   ]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0002
  Image Loss: 0.0168
  Total Loss: 0.0183
  Image grad max: 0.13811060786247253
  Output probs: [[0.    0.    0.552 0.    0.    0.    0.    0.    0.448 0.   ]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0005
  Image Loss: 0.0168
  Total Loss: 0.0222
  Image grad max: 0.26215383410453796
  Output probs: [[0.    0.    0.542 0.    0.    0.    0.    0.    0.458 0.   ]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0004
  Image Loss: 0.0168
  Total Loss: 0.0204
  Image grad max: 0.21348759531974792
  Output probs: [[0.    0.    0.506 0.    0.    0.    0.    0.    0.493 0.   ]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.032808687537908554
  Output probs: [[0.   0.   0.47 0.   0.   0.   0.   0.   0.53 0.  ]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0002
  Image Loss: 0.0167
  Total Loss: 0.0185
  Image grad max: 0.14937247335910797
  Output probs: [[0.    0.    0.457 0.    0.    0.    0.    0.    0.543 0.   ]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0004
  Image Loss: 0.0167
  Total Loss: 0.0205
  Image grad max: 0.21763986349105835
  Output probs: [[0.    0.    0.472 0.    0.    0.    0.    0.    0.528 0.   ]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0002
  Image Loss: 0.0167
  Total Loss: 0.0183
  Image grad max: 0.14014656841754913
  Output probs: [[0.    0.    0.504 0.    0.    0.    0.    0.    0.496 0.   ]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0168
  Image grad max: 0.022237464785575867
  Output probs: [[0.   0.   0.53 0.   0.   0.   0.   0.   0.47 0.  ]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0002
  Image Loss: 0.0167
  Total Loss: 0.0186
  Image grad max: 0.15275152027606964
  Output probs: [[0.    0.    0.534 0.    0.    0.    0.    0.    0.466 0.   ]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0002
  Image Loss: 0.0167
  Total Loss: 0.0190
  Image grad max: 0.1692505180835724
  Output probs: [[0.    0.    0.514 0.    0.    0.    0.    0.    0.486 0.   ]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0171
  Image grad max: 0.07108403742313385
  Output probs: [[0.    0.    0.487 0.    0.    0.    0.    0.    0.513 0.   ]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0171
  Image grad max: 0.0655314028263092
  Output probs: [[0.    0.    0.471 0.    0.    0.    0.    0.    0.529 0.   ]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0002
  Image Loss: 0.0167
  Total Loss: 0.0184
  Image grad max: 0.1434089094400406
  Output probs: [[0.    0.    0.477 0.    0.    0.    0.    0.    0.523 0.   ]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0001
  Image Loss: 0.0167
  Total Loss: 0.0178
  Image grad max: 0.11682281643152237
  Output probs: [[0.    0.    0.498 0.    0.    0.    0.    0.    0.502 0.   ]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0167
  Image grad max: 0.012038499116897583
  Output probs: [[0.    0.    0.518 0.    0.    0.    0.    0.    0.482 0.   ]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0001
  Image Loss: 0.0167
  Total Loss: 0.0174
  Image grad max: 0.09070050716400146
  Output probs: [[0.    0.    0.524 0.    0.    0.    0.    0.    0.476 0.   ]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0001
  Image Loss: 0.0167
  Total Loss: 0.0178
  Image grad max: 0.11901868879795074
  Output probs: [[0.    0.    0.512 0.    0.    0.    0.    0.    0.488 0.   ]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0170
  Image grad max: 0.061285655945539474
  Output probs: [[0.    0.    0.493 0.    0.    0.    0.    0.    0.507 0.   ]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0000
  Image Loss: 0.0167
  Total Loss: 0.0168
  Image grad max: 0.03476489707827568
  Output probs: [[0.    0.    0.481 0.    0.    0.    0.    0.    0.519 0.   ]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0001
  Image Loss: 0.0166
  Total Loss: 0.0174
  Image grad max: 0.09608191251754761
  Output probs: [[0.    0.    0.483 0.    0.    0.    0.    0.    0.517 0.   ]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0001
  Image Loss: 0.0166
  Total Loss: 0.0172
  Image grad max: 0.08304423838853836
  Output probs: [[0.    0.    0.498 0.    0.    0.    0.    0.    0.502 0.   ]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0167
  Image grad max: 0.011105997487902641
  Output probs: [[0.    0.    0.512 0.    0.    0.    0.    0.    0.488 0.   ]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0170
  Image grad max: 0.061706457287073135
  Output probs: [[0.    0.    0.516 0.    0.    0.    0.    0.    0.484 0.   ]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0001
  Image Loss: 0.0166
  Total Loss: 0.0172
  Image grad max: 0.08140239864587784
  Output probs: [[0.    0.    0.508 0.    0.    0.    0.    0.    0.492 0.   ]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.03943473473191261
  Output probs: [[0.    0.    0.494 0.    0.    0.    0.    0.    0.506 0.   ]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0167
  Image grad max: 0.028129711747169495
  Output probs: [[0.    0.    0.486 0.    0.    0.    0.    0.    0.514 0.   ]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0170
  Image grad max: 0.06792137026786804
  Output probs: [[0.    0.    0.489 0.    0.    0.    0.    0.    0.511 0.   ]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.05311385169625282
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0166
  Image grad max: 0.0018897939007729292
  Output probs: [[0.    0.    0.509 0.    0.    0.    0.    0.    0.491 0.   ]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.047814663499593735
  Output probs: [[0.    0.    0.511 0.    0.    0.    0.    0.    0.489 0.   ]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0168
  Image grad max: 0.053786955773830414
  Output probs: [[0.    0.    0.503 0.    0.    0.    0.    0.    0.496 0.   ]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0000
  Image Loss: 0.0166
  Total Loss: 0.0166
  Image grad max: 0.017752869054675102
  Output probs: [[0.    0.    0.494 0.    0.    0.    0.    0.    0.506 0.   ]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0166
  Image grad max: 0.028792554512619972
  Output probs: [[0.   0.   0.49 0.   0.   0.   0.   0.   0.51 0.  ]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0168
  Image grad max: 0.048402365297079086
  Output probs: [[0.    0.    0.494 0.    0.    0.    0.    0.    0.506 0.   ]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0166
  Image grad max: 0.028296492993831635
  Output probs: [[0.    0.    0.502 0.    0.    0.    0.    0.    0.498 0.   ]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0166
  Image grad max: 0.011972866021096706
  Output probs: [[0.    0.    0.507 0.    0.    0.    0.    0.    0.492 0.   ]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0167
  Image grad max: 0.03783305734395981
  Output probs: [[0.    0.    0.506 0.    0.    0.    0.    0.    0.494 0.   ]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0166
  Image grad max: 0.031550128012895584
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0165
  Image grad max: 0.001835821196436882
  Output probs: [[0.    0.    0.494 0.    0.    0.    0.    0.    0.506 0.   ]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0166
  Image grad max: 0.02832910604774952
  Output probs: [[0.    0.    0.494 0.    0.    0.    0.    0.    0.506 0.   ]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0166
  Image grad max: 0.03130489960312843
  Output probs: [[0.    0.    0.498 0.    0.    0.    0.    0.    0.502 0.   ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0165
  Image grad max: 0.008636596612632275
  Output probs: [[0.    0.    0.504 0.    0.    0.    0.    0.    0.496 0.   ]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0165
  Image grad max: 0.0180843323469162
  Output probs: [[0.    0.    0.505 0.    0.    0.    0.    0.    0.495 0.   ]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0166
  Image grad max: 0.02707013115286827
  Output probs: [[0.    0.    0.503 0.    0.    0.    0.    0.    0.497 0.   ]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0000
  Image Loss: 0.0165
  Total Loss: 0.0165
  Image grad max: 0.013095308095216751
  Output probs: [[0.    0.    0.498 0.    0.    0.    0.    0.    0.502 0.   ]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0165
  Image grad max: 0.010666854679584503
  Output probs: [[0.    0.    0.495 0.    0.    0.    0.    0.    0.505 0.   ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0165
  Image grad max: 0.02332265116274357
  Output probs: [[0.    0.    0.497 0.    0.    0.    0.    0.    0.503 0.   ]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0165
  Image grad max: 0.015343905426561832
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0165
  Image grad max: 0.004280891269445419
  Output probs: [[0.    0.    0.503 0.    0.    0.    0.    0.    0.496 0.   ]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0165
  Image grad max: 0.017806276679039
  Output probs: [[0.    0.    0.503 0.    0.    0.    0.    0.    0.497 0.   ]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0165
  Image grad max: 0.015101696364581585
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0164
  Image grad max: 0.0016589094884693623
  Output probs: [[0.    0.    0.497 0.    0.    0.    0.    0.    0.503 0.   ]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0164
  Image grad max: 0.014227887615561485
  Output probs: [[0.    0.    0.497 0.    0.    0.    0.    0.    0.503 0.   ]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0164
  Image grad max: 0.014605293050408363
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0164
  Image grad max: 0.002732339082285762
  Output probs: [[0.    0.    0.502 0.    0.    0.    0.    0.    0.498 0.   ]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0164
  Image grad max: 0.009759507142007351
  Output probs: [[0.    0.    0.502 0.    0.    0.    0.    0.    0.498 0.   ]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0000
  Image Loss: 0.0164
  Total Loss: 0.0164
  Image grad max: 0.012468875385820866
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0164
  Image grad max: 0.004061767365783453
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0164
  Image grad max: 0.0073231845162808895
  Output probs: [[0.    0.    0.498 0.    0.    0.    0.    0.    0.502 0.   ]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0164
  Image grad max: 0.011317460797727108
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0164
  Image grad max: 0.005263487808406353
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0163
  Image grad max: 0.0043440512381494045
  Output probs: [[0.    0.    0.502 0.    0.    0.    0.    0.    0.498 0.   ]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0163
  Image grad max: 0.008948749862611294
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0163
  Image grad max: 0.005149895325303078
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0163
  Image grad max: 0.0031170141883194447
  Output probs: [[0.    0.    0.498 0.    0.    0.    0.    0.    0.502 0.   ]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0163
  Image grad max: 0.007982718758285046
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0163
  Image grad max: 0.005451598204672337
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0000
  Image Loss: 0.0163
  Total Loss: 0.0163
  Image grad max: 0.0021065743640065193
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0163
  Image grad max: 0.005991917569190264
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0163
  Image grad max: 0.004658916965126991
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0163
  Image grad max: 0.0015021230792626739
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0163
  Image grad max: 0.005399714689701796
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0162
  Image grad max: 0.00471551064401865
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0162
  Image grad max: 0.001719716819934547
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0162
  Image grad max: 0.003856186755001545
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0162
  Image grad max: 0.0037384829483926296
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0162
  Image grad max: 0.0017556779785081744
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0162
  Image grad max: 0.0036355010233819485
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0162
  Image grad max: 0.003780643455684185
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0000
  Image Loss: 0.0162
  Total Loss: 0.0162
  Image grad max: 0.0015722193056717515
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0162
  Image grad max: 0.002505301032215357
  Output probs: [[0.    0.    0.501 0.    0.    0.    0.    0.    0.499 0.   ]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0162
  Image grad max: 0.0028168209828436375
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0162
  Image grad max: 0.0018250568537041545
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0161
  Image grad max: 0.0025580977089703083
  Output probs: [[0.    0.    0.499 0.    0.    0.    0.    0.    0.501 0.   ]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0161
  Image grad max: 0.002935967640951276
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0161
  Image grad max: 0.0015548785449936986
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0161
  Image grad max: 0.0022266695741564035
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0161
  Image grad max: 0.0023507927544414997
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0161
  Image grad max: 0.001801977283321321
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0000
  Image Loss: 0.0161
  Total Loss: 0.0161
  Image grad max: 0.001926705241203308
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0161
  Image grad max: 0.002232405822724104
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0161
  Image grad max: 0.0015999070601537824
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0161
  Image grad max: 0.002072329865768552
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0161
  Image grad max: 0.0021264494862407446
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0160
  Image grad max: 0.0017417463241145015
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0160
  Image grad max: 0.001605986850336194
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0160
  Image grad max: 0.0016604592092335224
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0160
  Image grad max: 0.0016554513713344932
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0160
  Image grad max: 0.0019887781236320734
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0160
  Image grad max: 0.0019914882723242044
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0000
  Image Loss: 0.0160
  Total Loss: 0.0160
  Image grad max: 0.0016897950554266572
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0160
  Image grad max: 0.0014642407186329365
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0160
  Image grad max: 0.0014471493195742369
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0160
  Image grad max: 0.0017065355787053704
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0160
  Image grad max: 0.0019291311036795378
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0159
  Image grad max: 0.0018844776786863804
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0159
  Image grad max: 0.001641654409468174
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0159
  Image grad max: 0.0014669493539258838
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0159
  Image grad max: 0.0015285658882930875
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0159
  Image grad max: 0.0017435284098610282
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0159
  Image grad max: 0.001876220339909196
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0000
  Image Loss: 0.0159
  Total Loss: 0.0159
  Image grad max: 0.0017982348799705505
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0159
  Image grad max: 0.0016100595239549875
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0159
  Image grad max: 0.0015147454105317593
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0159
  Image grad max: 0.001602953183464706
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0158
  Image grad max: 0.0017654653638601303
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0158
  Image grad max: 0.0018251703586429358
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0158
  Image grad max: 0.0017317839665338397
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0158
  Image grad max: 0.0015948530053719878
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0158
  Image grad max: 0.0015616252785548568
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0158
  Image grad max: 0.0016626701690256596
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0000
  Image Loss: 0.0158
  Total Loss: 0.0158
  Image grad max: 0.0017717014998197556
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0158
  Image grad max: 0.0017747129313647747
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0158
  Image grad max: 0.0016771391965448856
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0158
  Image grad max: 0.0015968821244314313
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0157
  Image grad max: 0.0016132981982082129
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0157
  Image grad max: 0.001700901542790234
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0157
  Image grad max: 0.0017606145702302456
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0157
  Image grad max: 0.0017258222214877605
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0157
  Image grad max: 0.0016433020355179906
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0157
  Image grad max: 0.0016071684658527374
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0000
  Image Loss: 0.0157
  Total Loss: 0.0157
  Image grad max: 0.0016528138658031821
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0157
  Image grad max: 0.0017253491096198559
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0157
  Image grad max: 0.0017414969624951482
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0157
  Image grad max: 0.0016855387948453426
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0156
  Image grad max: 0.0016273524379357696
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0156
  Image grad max: 0.0016351593658328056
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0156
  Image grad max: 0.001688776072114706
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0156
  Image grad max: 0.0017263955669477582
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0156
  Image grad max: 0.0017079233657568693
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0156
  Image grad max: 0.001655795844271779
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0000
  Image Loss: 0.0156
  Total Loss: 0.0156
  Image grad max: 0.0016363762551918626
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0156
  Image grad max: 0.001663079485297203
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0156
  Image grad max: 0.0017064663115888834
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0156
  Image grad max: 0.0017117090756073594
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0156
  Image grad max: 0.0016762391896918416
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0155
  Image grad max: 0.0016433363780379295
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0155
  Image grad max: 0.0016517816111445427
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0155
  Image grad max: 0.0016884400974959135
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0155
  Image grad max: 0.001707151415757835
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0155
  Image grad max: 0.0016909048426896334
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0000
  Image Loss: 0.0155
  Total Loss: 0.0155
  Image grad max: 0.0016573525499552488
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0155
  Image grad max: 0.0016481613274663687
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0155
  Image grad max: 0.0016723227454349399
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0155
  Image grad max: 0.001699045649729669
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0154
  Image grad max: 0.001695304992608726
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0154
  Image grad max: 0.001667190808802843
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0154
  Image grad max: 0.0016519021010026336
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0154
  Image grad max: 0.0016667646123096347
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0154
  Image grad max: 0.0016886782832443714
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0154
  Image grad max: 0.0016910280101001263
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0000
  Image Loss: 0.0154
  Total Loss: 0.0154
  Image grad max: 0.0016728491755202413
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0154
  Image grad max: 0.0016591588500887156
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0154
  Image grad max: 0.0016650399193167686
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0154
  Image grad max: 0.001679897541180253
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0153
  Image grad max: 0.0016854526475071907
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0153
  Image grad max: 0.001678180182352662
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0153
  Image grad max: 0.0016641642432659864
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0153
  Image grad max: 0.0016623383853584528
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0153
  Image grad max: 0.001673345803283155
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0153
  Image grad max: 0.0016833909321576357
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0000
  Image Loss: 0.0153
  Total Loss: 0.0153
  Image grad max: 0.0016783606261014938
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0153
  Image grad max: 0.0016669061733409762
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0153
  Image grad max: 0.0016647565644234419
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0153
  Image grad max: 0.0016690297052264214
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0152
  Image grad max: 0.0016774693503975868
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0152
  Image grad max: 0.00167788565158844
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0152
  Image grad max: 0.0016705989837646484
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0152
  Image grad max: 0.0016642791451886296
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0152
  Image grad max: 0.0016679033869877458
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0152
  Image grad max: 0.0016747369663789868
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0000
  Image Loss: 0.0152
  Total Loss: 0.0152
  Image grad max: 0.0016722617438063025
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0152
  Image grad max: 0.0016710697673261166
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0152
  Image grad max: 0.0016650671605020761
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0152
  Image grad max: 0.001665158080868423
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0151
  Image grad max: 0.0016739164711907506
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0151
  Image grad max: 0.001674011698924005
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0151
  Image grad max: 0.001665753312408924
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0151
  Image grad max: 0.0016616681823506951
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0151
  Image grad max: 0.0016617563087493181
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0151
  Image grad max: 0.0016698716208338737
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0000
  Image Loss: 0.0151
  Total Loss: 0.0151
  Image grad max: 0.001674781204201281
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0151
  Image grad max: 0.0016678033862262964
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0151
  Image grad max: 0.0016582552343606949
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0150
  Image grad max: 0.0016634792555123568
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0150
  Image grad max: 0.0016693456564098597
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0150
  Image grad max: 0.0016723264707252383
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0150
  Image grad max: 0.0016669505275785923
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0150
  Image grad max: 0.001662221155129373
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0150
  Image grad max: 0.0016613405896350741
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0000
  Image Loss: 0.0150
  Total Loss: 0.0150
  Image grad max: 0.001668168930336833
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0150
  Image grad max: 0.0016685726586729288
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0150
  Image grad max: 0.0016660843975842
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0150
  Image grad max: 0.001663921750150621
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0149
  Image grad max: 0.0016604680567979813
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0149
  Image grad max: 0.0016608693404123187
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0149
  Image grad max: 0.0016676965169608593
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0149
  Image grad max: 0.001668423879891634
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0149
  Image grad max: 0.001664325245656073
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0149
  Image grad max: 0.0016589404549449682
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0000
  Image Loss: 0.0149
  Total Loss: 0.0149
  Image grad max: 0.0016596607165411115
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0149
  Image grad max: 0.0016671346966177225
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0149
  Image grad max: 0.0016665688017383218
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0149
  Image grad max: 0.001662466675043106
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0148
  Image grad max: 0.001659649657085538
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0148
  Image grad max: 0.0016606890130788088
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0148
  Image grad max: 0.001663018949329853
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0148
  Image grad max: 0.0016650217585265636
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0148
  Image grad max: 0.0016628503799438477
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0000
  Image Loss: 0.0148
  Total Loss: 0.0148
  Image grad max: 0.0016571367159485817
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0148
  Image grad max: 0.001656566048040986
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0148
  Image grad max: 0.001655678846873343
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0148
  Image grad max: 0.0016621811082586646
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0147
  Image grad max: 0.001652284525334835
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0147
  Image grad max: 0.0016523601952940226
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0147
  Image grad max: 0.0016640066169202328
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0147
  Image grad max: 0.0016643982380628586
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0147
  Image grad max: 0.0016599653754383326
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0147
  Image grad max: 0.0016503859078511596
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0000
  Image Loss: 0.0147
  Total Loss: 0.0147
  Image grad max: 0.0016530320281162858
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0147
  Image grad max: 0.0016633920604363084
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0147
  Image grad max: 0.0016641030088067055
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0147
  Image grad max: 0.001654198276810348
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0146
  Image grad max: 0.0016481525963172317
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0146
  Image grad max: 0.001649188227020204
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0146
  Image grad max: 0.0016627651639282703
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0146
  Image grad max: 0.0016621870454400778
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0146
  Image grad max: 0.0016564656980335712
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0000
  Image Loss: 0.0146
  Total Loss: 0.0146
  Image grad max: 0.0016481638886034489
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0000
  Image Loss: 0.0145
  Total Loss: 0.0146
  Image grad max: 0.0016508010448887944
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0000
  Image Loss: 0.0145
  Total Loss: 0.0146
  Image grad max: 0.0016585866687819362
  Output probs: [[0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0000
  Image Loss: 0.0145
  Total Loss: 0.0146
  Image grad max: 0.0016621890245005488
Visualization saved to adversarial_figures/adversarial_training.png
Visualization saved to adversarial_figures/adversarial_testing.png
