Epoch [1/30], Batch [0/6000], Loss: 2.3913
Epoch [1/30], Batch [100/6000], Loss: 1.4770
Epoch [1/30], Batch [200/6000], Loss: 0.7408
Epoch [1/30], Batch [300/6000], Loss: 0.4520
Epoch [1/30], Batch [400/6000], Loss: 0.3529
Epoch [1/30], Batch [500/6000], Loss: 0.6477
Epoch [1/30], Batch [600/6000], Loss: 0.3775
Epoch [1/30], Batch [700/6000], Loss: 0.5810
Epoch [1/30], Batch [800/6000], Loss: 0.2595
Epoch [1/30], Batch [900/6000], Loss: 0.0867
Epoch [1/30], Batch [1000/6000], Loss: 0.7106
Epoch [1/30], Batch [1100/6000], Loss: 0.5200
Epoch [1/30], Batch [1200/6000], Loss: 0.4906
Epoch [1/30], Batch [1300/6000], Loss: 0.3648
Epoch [1/30], Batch [1400/6000], Loss: 0.1443
Epoch [1/30], Batch [1500/6000], Loss: 0.6039
Epoch [1/30], Batch [1600/6000], Loss: 0.2608
Epoch [1/30], Batch [1700/6000], Loss: 0.3945
Epoch [1/30], Batch [1800/6000], Loss: 0.1759
Epoch [1/30], Batch [1900/6000], Loss: 0.5003
Epoch [1/30], Batch [2000/6000], Loss: 0.3165
Epoch [1/30], Batch [2100/6000], Loss: 0.1633
Epoch [1/30], Batch [2200/6000], Loss: 0.9783
Epoch [1/30], Batch [2300/6000], Loss: 0.1152
Epoch [1/30], Batch [2400/6000], Loss: 0.4963
Epoch [1/30], Batch [2500/6000], Loss: 0.3182
Epoch [1/30], Batch [2600/6000], Loss: 0.2738
Epoch [1/30], Batch [2700/6000], Loss: 0.6739
Epoch [1/30], Batch [2800/6000], Loss: 0.1887
Epoch [1/30], Batch [2900/6000], Loss: 0.2728
Epoch [1/30], Batch [3000/6000], Loss: 0.0621
Epoch [1/30], Batch [3100/6000], Loss: 0.0837
Epoch [1/30], Batch [3200/6000], Loss: 0.1217
Epoch [1/30], Batch [3300/6000], Loss: 0.2534
Epoch [1/30], Batch [3400/6000], Loss: 0.2071
Epoch [1/30], Batch [3500/6000], Loss: 0.1347
Epoch [1/30], Batch [3600/6000], Loss: 0.7562
Epoch [1/30], Batch [3700/6000], Loss: 0.2184
Epoch [1/30], Batch [3800/6000], Loss: 0.3021
Epoch [1/30], Batch [3900/6000], Loss: 0.3429
Epoch [1/30], Batch [4000/6000], Loss: 0.1855
Epoch [1/30], Batch [4100/6000], Loss: 0.0905
Epoch [1/30], Batch [4200/6000], Loss: 0.2251
Epoch [1/30], Batch [4300/6000], Loss: 0.3206
Epoch [1/30], Batch [4400/6000], Loss: 0.1145
Epoch [1/30], Batch [4500/6000], Loss: 0.2243
Epoch [1/30], Batch [4600/6000], Loss: 0.2095
Epoch [1/30], Batch [4700/6000], Loss: 0.0983
Epoch [1/30], Batch [4800/6000], Loss: 0.3222
Epoch [1/30], Batch [4900/6000], Loss: 0.4787
Epoch [1/30], Batch [5000/6000], Loss: 0.3725
Epoch [1/30], Batch [5100/6000], Loss: 0.2665
Epoch [1/30], Batch [5200/6000], Loss: 1.2687
Epoch [1/30], Batch [5300/6000], Loss: 0.3273
Epoch [1/30], Batch [5400/6000], Loss: 0.4851
Epoch [1/30], Batch [5500/6000], Loss: 0.6453
Epoch [1/30], Batch [5600/6000], Loss: 0.1013
Epoch [1/30], Batch [5700/6000], Loss: 0.1447
Epoch [1/30], Batch [5800/6000], Loss: 0.0927
Epoch [1/30], Batch [5900/6000], Loss: 0.3489
Epoch [1/30], Loss: 0.3946
Visualization saved to figures/visualization_0.png
Epoch [2/30], Batch [0/6000], Loss: 0.8407
Epoch [2/30], Batch [100/6000], Loss: 0.1053
Epoch [2/30], Batch [200/6000], Loss: 0.0618
Epoch [2/30], Batch [300/6000], Loss: 0.0595
Epoch [2/30], Batch [400/6000], Loss: 0.0523
Epoch [2/30], Batch [500/6000], Loss: 0.0666
Epoch [2/30], Batch [600/6000], Loss: 0.0666
Epoch [2/30], Batch [700/6000], Loss: 0.3266
Epoch [2/30], Batch [800/6000], Loss: 0.1135
Epoch [2/30], Batch [900/6000], Loss: 0.3937
Epoch [2/30], Batch [1000/6000], Loss: 0.1727
Epoch [2/30], Batch [1100/6000], Loss: 0.0765
Epoch [2/30], Batch [1200/6000], Loss: 0.3018
Epoch [2/30], Batch [1300/6000], Loss: 0.2854
Epoch [2/30], Batch [1400/6000], Loss: 0.0513
Epoch [2/30], Batch [1500/6000], Loss: 0.0621
Epoch [2/30], Batch [1600/6000], Loss: 0.2256
Epoch [2/30], Batch [1700/6000], Loss: 0.0786
Epoch [2/30], Batch [1800/6000], Loss: 0.1785
Epoch [2/30], Batch [1900/6000], Loss: 0.1868
Epoch [2/30], Batch [2000/6000], Loss: 0.1399
Epoch [2/30], Batch [2100/6000], Loss: 0.0624
Epoch [2/30], Batch [2200/6000], Loss: 0.1058
Epoch [2/30], Batch [2300/6000], Loss: 0.2277
Epoch [2/30], Batch [2400/6000], Loss: 0.3470
Epoch [2/30], Batch [2500/6000], Loss: 0.4936
Epoch [2/30], Batch [2600/6000], Loss: 0.0390
Epoch [2/30], Batch [2700/6000], Loss: 0.6302
Epoch [2/30], Batch [2800/6000], Loss: 0.1235
Epoch [2/30], Batch [2900/6000], Loss: 0.8619
Epoch [2/30], Batch [3000/6000], Loss: 0.3492
Epoch [2/30], Batch [3100/6000], Loss: 0.0518
Epoch [2/30], Batch [3200/6000], Loss: 0.1600
Epoch [2/30], Batch [3300/6000], Loss: 0.1944
Epoch [2/30], Batch [3400/6000], Loss: 0.1907
Epoch [2/30], Batch [3500/6000], Loss: 0.1612
Epoch [2/30], Batch [3600/6000], Loss: 0.2533
Epoch [2/30], Batch [3700/6000], Loss: 0.3279
Epoch [2/30], Batch [3800/6000], Loss: 0.4506
Epoch [2/30], Batch [3900/6000], Loss: 0.0506
Epoch [2/30], Batch [4000/6000], Loss: 0.2169
Epoch [2/30], Batch [4100/6000], Loss: 0.3210
Epoch [2/30], Batch [4200/6000], Loss: 0.0763
Epoch [2/30], Batch [4300/6000], Loss: 0.0882
Epoch [2/30], Batch [4400/6000], Loss: 0.0414
Epoch [2/30], Batch [4500/6000], Loss: 0.6554
Epoch [2/30], Batch [4600/6000], Loss: 0.6927
Epoch [2/30], Batch [4700/6000], Loss: 0.0336
Epoch [2/30], Batch [4800/6000], Loss: 0.1064
Epoch [2/30], Batch [4900/6000], Loss: 0.0831
Epoch [2/30], Batch [5000/6000], Loss: 0.0329
Epoch [2/30], Batch [5100/6000], Loss: 0.1360
Epoch [2/30], Batch [5200/6000], Loss: 0.0340
Epoch [2/30], Batch [5300/6000], Loss: 0.0665
Epoch [2/30], Batch [5400/6000], Loss: 0.0865
Epoch [2/30], Batch [5500/6000], Loss: 0.0682
Epoch [2/30], Batch [5600/6000], Loss: 0.2772
Epoch [2/30], Batch [5700/6000], Loss: 0.0652
Epoch [2/30], Batch [5800/6000], Loss: 0.0840
Epoch [2/30], Batch [5900/6000], Loss: 0.4266
Epoch [2/30], Loss: 0.2216
Visualization saved to figures/visualization_0.png
Epoch [3/30], Batch [0/6000], Loss: 0.1219
Epoch [3/30], Batch [100/6000], Loss: 0.0532
Epoch [3/30], Batch [200/6000], Loss: 0.4251
Epoch [3/30], Batch [300/6000], Loss: 0.3034
Epoch [3/30], Batch [400/6000], Loss: 0.6389
Epoch [3/30], Batch [500/6000], Loss: 0.5469
Epoch [3/30], Batch [600/6000], Loss: 0.1724
Epoch [3/30], Batch [700/6000], Loss: 0.0827
Epoch [3/30], Batch [800/6000], Loss: 0.0533
Epoch [3/30], Batch [900/6000], Loss: 0.0536
Epoch [3/30], Batch [1000/6000], Loss: 1.1662
Epoch [3/30], Batch [1100/6000], Loss: 0.0350
Epoch [3/30], Batch [1200/6000], Loss: 0.0436
Epoch [3/30], Batch [1300/6000], Loss: 0.0597
Epoch [3/30], Batch [1400/6000], Loss: 0.0986
Epoch [3/30], Batch [1500/6000], Loss: 0.0770
Epoch [3/30], Batch [1600/6000], Loss: 0.2089
Epoch [3/30], Batch [1700/6000], Loss: 0.0679
Epoch [3/30], Batch [1800/6000], Loss: 0.0964
Epoch [3/30], Batch [1900/6000], Loss: 0.1211
Epoch [3/30], Batch [2000/6000], Loss: 0.2099
Epoch [3/30], Batch [2100/6000], Loss: 0.0335
Epoch [3/30], Batch [2200/6000], Loss: 0.3836
Epoch [3/30], Batch [2300/6000], Loss: 0.0415
Epoch [3/30], Batch [2400/6000], Loss: 0.2701
Epoch [3/30], Batch [2500/6000], Loss: 0.1026
Epoch [3/30], Batch [2600/6000], Loss: 0.0778
Epoch [3/30], Batch [2700/6000], Loss: 0.2610
Epoch [3/30], Batch [2800/6000], Loss: 0.0363
Epoch [3/30], Batch [2900/6000], Loss: 0.4101
Epoch [3/30], Batch [3000/6000], Loss: 0.0541
Epoch [3/30], Batch [3100/6000], Loss: 0.0544
Epoch [3/30], Batch [3200/6000], Loss: 0.0583
Epoch [3/30], Batch [3300/6000], Loss: 0.0904
Epoch [3/30], Batch [3400/6000], Loss: 0.3197
Epoch [3/30], Batch [3500/6000], Loss: 0.0517
Epoch [3/30], Batch [3600/6000], Loss: 0.0330
Epoch [3/30], Batch [3700/6000], Loss: 0.0362
Epoch [3/30], Batch [3800/6000], Loss: 0.3737
Epoch [3/30], Batch [3900/6000], Loss: 0.1516
Epoch [3/30], Batch [4000/6000], Loss: 0.0281
Epoch [3/30], Batch [4100/6000], Loss: 0.0658
Epoch [3/30], Batch [4200/6000], Loss: 0.6484
Epoch [3/30], Batch [4300/6000], Loss: 0.1168
Epoch [3/30], Batch [4400/6000], Loss: 0.0651
Epoch [3/30], Batch [4500/6000], Loss: 0.0871
Epoch [3/30], Batch [4600/6000], Loss: 0.0676
Epoch [3/30], Batch [4700/6000], Loss: 0.1016
Epoch [3/30], Batch [4800/6000], Loss: 0.1863
Epoch [3/30], Batch [4900/6000], Loss: 0.0475
Epoch [3/30], Batch [5000/6000], Loss: 0.0919
Epoch [3/30], Batch [5100/6000], Loss: 0.4929
Epoch [3/30], Batch [5200/6000], Loss: 0.3912
Epoch [3/30], Batch [5300/6000], Loss: 0.1816
Epoch [3/30], Batch [5400/6000], Loss: 0.0346
Epoch [3/30], Batch [5500/6000], Loss: 0.0304
Epoch [3/30], Batch [5600/6000], Loss: 0.3322
Epoch [3/30], Batch [5700/6000], Loss: 0.2376
Epoch [3/30], Batch [5800/6000], Loss: 0.0371
Epoch [3/30], Batch [5900/6000], Loss: 0.1070
Epoch [3/30], Loss: 0.1692
Visualization saved to figures/visualization_0.png
Epoch [4/30], Batch [0/6000], Loss: 0.0286
Epoch [4/30], Batch [100/6000], Loss: 0.0320
Epoch [4/30], Batch [200/6000], Loss: 0.0837
Epoch [4/30], Batch [300/6000], Loss: 0.0269
Epoch [4/30], Batch [400/6000], Loss: 0.0786
Epoch [4/30], Batch [500/6000], Loss: 0.2556
Epoch [4/30], Batch [600/6000], Loss: 0.0458
Epoch [4/30], Batch [700/6000], Loss: 0.1204
Epoch [4/30], Batch [800/6000], Loss: 0.0434
Epoch [4/30], Batch [900/6000], Loss: 0.1766
Epoch [4/30], Batch [1000/6000], Loss: 0.1807
Epoch [4/30], Batch [1100/6000], Loss: 0.0814
Epoch [4/30], Batch [1200/6000], Loss: 0.0407
Epoch [4/30], Batch [1300/6000], Loss: 0.0456
Epoch [4/30], Batch [1400/6000], Loss: 0.0426
Epoch [4/30], Batch [1500/6000], Loss: 0.0854
Epoch [4/30], Batch [1600/6000], Loss: 0.0399
Epoch [4/30], Batch [1700/6000], Loss: 0.0376
Epoch [4/30], Batch [1800/6000], Loss: 0.0331
Epoch [4/30], Batch [1900/6000], Loss: 0.0522
Epoch [4/30], Batch [2000/6000], Loss: 0.0383
Epoch [4/30], Batch [2100/6000], Loss: 0.0810
Epoch [4/30], Batch [2200/6000], Loss: 0.1062
Epoch [4/30], Batch [2300/6000], Loss: 0.0483
Epoch [4/30], Batch [2400/6000], Loss: 0.0385
Epoch [4/30], Batch [2500/6000], Loss: 0.2793
Epoch [4/30], Batch [2600/6000], Loss: 0.0387
Epoch [4/30], Batch [2700/6000], Loss: 0.0597
Epoch [4/30], Batch [2800/6000], Loss: 0.1068
Epoch [4/30], Batch [2900/6000], Loss: 0.1156
Epoch [4/30], Batch [3000/6000], Loss: 0.1023
Epoch [4/30], Batch [3100/6000], Loss: 0.0417
Epoch [4/30], Batch [3200/6000], Loss: 0.0461
Epoch [4/30], Batch [3300/6000], Loss: 0.0694
Epoch [4/30], Batch [3400/6000], Loss: 0.0409
Epoch [4/30], Batch [3500/6000], Loss: 0.0323
Epoch [4/30], Batch [3600/6000], Loss: 0.0391
Epoch [4/30], Batch [3700/6000], Loss: 0.0605
Epoch [4/30], Batch [3800/6000], Loss: 0.8802
Epoch [4/30], Batch [3900/6000], Loss: 0.0422
Epoch [4/30], Batch [4000/6000], Loss: 0.0376
Epoch [4/30], Batch [4100/6000], Loss: 0.0368
Epoch [4/30], Batch [4200/6000], Loss: 0.2454
Epoch [4/30], Batch [4300/6000], Loss: 0.0554
Epoch [4/30], Batch [4400/6000], Loss: 0.0403
Epoch [4/30], Batch [4500/6000], Loss: 0.1411
Epoch [4/30], Batch [4600/6000], Loss: 0.0395
Epoch [4/30], Batch [4700/6000], Loss: 0.3423
Epoch [4/30], Batch [4800/6000], Loss: 0.2292
Epoch [4/30], Batch [4900/6000], Loss: 0.1763
Epoch [4/30], Batch [5000/6000], Loss: 0.0373
Epoch [4/30], Batch [5100/6000], Loss: 0.1015
Epoch [4/30], Batch [5200/6000], Loss: 0.3759
Epoch [4/30], Batch [5300/6000], Loss: 0.0338
Epoch [4/30], Batch [5400/6000], Loss: 0.1344
Epoch [4/30], Batch [5500/6000], Loss: 0.3557
Epoch [4/30], Batch [5600/6000], Loss: 0.3284
Epoch [4/30], Batch [5700/6000], Loss: 0.0570
Epoch [4/30], Batch [5800/6000], Loss: 0.0539
Epoch [4/30], Batch [5900/6000], Loss: 0.3554
Epoch [4/30], Loss: 0.1399
Visualization saved to figures/visualization_0.png
Epoch [5/30], Batch [0/6000], Loss: 0.0483
Epoch [5/30], Batch [100/6000], Loss: 0.0309
Epoch [5/30], Batch [200/6000], Loss: 0.1414
Epoch [5/30], Batch [300/6000], Loss: 0.9654
Epoch [5/30], Batch [400/6000], Loss: 0.1100
Epoch [5/30], Batch [500/6000], Loss: 0.0262
Epoch [5/30], Batch [600/6000], Loss: 0.6544
Epoch [5/30], Batch [700/6000], Loss: 0.0321
Epoch [5/30], Batch [800/6000], Loss: 0.0266
Epoch [5/30], Batch [900/6000], Loss: 0.0428
Epoch [5/30], Batch [1000/6000], Loss: 0.0330
Epoch [5/30], Batch [1100/6000], Loss: 0.0350
Epoch [5/30], Batch [1200/6000], Loss: 0.0602
Epoch [5/30], Batch [1300/6000], Loss: 0.0577
Epoch [5/30], Batch [1400/6000], Loss: 0.0370
Epoch [5/30], Batch [1500/6000], Loss: 0.0824
Epoch [5/30], Batch [1600/6000], Loss: 0.0392
Epoch [5/30], Batch [1700/6000], Loss: 0.0466
Epoch [5/30], Batch [1800/6000], Loss: 0.0327
Epoch [5/30], Batch [1900/6000], Loss: 0.4410
Epoch [5/30], Batch [2000/6000], Loss: 0.0310
Epoch [5/30], Batch [2100/6000], Loss: 0.1516
Epoch [5/30], Batch [2200/6000], Loss: 0.0307
Epoch [5/30], Batch [2300/6000], Loss: 0.2668
Epoch [5/30], Batch [2400/6000], Loss: 0.0493
Epoch [5/30], Batch [2500/6000], Loss: 0.2122
Epoch [5/30], Batch [2600/6000], Loss: 0.0489
Epoch [5/30], Batch [2700/6000], Loss: 0.6146
Epoch [5/30], Batch [2800/6000], Loss: 0.1063
Epoch [5/30], Batch [2900/6000], Loss: 0.2015
Epoch [5/30], Batch [3000/6000], Loss: 0.0371
Epoch [5/30], Batch [3100/6000], Loss: 0.0428
Epoch [5/30], Batch [3200/6000], Loss: 0.0636
Epoch [5/30], Batch [3300/6000], Loss: 0.0670
Epoch [5/30], Batch [3400/6000], Loss: 0.0308
Epoch [5/30], Batch [3500/6000], Loss: 0.1141
Epoch [5/30], Batch [3600/6000], Loss: 0.1085
Epoch [5/30], Batch [3700/6000], Loss: 0.0293
Epoch [5/30], Batch [3800/6000], Loss: 0.2260
Epoch [5/30], Batch [3900/6000], Loss: 0.0377
Epoch [5/30], Batch [4000/6000], Loss: 0.0541
Epoch [5/30], Batch [4100/6000], Loss: 0.0559
Epoch [5/30], Batch [4200/6000], Loss: 0.0657
Epoch [5/30], Batch [4300/6000], Loss: 0.0306
Epoch [5/30], Batch [4400/6000], Loss: 0.0272
Epoch [5/30], Batch [4500/6000], Loss: 0.4342
Epoch [5/30], Batch [4600/6000], Loss: 0.0475
Epoch [5/30], Batch [4700/6000], Loss: 0.0446
Epoch [5/30], Batch [4800/6000], Loss: 0.0190
Epoch [5/30], Batch [4900/6000], Loss: 0.0301
Epoch [5/30], Batch [5000/6000], Loss: 0.0763
Epoch [5/30], Batch [5100/6000], Loss: 0.0314
Epoch [5/30], Batch [5200/6000], Loss: 0.0545
Epoch [5/30], Batch [5300/6000], Loss: 0.4292
Epoch [5/30], Batch [5400/6000], Loss: 0.2010
Epoch [5/30], Batch [5500/6000], Loss: 0.0852
Epoch [5/30], Batch [5600/6000], Loss: 0.1668
Epoch [5/30], Batch [5700/6000], Loss: 0.0278
Epoch [5/30], Batch [5800/6000], Loss: 0.0600
Epoch [5/30], Batch [5900/6000], Loss: 0.1465
Epoch [5/30], Loss: 0.1181
Visualization saved to figures/visualization_0.png
Epoch [6/30], Batch [0/6000], Loss: 0.0415
Epoch [6/30], Batch [100/6000], Loss: 0.0391
Epoch [6/30], Batch [200/6000], Loss: 0.0301
Epoch [6/30], Batch [300/6000], Loss: 0.0328
Epoch [6/30], Batch [400/6000], Loss: 0.2538
Epoch [6/30], Batch [500/6000], Loss: 0.0384
Epoch [6/30], Batch [600/6000], Loss: 0.0287
Epoch [6/30], Batch [700/6000], Loss: 0.1521
Epoch [6/30], Batch [800/6000], Loss: 0.1632
Epoch [6/30], Batch [900/6000], Loss: 0.0221
Epoch [6/30], Batch [1000/6000], Loss: 0.0493
Epoch [6/30], Batch [1100/6000], Loss: 0.5235
Epoch [6/30], Batch [1200/6000], Loss: 0.3121
Epoch [6/30], Batch [1300/6000], Loss: 0.0608
Epoch [6/30], Batch [1400/6000], Loss: 0.0269
Epoch [6/30], Batch [1500/6000], Loss: 0.3713
Epoch [6/30], Batch [1600/6000], Loss: 0.0324
Epoch [6/30], Batch [1700/6000], Loss: 0.0320
Epoch [6/30], Batch [1800/6000], Loss: 0.0275
Epoch [6/30], Batch [1900/6000], Loss: 0.0325
Epoch [6/30], Batch [2000/6000], Loss: 0.0525
Epoch [6/30], Batch [2100/6000], Loss: 0.0239
Epoch [6/30], Batch [2200/6000], Loss: 0.0903
Epoch [6/30], Batch [2300/6000], Loss: 0.0437
Epoch [6/30], Batch [2400/6000], Loss: 0.0817
Epoch [6/30], Batch [2500/6000], Loss: 0.0819
Epoch [6/30], Batch [2600/6000], Loss: 0.0689
Epoch [6/30], Batch [2700/6000], Loss: 0.0469
Epoch [6/30], Batch [2800/6000], Loss: 0.0265
Epoch [6/30], Batch [2900/6000], Loss: 0.2148
Epoch [6/30], Batch [3000/6000], Loss: 0.0382
Epoch [6/30], Batch [3100/6000], Loss: 0.3343
Epoch [6/30], Batch [3200/6000], Loss: 0.0301
Epoch [6/30], Batch [3300/6000], Loss: 0.4475
Epoch [6/30], Batch [3400/6000], Loss: 0.0686
Epoch [6/30], Batch [3500/6000], Loss: 0.0913
Epoch [6/30], Batch [3600/6000], Loss: 0.1675
Epoch [6/30], Batch [3700/6000], Loss: 0.0242
Epoch [6/30], Batch [3800/6000], Loss: 0.2061
Epoch [6/30], Batch [3900/6000], Loss: 0.0392
Epoch [6/30], Batch [4000/6000], Loss: 0.0286
Epoch [6/30], Batch [4100/6000], Loss: 0.0523
Epoch [6/30], Batch [4200/6000], Loss: 0.0702
Epoch [6/30], Batch [4300/6000], Loss: 0.0408
Epoch [6/30], Batch [4400/6000], Loss: 0.0872
Epoch [6/30], Batch [4500/6000], Loss: 0.3553
Epoch [6/30], Batch [4600/6000], Loss: 0.0432
Epoch [6/30], Batch [4700/6000], Loss: 0.1360
Epoch [6/30], Batch [4800/6000], Loss: 0.0342
Epoch [6/30], Batch [4900/6000], Loss: 0.0250
Epoch [6/30], Batch [5000/6000], Loss: 0.6468
Epoch [6/30], Batch [5100/6000], Loss: 0.0550
Epoch [6/30], Batch [5200/6000], Loss: 0.2581
Epoch [6/30], Batch [5300/6000], Loss: 0.0263
Epoch [6/30], Batch [5400/6000], Loss: 0.4893
Epoch [6/30], Batch [5500/6000], Loss: 0.0266
Epoch [6/30], Batch [5600/6000], Loss: 0.0310
Epoch [6/30], Batch [5700/6000], Loss: 0.0300
Epoch [6/30], Batch [5800/6000], Loss: 0.2017
Epoch [6/30], Batch [5900/6000], Loss: 0.0251
Epoch [6/30], Loss: 0.1035
Visualization saved to figures/visualization_0.png
Epoch [7/30], Batch [0/6000], Loss: 0.0404
Epoch [7/30], Batch [100/6000], Loss: 0.0346
Epoch [7/30], Batch [200/6000], Loss: 0.1398
Epoch [7/30], Batch [300/6000], Loss: 0.0286
Epoch [7/30], Batch [400/6000], Loss: 0.0223
Epoch [7/30], Batch [500/6000], Loss: 0.0385
Epoch [7/30], Batch [600/6000], Loss: 0.0277
Epoch [7/30], Batch [700/6000], Loss: 0.0232
Epoch [7/30], Batch [800/6000], Loss: 0.0446
Epoch [7/30], Batch [900/6000], Loss: 0.0247
Epoch [7/30], Batch [1000/6000], Loss: 0.1890
Epoch [7/30], Batch [1100/6000], Loss: 0.0239
Epoch [7/30], Batch [1200/6000], Loss: 0.0829
Epoch [7/30], Batch [1300/6000], Loss: 0.2247
Epoch [7/30], Batch [1400/6000], Loss: 0.0324
Epoch [7/30], Batch [1500/6000], Loss: 0.0354
Epoch [7/30], Batch [1600/6000], Loss: 0.0363
Epoch [7/30], Batch [1700/6000], Loss: 0.2644
Epoch [7/30], Batch [1800/6000], Loss: 0.0357
Epoch [7/30], Batch [1900/6000], Loss: 0.0397
Epoch [7/30], Batch [2000/6000], Loss: 0.0272
Epoch [7/30], Batch [2100/6000], Loss: 0.0342
Epoch [7/30], Batch [2200/6000], Loss: 0.0273
Epoch [7/30], Batch [2300/6000], Loss: 0.0457
Epoch [7/30], Batch [2400/6000], Loss: 0.0308
Epoch [7/30], Batch [2500/6000], Loss: 0.6353
Epoch [7/30], Batch [2600/6000], Loss: 0.2236
Epoch [7/30], Batch [2700/6000], Loss: 0.0770
Epoch [7/30], Batch [2800/6000], Loss: 0.0430
Epoch [7/30], Batch [2900/6000], Loss: 0.2118
Epoch [7/30], Batch [3000/6000], Loss: 0.0584
Epoch [7/30], Batch [3100/6000], Loss: 0.0263
Epoch [7/30], Batch [3200/6000], Loss: 0.0340
Epoch [7/30], Batch [3300/6000], Loss: 0.0296
Epoch [7/30], Batch [3400/6000], Loss: 0.0235
Epoch [7/30], Batch [3500/6000], Loss: 0.0283
Epoch [7/30], Batch [3600/6000], Loss: 0.0248
Epoch [7/30], Batch [3700/6000], Loss: 0.2426
Epoch [7/30], Batch [3800/6000], Loss: 0.0332
Epoch [7/30], Batch [3900/6000], Loss: 0.0277
Epoch [7/30], Batch [4000/6000], Loss: 0.0314
Epoch [7/30], Batch [4100/6000], Loss: 0.0220
Epoch [7/30], Batch [4200/6000], Loss: 0.0226
Epoch [7/30], Batch [4300/6000], Loss: 0.0408
Epoch [7/30], Batch [4400/6000], Loss: 0.0286
Epoch [7/30], Batch [4500/6000], Loss: 0.0346
Epoch [7/30], Batch [4600/6000], Loss: 0.0334
Epoch [7/30], Batch [4700/6000], Loss: 0.0306
Epoch [7/30], Batch [4800/6000], Loss: 0.0304
Epoch [7/30], Batch [4900/6000], Loss: 0.1467
Epoch [7/30], Batch [5000/6000], Loss: 0.0354
Epoch [7/30], Batch [5100/6000], Loss: 0.0373
Epoch [7/30], Batch [5200/6000], Loss: 0.0397
Epoch [7/30], Batch [5300/6000], Loss: 0.0611
Epoch [7/30], Batch [5400/6000], Loss: 0.0283
Epoch [7/30], Batch [5500/6000], Loss: 0.0463
Epoch [7/30], Batch [5600/6000], Loss: 0.2521
Epoch [7/30], Batch [5700/6000], Loss: 0.0928
Epoch [7/30], Batch [5800/6000], Loss: 0.0324
Epoch [7/30], Batch [5900/6000], Loss: 0.8031
Epoch [7/30], Loss: 0.0923
Visualization saved to figures/visualization_0.png
Epoch [8/30], Batch [0/6000], Loss: 0.0478
Epoch [8/30], Batch [100/6000], Loss: 0.0779
Epoch [8/30], Batch [200/6000], Loss: 0.7853
Epoch [8/30], Batch [300/6000], Loss: 0.0315
Epoch [8/30], Batch [400/6000], Loss: 0.0271
Epoch [8/30], Batch [500/6000], Loss: 0.0343
Epoch [8/30], Batch [600/6000], Loss: 0.0276
Epoch [8/30], Batch [700/6000], Loss: 0.0407
Epoch [8/30], Batch [800/6000], Loss: 0.0243
Epoch [8/30], Batch [900/6000], Loss: 0.0260
Epoch [8/30], Batch [1000/6000], Loss: 0.2910
Epoch [8/30], Batch [1100/6000], Loss: 0.0268
Epoch [8/30], Batch [1200/6000], Loss: 0.0222
Epoch [8/30], Batch [1300/6000], Loss: 0.1213
Epoch [8/30], Batch [1400/6000], Loss: 0.0381
Epoch [8/30], Batch [1500/6000], Loss: 0.0594
Epoch [8/30], Batch [1600/6000], Loss: 0.0270
Epoch [8/30], Batch [1700/6000], Loss: 0.0261
Epoch [8/30], Batch [1800/6000], Loss: 0.0276
Epoch [8/30], Batch [1900/6000], Loss: 0.4405
Epoch [8/30], Batch [2000/6000], Loss: 0.0761
Epoch [8/30], Batch [2100/6000], Loss: 0.0410
Epoch [8/30], Batch [2200/6000], Loss: 0.0227
Epoch [8/30], Batch [2300/6000], Loss: 0.0265
Epoch [8/30], Batch [2400/6000], Loss: 0.0189
Epoch [8/30], Batch [2500/6000], Loss: 0.0537
Epoch [8/30], Batch [2600/6000], Loss: 0.0257
Epoch [8/30], Batch [2700/6000], Loss: 0.0383
Epoch [8/30], Batch [2800/6000], Loss: 0.0214
Epoch [8/30], Batch [2900/6000], Loss: 0.0295
Epoch [8/30], Batch [3000/6000], Loss: 0.1444
Epoch [8/30], Batch [3100/6000], Loss: 0.0282
Epoch [8/30], Batch [3200/6000], Loss: 0.0504
Epoch [8/30], Batch [3300/6000], Loss: 0.2552
Epoch [8/30], Batch [3400/6000], Loss: 0.0323
Epoch [8/30], Batch [3500/6000], Loss: 0.0248
Epoch [8/30], Batch [3600/6000], Loss: 0.1190
Epoch [8/30], Batch [3700/6000], Loss: 0.0262
Epoch [8/30], Batch [3800/6000], Loss: 0.1476
Epoch [8/30], Batch [3900/6000], Loss: 0.0412
Epoch [8/30], Batch [4000/6000], Loss: 0.2783
Epoch [8/30], Batch [4100/6000], Loss: 0.0940
Epoch [8/30], Batch [4200/6000], Loss: 0.0245
Epoch [8/30], Batch [4300/6000], Loss: 0.0312
Epoch [8/30], Batch [4400/6000], Loss: 0.0692
Epoch [8/30], Batch [4500/6000], Loss: 0.0284
Epoch [8/30], Batch [4600/6000], Loss: 0.0267
Epoch [8/30], Batch [4700/6000], Loss: 0.0365
Epoch [8/30], Batch [4800/6000], Loss: 0.2348
Epoch [8/30], Batch [4900/6000], Loss: 0.0318
Epoch [8/30], Batch [5000/6000], Loss: 0.0215
Epoch [8/30], Batch [5100/6000], Loss: 0.0350
Epoch [8/30], Batch [5200/6000], Loss: 0.0284
Epoch [8/30], Batch [5300/6000], Loss: 0.0240
Epoch [8/30], Batch [5400/6000], Loss: 0.0367
Epoch [8/30], Batch [5500/6000], Loss: 0.0280
Epoch [8/30], Batch [5600/6000], Loss: 0.0406
Epoch [8/30], Batch [5700/6000], Loss: 0.0337
Epoch [8/30], Batch [5800/6000], Loss: 0.0723
Epoch [8/30], Batch [5900/6000], Loss: 0.0648
Epoch [8/30], Loss: 0.0828
Visualization saved to figures/visualization_0.png
Epoch [9/30], Batch [0/6000], Loss: 0.0411
Epoch [9/30], Batch [100/6000], Loss: 0.0232
Epoch [9/30], Batch [200/6000], Loss: 0.0237
Epoch [9/30], Batch [300/6000], Loss: 0.1004
Epoch [9/30], Batch [400/6000], Loss: 0.0311
Epoch [9/30], Batch [500/6000], Loss: 0.0390
Epoch [9/30], Batch [600/6000], Loss: 0.0219
Epoch [9/30], Batch [700/6000], Loss: 0.0187
Epoch [9/30], Batch [800/6000], Loss: 0.2922
Epoch [9/30], Batch [900/6000], Loss: 0.0372
Epoch [9/30], Batch [1000/6000], Loss: 0.1180
Epoch [9/30], Batch [1100/6000], Loss: 0.0226
Epoch [9/30], Batch [1200/6000], Loss: 0.0295
Epoch [9/30], Batch [1300/6000], Loss: 0.0347
Epoch [9/30], Batch [1400/6000], Loss: 0.0675
Epoch [9/30], Batch [1500/6000], Loss: 0.0276
Epoch [9/30], Batch [1600/6000], Loss: 0.0370
Epoch [9/30], Batch [1700/6000], Loss: 0.0303
Epoch [9/30], Batch [1800/6000], Loss: 0.0303
Epoch [9/30], Batch [1900/6000], Loss: 0.0246
Epoch [9/30], Batch [2000/6000], Loss: 0.0235
Epoch [9/30], Batch [2100/6000], Loss: 0.1127
Epoch [9/30], Batch [2200/6000], Loss: 0.0245
Epoch [9/30], Batch [2300/6000], Loss: 0.0262
Epoch [9/30], Batch [2400/6000], Loss: 0.0286
Epoch [9/30], Batch [2500/6000], Loss: 0.1341
Epoch [9/30], Batch [2600/6000], Loss: 0.1127
Epoch [9/30], Batch [2700/6000], Loss: 0.0282
Epoch [9/30], Batch [2800/6000], Loss: 0.1664
Epoch [9/30], Batch [2900/6000], Loss: 0.0260
Epoch [9/30], Batch [3000/6000], Loss: 0.1394
Epoch [9/30], Batch [3100/6000], Loss: 0.0232
Epoch [9/30], Batch [3200/6000], Loss: 0.0190
Epoch [9/30], Batch [3300/6000], Loss: 0.0298
Epoch [9/30], Batch [3400/6000], Loss: 0.0267
Epoch [9/30], Batch [3500/6000], Loss: 0.0359
Epoch [9/30], Batch [3600/6000], Loss: 0.0480
Epoch [9/30], Batch [3700/6000], Loss: 0.0296
Epoch [9/30], Batch [3800/6000], Loss: 0.0236
Epoch [9/30], Batch [3900/6000], Loss: 0.0219
Epoch [9/30], Batch [4000/6000], Loss: 0.0265
Epoch [9/30], Batch [4100/6000], Loss: 0.0230
Epoch [9/30], Batch [4200/6000], Loss: 0.0205
Epoch [9/30], Batch [4300/6000], Loss: 0.0264
Epoch [9/30], Batch [4400/6000], Loss: 0.0268
Epoch [9/30], Batch [4500/6000], Loss: 0.0262
Epoch [9/30], Batch [4600/6000], Loss: 0.0272
Epoch [9/30], Batch [4700/6000], Loss: 0.3778
Epoch [9/30], Batch [4800/6000], Loss: 0.0227
Epoch [9/30], Batch [4900/6000], Loss: 0.0233
Epoch [9/30], Batch [5000/6000], Loss: 0.0267
Epoch [9/30], Batch [5100/6000], Loss: 0.0246
Epoch [9/30], Batch [5200/6000], Loss: 0.0236
Epoch [9/30], Batch [5300/6000], Loss: 0.0197
Epoch [9/30], Batch [5400/6000], Loss: 0.1228
Epoch [9/30], Batch [5500/6000], Loss: 0.0241
Epoch [9/30], Batch [5600/6000], Loss: 0.0235
Epoch [9/30], Batch [5700/6000], Loss: 0.0253
Epoch [9/30], Batch [5800/6000], Loss: 0.3953
Epoch [9/30], Batch [5900/6000], Loss: 0.0911
Epoch [9/30], Loss: 0.0742
Visualization saved to figures/visualization_0.png
Epoch [10/30], Batch [0/6000], Loss: 0.0226
Epoch [10/30], Batch [100/6000], Loss: 0.1101
Epoch [10/30], Batch [200/6000], Loss: 0.0302
Epoch [10/30], Batch [300/6000], Loss: 0.0297
Epoch [10/30], Batch [400/6000], Loss: 0.0258
Epoch [10/30], Batch [500/6000], Loss: 0.0279
Epoch [10/30], Batch [600/6000], Loss: 0.0251
Epoch [10/30], Batch [700/6000], Loss: 0.0261
Epoch [10/30], Batch [800/6000], Loss: 0.0435
Epoch [10/30], Batch [900/6000], Loss: 0.0245
Epoch [10/30], Batch [1000/6000], Loss: 0.0259
Epoch [10/30], Batch [1100/6000], Loss: 0.0553
Epoch [10/30], Batch [1200/6000], Loss: 0.0425
Epoch [10/30], Batch [1300/6000], Loss: 0.0231
Epoch [10/30], Batch [1400/6000], Loss: 0.0289
Epoch [10/30], Batch [1500/6000], Loss: 0.0221
Epoch [10/30], Batch [1600/6000], Loss: 0.0310
Epoch [10/30], Batch [1700/6000], Loss: 0.0301
Epoch [10/30], Batch [1800/6000], Loss: 0.0391
Epoch [10/30], Batch [1900/6000], Loss: 0.0255
Epoch [10/30], Batch [2000/6000], Loss: 0.0551
Epoch [10/30], Batch [2100/6000], Loss: 0.0246
Epoch [10/30], Batch [2200/6000], Loss: 0.0483
Epoch [10/30], Batch [2300/6000], Loss: 0.2685
Epoch [10/30], Batch [2400/6000], Loss: 0.0208
Epoch [10/30], Batch [2500/6000], Loss: 0.0319
Epoch [10/30], Batch [2600/6000], Loss: 0.0318
Epoch [10/30], Batch [2700/6000], Loss: 0.1265
Epoch [10/30], Batch [2800/6000], Loss: 0.0202
Epoch [10/30], Batch [2900/6000], Loss: 0.0263
Epoch [10/30], Batch [3000/6000], Loss: 0.0315
Epoch [10/30], Batch [3100/6000], Loss: 0.0564
Epoch [10/30], Batch [3200/6000], Loss: 0.0197
Epoch [10/30], Batch [3300/6000], Loss: 0.0266
Epoch [10/30], Batch [3400/6000], Loss: 0.0370
Epoch [10/30], Batch [3500/6000], Loss: 0.1210
Epoch [10/30], Batch [3600/6000], Loss: 0.0994
Epoch [10/30], Batch [3700/6000], Loss: 0.0530
Epoch [10/30], Batch [3800/6000], Loss: 0.0426
Epoch [10/30], Batch [3900/6000], Loss: 0.0709
Epoch [10/30], Batch [4000/6000], Loss: 0.0268
Epoch [10/30], Batch [4100/6000], Loss: 0.0258
Epoch [10/30], Batch [4200/6000], Loss: 0.0210
Epoch [10/30], Batch [4300/6000], Loss: 0.0242
Epoch [10/30], Batch [4400/6000], Loss: 0.2177
Epoch [10/30], Batch [4500/6000], Loss: 0.0448
Epoch [10/30], Batch [4600/6000], Loss: 0.0312
Epoch [10/30], Batch [4700/6000], Loss: 0.0269
Epoch [10/30], Batch [4800/6000], Loss: 0.0281
Epoch [10/30], Batch [4900/6000], Loss: 0.1474
Epoch [10/30], Batch [5000/6000], Loss: 0.0277
Epoch [10/30], Batch [5100/6000], Loss: 0.0231
Epoch [10/30], Batch [5200/6000], Loss: 0.0310
Epoch [10/30], Batch [5300/6000], Loss: 0.0640
Epoch [10/30], Batch [5400/6000], Loss: 0.0342
Epoch [10/30], Batch [5500/6000], Loss: 0.2237
Epoch [10/30], Batch [5600/6000], Loss: 0.2258
Epoch [10/30], Batch [5700/6000], Loss: 0.0641
Epoch [10/30], Batch [5800/6000], Loss: 0.0235
Epoch [10/30], Batch [5900/6000], Loss: 0.0249
Epoch [10/30], Loss: 0.0680
Visualization saved to figures/visualization_0.png
Epoch [11/30], Batch [0/6000], Loss: 0.1965
Epoch [11/30], Batch [100/6000], Loss: 0.0215
Epoch [11/30], Batch [200/6000], Loss: 0.0225
Epoch [11/30], Batch [300/6000], Loss: 0.0380
Epoch [11/30], Batch [400/6000], Loss: 0.0236
Epoch [11/30], Batch [500/6000], Loss: 0.0536
Epoch [11/30], Batch [600/6000], Loss: 0.0306
Epoch [11/30], Batch [700/6000], Loss: 0.0212
Epoch [11/30], Batch [800/6000], Loss: 0.0264
Epoch [11/30], Batch [900/6000], Loss: 0.0311
Epoch [11/30], Batch [1000/6000], Loss: 0.0223
Epoch [11/30], Batch [1100/6000], Loss: 0.0225
Epoch [11/30], Batch [1200/6000], Loss: 0.0220
Epoch [11/30], Batch [1300/6000], Loss: 0.0226
Epoch [11/30], Batch [1400/6000], Loss: 0.0155
Epoch [11/30], Batch [1500/6000], Loss: 0.0208
Epoch [11/30], Batch [1600/6000], Loss: 0.0634
Epoch [11/30], Batch [1700/6000], Loss: 0.2970
Epoch [11/30], Batch [1800/6000], Loss: 0.0474
Epoch [11/30], Batch [1900/6000], Loss: 0.0939
Epoch [11/30], Batch [2000/6000], Loss: 0.0397
Epoch [11/30], Batch [2100/6000], Loss: 0.0285
Epoch [11/30], Batch [2200/6000], Loss: 0.0332
Epoch [11/30], Batch [2300/6000], Loss: 0.5717
Epoch [11/30], Batch [2400/6000], Loss: 0.3450
Epoch [11/30], Batch [2500/6000], Loss: 0.0237
Epoch [11/30], Batch [2600/6000], Loss: 0.0194
Epoch [11/30], Batch [2700/6000], Loss: 0.0237
Epoch [11/30], Batch [2800/6000], Loss: 0.0236
Epoch [11/30], Batch [2900/6000], Loss: 0.0376
Epoch [11/30], Batch [3000/6000], Loss: 0.0376
Epoch [11/30], Batch [3100/6000], Loss: 0.0293
Epoch [11/30], Batch [3200/6000], Loss: 0.0233
Epoch [11/30], Batch [3300/6000], Loss: 0.0241
Epoch [11/30], Batch [3400/6000], Loss: 0.0241
Epoch [11/30], Batch [3500/6000], Loss: 0.0425
Epoch [11/30], Batch [3600/6000], Loss: 0.0325
Epoch [11/30], Batch [3700/6000], Loss: 0.0340
Epoch [11/30], Batch [3800/6000], Loss: 0.1223
Epoch [11/30], Batch [3900/6000], Loss: 0.0545
Epoch [11/30], Batch [4000/6000], Loss: 0.0358
Epoch [11/30], Batch [4100/6000], Loss: 0.0339
Epoch [11/30], Batch [4200/6000], Loss: 0.0215
Epoch [11/30], Batch [4300/6000], Loss: 0.0296
Epoch [11/30], Batch [4400/6000], Loss: 0.0369
Epoch [11/30], Batch [4500/6000], Loss: 0.1194
Epoch [11/30], Batch [4600/6000], Loss: 0.0270
Epoch [11/30], Batch [4700/6000], Loss: 0.0276
Epoch [11/30], Batch [4800/6000], Loss: 0.0336
Epoch [11/30], Batch [4900/6000], Loss: 0.0265
Epoch [11/30], Batch [5000/6000], Loss: 0.0988
Epoch [11/30], Batch [5100/6000], Loss: 0.3002
Epoch [11/30], Batch [5200/6000], Loss: 0.0939
Epoch [11/30], Batch [5300/6000], Loss: 0.0304
Epoch [11/30], Batch [5400/6000], Loss: 0.0285
Epoch [11/30], Batch [5500/6000], Loss: 0.0297
Epoch [11/30], Batch [5600/6000], Loss: 0.0248
Epoch [11/30], Batch [5700/6000], Loss: 0.0456
Epoch [11/30], Batch [5800/6000], Loss: 0.0315
Epoch [11/30], Batch [5900/6000], Loss: 0.7027
Epoch [11/30], Loss: 0.0625
Visualization saved to figures/visualization_0.png
Epoch [12/30], Batch [0/6000], Loss: 0.0702
Epoch [12/30], Batch [100/6000], Loss: 0.0177
Epoch [12/30], Batch [200/6000], Loss: 0.0261
Epoch [12/30], Batch [300/6000], Loss: 0.0183
Epoch [12/30], Batch [400/6000], Loss: 0.0263
Epoch [12/30], Batch [500/6000], Loss: 0.0207
Epoch [12/30], Batch [600/6000], Loss: 0.0354
Epoch [12/30], Batch [700/6000], Loss: 0.0200
Epoch [12/30], Batch [800/6000], Loss: 0.0276
Epoch [12/30], Batch [900/6000], Loss: 0.0361
Epoch [12/30], Batch [1000/6000], Loss: 0.0427
Epoch [12/30], Batch [1100/6000], Loss: 0.0264
Epoch [12/30], Batch [1200/6000], Loss: 0.0304
Epoch [12/30], Batch [1300/6000], Loss: 0.0185
Epoch [12/30], Batch [1400/6000], Loss: 0.0940
Epoch [12/30], Batch [1500/6000], Loss: 0.0192
Epoch [12/30], Batch [1600/6000], Loss: 0.0278
Epoch [12/30], Batch [1700/6000], Loss: 0.0348
Epoch [12/30], Batch [1800/6000], Loss: 0.0258
Epoch [12/30], Batch [1900/6000], Loss: 0.0435
Epoch [12/30], Batch [2000/6000], Loss: 0.0375
Epoch [12/30], Batch [2100/6000], Loss: 0.0252
Epoch [12/30], Batch [2200/6000], Loss: 0.0211
Epoch [12/30], Batch [2300/6000], Loss: 0.0191
Epoch [12/30], Batch [2400/6000], Loss: 0.0210
Epoch [12/30], Batch [2500/6000], Loss: 0.0394
Epoch [12/30], Batch [2600/6000], Loss: 0.0173
Epoch [12/30], Batch [2700/6000], Loss: 0.2466
Epoch [12/30], Batch [2800/6000], Loss: 0.0260
Epoch [12/30], Batch [2900/6000], Loss: 0.0207
Epoch [12/30], Batch [3000/6000], Loss: 0.0252
Epoch [12/30], Batch [3100/6000], Loss: 0.0354
Epoch [12/30], Batch [3200/6000], Loss: 0.0315
Epoch [12/30], Batch [3300/6000], Loss: 0.0225
Epoch [12/30], Batch [3400/6000], Loss: 0.4082
Epoch [12/30], Batch [3500/6000], Loss: 0.0204
Epoch [12/30], Batch [3600/6000], Loss: 0.0748
Epoch [12/30], Batch [3700/6000], Loss: 0.0244
Epoch [12/30], Batch [3800/6000], Loss: 0.0252
Epoch [12/30], Batch [3900/6000], Loss: 0.0204
Epoch [12/30], Batch [4000/6000], Loss: 0.0173
Epoch [12/30], Batch [4100/6000], Loss: 0.3747
Epoch [12/30], Batch [4200/6000], Loss: 0.0261
Epoch [12/30], Batch [4300/6000], Loss: 0.0288
Epoch [12/30], Batch [4400/6000], Loss: 0.0700
Epoch [12/30], Batch [4500/6000], Loss: 0.0232
Epoch [12/30], Batch [4600/6000], Loss: 0.0982
Epoch [12/30], Batch [4700/6000], Loss: 0.0203
Epoch [12/30], Batch [4800/6000], Loss: 0.0220
Epoch [12/30], Batch [4900/6000], Loss: 0.0158
Epoch [12/30], Batch [5000/6000], Loss: 0.0628
Epoch [12/30], Batch [5100/6000], Loss: 0.0217
Epoch [12/30], Batch [5200/6000], Loss: 0.3021
Epoch [12/30], Batch [5300/6000], Loss: 0.0225
Epoch [12/30], Batch [5400/6000], Loss: 0.1988
Epoch [12/30], Batch [5500/6000], Loss: 0.0188
Epoch [12/30], Batch [5600/6000], Loss: 0.0230
Epoch [12/30], Batch [5700/6000], Loss: 0.0207
Epoch [12/30], Batch [5800/6000], Loss: 0.0196
Epoch [12/30], Batch [5900/6000], Loss: 0.0232
Epoch [12/30], Loss: 0.0570
Visualization saved to figures/visualization_0.png
Epoch [13/30], Batch [0/6000], Loss: 0.0237
Epoch [13/30], Batch [100/6000], Loss: 0.0326
Epoch [13/30], Batch [200/6000], Loss: 0.0284
Epoch [13/30], Batch [300/6000], Loss: 0.0720
Epoch [13/30], Batch [400/6000], Loss: 0.0249
Epoch [13/30], Batch [500/6000], Loss: 0.0219
Epoch [13/30], Batch [600/6000], Loss: 0.0244
Epoch [13/30], Batch [700/6000], Loss: 0.0356
Epoch [13/30], Batch [800/6000], Loss: 0.0341
Epoch [13/30], Batch [900/6000], Loss: 0.0254
Epoch [13/30], Batch [1000/6000], Loss: 0.0329
Epoch [13/30], Batch [1100/6000], Loss: 0.1872
Epoch [13/30], Batch [1200/6000], Loss: 0.0223
Epoch [13/30], Batch [1300/6000], Loss: 0.0230
Epoch [13/30], Batch [1400/6000], Loss: 0.1950
Epoch [13/30], Batch [1500/6000], Loss: 0.0230
Epoch [13/30], Batch [1600/6000], Loss: 0.1878
Epoch [13/30], Batch [1700/6000], Loss: 0.0273
Epoch [13/30], Batch [1800/6000], Loss: 0.0228
Epoch [13/30], Batch [1900/6000], Loss: 0.0462
Epoch [13/30], Batch [2000/6000], Loss: 0.0197
Epoch [13/30], Batch [2100/6000], Loss: 0.0267
Epoch [13/30], Batch [2200/6000], Loss: 0.0390
Epoch [13/30], Batch [2300/6000], Loss: 0.0304
Epoch [13/30], Batch [2400/6000], Loss: 0.0191
Epoch [13/30], Batch [2500/6000], Loss: 0.0219
Epoch [13/30], Batch [2600/6000], Loss: 0.0167
Epoch [13/30], Batch [2700/6000], Loss: 0.0229
Epoch [13/30], Batch [2800/6000], Loss: 0.0582
Epoch [13/30], Batch [2900/6000], Loss: 0.0254
Epoch [13/30], Batch [3000/6000], Loss: 0.0257
Epoch [13/30], Batch [3100/6000], Loss: 0.0254
Epoch [13/30], Batch [3200/6000], Loss: 0.0200
Epoch [13/30], Batch [3300/6000], Loss: 0.2119
Epoch [13/30], Batch [3400/6000], Loss: 0.3501
Epoch [13/30], Batch [3500/6000], Loss: 0.0381
Epoch [13/30], Batch [3600/6000], Loss: 0.0280
Epoch [13/30], Batch [3700/6000], Loss: 0.0212
Epoch [13/30], Batch [3800/6000], Loss: 0.0271
Epoch [13/30], Batch [3900/6000], Loss: 0.3757
Epoch [13/30], Batch [4000/6000], Loss: 0.0255
Epoch [13/30], Batch [4100/6000], Loss: 0.0209
Epoch [13/30], Batch [4200/6000], Loss: 0.0172
Epoch [13/30], Batch [4300/6000], Loss: 0.0241
Epoch [13/30], Batch [4400/6000], Loss: 0.0201
Epoch [13/30], Batch [4500/6000], Loss: 0.0219
Epoch [13/30], Batch [4600/6000], Loss: 0.0236
Epoch [13/30], Batch [4700/6000], Loss: 0.0188
Epoch [13/30], Batch [4800/6000], Loss: 0.0189
Epoch [13/30], Batch [4900/6000], Loss: 0.0328
Epoch [13/30], Batch [5000/6000], Loss: 0.0221
Epoch [13/30], Batch [5100/6000], Loss: 0.0216
Epoch [13/30], Batch [5200/6000], Loss: 0.0393
Epoch [13/30], Batch [5300/6000], Loss: 0.0200
Epoch [13/30], Batch [5400/6000], Loss: 0.0224
Epoch [13/30], Batch [5500/6000], Loss: 0.0202
Epoch [13/30], Batch [5600/6000], Loss: 0.0162
Epoch [13/30], Batch [5700/6000], Loss: 0.0351
Epoch [13/30], Batch [5800/6000], Loss: 0.4156
Epoch [13/30], Batch [5900/6000], Loss: 0.0381
Epoch [13/30], Loss: 0.0523
Visualization saved to figures/visualization_0.png
Epoch [14/30], Batch [0/6000], Loss: 0.0164
Epoch [14/30], Batch [100/6000], Loss: 0.0481
Epoch [14/30], Batch [200/6000], Loss: 0.0223
Epoch [14/30], Batch [300/6000], Loss: 0.0221
Epoch [14/30], Batch [400/6000], Loss: 0.0287
Epoch [14/30], Batch [500/6000], Loss: 0.0186
Epoch [14/30], Batch [600/6000], Loss: 0.0321
Epoch [14/30], Batch [700/6000], Loss: 0.0233
Epoch [14/30], Batch [800/6000], Loss: 0.0240
Epoch [14/30], Batch [900/6000], Loss: 0.0251
Epoch [14/30], Batch [1000/6000], Loss: 0.2950
Epoch [14/30], Batch [1100/6000], Loss: 0.0255
Epoch [14/30], Batch [1200/6000], Loss: 0.0237
Epoch [14/30], Batch [1300/6000], Loss: 0.0337
Epoch [14/30], Batch [1400/6000], Loss: 0.0234
Epoch [14/30], Batch [1500/6000], Loss: 0.0251
Epoch [14/30], Batch [1600/6000], Loss: 0.0173
Epoch [14/30], Batch [1700/6000], Loss: 0.0204
Epoch [14/30], Batch [1800/6000], Loss: 0.0214
Epoch [14/30], Batch [1900/6000], Loss: 0.0212
Epoch [14/30], Batch [2000/6000], Loss: 0.0276
Epoch [14/30], Batch [2100/6000], Loss: 0.2785
Epoch [14/30], Batch [2200/6000], Loss: 0.0258
Epoch [14/30], Batch [2300/6000], Loss: 0.3174
Epoch [14/30], Batch [2400/6000], Loss: 0.0351
Epoch [14/30], Batch [2500/6000], Loss: 0.0306
Epoch [14/30], Batch [2600/6000], Loss: 0.0236
Epoch [14/30], Batch [2700/6000], Loss: 0.0643
Epoch [14/30], Batch [2800/6000], Loss: 0.0230
Epoch [14/30], Batch [2900/6000], Loss: 0.0730
Epoch [14/30], Batch [3000/6000], Loss: 0.0202
Epoch [14/30], Batch [3100/6000], Loss: 0.0229
Epoch [14/30], Batch [3200/6000], Loss: 0.0234
Epoch [14/30], Batch [3300/6000], Loss: 0.0230
Epoch [14/30], Batch [3400/6000], Loss: 0.0275
Epoch [14/30], Batch [3500/6000], Loss: 0.0294
Epoch [14/30], Batch [3600/6000], Loss: 0.0197
Epoch [14/30], Batch [3700/6000], Loss: 0.0228
Epoch [14/30], Batch [3800/6000], Loss: 0.0794
Epoch [14/30], Batch [3900/6000], Loss: 0.0890
Epoch [14/30], Batch [4000/6000], Loss: 0.0266
Epoch [14/30], Batch [4100/6000], Loss: 0.0208
Epoch [14/30], Batch [4200/6000], Loss: 0.1395
Epoch [14/30], Batch [4300/6000], Loss: 0.0208
Epoch [14/30], Batch [4400/6000], Loss: 0.8325
Epoch [14/30], Batch [4500/6000], Loss: 0.0739
Epoch [14/30], Batch [4600/6000], Loss: 0.0156
Epoch [14/30], Batch [4700/6000], Loss: 0.0232
Epoch [14/30], Batch [4800/6000], Loss: 0.2617
Epoch [14/30], Batch [4900/6000], Loss: 0.0350
Epoch [14/30], Batch [5000/6000], Loss: 0.0231
Epoch [14/30], Batch [5100/6000], Loss: 0.0221
Epoch [14/30], Batch [5200/6000], Loss: 0.0641
Epoch [14/30], Batch [5300/6000], Loss: 0.0232
Epoch [14/30], Batch [5400/6000], Loss: 0.0246
Epoch [14/30], Batch [5500/6000], Loss: 0.0245
Epoch [14/30], Batch [5600/6000], Loss: 0.0229
Epoch [14/30], Batch [5700/6000], Loss: 0.0316
Epoch [14/30], Batch [5800/6000], Loss: 0.0331
Epoch [14/30], Batch [5900/6000], Loss: 0.0169
Epoch [14/30], Loss: 0.0495
Visualization saved to figures/visualization_0.png
Epoch [15/30], Batch [0/6000], Loss: 0.0259
Epoch [15/30], Batch [100/6000], Loss: 0.0229
Epoch [15/30], Batch [200/6000], Loss: 0.1230
Epoch [15/30], Batch [300/6000], Loss: 0.0242
Epoch [15/30], Batch [400/6000], Loss: 0.6041
Epoch [15/30], Batch [500/6000], Loss: 0.0220
Epoch [15/30], Batch [600/6000], Loss: 0.0256
Epoch [15/30], Batch [700/6000], Loss: 0.0172
Epoch [15/30], Batch [800/6000], Loss: 0.0176
Epoch [15/30], Batch [900/6000], Loss: 0.0214
Epoch [15/30], Batch [1000/6000], Loss: 0.0292
Epoch [15/30], Batch [1100/6000], Loss: 0.0188
Epoch [15/30], Batch [1200/6000], Loss: 0.0301
Epoch [15/30], Batch [1300/6000], Loss: 0.0234
Epoch [15/30], Batch [1400/6000], Loss: 0.0280
Epoch [15/30], Batch [1500/6000], Loss: 0.0256
Epoch [15/30], Batch [1600/6000], Loss: 0.0280
Epoch [15/30], Batch [1700/6000], Loss: 0.0162
Epoch [15/30], Batch [1800/6000], Loss: 0.0218
Epoch [15/30], Batch [1900/6000], Loss: 0.0333
Epoch [15/30], Batch [2000/6000], Loss: 0.0201
Epoch [15/30], Batch [2100/6000], Loss: 0.0174
Epoch [15/30], Batch [2200/6000], Loss: 0.0529
Epoch [15/30], Batch [2300/6000], Loss: 0.0187
Epoch [15/30], Batch [2400/6000], Loss: 0.0257
Epoch [15/30], Batch [2500/6000], Loss: 0.0253
Epoch [15/30], Batch [2600/6000], Loss: 0.0448
Epoch [15/30], Batch [2700/6000], Loss: 0.0390
Epoch [15/30], Batch [2800/6000], Loss: 0.0234
Epoch [15/30], Batch [2900/6000], Loss: 0.0190
Epoch [15/30], Batch [3000/6000], Loss: 0.0212
Epoch [15/30], Batch [3100/6000], Loss: 0.0202
Epoch [15/30], Batch [3200/6000], Loss: 0.0243
Epoch [15/30], Batch [3300/6000], Loss: 0.1196
Epoch [15/30], Batch [3400/6000], Loss: 0.0193
Epoch [15/30], Batch [3500/6000], Loss: 0.0226
Epoch [15/30], Batch [3600/6000], Loss: 0.0226
Epoch [15/30], Batch [3700/6000], Loss: 0.0245
Epoch [15/30], Batch [3800/6000], Loss: 0.0360
Epoch [15/30], Batch [3900/6000], Loss: 0.0417
Epoch [15/30], Batch [4000/6000], Loss: 0.0229
Epoch [15/30], Batch [4100/6000], Loss: 0.0186
Epoch [15/30], Batch [4200/6000], Loss: 0.0261
Epoch [15/30], Batch [4300/6000], Loss: 0.0183
Epoch [15/30], Batch [4400/6000], Loss: 0.0283
Epoch [15/30], Batch [4500/6000], Loss: 0.0295
Epoch [15/30], Batch [4600/6000], Loss: 0.0391
Epoch [15/30], Batch [4700/6000], Loss: 0.0701
Epoch [15/30], Batch [4800/6000], Loss: 0.0219
Epoch [15/30], Batch [4900/6000], Loss: 0.0275
Epoch [15/30], Batch [5000/6000], Loss: 0.1276
Epoch [15/30], Batch [5100/6000], Loss: 0.0186
Epoch [15/30], Batch [5200/6000], Loss: 0.0329
Epoch [15/30], Batch [5300/6000], Loss: 0.0243
Epoch [15/30], Batch [5400/6000], Loss: 0.0235
Epoch [15/30], Batch [5500/6000], Loss: 0.3026
Epoch [15/30], Batch [5600/6000], Loss: 0.0171
Epoch [15/30], Batch [5700/6000], Loss: 0.0323
Epoch [15/30], Batch [5800/6000], Loss: 0.0382
Epoch [15/30], Batch [5900/6000], Loss: 0.0567
Epoch [15/30], Loss: 0.0459
Visualization saved to figures/visualization_0.png
Epoch [16/30], Batch [0/6000], Loss: 0.0221
Epoch [16/30], Batch [100/6000], Loss: 0.0258
Epoch [16/30], Batch [200/6000], Loss: 0.0210
Epoch [16/30], Batch [300/6000], Loss: 0.0217
Epoch [16/30], Batch [400/6000], Loss: 0.0171
Epoch [16/30], Batch [500/6000], Loss: 0.0530
Epoch [16/30], Batch [600/6000], Loss: 0.0212
Epoch [16/30], Batch [700/6000], Loss: 0.0202
Epoch [16/30], Batch [800/6000], Loss: 0.0210
Epoch [16/30], Batch [900/6000], Loss: 0.0551
Epoch [16/30], Batch [1000/6000], Loss: 0.0192
Epoch [16/30], Batch [1100/6000], Loss: 0.0426
Epoch [16/30], Batch [1200/6000], Loss: 0.0226
Epoch [16/30], Batch [1300/6000], Loss: 0.0235
Epoch [16/30], Batch [1400/6000], Loss: 0.0203
Epoch [16/30], Batch [1500/6000], Loss: 0.0309
Epoch [16/30], Batch [1600/6000], Loss: 0.0226
Epoch [16/30], Batch [1700/6000], Loss: 0.0349
Epoch [16/30], Batch [1800/6000], Loss: 0.0193
Epoch [16/30], Batch [1900/6000], Loss: 0.0183
Epoch [16/30], Batch [2000/6000], Loss: 0.0223
Epoch [16/30], Batch [2100/6000], Loss: 0.0208
Epoch [16/30], Batch [2200/6000], Loss: 0.0163
Epoch [16/30], Batch [2300/6000], Loss: 0.0232
Epoch [16/30], Batch [2400/6000], Loss: 0.0286
Epoch [16/30], Batch [2500/6000], Loss: 0.0198
Epoch [16/30], Batch [2600/6000], Loss: 0.1145
Epoch [16/30], Batch [2700/6000], Loss: 0.0219
Epoch [16/30], Batch [2800/6000], Loss: 0.0193
Epoch [16/30], Batch [2900/6000], Loss: 0.0241
Epoch [16/30], Batch [3000/6000], Loss: 0.0712
Epoch [16/30], Batch [3100/6000], Loss: 0.0216
Epoch [16/30], Batch [3200/6000], Loss: 0.0240
Epoch [16/30], Batch [3300/6000], Loss: 0.0160
Epoch [16/30], Batch [3400/6000], Loss: 0.0216
Epoch [16/30], Batch [3500/6000], Loss: 0.0245
Epoch [16/30], Batch [3600/6000], Loss: 0.0204
Epoch [16/30], Batch [3700/6000], Loss: 0.0241
Epoch [16/30], Batch [3800/6000], Loss: 0.0217
Epoch [16/30], Batch [3900/6000], Loss: 0.0245
Epoch [16/30], Batch [4000/6000], Loss: 0.0891
Epoch [16/30], Batch [4100/6000], Loss: 0.0233
Epoch [16/30], Batch [4200/6000], Loss: 0.0206
Epoch [16/30], Batch [4300/6000], Loss: 0.0229
Epoch [16/30], Batch [4400/6000], Loss: 0.0221
Epoch [16/30], Batch [4500/6000], Loss: 0.0223
Epoch [16/30], Batch [4600/6000], Loss: 0.0245
Epoch [16/30], Batch [4700/6000], Loss: 0.0279
Epoch [16/30], Batch [4800/6000], Loss: 0.1148
Epoch [16/30], Batch [4900/6000], Loss: 0.0203
Epoch [16/30], Batch [5000/6000], Loss: 0.3233
Epoch [16/30], Batch [5100/6000], Loss: 0.1790
Epoch [16/30], Batch [5200/6000], Loss: 0.0220
Epoch [16/30], Batch [5300/6000], Loss: 0.0287
Epoch [16/30], Batch [5400/6000], Loss: 0.0245
Epoch [16/30], Batch [5500/6000], Loss: 0.0593
Epoch [16/30], Batch [5600/6000], Loss: 0.0189
Epoch [16/30], Batch [5700/6000], Loss: 0.0253
Epoch [16/30], Batch [5800/6000], Loss: 0.0244
Epoch [16/30], Batch [5900/6000], Loss: 0.0205
Epoch [16/30], Loss: 0.0439
Visualization saved to figures/visualization_0.png
Epoch [17/30], Batch [0/6000], Loss: 0.0202
Epoch [17/30], Batch [100/6000], Loss: 0.0198
Epoch [17/30], Batch [200/6000], Loss: 0.0206
Epoch [17/30], Batch [300/6000], Loss: 0.0269
Epoch [17/30], Batch [400/6000], Loss: 0.0181
Epoch [17/30], Batch [500/6000], Loss: 0.0202
Epoch [17/30], Batch [600/6000], Loss: 0.0125
Epoch [17/30], Batch [700/6000], Loss: 0.0214
Epoch [17/30], Batch [800/6000], Loss: 0.3756
Epoch [17/30], Batch [900/6000], Loss: 0.0185
Epoch [17/30], Batch [1000/6000], Loss: 0.1090
Epoch [17/30], Batch [1100/6000], Loss: 0.0203
Epoch [17/30], Batch [1200/6000], Loss: 0.0211
Epoch [17/30], Batch [1300/6000], Loss: 0.7460
Epoch [17/30], Batch [1400/6000], Loss: 0.0202
Epoch [17/30], Batch [1500/6000], Loss: 0.0175
Epoch [17/30], Batch [1600/6000], Loss: 0.0230
Epoch [17/30], Batch [1700/6000], Loss: 0.0222
Epoch [17/30], Batch [1800/6000], Loss: 0.0145
Epoch [17/30], Batch [1900/6000], Loss: 0.0163
Epoch [17/30], Batch [2000/6000], Loss: 0.0217
Epoch [17/30], Batch [2100/6000], Loss: 0.0596
Epoch [17/30], Batch [2200/6000], Loss: 0.0299
Epoch [17/30], Batch [2300/6000], Loss: 0.0231
Epoch [17/30], Batch [2400/6000], Loss: 0.0401
Epoch [17/30], Batch [2500/6000], Loss: 0.0251
Epoch [17/30], Batch [2600/6000], Loss: 0.0222
Epoch [17/30], Batch [2700/6000], Loss: 0.0199
Epoch [17/30], Batch [2800/6000], Loss: 0.0280
Epoch [17/30], Batch [2900/6000], Loss: 0.0255
Epoch [17/30], Batch [3000/6000], Loss: 0.0174
Epoch [17/30], Batch [3100/6000], Loss: 0.1004
Epoch [17/30], Batch [3200/6000], Loss: 0.0225
Epoch [17/30], Batch [3300/6000], Loss: 0.0904
Epoch [17/30], Batch [3400/6000], Loss: 0.0198
Epoch [17/30], Batch [3500/6000], Loss: 0.0288
Epoch [17/30], Batch [3600/6000], Loss: 0.0220
Epoch [17/30], Batch [3700/6000], Loss: 0.0205
Epoch [17/30], Batch [3800/6000], Loss: 0.0173
Epoch [17/30], Batch [3900/6000], Loss: 0.0208
Epoch [17/30], Batch [4000/6000], Loss: 0.0194
Epoch [17/30], Batch [4100/6000], Loss: 0.0211
Epoch [17/30], Batch [4200/6000], Loss: 0.0290
Epoch [17/30], Batch [4300/6000], Loss: 0.0220
Epoch [17/30], Batch [4400/6000], Loss: 0.0195
Epoch [17/30], Batch [4500/6000], Loss: 0.0230
Epoch [17/30], Batch [4600/6000], Loss: 0.0265
Epoch [17/30], Batch [4700/6000], Loss: 0.0196
Epoch [17/30], Batch [4800/6000], Loss: 0.0199
Epoch [17/30], Batch [4900/6000], Loss: 0.0537
Epoch [17/30], Batch [5000/6000], Loss: 0.0309
Epoch [17/30], Batch [5100/6000], Loss: 0.0943
Epoch [17/30], Batch [5200/6000], Loss: 0.0207
Epoch [17/30], Batch [5300/6000], Loss: 0.0216
Epoch [17/30], Batch [5400/6000], Loss: 0.0270
Epoch [17/30], Batch [5500/6000], Loss: 0.0177
Epoch [17/30], Batch [5600/6000], Loss: 0.0329
Epoch [17/30], Batch [5700/6000], Loss: 0.0195
Epoch [17/30], Batch [5800/6000], Loss: 0.0419
Epoch [17/30], Batch [5900/6000], Loss: 0.0258
Epoch [17/30], Loss: 0.0407
Visualization saved to figures/visualization_0.png
Epoch [18/30], Batch [0/6000], Loss: 0.0234
Epoch [18/30], Batch [100/6000], Loss: 0.0244
Epoch [18/30], Batch [200/6000], Loss: 0.0245
Epoch [18/30], Batch [300/6000], Loss: 0.0196
Epoch [18/30], Batch [400/6000], Loss: 0.0456
Epoch [18/30], Batch [500/6000], Loss: 0.0193
Epoch [18/30], Batch [600/6000], Loss: 0.0161
Epoch [18/30], Batch [700/6000], Loss: 0.0225
Epoch [18/30], Batch [800/6000], Loss: 0.0287
Epoch [18/30], Batch [900/6000], Loss: 0.1123
Epoch [18/30], Batch [1000/6000], Loss: 0.0335
Epoch [18/30], Batch [1100/6000], Loss: 0.0197
Epoch [18/30], Batch [1200/6000], Loss: 0.0194
Epoch [18/30], Batch [1300/6000], Loss: 0.0212
Epoch [18/30], Batch [1400/6000], Loss: 0.0186
Epoch [18/30], Batch [1500/6000], Loss: 0.0167
Epoch [18/30], Batch [1600/6000], Loss: 0.0240
Epoch [18/30], Batch [1700/6000], Loss: 0.0186
Epoch [18/30], Batch [1800/6000], Loss: 0.0250
Epoch [18/30], Batch [1900/6000], Loss: 0.0202
Epoch [18/30], Batch [2000/6000], Loss: 0.0315
Epoch [18/30], Batch [2100/6000], Loss: 0.0295
Epoch [18/30], Batch [2200/6000], Loss: 0.0230
Epoch [18/30], Batch [2300/6000], Loss: 0.0180
Epoch [18/30], Batch [2400/6000], Loss: 0.0255
Epoch [18/30], Batch [2500/6000], Loss: 0.0270
Epoch [18/30], Batch [2600/6000], Loss: 0.0245
Epoch [18/30], Batch [2700/6000], Loss: 0.0422
Epoch [18/30], Batch [2800/6000], Loss: 0.0170
Epoch [18/30], Batch [2900/6000], Loss: 0.0320
Epoch [18/30], Batch [3000/6000], Loss: 0.0242
Epoch [18/30], Batch [3100/6000], Loss: 0.0204
Epoch [18/30], Batch [3200/6000], Loss: 0.0247
Epoch [18/30], Batch [3300/6000], Loss: 0.0252
Epoch [18/30], Batch [3400/6000], Loss: 0.0235
Epoch [18/30], Batch [3500/6000], Loss: 0.0199
Epoch [18/30], Batch [3600/6000], Loss: 0.0177
Epoch [18/30], Batch [3700/6000], Loss: 0.0270
Epoch [18/30], Batch [3800/6000], Loss: 0.0171
Epoch [18/30], Batch [3900/6000], Loss: 0.0579
Epoch [18/30], Batch [4000/6000], Loss: 0.0261
Epoch [18/30], Batch [4100/6000], Loss: 0.0230
Epoch [18/30], Batch [4200/6000], Loss: 0.0168
Epoch [18/30], Batch [4300/6000], Loss: 0.0207
Epoch [18/30], Batch [4400/6000], Loss: 0.0975
Epoch [18/30], Batch [4500/6000], Loss: 0.0210
Epoch [18/30], Batch [4600/6000], Loss: 0.0179
Epoch [18/30], Batch [4700/6000], Loss: 0.0267
Epoch [18/30], Batch [4800/6000], Loss: 0.1621
Epoch [18/30], Batch [4900/6000], Loss: 0.0230
Epoch [18/30], Batch [5000/6000], Loss: 0.0194
Epoch [18/30], Batch [5100/6000], Loss: 0.0215
Epoch [18/30], Batch [5200/6000], Loss: 0.0208
Epoch [18/30], Batch [5300/6000], Loss: 0.0135
Epoch [18/30], Batch [5400/6000], Loss: 0.0209
Epoch [18/30], Batch [5500/6000], Loss: 0.0255
Epoch [18/30], Batch [5600/6000], Loss: 0.0210
Epoch [18/30], Batch [5700/6000], Loss: 0.0249
Epoch [18/30], Batch [5800/6000], Loss: 0.0185
Epoch [18/30], Batch [5900/6000], Loss: 0.0199
Epoch [18/30], Loss: 0.0393
Visualization saved to figures/visualization_0.png
Epoch [19/30], Batch [0/6000], Loss: 0.1067
Epoch [19/30], Batch [100/6000], Loss: 0.0194
Epoch [19/30], Batch [200/6000], Loss: 0.0251
Epoch [19/30], Batch [300/6000], Loss: 0.0221
Epoch [19/30], Batch [400/6000], Loss: 0.0177
Epoch [19/30], Batch [500/6000], Loss: 0.0192
Epoch [19/30], Batch [600/6000], Loss: 0.0919
Epoch [19/30], Batch [700/6000], Loss: 0.0186
Epoch [19/30], Batch [800/6000], Loss: 0.0245
Epoch [19/30], Batch [900/6000], Loss: 0.0170
Epoch [19/30], Batch [1000/6000], Loss: 0.0200
Epoch [19/30], Batch [1100/6000], Loss: 0.0305
Epoch [19/30], Batch [1200/6000], Loss: 0.0212
Epoch [19/30], Batch [1300/6000], Loss: 0.0169
Epoch [19/30], Batch [1400/6000], Loss: 0.0172
Epoch [19/30], Batch [1500/6000], Loss: 0.0394
Epoch [19/30], Batch [1600/6000], Loss: 0.0203
Epoch [19/30], Batch [1700/6000], Loss: 0.0307
Epoch [19/30], Batch [1800/6000], Loss: 0.0204
Epoch [19/30], Batch [1900/6000], Loss: 0.0288
Epoch [19/30], Batch [2000/6000], Loss: 0.0160
Epoch [19/30], Batch [2100/6000], Loss: 0.0211
Epoch [19/30], Batch [2200/6000], Loss: 0.0246
Epoch [19/30], Batch [2300/6000], Loss: 0.0334
Epoch [19/30], Batch [2400/6000], Loss: 0.0198
Epoch [19/30], Batch [2500/6000], Loss: 0.0348
Epoch [19/30], Batch [2600/6000], Loss: 0.0238
Epoch [19/30], Batch [2700/6000], Loss: 0.0177
Epoch [19/30], Batch [2800/6000], Loss: 0.0224
Epoch [19/30], Batch [2900/6000], Loss: 0.0396
Epoch [19/30], Batch [3000/6000], Loss: 0.0186
Epoch [19/30], Batch [3100/6000], Loss: 0.0213
Epoch [19/30], Batch [3200/6000], Loss: 0.0224
Epoch [19/30], Batch [3300/6000], Loss: 0.0200
Epoch [19/30], Batch [3400/6000], Loss: 0.0364
Epoch [19/30], Batch [3500/6000], Loss: 0.0246
Epoch [19/30], Batch [3600/6000], Loss: 0.0594
Epoch [19/30], Batch [3700/6000], Loss: 0.2355
Epoch [19/30], Batch [3800/6000], Loss: 0.0206
Epoch [19/30], Batch [3900/6000], Loss: 0.0201
Epoch [19/30], Batch [4000/6000], Loss: 0.1352
Epoch [19/30], Batch [4100/6000], Loss: 0.0319
Epoch [19/30], Batch [4200/6000], Loss: 0.0830
Epoch [19/30], Batch [4300/6000], Loss: 0.0671
Epoch [19/30], Batch [4400/6000], Loss: 0.0241
Epoch [19/30], Batch [4500/6000], Loss: 0.0203
Epoch [19/30], Batch [4600/6000], Loss: 0.0146
Epoch [19/30], Batch [4700/6000], Loss: 0.0198
Epoch [19/30], Batch [4800/6000], Loss: 0.0257
Epoch [19/30], Batch [4900/6000], Loss: 0.0192
Epoch [19/30], Batch [5000/6000], Loss: 0.0219
Epoch [19/30], Batch [5100/6000], Loss: 0.0225
Epoch [19/30], Batch [5200/6000], Loss: 0.0198
Epoch [19/30], Batch [5300/6000], Loss: 0.0223
Epoch [19/30], Batch [5400/6000], Loss: 0.0194
Epoch [19/30], Batch [5500/6000], Loss: 0.1094
Epoch [19/30], Batch [5600/6000], Loss: 0.0206
Epoch [19/30], Batch [5700/6000], Loss: 0.0186
Epoch [19/30], Batch [5800/6000], Loss: 0.0198
Epoch [19/30], Batch [5900/6000], Loss: 0.0176
Epoch [19/30], Loss: 0.0377
Visualization saved to figures/visualization_0.png
Epoch [20/30], Batch [0/6000], Loss: 0.0187
Epoch [20/30], Batch [100/6000], Loss: 0.0183
Epoch [20/30], Batch [200/6000], Loss: 0.0369
Epoch [20/30], Batch [300/6000], Loss: 0.0227
Epoch [20/30], Batch [400/6000], Loss: 0.0212
Epoch [20/30], Batch [500/6000], Loss: 0.0180
Epoch [20/30], Batch [600/6000], Loss: 0.0222
Epoch [20/30], Batch [700/6000], Loss: 0.0321
Epoch [20/30], Batch [800/6000], Loss: 0.1116
Epoch [20/30], Batch [900/6000], Loss: 0.0218
Epoch [20/30], Batch [1000/6000], Loss: 0.0187
Epoch [20/30], Batch [1100/6000], Loss: 0.0161
Epoch [20/30], Batch [1200/6000], Loss: 0.0210
Epoch [20/30], Batch [1300/6000], Loss: 0.0206
Epoch [20/30], Batch [1400/6000], Loss: 0.0184
Epoch [20/30], Batch [1500/6000], Loss: 0.0175
Epoch [20/30], Batch [1600/6000], Loss: 0.2970
Epoch [20/30], Batch [1700/6000], Loss: 0.0199
Epoch [20/30], Batch [1800/6000], Loss: 0.0712
Epoch [20/30], Batch [1900/6000], Loss: 0.0175
Epoch [20/30], Batch [2000/6000], Loss: 0.0195
Epoch [20/30], Batch [2100/6000], Loss: 0.0273
Epoch [20/30], Batch [2200/6000], Loss: 0.0184
Epoch [20/30], Batch [2300/6000], Loss: 0.0195
Epoch [20/30], Batch [2400/6000], Loss: 0.0332
Epoch [20/30], Batch [2500/6000], Loss: 0.0551
Epoch [20/30], Batch [2600/6000], Loss: 0.0203
Epoch [20/30], Batch [2700/6000], Loss: 0.0213
Epoch [20/30], Batch [2800/6000], Loss: 0.0171
Epoch [20/30], Batch [2900/6000], Loss: 0.0196
Epoch [20/30], Batch [3000/6000], Loss: 0.0232
Epoch [20/30], Batch [3100/6000], Loss: 0.0200
Epoch [20/30], Batch [3200/6000], Loss: 0.0234
Epoch [20/30], Batch [3300/6000], Loss: 0.0193
Epoch [20/30], Batch [3400/6000], Loss: 0.0190
Epoch [20/30], Batch [3500/6000], Loss: 0.0227
Epoch [20/30], Batch [3600/6000], Loss: 0.0267
Epoch [20/30], Batch [3700/6000], Loss: 0.0263
Epoch [20/30], Batch [3800/6000], Loss: 0.0225
Epoch [20/30], Batch [3900/6000], Loss: 0.0232
Epoch [20/30], Batch [4000/6000], Loss: 0.0198
Epoch [20/30], Batch [4100/6000], Loss: 0.0195
Epoch [20/30], Batch [4200/6000], Loss: 0.0198
Epoch [20/30], Batch [4300/6000], Loss: 0.0229
Epoch [20/30], Batch [4400/6000], Loss: 0.0186
Epoch [20/30], Batch [4500/6000], Loss: 0.0168
Epoch [20/30], Batch [4600/6000], Loss: 0.0854
Epoch [20/30], Batch [4700/6000], Loss: 0.0211
Epoch [20/30], Batch [4800/6000], Loss: 0.0260
Epoch [20/30], Batch [4900/6000], Loss: 0.3058
Epoch [20/30], Batch [5000/6000], Loss: 0.0332
Epoch [20/30], Batch [5100/6000], Loss: 0.0218
Epoch [20/30], Batch [5200/6000], Loss: 0.0215
Epoch [20/30], Batch [5300/6000], Loss: 0.0315
Epoch [20/30], Batch [5400/6000], Loss: 0.0191
Epoch [20/30], Batch [5500/6000], Loss: 0.0196
Epoch [20/30], Batch [5600/6000], Loss: 0.0176
Epoch [20/30], Batch [5700/6000], Loss: 0.0343
Epoch [20/30], Batch [5800/6000], Loss: 0.0183
Epoch [20/30], Batch [5900/6000], Loss: 0.0642
Epoch [20/30], Loss: 0.0356
Visualization saved to figures/visualization_0.png
Epoch [21/30], Batch [0/6000], Loss: 0.0221
Epoch [21/30], Batch [100/6000], Loss: 0.0173
Epoch [21/30], Batch [200/6000], Loss: 0.0213
Epoch [21/30], Batch [300/6000], Loss: 0.0231
Epoch [21/30], Batch [400/6000], Loss: 0.0219
Epoch [21/30], Batch [500/6000], Loss: 0.0173
Epoch [21/30], Batch [600/6000], Loss: 0.0168
Epoch [21/30], Batch [700/6000], Loss: 0.0194
Epoch [21/30], Batch [800/6000], Loss: 0.0214
Epoch [21/30], Batch [900/6000], Loss: 0.0189
Epoch [21/30], Batch [1000/6000], Loss: 0.0218
Epoch [21/30], Batch [1100/6000], Loss: 0.0217
Epoch [21/30], Batch [1200/6000], Loss: 0.0195
Epoch [21/30], Batch [1300/6000], Loss: 0.0358
Epoch [21/30], Batch [1400/6000], Loss: 0.0230
Epoch [21/30], Batch [1500/6000], Loss: 0.0159
Epoch [21/30], Batch [1600/6000], Loss: 0.0194
Epoch [21/30], Batch [1700/6000], Loss: 0.0193
Epoch [21/30], Batch [1800/6000], Loss: 0.0143
Epoch [21/30], Batch [1900/6000], Loss: 0.0347
Epoch [21/30], Batch [2000/6000], Loss: 0.1096
Epoch [21/30], Batch [2100/6000], Loss: 0.1112
Epoch [21/30], Batch [2200/6000], Loss: 0.0202
Epoch [21/30], Batch [2300/6000], Loss: 0.0209
Epoch [21/30], Batch [2400/6000], Loss: 0.0172
Epoch [21/30], Batch [2500/6000], Loss: 0.0243
Epoch [21/30], Batch [2600/6000], Loss: 0.0258
Epoch [21/30], Batch [2700/6000], Loss: 0.0216
Epoch [21/30], Batch [2800/6000], Loss: 0.0223
Epoch [21/30], Batch [2900/6000], Loss: 0.0281
Epoch [21/30], Batch [3000/6000], Loss: 0.0266
Epoch [21/30], Batch [3100/6000], Loss: 0.0269
Epoch [21/30], Batch [3200/6000], Loss: 0.0412
Epoch [21/30], Batch [3300/6000], Loss: 0.2007
Epoch [21/30], Batch [3400/6000], Loss: 0.0154
Epoch [21/30], Batch [3500/6000], Loss: 0.0142
Epoch [21/30], Batch [3600/6000], Loss: 0.0421
Epoch [21/30], Batch [3700/6000], Loss: 0.1178
Epoch [21/30], Batch [3800/6000], Loss: 0.0262
Epoch [21/30], Batch [3900/6000], Loss: 0.0176
Epoch [21/30], Batch [4000/6000], Loss: 0.0181
Epoch [21/30], Batch [4100/6000], Loss: 0.0205
Epoch [21/30], Batch [4200/6000], Loss: 0.0198
Epoch [21/30], Batch [4300/6000], Loss: 0.0188
Epoch [21/30], Batch [4400/6000], Loss: 0.0248
Epoch [21/30], Batch [4500/6000], Loss: 0.0250
Epoch [21/30], Batch [4600/6000], Loss: 0.0147
Epoch [21/30], Batch [4700/6000], Loss: 0.0221
Epoch [21/30], Batch [4800/6000], Loss: 0.0178
Epoch [21/30], Batch [4900/6000], Loss: 0.0187
Epoch [21/30], Batch [5000/6000], Loss: 0.0203
Epoch [21/30], Batch [5100/6000], Loss: 0.0172
Epoch [21/30], Batch [5200/6000], Loss: 0.0304
Epoch [21/30], Batch [5300/6000], Loss: 0.0502
Epoch [21/30], Batch [5400/6000], Loss: 0.0263
Epoch [21/30], Batch [5500/6000], Loss: 0.0175
Epoch [21/30], Batch [5600/6000], Loss: 0.0274
Epoch [21/30], Batch [5700/6000], Loss: 0.0694
Epoch [21/30], Batch [5800/6000], Loss: 0.0223
Epoch [21/30], Batch [5900/6000], Loss: 0.0197
Epoch [21/30], Loss: 0.0334
Visualization saved to figures/visualization_0.png
Epoch [22/30], Batch [0/6000], Loss: 0.0162
Epoch [22/30], Batch [100/6000], Loss: 0.0203
Epoch [22/30], Batch [200/6000], Loss: 0.0186
Epoch [22/30], Batch [300/6000], Loss: 0.0202
Epoch [22/30], Batch [400/6000], Loss: 0.0174
Epoch [22/30], Batch [500/6000], Loss: 0.0186
Epoch [22/30], Batch [600/6000], Loss: 0.0227
Epoch [22/30], Batch [700/6000], Loss: 0.0204
Epoch [22/30], Batch [800/6000], Loss: 0.0163
Epoch [22/30], Batch [900/6000], Loss: 0.0162
Epoch [22/30], Batch [1000/6000], Loss: 0.0206
Epoch [22/30], Batch [1100/6000], Loss: 0.0258
Epoch [22/30], Batch [1200/6000], Loss: 0.0177
Epoch [22/30], Batch [1300/6000], Loss: 0.0213
Epoch [22/30], Batch [1400/6000], Loss: 0.0262
Epoch [22/30], Batch [1500/6000], Loss: 0.0204
Epoch [22/30], Batch [1600/6000], Loss: 0.0190
Epoch [22/30], Batch [1700/6000], Loss: 0.0198
Epoch [22/30], Batch [1800/6000], Loss: 0.0404
Epoch [22/30], Batch [1900/6000], Loss: 0.0142
Epoch [22/30], Batch [2000/6000], Loss: 0.0490
Epoch [22/30], Batch [2100/6000], Loss: 0.0163
Epoch [22/30], Batch [2200/6000], Loss: 0.0198
Epoch [22/30], Batch [2300/6000], Loss: 0.0258
Epoch [22/30], Batch [2400/6000], Loss: 0.0168
Epoch [22/30], Batch [2500/6000], Loss: 0.0184
Epoch [22/30], Batch [2600/6000], Loss: 0.0508
Epoch [22/30], Batch [2700/6000], Loss: 0.0160
Epoch [22/30], Batch [2800/6000], Loss: 0.1023
Epoch [22/30], Batch [2900/6000], Loss: 0.0261
Epoch [22/30], Batch [3000/6000], Loss: 0.0205
Epoch [22/30], Batch [3100/6000], Loss: 0.0196
Epoch [22/30], Batch [3200/6000], Loss: 0.0202
Epoch [22/30], Batch [3300/6000], Loss: 0.0199
Epoch [22/30], Batch [3400/6000], Loss: 0.0192
Epoch [22/30], Batch [3500/6000], Loss: 0.0213
Epoch [22/30], Batch [3600/6000], Loss: 0.0190
Epoch [22/30], Batch [3700/6000], Loss: 0.0183
Epoch [22/30], Batch [3800/6000], Loss: 0.0235
Epoch [22/30], Batch [3900/6000], Loss: 0.0187
Epoch [22/30], Batch [4000/6000], Loss: 0.0256
Epoch [22/30], Batch [4100/6000], Loss: 0.0206
Epoch [22/30], Batch [4200/6000], Loss: 0.0208
Epoch [22/30], Batch [4300/6000], Loss: 0.0267
Epoch [22/30], Batch [4400/6000], Loss: 0.0163
Epoch [22/30], Batch [4500/6000], Loss: 0.0233
Epoch [22/30], Batch [4600/6000], Loss: 0.0228
Epoch [22/30], Batch [4700/6000], Loss: 0.0167
Epoch [22/30], Batch [4800/6000], Loss: 0.0273
Epoch [22/30], Batch [4900/6000], Loss: 0.0185
Epoch [22/30], Batch [5000/6000], Loss: 0.0181
Epoch [22/30], Batch [5100/6000], Loss: 0.0663
Epoch [22/30], Batch [5200/6000], Loss: 0.0191
Epoch [22/30], Batch [5300/6000], Loss: 0.0197
Epoch [22/30], Batch [5400/6000], Loss: 0.0473
Epoch [22/30], Batch [5500/6000], Loss: 0.0153
Epoch [22/30], Batch [5600/6000], Loss: 0.0295
Epoch [22/30], Batch [5700/6000], Loss: 0.0213
Epoch [22/30], Batch [5800/6000], Loss: 0.0206
Epoch [22/30], Batch [5900/6000], Loss: 0.0191
Epoch [22/30], Loss: 0.0332
Visualization saved to figures/visualization_0.png
Epoch [23/30], Batch [0/6000], Loss: 0.0234
Epoch [23/30], Batch [100/6000], Loss: 0.0175
Epoch [23/30], Batch [200/6000], Loss: 0.0222
Epoch [23/30], Batch [300/6000], Loss: 0.0221
Epoch [23/30], Batch [400/6000], Loss: 0.0191
Epoch [23/30], Batch [500/6000], Loss: 0.0196
Epoch [23/30], Batch [600/6000], Loss: 0.0176
Epoch [23/30], Batch [700/6000], Loss: 0.0209
Epoch [23/30], Batch [800/6000], Loss: 0.0236
Epoch [23/30], Batch [900/6000], Loss: 0.0264
Epoch [23/30], Batch [1000/6000], Loss: 0.0197
Epoch [23/30], Batch [1100/6000], Loss: 0.0226
Epoch [23/30], Batch [1200/6000], Loss: 0.0192
Epoch [23/30], Batch [1300/6000], Loss: 0.0207
Epoch [23/30], Batch [1400/6000], Loss: 0.0209
Epoch [23/30], Batch [1500/6000], Loss: 0.0161
Epoch [23/30], Batch [1600/6000], Loss: 0.0153
Epoch [23/30], Batch [1700/6000], Loss: 0.0188
Epoch [23/30], Batch [1800/6000], Loss: 0.0263
Epoch [23/30], Batch [1900/6000], Loss: 0.0267
Epoch [23/30], Batch [2000/6000], Loss: 0.0230
Epoch [23/30], Batch [2100/6000], Loss: 0.0231
Epoch [23/30], Batch [2200/6000], Loss: 0.0202
Epoch [23/30], Batch [2300/6000], Loss: 0.0150
Epoch [23/30], Batch [2400/6000], Loss: 0.0189
Epoch [23/30], Batch [2500/6000], Loss: 0.0211
Epoch [23/30], Batch [2600/6000], Loss: 0.0189
Epoch [23/30], Batch [2700/6000], Loss: 0.0160
Epoch [23/30], Batch [2800/6000], Loss: 0.0198
Epoch [23/30], Batch [2900/6000], Loss: 0.0174
Epoch [23/30], Batch [3000/6000], Loss: 0.0234
Epoch [23/30], Batch [3100/6000], Loss: 0.0188
Epoch [23/30], Batch [3200/6000], Loss: 0.0156
Epoch [23/30], Batch [3300/6000], Loss: 0.0268
Epoch [23/30], Batch [3400/6000], Loss: 0.0201
Epoch [23/30], Batch [3500/6000], Loss: 0.0535
Epoch [23/30], Batch [3600/6000], Loss: 0.0187
Epoch [23/30], Batch [3700/6000], Loss: 0.0186
Epoch [23/30], Batch [3800/6000], Loss: 0.0206
Epoch [23/30], Batch [3900/6000], Loss: 0.0164
Epoch [23/30], Batch [4000/6000], Loss: 0.0217
Epoch [23/30], Batch [4100/6000], Loss: 0.0199
Epoch [23/30], Batch [4200/6000], Loss: 0.0182
Epoch [23/30], Batch [4300/6000], Loss: 0.1790
Epoch [23/30], Batch [4400/6000], Loss: 0.0260
Epoch [23/30], Batch [4500/6000], Loss: 0.0236
Epoch [23/30], Batch [4600/6000], Loss: 0.0173
Epoch [23/30], Batch [4700/6000], Loss: 0.0141
Epoch [23/30], Batch [4800/6000], Loss: 0.0200
Epoch [23/30], Batch [4900/6000], Loss: 0.0183
Epoch [23/30], Batch [5000/6000], Loss: 0.0208
Epoch [23/30], Batch [5100/6000], Loss: 0.0213
Epoch [23/30], Batch [5200/6000], Loss: 0.0215
Epoch [23/30], Batch [5300/6000], Loss: 0.0189
Epoch [23/30], Batch [5400/6000], Loss: 0.0232
Epoch [23/30], Batch [5500/6000], Loss: 0.0197
Epoch [23/30], Batch [5600/6000], Loss: 0.0194
Epoch [23/30], Batch [5700/6000], Loss: 0.0173
Epoch [23/30], Batch [5800/6000], Loss: 0.0128
Epoch [23/30], Batch [5900/6000], Loss: 0.0149
Epoch [23/30], Loss: 0.0319
Visualization saved to figures/visualization_0.png
Epoch [24/30], Batch [0/6000], Loss: 0.0174
Epoch [24/30], Batch [100/6000], Loss: 0.0172
Epoch [24/30], Batch [200/6000], Loss: 0.0182
Epoch [24/30], Batch [300/6000], Loss: 0.0227
Epoch [24/30], Batch [400/6000], Loss: 0.0164
Epoch [24/30], Batch [500/6000], Loss: 0.0680
Epoch [24/30], Batch [600/6000], Loss: 0.0180
Epoch [24/30], Batch [700/6000], Loss: 0.0148
Epoch [24/30], Batch [800/6000], Loss: 0.0204
Epoch [24/30], Batch [900/6000], Loss: 0.0251
Epoch [24/30], Batch [1000/6000], Loss: 0.0193
Epoch [24/30], Batch [1100/6000], Loss: 0.0280
Epoch [24/30], Batch [1200/6000], Loss: 0.0169
Epoch [24/30], Batch [1300/6000], Loss: 0.0161
Epoch [24/30], Batch [1400/6000], Loss: 0.0307
Epoch [24/30], Batch [1500/6000], Loss: 0.0178
Epoch [24/30], Batch [1600/6000], Loss: 0.0146
Epoch [24/30], Batch [1700/6000], Loss: 0.0308
Epoch [24/30], Batch [1800/6000], Loss: 0.0197
Epoch [24/30], Batch [1900/6000], Loss: 0.0131
Epoch [24/30], Batch [2000/6000], Loss: 0.0229
Epoch [24/30], Batch [2100/6000], Loss: 0.1788
Epoch [24/30], Batch [2200/6000], Loss: 0.0177
Epoch [24/30], Batch [2300/6000], Loss: 0.0306
Epoch [24/30], Batch [2400/6000], Loss: 0.0143
Epoch [24/30], Batch [2500/6000], Loss: 0.0216
Epoch [24/30], Batch [2600/6000], Loss: 0.0205
Epoch [24/30], Batch [2700/6000], Loss: 0.0177
Epoch [24/30], Batch [2800/6000], Loss: 0.0182
Epoch [24/30], Batch [2900/6000], Loss: 0.0180
Epoch [24/30], Batch [3000/6000], Loss: 0.0219
Epoch [24/30], Batch [3100/6000], Loss: 0.0124
Epoch [24/30], Batch [3200/6000], Loss: 0.0333
Epoch [24/30], Batch [3300/6000], Loss: 0.0180
Epoch [24/30], Batch [3400/6000], Loss: 0.0161
Epoch [24/30], Batch [3500/6000], Loss: 0.0161
Epoch [24/30], Batch [3600/6000], Loss: 0.0227
Epoch [24/30], Batch [3700/6000], Loss: 0.0190
Epoch [24/30], Batch [3800/6000], Loss: 0.0203
Epoch [24/30], Batch [3900/6000], Loss: 0.0167
Epoch [24/30], Batch [4000/6000], Loss: 0.0172
Epoch [24/30], Batch [4100/6000], Loss: 0.0172
Epoch [24/30], Batch [4200/6000], Loss: 0.1254
Epoch [24/30], Batch [4300/6000], Loss: 0.0192
Epoch [24/30], Batch [4400/6000], Loss: 0.0269
Epoch [24/30], Batch [4500/6000], Loss: 0.0224
Epoch [24/30], Batch [4600/6000], Loss: 0.0203
Epoch [24/30], Batch [4700/6000], Loss: 0.0170
Epoch [24/30], Batch [4800/6000], Loss: 0.0206
Epoch [24/30], Batch [4900/6000], Loss: 0.0203
Epoch [24/30], Batch [5000/6000], Loss: 0.0200
Epoch [24/30], Batch [5100/6000], Loss: 0.0212
Epoch [24/30], Batch [5200/6000], Loss: 0.0154
Epoch [24/30], Batch [5300/6000], Loss: 0.0246
Epoch [24/30], Batch [5400/6000], Loss: 0.0174
Epoch [24/30], Batch [5500/6000], Loss: 0.1155
Epoch [24/30], Batch [5600/6000], Loss: 0.0224
Epoch [24/30], Batch [5700/6000], Loss: 0.0221
Epoch [24/30], Batch [5800/6000], Loss: 0.0170
Epoch [24/30], Batch [5900/6000], Loss: 0.0204
Epoch [24/30], Loss: 0.0297
Visualization saved to figures/visualization_0.png
Epoch [25/30], Batch [0/6000], Loss: 0.0222
Epoch [25/30], Batch [100/6000], Loss: 0.0310
Epoch [25/30], Batch [200/6000], Loss: 0.0179
Epoch [25/30], Batch [300/6000], Loss: 0.0220
Epoch [25/30], Batch [400/6000], Loss: 0.0188
Epoch [25/30], Batch [500/6000], Loss: 0.0212
Epoch [25/30], Batch [600/6000], Loss: 0.0191
Epoch [25/30], Batch [700/6000], Loss: 0.0205
Epoch [25/30], Batch [800/6000], Loss: 0.0178
Epoch [25/30], Batch [900/6000], Loss: 0.0156
Epoch [25/30], Batch [1000/6000], Loss: 0.0579
Epoch [25/30], Batch [1100/6000], Loss: 0.0162
Epoch [25/30], Batch [1200/6000], Loss: 0.0125
Epoch [25/30], Batch [1300/6000], Loss: 0.0212
Epoch [25/30], Batch [1400/6000], Loss: 0.0202
Epoch [25/30], Batch [1500/6000], Loss: 0.0185
Epoch [25/30], Batch [1600/6000], Loss: 0.0152
Epoch [25/30], Batch [1700/6000], Loss: 0.0142
Epoch [25/30], Batch [1800/6000], Loss: 0.0171
Epoch [25/30], Batch [1900/6000], Loss: 0.0171
Epoch [25/30], Batch [2000/6000], Loss: 0.0245
Epoch [25/30], Batch [2100/6000], Loss: 0.0201
Epoch [25/30], Batch [2200/6000], Loss: 0.0155
Epoch [25/30], Batch [2300/6000], Loss: 0.0206
Epoch [25/30], Batch [2400/6000], Loss: 0.0195
Epoch [25/30], Batch [2500/6000], Loss: 0.0158
Epoch [25/30], Batch [2600/6000], Loss: 0.0286
Epoch [25/30], Batch [2700/6000], Loss: 0.0368
Epoch [25/30], Batch [2800/6000], Loss: 0.0195
Epoch [25/30], Batch [2900/6000], Loss: 0.0246
Epoch [25/30], Batch [3000/6000], Loss: 0.0199
Epoch [25/30], Batch [3100/6000], Loss: 0.0242
Epoch [25/30], Batch [3200/6000], Loss: 0.0190
Epoch [25/30], Batch [3300/6000], Loss: 0.0260
Epoch [25/30], Batch [3400/6000], Loss: 0.0521
Epoch [25/30], Batch [3500/6000], Loss: 0.0229
Epoch [25/30], Batch [3600/6000], Loss: 0.0172
Epoch [25/30], Batch [3700/6000], Loss: 0.0170
Epoch [25/30], Batch [3800/6000], Loss: 0.0392
Epoch [25/30], Batch [3900/6000], Loss: 0.0201
Epoch [25/30], Batch [4000/6000], Loss: 0.0583
Epoch [25/30], Batch [4100/6000], Loss: 0.0168
Epoch [25/30], Batch [4200/6000], Loss: 0.0179
Epoch [25/30], Batch [4300/6000], Loss: 0.0202
Epoch [25/30], Batch [4400/6000], Loss: 0.0207
Epoch [25/30], Batch [4500/6000], Loss: 0.0280
Epoch [25/30], Batch [4600/6000], Loss: 0.0186
Epoch [25/30], Batch [4700/6000], Loss: 0.0190
Epoch [25/30], Batch [4800/6000], Loss: 0.0194
Epoch [25/30], Batch [4900/6000], Loss: 0.0215
Epoch [25/30], Batch [5000/6000], Loss: 0.0198
Epoch [25/30], Batch [5100/6000], Loss: 0.0182
Epoch [25/30], Batch [5200/6000], Loss: 0.0168
Epoch [25/30], Batch [5300/6000], Loss: 0.0156
Epoch [25/30], Batch [5400/6000], Loss: 0.0169
Epoch [25/30], Batch [5500/6000], Loss: 0.0164
Epoch [25/30], Batch [5600/6000], Loss: 0.0220
Epoch [25/30], Batch [5700/6000], Loss: 0.0175
Epoch [25/30], Batch [5800/6000], Loss: 0.0598
Epoch [25/30], Batch [5900/6000], Loss: 0.0192
Epoch [25/30], Loss: 0.0297
Visualization saved to figures/visualization_0.png
Epoch [26/30], Batch [0/6000], Loss: 0.0162
Epoch [26/30], Batch [100/6000], Loss: 0.0201
Epoch [26/30], Batch [200/6000], Loss: 0.0175
Epoch [26/30], Batch [300/6000], Loss: 0.0198
Epoch [26/30], Batch [400/6000], Loss: 0.0197
Epoch [26/30], Batch [500/6000], Loss: 0.0150
Epoch [26/30], Batch [600/6000], Loss: 0.2715
Epoch [26/30], Batch [700/6000], Loss: 0.0250
Epoch [26/30], Batch [800/6000], Loss: 0.0189
Epoch [26/30], Batch [900/6000], Loss: 0.0174
Epoch [26/30], Batch [1000/6000], Loss: 0.0165
Epoch [26/30], Batch [1100/6000], Loss: 0.0180
Epoch [26/30], Batch [1200/6000], Loss: 0.4198
Epoch [26/30], Batch [1300/6000], Loss: 0.0684
Epoch [26/30], Batch [1400/6000], Loss: 0.0154
Epoch [26/30], Batch [1500/6000], Loss: 0.0181
Epoch [26/30], Batch [1600/6000], Loss: 0.0202
Epoch [26/30], Batch [1700/6000], Loss: 0.0188
Epoch [26/30], Batch [1800/6000], Loss: 0.0205
Epoch [26/30], Batch [1900/6000], Loss: 0.0184
Epoch [26/30], Batch [2000/6000], Loss: 0.1003
Epoch [26/30], Batch [2100/6000], Loss: 0.0169
Epoch [26/30], Batch [2200/6000], Loss: 0.0229
Epoch [26/30], Batch [2300/6000], Loss: 0.0168
Epoch [26/30], Batch [2400/6000], Loss: 0.0186
Epoch [26/30], Batch [2500/6000], Loss: 0.0207
Epoch [26/30], Batch [2600/6000], Loss: 0.0187
Epoch [26/30], Batch [2700/6000], Loss: 0.0176
Epoch [26/30], Batch [2800/6000], Loss: 0.0205
Epoch [26/30], Batch [2900/6000], Loss: 0.0268
Epoch [26/30], Batch [3000/6000], Loss: 0.0133
Epoch [26/30], Batch [3100/6000], Loss: 0.0185
Epoch [26/30], Batch [3200/6000], Loss: 0.0239
Epoch [26/30], Batch [3300/6000], Loss: 0.0207
Epoch [26/30], Batch [3400/6000], Loss: 0.0194
Epoch [26/30], Batch [3500/6000], Loss: 0.0170
Epoch [26/30], Batch [3600/6000], Loss: 0.0163
Epoch [26/30], Batch [3700/6000], Loss: 0.0168
Epoch [26/30], Batch [3800/6000], Loss: 0.0310
Epoch [26/30], Batch [3900/6000], Loss: 0.0180
Epoch [26/30], Batch [4000/6000], Loss: 0.0181
Epoch [26/30], Batch [4100/6000], Loss: 0.0204
Epoch [26/30], Batch [4200/6000], Loss: 0.0191
Epoch [26/30], Batch [4300/6000], Loss: 0.0146
Epoch [26/30], Batch [4400/6000], Loss: 0.0160
Epoch [26/30], Batch [4500/6000], Loss: 0.0219
Epoch [26/30], Batch [4600/6000], Loss: 0.0134
Epoch [26/30], Batch [4700/6000], Loss: 0.0168
Epoch [26/30], Batch [4800/6000], Loss: 0.0165
Epoch [26/30], Batch [4900/6000], Loss: 0.0161
Epoch [26/30], Batch [5000/6000], Loss: 0.0197
Epoch [26/30], Batch [5100/6000], Loss: 0.0159
Epoch [26/30], Batch [5200/6000], Loss: 0.0177
Epoch [26/30], Batch [5300/6000], Loss: 0.0189
Epoch [26/30], Batch [5400/6000], Loss: 0.0349
Epoch [26/30], Batch [5500/6000], Loss: 0.0209
Epoch [26/30], Batch [5600/6000], Loss: 0.0235
Epoch [26/30], Batch [5700/6000], Loss: 0.0182
Epoch [26/30], Batch [5800/6000], Loss: 0.0219
Epoch [26/30], Batch [5900/6000], Loss: 0.0314
Epoch [26/30], Loss: 0.0295
Visualization saved to figures/visualization_0.png
Epoch [27/30], Batch [0/6000], Loss: 0.0206
Epoch [27/30], Batch [100/6000], Loss: 0.0199
Epoch [27/30], Batch [200/6000], Loss: 0.0147
Epoch [27/30], Batch [300/6000], Loss: 0.0200
Epoch [27/30], Batch [400/6000], Loss: 0.0158
Epoch [27/30], Batch [500/6000], Loss: 0.1106
Epoch [27/30], Batch [600/6000], Loss: 0.0235
Epoch [27/30], Batch [700/6000], Loss: 0.0192
Epoch [27/30], Batch [800/6000], Loss: 0.0144
Epoch [27/30], Batch [900/6000], Loss: 0.0187
Epoch [27/30], Batch [1000/6000], Loss: 0.0207
Epoch [27/30], Batch [1100/6000], Loss: 0.0182
Epoch [27/30], Batch [1200/6000], Loss: 0.0188
Epoch [27/30], Batch [1300/6000], Loss: 0.0170
Epoch [27/30], Batch [1400/6000], Loss: 0.0187
Epoch [27/30], Batch [1500/6000], Loss: 0.0153
Epoch [27/30], Batch [1600/6000], Loss: 0.0147
Epoch [27/30], Batch [1700/6000], Loss: 0.0199
Epoch [27/30], Batch [1800/6000], Loss: 0.0168
Epoch [27/30], Batch [1900/6000], Loss: 0.0212
Epoch [27/30], Batch [2000/6000], Loss: 0.0207
Epoch [27/30], Batch [2100/6000], Loss: 0.0180
Epoch [27/30], Batch [2200/6000], Loss: 0.0178
Epoch [27/30], Batch [2300/6000], Loss: 0.0281
Epoch [27/30], Batch [2400/6000], Loss: 0.0195
Epoch [27/30], Batch [2500/6000], Loss: 0.0218
Epoch [27/30], Batch [2600/6000], Loss: 0.0550
Epoch [27/30], Batch [2700/6000], Loss: 0.0194
Epoch [27/30], Batch [2800/6000], Loss: 0.0178
Epoch [27/30], Batch [2900/6000], Loss: 0.0158
Epoch [27/30], Batch [3000/6000], Loss: 0.0203
Epoch [27/30], Batch [3100/6000], Loss: 0.0217
Epoch [27/30], Batch [3200/6000], Loss: 0.0199
Epoch [27/30], Batch [3300/6000], Loss: 0.0224
Epoch [27/30], Batch [3400/6000], Loss: 0.0240
Epoch [27/30], Batch [3500/6000], Loss: 0.0240
Epoch [27/30], Batch [3600/6000], Loss: 0.0183
Epoch [27/30], Batch [3700/6000], Loss: 0.0181
Epoch [27/30], Batch [3800/6000], Loss: 0.0167
Epoch [27/30], Batch [3900/6000], Loss: 0.0242
Epoch [27/30], Batch [4000/6000], Loss: 0.0226
Epoch [27/30], Batch [4100/6000], Loss: 0.0170
Epoch [27/30], Batch [4200/6000], Loss: 0.0185
Epoch [27/30], Batch [4300/6000], Loss: 0.2489
Epoch [27/30], Batch [4400/6000], Loss: 0.0202
Epoch [27/30], Batch [4500/6000], Loss: 0.0224
Epoch [27/30], Batch [4600/6000], Loss: 0.0197
Epoch [27/30], Batch [4700/6000], Loss: 0.0209
Epoch [27/30], Batch [4800/6000], Loss: 0.0195
Epoch [27/30], Batch [4900/6000], Loss: 0.0209
Epoch [27/30], Batch [5000/6000], Loss: 0.0176
Epoch [27/30], Batch [5100/6000], Loss: 0.0209
Epoch [27/30], Batch [5200/6000], Loss: 0.0219
Epoch [27/30], Batch [5300/6000], Loss: 0.0168
Epoch [27/30], Batch [5400/6000], Loss: 0.0205
Epoch [27/30], Batch [5500/6000], Loss: 0.0148
Epoch [27/30], Batch [5600/6000], Loss: 0.0191
Epoch [27/30], Batch [5700/6000], Loss: 0.0165
Epoch [27/30], Batch [5800/6000], Loss: 0.0182
Epoch [27/30], Batch [5900/6000], Loss: 0.0165
Epoch [27/30], Loss: 0.0278
Visualization saved to figures/visualization_0.png
Epoch [28/30], Batch [0/6000], Loss: 0.0259
Epoch [28/30], Batch [100/6000], Loss: 0.0188
Epoch [28/30], Batch [200/6000], Loss: 0.0187
Epoch [28/30], Batch [300/6000], Loss: 0.0235
Epoch [28/30], Batch [400/6000], Loss: 0.0209
Epoch [28/30], Batch [500/6000], Loss: 0.0187
Epoch [28/30], Batch [600/6000], Loss: 0.0223
Epoch [28/30], Batch [700/6000], Loss: 0.0146
Epoch [28/30], Batch [800/6000], Loss: 0.0160
Epoch [28/30], Batch [900/6000], Loss: 0.0139
Epoch [28/30], Batch [1000/6000], Loss: 0.0222
Epoch [28/30], Batch [1100/6000], Loss: 0.0175
Epoch [28/30], Batch [1200/6000], Loss: 0.0238
Epoch [28/30], Batch [1300/6000], Loss: 0.0166
Epoch [28/30], Batch [1400/6000], Loss: 0.0190
Epoch [28/30], Batch [1500/6000], Loss: 0.4239
Epoch [28/30], Batch [1600/6000], Loss: 0.0156
Epoch [28/30], Batch [1700/6000], Loss: 0.0178
Epoch [28/30], Batch [1800/6000], Loss: 0.0210
Epoch [28/30], Batch [1900/6000], Loss: 0.0302
Epoch [28/30], Batch [2000/6000], Loss: 0.0186
Epoch [28/30], Batch [2100/6000], Loss: 0.0183
Epoch [28/30], Batch [2200/6000], Loss: 0.0195
Epoch [28/30], Batch [2300/6000], Loss: 0.0190
Epoch [28/30], Batch [2400/6000], Loss: 0.0893
Epoch [28/30], Batch [2500/6000], Loss: 0.0184
Epoch [28/30], Batch [2600/6000], Loss: 0.0201
Epoch [28/30], Batch [2700/6000], Loss: 0.0205
Epoch [28/30], Batch [2800/6000], Loss: 0.0140
Epoch [28/30], Batch [2900/6000], Loss: 0.0169
Epoch [28/30], Batch [3000/6000], Loss: 0.0188
Epoch [28/30], Batch [3100/6000], Loss: 0.0181
Epoch [28/30], Batch [3200/6000], Loss: 0.0179
Epoch [28/30], Batch [3300/6000], Loss: 0.0179
Epoch [28/30], Batch [3400/6000], Loss: 0.0152
Epoch [28/30], Batch [3500/6000], Loss: 0.0160
Epoch [28/30], Batch [3600/6000], Loss: 0.0175
Epoch [28/30], Batch [3700/6000], Loss: 0.0174
Epoch [28/30], Batch [3800/6000], Loss: 0.0133
Epoch [28/30], Batch [3900/6000], Loss: 0.0205
Epoch [28/30], Batch [4000/6000], Loss: 0.0182
Epoch [28/30], Batch [4100/6000], Loss: 0.0219
Epoch [28/30], Batch [4200/6000], Loss: 0.0189
Epoch [28/30], Batch [4300/6000], Loss: 0.0154
Epoch [28/30], Batch [4400/6000], Loss: 0.0170
Epoch [28/30], Batch [4500/6000], Loss: 0.0184
Epoch [28/30], Batch [4600/6000], Loss: 0.0201
Epoch [28/30], Batch [4700/6000], Loss: 0.0189
Epoch [28/30], Batch [4800/6000], Loss: 0.0207
Epoch [28/30], Batch [4900/6000], Loss: 0.0188
Epoch [28/30], Batch [5000/6000], Loss: 0.0470
Epoch [28/30], Batch [5100/6000], Loss: 0.0295
Epoch [28/30], Batch [5200/6000], Loss: 0.0209
Epoch [28/30], Batch [5300/6000], Loss: 0.0165
Epoch [28/30], Batch [5400/6000], Loss: 0.0182
Epoch [28/30], Batch [5500/6000], Loss: 0.2348
Epoch [28/30], Batch [5600/6000], Loss: 0.0137
Epoch [28/30], Batch [5700/6000], Loss: 0.0219
Epoch [28/30], Batch [5800/6000], Loss: 0.0305
Epoch [28/30], Batch [5900/6000], Loss: 0.0193
Epoch [28/30], Loss: 0.0280
Visualization saved to figures/visualization_0.png
Epoch [29/30], Batch [0/6000], Loss: 0.0213
Epoch [29/30], Batch [100/6000], Loss: 0.0244
Epoch [29/30], Batch [200/6000], Loss: 0.0132
Epoch [29/30], Batch [300/6000], Loss: 0.0163
Epoch [29/30], Batch [400/6000], Loss: 0.0171
Epoch [29/30], Batch [500/6000], Loss: 0.0178
Epoch [29/30], Batch [600/6000], Loss: 0.0304
Epoch [29/30], Batch [700/6000], Loss: 0.0152
Epoch [29/30], Batch [800/6000], Loss: 0.0192
Epoch [29/30], Batch [900/6000], Loss: 0.1623
Epoch [29/30], Batch [1000/6000], Loss: 0.0174
Epoch [29/30], Batch [1100/6000], Loss: 0.0271
Epoch [29/30], Batch [1200/6000], Loss: 0.0195
Epoch [29/30], Batch [1300/6000], Loss: 0.0211
Epoch [29/30], Batch [1400/6000], Loss: 0.0200
Epoch [29/30], Batch [1500/6000], Loss: 0.0171
Epoch [29/30], Batch [1600/6000], Loss: 0.0166
Epoch [29/30], Batch [1700/6000], Loss: 0.0131
Epoch [29/30], Batch [1800/6000], Loss: 0.0178
Epoch [29/30], Batch [1900/6000], Loss: 0.0175
Epoch [29/30], Batch [2000/6000], Loss: 0.0157
Epoch [29/30], Batch [2100/6000], Loss: 0.0176
Epoch [29/30], Batch [2200/6000], Loss: 0.0125
Epoch [29/30], Batch [2300/6000], Loss: 0.0172
Epoch [29/30], Batch [2400/6000], Loss: 0.0178
Epoch [29/30], Batch [2500/6000], Loss: 0.0272
Epoch [29/30], Batch [2600/6000], Loss: 0.0178
Epoch [29/30], Batch [2700/6000], Loss: 0.0176
Epoch [29/30], Batch [2800/6000], Loss: 0.0173
Epoch [29/30], Batch [2900/6000], Loss: 0.0241
Epoch [29/30], Batch [3000/6000], Loss: 0.0217
Epoch [29/30], Batch [3100/6000], Loss: 0.0235
Epoch [29/30], Batch [3200/6000], Loss: 0.0161
Epoch [29/30], Batch [3300/6000], Loss: 0.0246
Epoch [29/30], Batch [3400/6000], Loss: 0.0167
Epoch [29/30], Batch [3500/6000], Loss: 0.0155
Epoch [29/30], Batch [3600/6000], Loss: 0.0187
Epoch [29/30], Batch [3700/6000], Loss: 0.0164
Epoch [29/30], Batch [3800/6000], Loss: 0.0156
Epoch [29/30], Batch [3900/6000], Loss: 0.0164
Epoch [29/30], Batch [4000/6000], Loss: 0.0125
Epoch [29/30], Batch [4100/6000], Loss: 0.0172
Epoch [29/30], Batch [4200/6000], Loss: 0.0149
Epoch [29/30], Batch [4300/6000], Loss: 0.0233
Epoch [29/30], Batch [4400/6000], Loss: 0.0161
Epoch [29/30], Batch [4500/6000], Loss: 0.0170
Epoch [29/30], Batch [4600/6000], Loss: 0.0192
Epoch [29/30], Batch [4700/6000], Loss: 0.0181
Epoch [29/30], Batch [4800/6000], Loss: 0.0194
Epoch [29/30], Batch [4900/6000], Loss: 0.0181
Epoch [29/30], Batch [5000/6000], Loss: 0.0497
Epoch [29/30], Batch [5100/6000], Loss: 0.0170
Epoch [29/30], Batch [5200/6000], Loss: 0.0176
Epoch [29/30], Batch [5300/6000], Loss: 0.0240
Epoch [29/30], Batch [5400/6000], Loss: 0.0164
Epoch [29/30], Batch [5500/6000], Loss: 0.0177
Epoch [29/30], Batch [5600/6000], Loss: 0.0174
Epoch [29/30], Batch [5700/6000], Loss: 0.2378
Epoch [29/30], Batch [5800/6000], Loss: 0.0154
Epoch [29/30], Batch [5900/6000], Loss: 0.0177
Epoch [29/30], Loss: 0.0265
Visualization saved to figures/visualization_0.png
Epoch [30/30], Batch [0/6000], Loss: 0.0136
Epoch [30/30], Batch [100/6000], Loss: 0.0168
Epoch [30/30], Batch [200/6000], Loss: 0.0188
Epoch [30/30], Batch [300/6000], Loss: 0.0151
Epoch [30/30], Batch [400/6000], Loss: 0.0173
Epoch [30/30], Batch [500/6000], Loss: 0.0134
Epoch [30/30], Batch [600/6000], Loss: 0.0290
Epoch [30/30], Batch [700/6000], Loss: 0.0191
Epoch [30/30], Batch [800/6000], Loss: 0.0210
Epoch [30/30], Batch [900/6000], Loss: 0.0184
Epoch [30/30], Batch [1000/6000], Loss: 0.0164
Epoch [30/30], Batch [1100/6000], Loss: 0.0153
Epoch [30/30], Batch [1200/6000], Loss: 0.0791
Epoch [30/30], Batch [1300/6000], Loss: 0.0179
Epoch [30/30], Batch [1400/6000], Loss: 0.0315
Epoch [30/30], Batch [1500/6000], Loss: 0.0138
Epoch [30/30], Batch [1600/6000], Loss: 0.0161
Epoch [30/30], Batch [1700/6000], Loss: 0.0204
Epoch [30/30], Batch [1800/6000], Loss: 0.0206
Epoch [30/30], Batch [1900/6000], Loss: 0.0201
Epoch [30/30], Batch [2000/6000], Loss: 0.0216
Epoch [30/30], Batch [2100/6000], Loss: 0.0145
Epoch [30/30], Batch [2200/6000], Loss: 0.0164
Epoch [30/30], Batch [2300/6000], Loss: 0.3647
Epoch [30/30], Batch [2400/6000], Loss: 0.0183
Epoch [30/30], Batch [2500/6000], Loss: 0.0196
Epoch [30/30], Batch [2600/6000], Loss: 0.0167
Epoch [30/30], Batch [2700/6000], Loss: 0.0385
Epoch [30/30], Batch [2800/6000], Loss: 0.0194
Epoch [30/30], Batch [2900/6000], Loss: 0.0217
Epoch [30/30], Batch [3000/6000], Loss: 0.0165
Epoch [30/30], Batch [3100/6000], Loss: 0.0967
Epoch [30/30], Batch [3200/6000], Loss: 0.0546
Epoch [30/30], Batch [3300/6000], Loss: 0.0156
Epoch [30/30], Batch [3400/6000], Loss: 0.0205
Epoch [30/30], Batch [3500/6000], Loss: 0.0153
Epoch [30/30], Batch [3600/6000], Loss: 0.0166
Epoch [30/30], Batch [3700/6000], Loss: 0.0182
Epoch [30/30], Batch [3800/6000], Loss: 0.0188
Epoch [30/30], Batch [3900/6000], Loss: 0.0162
Epoch [30/30], Batch [4000/6000], Loss: 0.0178
Epoch [30/30], Batch [4100/6000], Loss: 0.0242
Epoch [30/30], Batch [4200/6000], Loss: 0.0211
Epoch [30/30], Batch [4300/6000], Loss: 0.0204
Epoch [30/30], Batch [4400/6000], Loss: 0.0195
Epoch [30/30], Batch [4500/6000], Loss: 0.0189
Epoch [30/30], Batch [4600/6000], Loss: 0.0155
Epoch [30/30], Batch [4700/6000], Loss: 0.0146
Epoch [30/30], Batch [4800/6000], Loss: 0.0354
Epoch [30/30], Batch [4900/6000], Loss: 0.0185
Epoch [30/30], Batch [5000/6000], Loss: 0.0223
Epoch [30/30], Batch [5100/6000], Loss: 0.0188
Epoch [30/30], Batch [5200/6000], Loss: 0.0162
Epoch [30/30], Batch [5300/6000], Loss: 0.0164
Epoch [30/30], Batch [5400/6000], Loss: 0.0217
Epoch [30/30], Batch [5500/6000], Loss: 0.0491
Epoch [30/30], Batch [5600/6000], Loss: 0.0177
Epoch [30/30], Batch [5700/6000], Loss: 0.0175
Epoch [30/30], Batch [5800/6000], Loss: 0.0198
Epoch [30/30], Batch [5900/6000], Loss: 0.0271
Epoch [30/30], Loss: 0.0267
Visualization saved to figures/visualization_0.png
Test Loss: 0.1086, Accuracy: 97.99%
  Output probs: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
Adversarial Training Loop 1/300:
  Label Loss: 1.7381
  Image Loss: 0.0172
  Total Loss: 173.8293
  Image grad max: 14.553287506103516
  Output probs: [[0.976 0.    0.    0.    0.002 0.    0.021 0.    0.    0.001]]
Adversarial Training Loop 2/300:
  Label Loss: 0.4686
  Image Loss: 0.0211
  Total Loss: 46.8769
  Image grad max: 31.38284683227539
  Output probs: [[0.    0.    0.163 0.    0.004 0.    0.832 0.    0.    0.   ]]
Adversarial Training Loop 3/300:
  Label Loss: 0.7173
  Image Loss: 0.0267
  Total Loss: 71.7561
  Image grad max: 19.460744857788086
  Output probs: [[0.    0.    0.222 0.    0.002 0.    0.776 0.    0.    0.   ]]
Adversarial Training Loop 4/300:
  Label Loss: 0.3957
  Image Loss: 0.0326
  Total Loss: 39.6006
  Image grad max: 17.48468017578125
  Output probs: [[0.    0.    0.08  0.    0.01  0.    0.129 0.    0.    0.782]]
Adversarial Training Loop 5/300:
  Label Loss: 0.0455
  Image Loss: 0.0390
  Total Loss: 4.5868
  Image grad max: 11.891767501831055
  Output probs: [[0.    0.    0.001 0.    0.001 0.    0.543 0.    0.    0.455]]
Adversarial Training Loop 6/300:
  Label Loss: 0.0005
  Image Loss: 0.0463
  Total Loss: 0.0986
  Image grad max: 1.2680773735046387
  Output probs: [[0.    0.    0.    0.    0.    0.    0.698 0.    0.    0.302]]
Adversarial Training Loop 7/300:
  Label Loss: 0.0085
  Image Loss: 0.0538
  Total Loss: 0.9066
  Image grad max: 5.129815101623535
  Output probs: [[0.    0.    0.    0.    0.    0.    0.382 0.    0.    0.617]]
Adversarial Training Loop 8/300:
  Label Loss: 0.0029
  Image Loss: 0.0611
  Total Loss: 0.3462
  Image grad max: 2.8128609657287598
  Output probs: [[0.    0.    0.    0.    0.    0.    0.384 0.    0.    0.616]]
Adversarial Training Loop 9/300:
  Label Loss: 0.0028
  Image Loss: 0.0683
  Total Loss: 0.3466
  Image grad max: 2.595316171646118
  Output probs: [[0.    0.    0.    0.    0.    0.    0.565 0.    0.    0.435]]
Adversarial Training Loop 10/300:
  Label Loss: 0.0009
  Image Loss: 0.0753
  Total Loss: 0.1622
  Image grad max: 1.3846160173416138
  Output probs: [[0.    0.    0.    0.    0.    0.    0.623 0.    0.    0.377]]
Adversarial Training Loop 11/300:
  Label Loss: 0.0031
  Image Loss: 0.0819
  Total Loss: 0.3936
  Image grad max: 2.4645330905914307
  Output probs: [[0.    0.    0.    0.    0.    0.    0.532 0.    0.    0.468]]
Adversarial Training Loop 12/300:
  Label Loss: 0.0002
  Image Loss: 0.0880
  Total Loss: 0.1091
  Image grad max: 0.6083423495292664
  Output probs: [[0.    0.    0.    0.    0.    0.    0.427 0.    0.    0.573]]
Adversarial Training Loop 13/300:
  Label Loss: 0.0011
  Image Loss: 0.0937
  Total Loss: 0.2022
  Image grad max: 1.323639988899231
  Output probs: [[0.    0.    0.    0.    0.    0.    0.413 0.    0.    0.587]]
Adversarial Training Loop 14/300:
  Label Loss: 0.0015
  Image Loss: 0.0991
  Total Loss: 0.2529
  Image grad max: 1.4979490041732788
  Output probs: [[0.    0.    0.    0.    0.    0.    0.477 0.    0.    0.523]]
Adversarial Training Loop 15/300:
  Label Loss: 0.0001
  Image Loss: 0.1042
  Total Loss: 0.1161
  Image grad max: 0.3773805499076843
  Output probs: [[0.    0.    0.    0.    0.    0.    0.544 0.    0.    0.455]]
Adversarial Training Loop 16/300:
  Label Loss: 0.0004
  Image Loss: 0.1090
  Total Loss: 0.1503
  Image grad max: 0.6952067017555237
  Output probs: [[0.    0.    0.    0.    0.    0.    0.566 0.    0.    0.434]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0009
  Image Loss: 0.1134
  Total Loss: 0.2039
  Image grad max: 0.9936237335205078
  Output probs: [[0.    0.    0.    0.    0.    0.    0.538 0.    0.    0.462]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0003
  Image Loss: 0.1174
  Total Loss: 0.1488
  Image grad max: 0.5452183485031128
  Output probs: [[0.    0.    0.    0.    0.    0.    0.486 0.    0.    0.513]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0001
  Image Loss: 0.1210
  Total Loss: 0.1282
  Image grad max: 0.19103144109249115
  Output probs: [[0.   0.   0.   0.   0.   0.   0.45 0.   0.   0.55]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0005
  Image Loss: 0.1243
  Total Loss: 0.1786
  Image grad max: 0.6790191531181335
  Output probs: [[0.    0.    0.    0.    0.    0.    0.447 0.    0.    0.553]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0006
  Image Loss: 0.1274
  Total Loss: 0.1889
  Image grad max: 0.7048797607421875
  Output probs: [[0.    0.    0.    0.    0.    0.    0.473 0.    0.    0.527]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0002
  Image Loss: 0.1302
  Total Loss: 0.1504
  Image grad max: 0.3506862223148346
  Output probs: [[0.    0.    0.    0.    0.    0.    0.51  0.001 0.    0.489]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0001
  Image Loss: 0.1326
  Total Loss: 0.1415
  Image grad max: 0.13752086460590363
  Output probs: [[0.    0.    0.    0.    0.    0.    0.537 0.001 0.    0.462]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0004
  Image Loss: 0.1349
  Total Loss: 0.1705
  Image grad max: 0.4711841344833374
  Output probs: [[0.    0.    0.    0.    0.    0.    0.54  0.001 0.    0.459]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0004
  Image Loss: 0.1368
  Total Loss: 0.1778
  Image grad max: 0.5015891790390015
  Output probs: [[0.    0.    0.    0.    0.    0.    0.521 0.001 0.    0.478]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0002
  Image Loss: 0.1386
  Total Loss: 0.1568
  Image grad max: 0.2632793188095093
  Output probs: [[0.    0.    0.    0.    0.    0.    0.492 0.001 0.    0.507]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0001
  Image Loss: 0.1403
  Total Loss: 0.1510
  Image grad max: 0.08852767199277878
  Output probs: [[0.    0.    0.    0.    0.    0.    0.471 0.001 0.    0.528]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0003
  Image Loss: 0.1417
  Total Loss: 0.1688
  Image grad max: 0.34956085681915283
  Output probs: [[0.    0.    0.    0.    0.    0.    0.467 0.001 0.    0.532]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0003
  Image Loss: 0.1431
  Total Loss: 0.1749
  Image grad max: 0.3875395357608795
  Output probs: [[0.    0.    0.    0.    0.    0.    0.481 0.001 0.    0.517]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0002
  Image Loss: 0.1444
  Total Loss: 0.1626
  Image grad max: 0.21532370150089264
  Output probs: [[0.    0.    0.    0.    0.    0.    0.504 0.001 0.    0.495]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0001
  Image Loss: 0.1456
  Total Loss: 0.1582
  Image grad max: 0.05301559343934059
  Output probs: [[0.    0.    0.    0.    0.    0.    0.521 0.001 0.    0.477]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0002
  Image Loss: 0.1466
  Total Loss: 0.1689
  Image grad max: 0.2572731375694275
  Output probs: [[0.    0.    0.    0.    0.    0.    0.525 0.001 0.    0.474]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0003
  Image Loss: 0.1475
  Total Loss: 0.1735
  Image grad max: 0.29572680592536926
  Output probs: [[0.    0.    0.    0.    0.    0.    0.514 0.001 0.    0.485]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0002
  Image Loss: 0.1484
  Total Loss: 0.1660
  Image grad max: 0.1689721643924713
  Output probs: [[0.    0.    0.    0.    0.    0.    0.496 0.001 0.    0.503]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0001
  Image Loss: 0.1491
  Total Loss: 0.1630
  Image grad max: 0.038344815373420715
  Output probs: [[0.    0.    0.    0.    0.    0.    0.482 0.001 0.    0.517]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0002
  Image Loss: 0.1497
  Total Loss: 0.1698
  Image grad max: 0.20240667462348938
  Output probs: [[0.    0.    0.    0.    0.    0.    0.479 0.001 0.    0.519]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0002
  Image Loss: 0.1503
  Total Loss: 0.1725
  Image grad max: 0.23251654207706451
  Output probs: [[0.    0.    0.    0.    0.    0.    0.488 0.001 0.    0.51 ]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0002
  Image Loss: 0.1509
  Total Loss: 0.1675
  Image grad max: 0.130111962556839
  Output probs: [[0.    0.    0.    0.    0.    0.    0.502 0.001 0.    0.496]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0001
  Image Loss: 0.1513
  Total Loss: 0.1656
  Image grad max: 0.034448862075805664
  Output probs: [[0.    0.    0.    0.    0.    0.    0.513 0.001 0.    0.485]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0002
  Image Loss: 0.1518
  Total Loss: 0.1697
  Image grad max: 0.16073386371135712
  Output probs: [[0.    0.    0.    0.    0.    0.    0.515 0.001 0.    0.483]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0002
  Image Loss: 0.1521
  Total Loss: 0.1711
  Image grad max: 0.18185146152973175
  Output probs: [[0.    0.    0.    0.    0.    0.    0.507 0.001 0.    0.491]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0002
  Image Loss: 0.1524
  Total Loss: 0.1675
  Image grad max: 0.09267663210630417
  Output probs: [[0.    0.    0.    0.    0.    0.    0.495 0.001 0.    0.504]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0001
  Image Loss: 0.1527
  Total Loss: 0.1666
  Image grad max: 0.04911818727850914
  Output probs: [[0.    0.    0.    0.    0.    0.    0.487 0.001 0.    0.512]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0002
  Image Loss: 0.1529
  Total Loss: 0.1693
  Image grad max: 0.14356876909732819
  Output probs: [[0.    0.    0.    0.    0.    0.    0.488 0.001 0.    0.511]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0002
  Image Loss: 0.1531
  Total Loss: 0.1689
  Image grad max: 0.13477283716201782
  Output probs: [[0.    0.    0.    0.    0.    0.    0.496 0.001 0.    0.503]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0001
  Image Loss: 0.1533
  Total Loss: 0.1663
  Image grad max: 0.04163844510912895
  Output probs: [[0.    0.    0.    0.    0.    0.    0.505 0.001 0.    0.493]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0001
  Image Loss: 0.1534
  Total Loss: 0.1665
  Image grad max: 0.06768175214529037
  Output probs: [[0.    0.    0.    0.    0.    0.    0.51  0.001 0.    0.489]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0001
  Image Loss: 0.1535
  Total Loss: 0.1678
  Image grad max: 0.12056471407413483
  Output probs: [[0.    0.    0.    0.    0.    0.    0.507 0.001 0.    0.492]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0001
  Image Loss: 0.1536
  Total Loss: 0.1665
  Image grad max: 0.08795692771673203
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.001 0.    0.5  ]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0001
  Image Loss: 0.1537
  Total Loss: 0.1650
  Image grad max: 0.015663862228393555
  Output probs: [[0.    0.    0.    0.    0.    0.    0.492 0.001 0.    0.507]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0001
  Image Loss: 0.1537
  Total Loss: 0.1658
  Image grad max: 0.08881857991218567
  Output probs: [[0.    0.    0.    0.    0.    0.    0.491 0.001 0.    0.508]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0001
  Image Loss: 0.1537
  Total Loss: 0.1655
  Image grad max: 0.09415460377931595
  Output probs: [[0.    0.    0.    0.    0.    0.    0.497 0.001 0.    0.502]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0001
  Image Loss: 0.1537
  Total Loss: 0.1639
  Image grad max: 0.028400620445609093
  Output probs: [[0.    0.    0.    0.    0.    0.    0.504 0.001 0.    0.495]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0001
  Image Loss: 0.1537
  Total Loss: 0.1638
  Image grad max: 0.051572345197200775
  Output probs: [[0.    0.    0.    0.    0.    0.    0.507 0.001 0.    0.492]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0001
  Image Loss: 0.1537
  Total Loss: 0.1640
  Image grad max: 0.08169548958539963
  Output probs: [[0.    0.    0.    0.    0.    0.    0.503 0.001 0.    0.496]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0001
  Image Loss: 0.1537
  Total Loss: 0.1628
  Image grad max: 0.043220967054367065
  Output probs: [[0.    0.    0.    0.    0.    0.    0.497 0.001 0.    0.502]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0001
  Image Loss: 0.1536
  Total Loss: 0.1622
  Image grad max: 0.030438603833317757
  Output probs: [[0.    0.    0.    0.    0.    0.    0.494 0.001 0.    0.506]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0001
  Image Loss: 0.1535
  Total Loss: 0.1624
  Image grad max: 0.07065288722515106
  Output probs: [[0.    0.    0.    0.    0.    0.    0.496 0.001 0.    0.503]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0001
  Image Loss: 0.1534
  Total Loss: 0.1615
  Image grad max: 0.04453670606017113
  Output probs: [[0.    0.    0.    0.    0.    0.    0.501 0.001 0.    0.498]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0001
  Image Loss: 0.1534
  Total Loss: 0.1608
  Image grad max: 0.017524749040603638
  Output probs: [[0.    0.    0.    0.    0.    0.    0.504 0.001 0.    0.495]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0001
  Image Loss: 0.1533
  Total Loss: 0.1608
  Image grad max: 0.05515412613749504
  Output probs: [[0.    0.    0.    0.    0.    0.    0.503 0.001 0.    0.496]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0001
  Image Loss: 0.1532
  Total Loss: 0.1602
  Image grad max: 0.03864474967122078
  Output probs: [[0.    0.    0.    0.    0.    0.    0.498 0.001 0.    0.501]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0001
  Image Loss: 0.1531
  Total Loss: 0.1596
  Image grad max: 0.014758557081222534
  Output probs: [[0.    0.    0.    0.    0.    0.    0.496 0.001 0.    0.504]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0001
  Image Loss: 0.1529
  Total Loss: 0.1595
  Image grad max: 0.04947587475180626
  Output probs: [[0.    0.    0.    0.    0.    0.    0.497 0.001 0.    0.502]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0001
  Image Loss: 0.1528
  Total Loss: 0.1589
  Image grad max: 0.032276663929224014
  Output probs: [[0.    0.    0.    0.    0.    0.    0.501 0.    0.    0.498]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0001
  Image Loss: 0.1527
  Total Loss: 0.1584
  Image grad max: 0.015161854214966297
  Output probs: [[0.    0.    0.    0.    0.    0.    0.503 0.    0.    0.496]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0001
  Image Loss: 0.1526
  Total Loss: 0.1582
  Image grad max: 0.040913719683885574
  Output probs: [[0.    0.    0.    0.    0.    0.    0.502 0.    0.    0.498]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0001
  Image Loss: 0.1524
  Total Loss: 0.1577
  Image grad max: 0.020821135491132736
  Output probs: [[0.    0.    0.    0.    0.    0.    0.498 0.    0.    0.501]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0000
  Image Loss: 0.1523
  Total Loss: 0.1573
  Image grad max: 0.020189957693219185
  Output probs: [[0.    0.    0.    0.    0.    0.    0.497 0.    0.    0.503]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0000
  Image Loss: 0.1521
  Total Loss: 0.1571
  Image grad max: 0.03643573448061943
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.501]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0000
  Image Loss: 0.1520
  Total Loss: 0.1566
  Image grad max: 0.012595058418810368
  Output probs: [[0.    0.    0.    0.    0.    0.    0.502 0.    0.    0.498]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0000
  Image Loss: 0.1518
  Total Loss: 0.1563
  Image grad max: 0.022000815719366074
  Output probs: [[0.    0.    0.    0.    0.    0.    0.502 0.    0.    0.497]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0000
  Image Loss: 0.1517
  Total Loss: 0.1560
  Image grad max: 0.02752833254635334
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0000
  Image Loss: 0.1515
  Total Loss: 0.1556
  Image grad max: 0.007027157582342625
  Output probs: [[0.    0.    0.    0.    0.    0.    0.498 0.    0.    0.502]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0000
  Image Loss: 0.1514
  Total Loss: 0.1553
  Image grad max: 0.02538112923502922
  Output probs: [[0.    0.    0.    0.    0.    0.    0.498 0.    0.    0.501]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0000
  Image Loss: 0.1512
  Total Loss: 0.1550
  Image grad max: 0.020265793427824974
  Output probs: [[0.    0.    0.    0.    0.    0.    0.5   0.    0.    0.499]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0000
  Image Loss: 0.1511
  Total Loss: 0.1547
  Image grad max: 0.007647065911442041
  Output probs: [[0.    0.    0.    0.    0.    0.    0.502 0.    0.    0.498]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0000
  Image Loss: 0.1509
  Total Loss: 0.1544
  Image grad max: 0.022455798462033272
  Output probs: [[0.    0.    0.    0.    0.    0.    0.501 0.    0.    0.499]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0000
  Image Loss: 0.1507
  Total Loss: 0.1541
  Image grad max: 0.00812677014619112
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.501]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0000
  Image Loss: 0.1506
  Total Loss: 0.1538
  Image grad max: 0.01574096269905567
  Output probs: [[0.    0.    0.    0.    0.    0.    0.498 0.    0.    0.501]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0000
  Image Loss: 0.1504
  Total Loss: 0.1536
  Image grad max: 0.01880144327878952
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0000
  Image Loss: 0.1502
  Total Loss: 0.1532
  Image grad max: 0.005746303126215935
  Output probs: [[0.    0.    0.    0.    0.    0.    0.501 0.    0.    0.498]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0000
  Image Loss: 0.1501
  Total Loss: 0.1530
  Image grad max: 0.016374381259083748
  Output probs: [[0.    0.    0.    0.    0.    0.    0.501 0.    0.    0.499]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0000
  Image Loss: 0.1499
  Total Loss: 0.1527
  Image grad max: 0.008735176175832748
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.501]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0000
  Image Loss: 0.1497
  Total Loss: 0.1525
  Image grad max: 0.010593637824058533
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.501]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0000
  Image Loss: 0.1496
  Total Loss: 0.1522
  Image grad max: 0.015212913043797016
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0000
  Image Loss: 0.1494
  Total Loss: 0.1520
  Image grad max: 0.005250262096524239
  Output probs: [[0.    0.    0.    0.    0.    0.    0.501 0.    0.    0.499]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0000
  Image Loss: 0.1492
  Total Loss: 0.1517
  Image grad max: 0.01232845988124609
  Output probs: [[0.    0.    0.    0.    0.    0.    0.5   0.    0.    0.499]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0000
  Image Loss: 0.1491
  Total Loss: 0.1515
  Image grad max: 0.006493268068879843
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.501]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0000
  Image Loss: 0.1489
  Total Loss: 0.1512
  Image grad max: 0.008714946918189526
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.501]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0000
  Image Loss: 0.1487
  Total Loss: 0.1510
  Image grad max: 0.011663094162940979
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0000
  Image Loss: 0.1486
  Total Loss: 0.1508
  Image grad max: 0.004767495207488537
  Output probs: [[0.    0.    0.    0.    0.    0.    0.501 0.    0.    0.499]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0000
  Image Loss: 0.1484
  Total Loss: 0.1506
  Image grad max: 0.009750796481966972
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0000
  Image Loss: 0.1482
  Total Loss: 0.1503
  Image grad max: 0.004809706471860409
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.501]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0000
  Image Loss: 0.1480
  Total Loss: 0.1501
  Image grad max: 0.008150966838002205
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.501]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0000
  Image Loss: 0.1479
  Total Loss: 0.1499
  Image grad max: 0.008203859440982342
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0000
  Image Loss: 0.1477
  Total Loss: 0.1496
  Image grad max: 0.004340684041380882
  Output probs: [[0.    0.    0.    0.    0.    0.    0.501 0.    0.    0.499]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0000
  Image Loss: 0.1475
  Total Loss: 0.1494
  Image grad max: 0.007440911140292883
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0000
  Image Loss: 0.1473
  Total Loss: 0.1492
  Image grad max: 0.004300212487578392
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.5  ]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0000
  Image Loss: 0.1472
  Total Loss: 0.1490
  Image grad max: 0.007685844786465168
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0000
  Image Loss: 0.1470
  Total Loss: 0.1488
  Image grad max: 0.004796330351382494
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0000
  Image Loss: 0.1468
  Total Loss: 0.1486
  Image grad max: 0.004508340731263161
  Output probs: [[0.    0.    0.    0.    0.    0.    0.5   0.    0.    0.499]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0000
  Image Loss: 0.1467
  Total Loss: 0.1484
  Image grad max: 0.004951404873281717
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0000
  Image Loss: 0.1465
  Total Loss: 0.1481
  Image grad max: 0.004186663776636124
  Output probs: [[0.    0.    0.    0.    0.    0.    0.499 0.    0.    0.5  ]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0000
  Image Loss: 0.1463
  Total Loss: 0.1479
  Image grad max: 0.00647391751408577
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0000
  Image Loss: 0.1461
  Total Loss: 0.1477
  Image grad max: 0.00404583103954792
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0000
  Image Loss: 0.1460
  Total Loss: 0.1475
  Image grad max: 0.004512853920459747
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0000
  Image Loss: 0.1458
  Total Loss: 0.1473
  Image grad max: 0.0037776418030261993
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0000
  Image Loss: 0.1456
  Total Loss: 0.1471
  Image grad max: 0.004327712580561638
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0000
  Image Loss: 0.1454
  Total Loss: 0.1469
  Image grad max: 0.004371169023215771
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0000
  Image Loss: 0.1453
  Total Loss: 0.1467
  Image grad max: 0.003698793239891529
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0000
  Image Loss: 0.1451
  Total Loss: 0.1465
  Image grad max: 0.004015150014311075
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0000
  Image Loss: 0.1449
  Total Loss: 0.1463
  Image grad max: 0.003758841659873724
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0000
  Image Loss: 0.1447
  Total Loss: 0.1461
  Image grad max: 0.004342151805758476
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0000
  Image Loss: 0.1446
  Total Loss: 0.1459
  Image grad max: 0.003717482555657625
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0000
  Image Loss: 0.1444
  Total Loss: 0.1457
  Image grad max: 0.0036944302264600992
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0000
  Image Loss: 0.1442
  Total Loss: 0.1455
  Image grad max: 0.003512829774990678
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0000
  Image Loss: 0.1440
  Total Loss: 0.1453
  Image grad max: 0.003718916093930602
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0000
  Image Loss: 0.1439
  Total Loss: 0.1451
  Image grad max: 0.003680302994325757
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0000
  Image Loss: 0.1437
  Total Loss: 0.1449
  Image grad max: 0.003428863827139139
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0000
  Image Loss: 0.1435
  Total Loss: 0.1448
  Image grad max: 0.0034868812654167414
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0000
  Image Loss: 0.1434
  Total Loss: 0.1446
  Image grad max: 0.0035229146014899015
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0000
  Image Loss: 0.1432
  Total Loss: 0.1444
  Image grad max: 0.0035850531421601772
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0000
  Image Loss: 0.1430
  Total Loss: 0.1442
  Image grad max: 0.003403443144634366
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0000
  Image Loss: 0.1428
  Total Loss: 0.1440
  Image grad max: 0.0034319127444177866
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0000
  Image Loss: 0.1427
  Total Loss: 0.1438
  Image grad max: 0.0033573415130376816
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0000
  Image Loss: 0.1425
  Total Loss: 0.1436
  Image grad max: 0.0034692122135311365
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0000
  Image Loss: 0.1423
  Total Loss: 0.1434
  Image grad max: 0.0033612227998673916
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0000
  Image Loss: 0.1421
  Total Loss: 0.1432
  Image grad max: 0.0033453190699219704
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0000
  Image Loss: 0.1420
  Total Loss: 0.1430
  Image grad max: 0.0032425601966679096
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0000
  Image Loss: 0.1418
  Total Loss: 0.1428
  Image grad max: 0.003349528182297945
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0000
  Image Loss: 0.1416
  Total Loss: 0.1427
  Image grad max: 0.003299248870462179
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0000
  Image Loss: 0.1415
  Total Loss: 0.1425
  Image grad max: 0.003258573357015848
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0000
  Image Loss: 0.1413
  Total Loss: 0.1423
  Image grad max: 0.003235655603930354
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0000
  Image Loss: 0.1411
  Total Loss: 0.1421
  Image grad max: 0.0032396044116467237
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0000
  Image Loss: 0.1409
  Total Loss: 0.1419
  Image grad max: 0.0032259479630738497
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0000
  Image Loss: 0.1408
  Total Loss: 0.1417
  Image grad max: 0.0031838405411690474
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0000
  Image Loss: 0.1406
  Total Loss: 0.1415
  Image grad max: 0.003208867972716689
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0000
  Image Loss: 0.1404
  Total Loss: 0.1414
  Image grad max: 0.0031425864435732365
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0000
  Image Loss: 0.1403
  Total Loss: 0.1412
  Image grad max: 0.0031737107783555984
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0000
  Image Loss: 0.1401
  Total Loss: 0.1410
  Image grad max: 0.0031309782061725855
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0000
  Image Loss: 0.1399
  Total Loss: 0.1408
  Image grad max: 0.003176252357661724
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0000
  Image Loss: 0.1397
  Total Loss: 0.1406
  Image grad max: 0.0031054464634507895
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0000
  Image Loss: 0.1396
  Total Loss: 0.1404
  Image grad max: 0.003147238865494728
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0000
  Image Loss: 0.1394
  Total Loss: 0.1403
  Image grad max: 0.0030968869104981422
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0000
  Image Loss: 0.1392
  Total Loss: 0.1401
  Image grad max: 0.003140291664749384
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0000
  Image Loss: 0.1391
  Total Loss: 0.1399
  Image grad max: 0.003077399916946888
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0000
  Image Loss: 0.1389
  Total Loss: 0.1397
  Image grad max: 0.003119841916486621
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0000
  Image Loss: 0.1387
  Total Loss: 0.1395
  Image grad max: 0.0030667902901768684
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0000
  Image Loss: 0.1385
  Total Loss: 0.1394
  Image grad max: 0.0031092665158212185
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0000
  Image Loss: 0.1384
  Total Loss: 0.1392
  Image grad max: 0.0030583315528929234
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0000
  Image Loss: 0.1382
  Total Loss: 0.1390
  Image grad max: 0.003096861531957984
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0000
  Image Loss: 0.1380
  Total Loss: 0.1388
  Image grad max: 0.003049140330404043
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0000
  Image Loss: 0.1379
  Total Loss: 0.1386
  Image grad max: 0.003079898888245225
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0000
  Image Loss: 0.1377
  Total Loss: 0.1385
  Image grad max: 0.003049789695069194
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0000
  Image Loss: 0.1375
  Total Loss: 0.1383
  Image grad max: 0.003072802210226655
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0000
  Image Loss: 0.1373
  Total Loss: 0.1381
  Image grad max: 0.0030335746705532074
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0000
  Image Loss: 0.1372
  Total Loss: 0.1379
  Image grad max: 0.003050791332498193
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0000
  Image Loss: 0.1370
  Total Loss: 0.1377
  Image grad max: 0.0030407863669097424
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0000
  Image Loss: 0.1368
  Total Loss: 0.1376
  Image grad max: 0.003048545680940151
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0000
  Image Loss: 0.1367
  Total Loss: 0.1374
  Image grad max: 0.00302313850261271
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0000
  Image Loss: 0.1365
  Total Loss: 0.1372
  Image grad max: 0.003018181538209319
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0000
  Image Loss: 0.1363
  Total Loss: 0.1370
  Image grad max: 0.0030289823189377785
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0000
  Image Loss: 0.1362
  Total Loss: 0.1369
  Image grad max: 0.0030239890329539776
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0000
  Image Loss: 0.1360
  Total Loss: 0.1367
  Image grad max: 0.003006432205438614
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0000
  Image Loss: 0.1358
  Total Loss: 0.1365
  Image grad max: 0.0029961324762552977
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0000
  Image Loss: 0.1357
  Total Loss: 0.1363
  Image grad max: 0.0030211422126740217
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0000
  Image Loss: 0.1355
  Total Loss: 0.1361
  Image grad max: 0.0030068422202020884
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0000
  Image Loss: 0.1353
  Total Loss: 0.1360
  Image grad max: 0.002991971094161272
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0000
  Image Loss: 0.1352
  Total Loss: 0.1358
  Image grad max: 0.0029797463212162256
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0000
  Image Loss: 0.1350
  Total Loss: 0.1356
  Image grad max: 0.003006461774930358
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0000
  Image Loss: 0.1348
  Total Loss: 0.1354
  Image grad max: 0.002984829479828477
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0000
  Image Loss: 0.1346
  Total Loss: 0.1353
  Image grad max: 0.0029755972791463137
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0000
  Image Loss: 0.1345
  Total Loss: 0.1351
  Image grad max: 0.0029765916988253593
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0000
  Image Loss: 0.1343
  Total Loss: 0.1349
  Image grad max: 0.002993622561916709
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0000
  Image Loss: 0.1341
  Total Loss: 0.1348
  Image grad max: 0.002965166000649333
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0000
  Image Loss: 0.1340
  Total Loss: 0.1346
  Image grad max: 0.002956921933218837
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0000
  Image Loss: 0.1338
  Total Loss: 0.1344
  Image grad max: 0.0029729597736150026
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0000
  Image Loss: 0.1336
  Total Loss: 0.1342
  Image grad max: 0.0029771681874990463
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0000
  Image Loss: 0.1335
  Total Loss: 0.1341
  Image grad max: 0.0029475162737071514
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0000
  Image Loss: 0.1333
  Total Loss: 0.1339
  Image grad max: 0.002951098373159766
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0000
  Image Loss: 0.1331
  Total Loss: 0.1337
  Image grad max: 0.0029679033905267715
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0000
  Image Loss: 0.1330
  Total Loss: 0.1335
  Image grad max: 0.002957430435344577
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0000
  Image Loss: 0.1328
  Total Loss: 0.1334
  Image grad max: 0.002934859599918127
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0000
  Image Loss: 0.1326
  Total Loss: 0.1332
  Image grad max: 0.0029481432866305113
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0000
  Image Loss: 0.1325
  Total Loss: 0.1330
  Image grad max: 0.0029637969564646482
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0000
  Image Loss: 0.1323
  Total Loss: 0.1329
  Image grad max: 0.002937604673206806
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0000
  Image Loss: 0.1321
  Total Loss: 0.1327
  Image grad max: 0.0029267005156725645
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0000
  Image Loss: 0.1320
  Total Loss: 0.1325
  Image grad max: 0.0029485372360795736
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0000
  Image Loss: 0.1318
  Total Loss: 0.1323
  Image grad max: 0.0029492296744138002
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0000
  Image Loss: 0.1316
  Total Loss: 0.1322
  Image grad max: 0.0029211966320872307
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0000
  Image Loss: 0.1315
  Total Loss: 0.1320
  Image grad max: 0.0029239647556096315
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0000
  Image Loss: 0.1313
  Total Loss: 0.1318
  Image grad max: 0.0029440075159072876
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0000
  Image Loss: 0.1312
  Total Loss: 0.1317
  Image grad max: 0.0029264797922223806
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0000
  Image Loss: 0.1310
  Total Loss: 0.1315
  Image grad max: 0.002908809343352914
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0000
  Image Loss: 0.1308
  Total Loss: 0.1313
  Image grad max: 0.0029307259246706963
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0000
  Image Loss: 0.1307
  Total Loss: 0.1311
  Image grad max: 0.002929680747911334
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0000
  Image Loss: 0.1305
  Total Loss: 0.1310
  Image grad max: 0.002902392530813813
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0000
  Image Loss: 0.1303
  Total Loss: 0.1308
  Image grad max: 0.002914952114224434
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0000
  Image Loss: 0.1302
  Total Loss: 0.1306
  Image grad max: 0.0029251263476908207
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0000
  Image Loss: 0.1300
  Total Loss: 0.1305
  Image grad max: 0.002904827706515789
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0000
  Image Loss: 0.1298
  Total Loss: 0.1303
  Image grad max: 0.0028977999463677406
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0000
  Image Loss: 0.1297
  Total Loss: 0.1301
  Image grad max: 0.002917899750173092
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0000
  Image Loss: 0.1295
  Total Loss: 0.1300
  Image grad max: 0.0029047238640487194
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0000
  Image Loss: 0.1293
  Total Loss: 0.1298
  Image grad max: 0.0028926485683768988
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0000
  Image Loss: 0.1292
  Total Loss: 0.1296
  Image grad max: 0.002901047933846712
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0000
  Image Loss: 0.1290
  Total Loss: 0.1295
  Image grad max: 0.0029062845278531313
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0000
  Image Loss: 0.1289
  Total Loss: 0.1293
  Image grad max: 0.002888562623411417
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0000
  Image Loss: 0.1287
  Total Loss: 0.1291
  Image grad max: 0.002890181029215455
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0000
  Image Loss: 0.1285
  Total Loss: 0.1290
  Image grad max: 0.0028965044766664505
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0000
  Image Loss: 0.1284
  Total Loss: 0.1288
  Image grad max: 0.0028874455019831657
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0000
  Image Loss: 0.1282
  Total Loss: 0.1286
  Image grad max: 0.0028807464987039566
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0000
  Image Loss: 0.1280
  Total Loss: 0.1285
  Image grad max: 0.002889564959332347
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0000
  Image Loss: 0.1279
  Total Loss: 0.1283
  Image grad max: 0.0028816224075853825
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0000
  Image Loss: 0.1277
  Total Loss: 0.1281
  Image grad max: 0.0028769734781235456
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0000
  Image Loss: 0.1276
  Total Loss: 0.1280
  Image grad max: 0.002876081271097064
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0000
  Image Loss: 0.1274
  Total Loss: 0.1278
  Image grad max: 0.0028806570917367935
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0000
  Image Loss: 0.1272
  Total Loss: 0.1276
  Image grad max: 0.0028702348936349154
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0000
  Image Loss: 0.1271
  Total Loss: 0.1275
  Image grad max: 0.0028685606084764004
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0000
  Image Loss: 0.1269
  Total Loss: 0.1273
  Image grad max: 0.0028739054687321186
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0000
  Image Loss: 0.1267
  Total Loss: 0.1271
  Image grad max: 0.002864746144041419
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0000
  Image Loss: 0.1266
  Total Loss: 0.1270
  Image grad max: 0.0028626052662730217
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0000
  Image Loss: 0.1264
  Total Loss: 0.1268
  Image grad max: 0.0028660667594522238
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0000
  Image Loss: 0.1263
  Total Loss: 0.1266
  Image grad max: 0.0028598769567906857
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0000
  Image Loss: 0.1261
  Total Loss: 0.1265
  Image grad max: 0.0028583533130586147
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0000
  Image Loss: 0.1259
  Total Loss: 0.1263
  Image grad max: 0.0028546571265906096
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0000
  Image Loss: 0.1258
  Total Loss: 0.1261
  Image grad max: 0.002858723048120737
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0000
  Image Loss: 0.1256
  Total Loss: 0.1260
  Image grad max: 0.0028491299599409103
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0000
  Image Loss: 0.1255
  Total Loss: 0.1258
  Image grad max: 0.002850712276995182
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0000
  Image Loss: 0.1253
  Total Loss: 0.1257
  Image grad max: 0.0028488801326602697
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0000
  Image Loss: 0.1251
  Total Loss: 0.1255
  Image grad max: 0.0028470444958657026
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0000
  Image Loss: 0.1250
  Total Loss: 0.1253
  Image grad max: 0.0028384020552039146
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0000
  Image Loss: 0.1248
  Total Loss: 0.1252
  Image grad max: 0.0028458512388169765
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0000
  Image Loss: 0.1247
  Total Loss: 0.1250
  Image grad max: 0.0028409252408891916
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0000
  Image Loss: 0.1245
  Total Loss: 0.1248
  Image grad max: 0.0028355370741337538
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0000
  Image Loss: 0.1243
  Total Loss: 0.1247
  Image grad max: 0.0028344769962131977
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0000
  Image Loss: 0.1242
  Total Loss: 0.1245
  Image grad max: 0.0028380444273352623
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0000
  Image Loss: 0.1240
  Total Loss: 0.1244
  Image grad max: 0.002828495344147086
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0000
  Image Loss: 0.1239
  Total Loss: 0.1242
  Image grad max: 0.002829439239576459
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0000
  Image Loss: 0.1237
  Total Loss: 0.1240
  Image grad max: 0.00283237942494452
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0000
  Image Loss: 0.1235
  Total Loss: 0.1239
  Image grad max: 0.00282191950827837
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0000
  Image Loss: 0.1234
  Total Loss: 0.1237
  Image grad max: 0.002823932096362114
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0000
  Image Loss: 0.1232
  Total Loss: 0.1235
  Image grad max: 0.002825017087161541
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0000
  Image Loss: 0.1231
  Total Loss: 0.1234
  Image grad max: 0.0028168796561658382
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0000
  Image Loss: 0.1229
  Total Loss: 0.1232
  Image grad max: 0.0028193413745611906
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0000
  Image Loss: 0.1227
  Total Loss: 0.1231
  Image grad max: 0.0028158091008663177
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0000
  Image Loss: 0.1226
  Total Loss: 0.1229
  Image grad max: 0.0028142749797552824
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0000
  Image Loss: 0.1224
  Total Loss: 0.1227
  Image grad max: 0.0028098192997276783
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0000
  Image Loss: 0.1223
  Total Loss: 0.1226
  Image grad max: 0.002815025392919779
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0000
  Image Loss: 0.1221
  Total Loss: 0.1224
  Image grad max: 0.0028027622029185295
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0000
  Image Loss: 0.1219
  Total Loss: 0.1222
  Image grad max: 0.002806586679071188
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0000
  Image Loss: 0.1218
  Total Loss: 0.1221
  Image grad max: 0.0028093308210372925
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0000
  Image Loss: 0.1216
  Total Loss: 0.1219
  Image grad max: 0.002799367066472769
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0000
  Image Loss: 0.1215
  Total Loss: 0.1218
  Image grad max: 0.0027967446949332952
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0000
  Image Loss: 0.1213
  Total Loss: 0.1216
  Image grad max: 0.002805288415402174
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0000
  Image Loss: 0.1212
  Total Loss: 0.1214
  Image grad max: 0.00279135606251657
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0000
  Image Loss: 0.1210
  Total Loss: 0.1213
  Image grad max: 0.002797142369672656
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0000
  Image Loss: 0.1208
  Total Loss: 0.1211
  Image grad max: 0.0027945239562541246
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0000
  Image Loss: 0.1207
  Total Loss: 0.1210
  Image grad max: 0.0027865564916282892
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0000
  Image Loss: 0.1205
  Total Loss: 0.1208
  Image grad max: 0.002792184706777334
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0000
  Image Loss: 0.1204
  Total Loss: 0.1206
  Image grad max: 0.0027860517147928476
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0000
  Image Loss: 0.1202
  Total Loss: 0.1205
  Image grad max: 0.0027822130359709263
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0000
  Image Loss: 0.1200
  Total Loss: 0.1203
  Image grad max: 0.002787673380225897
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0000
  Image Loss: 0.1199
  Total Loss: 0.1202
  Image grad max: 0.0027766588609665632
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0000
  Image Loss: 0.1197
  Total Loss: 0.1200
  Image grad max: 0.0027799783274531364
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0000
  Image Loss: 0.1196
  Total Loss: 0.1199
  Image grad max: 0.002780698938295245
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0000
  Image Loss: 0.1194
  Total Loss: 0.1197
  Image grad max: 0.002768327249214053
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0000
  Image Loss: 0.1193
  Total Loss: 0.1195
  Image grad max: 0.0027819941751658916
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0000
  Image Loss: 0.1191
  Total Loss: 0.1194
  Image grad max: 0.0027644538786262274
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0000
  Image Loss: 0.1190
  Total Loss: 0.1192
  Image grad max: 0.0027712576556950808
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0000
  Image Loss: 0.1188
  Total Loss: 0.1191
  Image grad max: 0.0027701559010893106
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0000
  Image Loss: 0.1186
  Total Loss: 0.1189
  Image grad max: 0.0027622065972536802
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0000
  Image Loss: 0.1185
  Total Loss: 0.1187
  Image grad max: 0.0027668701950460672
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0000
  Image Loss: 0.1183
  Total Loss: 0.1186
  Image grad max: 0.002759078983217478
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0000
  Image Loss: 0.1182
  Total Loss: 0.1184
  Image grad max: 0.0027619008906185627
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0000
  Image Loss: 0.1180
  Total Loss: 0.1183
  Image grad max: 0.0027583392802625895
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0000
  Image Loss: 0.1179
  Total Loss: 0.1181
  Image grad max: 0.002752663567662239
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0000
  Image Loss: 0.1177
  Total Loss: 0.1180
  Image grad max: 0.002761094830930233
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0000
  Image Loss: 0.1175
  Total Loss: 0.1178
  Image grad max: 0.0027458583936095238
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0000
  Image Loss: 0.1174
  Total Loss: 0.1176
  Image grad max: 0.0027535175904631615
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0000
  Image Loss: 0.1172
  Total Loss: 0.1175
  Image grad max: 0.002750111510977149
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0000
  Image Loss: 0.1171
  Total Loss: 0.1173
  Image grad max: 0.0027400406543165445
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0000
  Image Loss: 0.1169
  Total Loss: 0.1172
  Image grad max: 0.0027548116631805897
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0000
  Image Loss: 0.1168
  Total Loss: 0.1170
  Image grad max: 0.0027348967269062996
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0000
  Image Loss: 0.1166
  Total Loss: 0.1169
  Image grad max: 0.002742852782830596
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0000
  Image Loss: 0.1165
  Total Loss: 0.1167
  Image grad max: 0.0027450392954051495
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0000
  Image Loss: 0.1163
  Total Loss: 0.1165
  Image grad max: 0.002726352773606777
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0000
  Image Loss: 0.1161
  Total Loss: 0.1164
  Image grad max: 0.0027463920414447784
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0000
  Image Loss: 0.1160
  Total Loss: 0.1162
  Image grad max: 0.002727413084357977
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0000
  Image Loss: 0.1158
  Total Loss: 0.1161
  Image grad max: 0.00272960914298892
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0000
  Image Loss: 0.1157
  Total Loss: 0.1159
  Image grad max: 0.002738130046054721
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0000
  Image Loss: 0.1155
  Total Loss: 0.1158
  Image grad max: 0.002716449787840247
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0000
  Image Loss: 0.1154
  Total Loss: 0.1156
  Image grad max: 0.002737349597737193
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0000
  Image Loss: 0.1152
  Total Loss: 0.1154
  Image grad max: 0.0027162833139300346
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0000
  Image Loss: 0.1151
  Total Loss: 0.1153
  Image grad max: 0.0027241806965321302
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0000
  Image Loss: 0.1149
  Total Loss: 0.1151
  Image grad max: 0.002723628655076027
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0000
  Image Loss: 0.1148
  Total Loss: 0.1150
  Image grad max: 0.002713592955842614
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0000
  Image Loss: 0.1146
  Total Loss: 0.1148
  Image grad max: 0.0027202689088881016
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0000
  Image Loss: 0.1145
  Total Loss: 0.1147
  Image grad max: 0.0027186674997210503
  Output probs: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0000
  Image Loss: 0.1143
  Total Loss: 0.1145
  Image grad max: 0.00270111788995564
Adversarial example training visualization saved to adversarial_figures/adversarial_training.png
Adversarial example training visualization saved to adversarial_figures/adversarial_testing.png
Target label was: [[0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5]]
