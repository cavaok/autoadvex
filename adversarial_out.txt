running dynamical.py
Epoch [1/30], Batch [0/6000], Loss: 0.2138
Epoch [1/30], Batch [100/6000], Loss: 0.1836
Epoch [1/30], Batch [200/6000], Loss: 0.1431
Epoch [1/30], Batch [300/6000], Loss: 0.0979
Epoch [1/30], Batch [400/6000], Loss: 0.1145
Epoch [1/30], Batch [500/6000], Loss: 0.0812
Epoch [1/30], Batch [600/6000], Loss: 0.0796
Epoch [1/30], Batch [700/6000], Loss: 0.1176
Epoch [1/30], Batch [800/6000], Loss: 0.0589
Epoch [1/30], Batch [900/6000], Loss: 0.0755
Epoch [1/30], Batch [1000/6000], Loss: 0.0718
Epoch [1/30], Batch [1100/6000], Loss: 0.0625
Epoch [1/30], Batch [1200/6000], Loss: 0.0764
Epoch [1/30], Batch [1300/6000], Loss: 0.0616
Epoch [1/30], Batch [1400/6000], Loss: 0.0574
Epoch [1/30], Batch [1500/6000], Loss: 0.0594
Epoch [1/30], Batch [1600/6000], Loss: 0.0836
Epoch [1/30], Batch [1700/6000], Loss: 0.0693
Epoch [1/30], Batch [1800/6000], Loss: 0.0493
Epoch [1/30], Batch [1900/6000], Loss: 0.0589
Epoch [1/30], Batch [2000/6000], Loss: 0.0810
Epoch [1/30], Batch [2100/6000], Loss: 0.0617
Epoch [1/30], Batch [2200/6000], Loss: 0.0733
Epoch [1/30], Batch [2300/6000], Loss: 0.0675
Epoch [1/30], Batch [2400/6000], Loss: 0.0534
Epoch [1/30], Batch [2500/6000], Loss: 0.0737
Epoch [1/30], Batch [2600/6000], Loss: 0.0540
Epoch [1/30], Batch [2700/6000], Loss: 0.0759
Epoch [1/30], Batch [2800/6000], Loss: 0.0708
Epoch [1/30], Batch [2900/6000], Loss: 0.0687
Epoch [1/30], Batch [3000/6000], Loss: 0.0731
Epoch [1/30], Batch [3100/6000], Loss: 0.0591
Epoch [1/30], Batch [3200/6000], Loss: 0.0419
Epoch [1/30], Batch [3300/6000], Loss: 0.0458
Epoch [1/30], Batch [3400/6000], Loss: 0.0671
Epoch [1/30], Batch [3500/6000], Loss: 0.0585
Epoch [1/30], Batch [3600/6000], Loss: 0.0374
Epoch [1/30], Batch [3700/6000], Loss: 0.0645
Epoch [1/30], Batch [3800/6000], Loss: 0.0405
Epoch [1/30], Batch [3900/6000], Loss: 0.0816
Epoch [1/30], Batch [4000/6000], Loss: 0.0418
Epoch [1/30], Batch [4100/6000], Loss: 0.0489
Epoch [1/30], Batch [4200/6000], Loss: 0.1036
Epoch [1/30], Batch [4300/6000], Loss: 0.0489
Epoch [1/30], Batch [4400/6000], Loss: 0.0482
Epoch [1/30], Batch [4500/6000], Loss: 0.0389
Epoch [1/30], Batch [4600/6000], Loss: 0.0881
Epoch [1/30], Batch [4700/6000], Loss: 0.0998
Epoch [1/30], Batch [4800/6000], Loss: 0.0563
Epoch [1/30], Batch [4900/6000], Loss: 0.1004
Epoch [1/30], Batch [5000/6000], Loss: 0.0475
Epoch [1/30], Batch [5100/6000], Loss: 0.0597
Epoch [1/30], Batch [5200/6000], Loss: 0.0526
Epoch [1/30], Batch [5300/6000], Loss: 0.0534
Epoch [1/30], Batch [5400/6000], Loss: 0.0611
Epoch [1/30], Batch [5500/6000], Loss: 0.0509
Epoch [1/30], Batch [5600/6000], Loss: 0.0823
Epoch [1/30], Batch [5700/6000], Loss: 0.0535
Epoch [1/30], Batch [5800/6000], Loss: 0.0642
Epoch [1/30], Batch [5900/6000], Loss: 0.0488
Epoch [1/30], Loss: 0.0708
Epoch [2/30], Batch [0/6000], Loss: 0.0430
Epoch [2/30], Batch [100/6000], Loss: 0.0451
Epoch [2/30], Batch [200/6000], Loss: 0.0381
Epoch [2/30], Batch [300/6000], Loss: 0.0594
Epoch [2/30], Batch [400/6000], Loss: 0.0408
Epoch [2/30], Batch [500/6000], Loss: 0.0482
Epoch [2/30], Batch [600/6000], Loss: 0.0312
Epoch [2/30], Batch [700/6000], Loss: 0.0400
Epoch [2/30], Batch [800/6000], Loss: 0.1052
Epoch [2/30], Batch [900/6000], Loss: 0.0470
Epoch [2/30], Batch [1000/6000], Loss: 0.0457
Epoch [2/30], Batch [1100/6000], Loss: 0.0663
Epoch [2/30], Batch [1200/6000], Loss: 0.0489
Epoch [2/30], Batch [1300/6000], Loss: 0.0374
Epoch [2/30], Batch [1400/6000], Loss: 0.0438
Epoch [2/30], Batch [1500/6000], Loss: 0.0390
Epoch [2/30], Batch [1600/6000], Loss: 0.0315
Epoch [2/30], Batch [1700/6000], Loss: 0.0369
Epoch [2/30], Batch [1800/6000], Loss: 0.0401
Epoch [2/30], Batch [1900/6000], Loss: 0.0379
Epoch [2/30], Batch [2000/6000], Loss: 0.0402
Epoch [2/30], Batch [2100/6000], Loss: 0.0416
Epoch [2/30], Batch [2200/6000], Loss: 0.0452
Epoch [2/30], Batch [2300/6000], Loss: 0.0398
Epoch [2/30], Batch [2400/6000], Loss: 0.0390
Epoch [2/30], Batch [2500/6000], Loss: 0.0630
Epoch [2/30], Batch [2600/6000], Loss: 0.0411
Epoch [2/30], Batch [2700/6000], Loss: 0.0370
Epoch [2/30], Batch [2800/6000], Loss: 0.0458
Epoch [2/30], Batch [2900/6000], Loss: 0.0480
Epoch [2/30], Batch [3000/6000], Loss: 0.0436
Epoch [2/30], Batch [3100/6000], Loss: 0.0366
Epoch [2/30], Batch [3200/6000], Loss: 0.0426
Epoch [2/30], Batch [3300/6000], Loss: 0.0355
Epoch [2/30], Batch [3400/6000], Loss: 0.0306
Epoch [2/30], Batch [3500/6000], Loss: 0.0474
Epoch [2/30], Batch [3600/6000], Loss: 0.0358
Epoch [2/30], Batch [3700/6000], Loss: 0.0343
Epoch [2/30], Batch [3800/6000], Loss: 0.0301
Epoch [2/30], Batch [3900/6000], Loss: 0.0343
Epoch [2/30], Batch [4000/6000], Loss: 0.0483
Epoch [2/30], Batch [4100/6000], Loss: 0.0402
Epoch [2/30], Batch [4200/6000], Loss: 0.0505
Epoch [2/30], Batch [4300/6000], Loss: 0.0539
Epoch [2/30], Batch [4400/6000], Loss: 0.0514
Epoch [2/30], Batch [4500/6000], Loss: 0.0439
Epoch [2/30], Batch [4600/6000], Loss: 0.0358
Epoch [2/30], Batch [4700/6000], Loss: 0.0362
Epoch [2/30], Batch [4800/6000], Loss: 0.0262
Epoch [2/30], Batch [4900/6000], Loss: 0.0481
Epoch [2/30], Batch [5000/6000], Loss: 0.0474
Epoch [2/30], Batch [5100/6000], Loss: 0.0337
Epoch [2/30], Batch [5200/6000], Loss: 0.0533
Epoch [2/30], Batch [5300/6000], Loss: 0.0928
Epoch [2/30], Batch [5400/6000], Loss: 0.0657
Epoch [2/30], Batch [5500/6000], Loss: 0.0312
Epoch [2/30], Batch [5600/6000], Loss: 0.0451
Epoch [2/30], Batch [5700/6000], Loss: 0.0361
Epoch [2/30], Batch [5800/6000], Loss: 0.0461
Epoch [2/30], Batch [5900/6000], Loss: 0.0372
Epoch [2/30], Loss: 0.0451
Epoch [3/30], Batch [0/6000], Loss: 0.0349
Epoch [3/30], Batch [100/6000], Loss: 0.0316
Epoch [3/30], Batch [200/6000], Loss: 0.0332
Epoch [3/30], Batch [300/6000], Loss: 0.0480
Epoch [3/30], Batch [400/6000], Loss: 0.0388
Epoch [3/30], Batch [500/6000], Loss: 0.0381
Epoch [3/30], Batch [600/6000], Loss: 0.0284
Epoch [3/30], Batch [700/6000], Loss: 0.0421
Epoch [3/30], Batch [800/6000], Loss: 0.0356
Epoch [3/30], Batch [900/6000], Loss: 0.0702
Epoch [3/30], Batch [1000/6000], Loss: 0.0321
Epoch [3/30], Batch [1100/6000], Loss: 0.0278
Epoch [3/30], Batch [1200/6000], Loss: 0.0433
Epoch [3/30], Batch [1300/6000], Loss: 0.0316
Epoch [3/30], Batch [1400/6000], Loss: 0.0293
Epoch [3/30], Batch [1500/6000], Loss: 0.0453
Epoch [3/30], Batch [1600/6000], Loss: 0.0332
Epoch [3/30], Batch [1700/6000], Loss: 0.0388
Epoch [3/30], Batch [1800/6000], Loss: 0.0332
Epoch [3/30], Batch [1900/6000], Loss: 0.0328
Epoch [3/30], Batch [2000/6000], Loss: 0.0389
Epoch [3/30], Batch [2100/6000], Loss: 0.0507
Epoch [3/30], Batch [2200/6000], Loss: 0.0322
Epoch [3/30], Batch [2300/6000], Loss: 0.0354
Epoch [3/30], Batch [2400/6000], Loss: 0.0322
Epoch [3/30], Batch [2500/6000], Loss: 0.0635
Epoch [3/30], Batch [2600/6000], Loss: 0.0299
Epoch [3/30], Batch [2700/6000], Loss: 0.0430
Epoch [3/30], Batch [2800/6000], Loss: 0.0331
Epoch [3/30], Batch [2900/6000], Loss: 0.0358
Epoch [3/30], Batch [3000/6000], Loss: 0.0360
Epoch [3/30], Batch [3100/6000], Loss: 0.0317
Epoch [3/30], Batch [3200/6000], Loss: 0.0610
Epoch [3/30], Batch [3300/6000], Loss: 0.0307
Epoch [3/30], Batch [3400/6000], Loss: 0.0399
Epoch [3/30], Batch [3500/6000], Loss: 0.0353
Epoch [3/30], Batch [3600/6000], Loss: 0.0513
Epoch [3/30], Batch [3700/6000], Loss: 0.0371
Epoch [3/30], Batch [3800/6000], Loss: 0.0325
Epoch [3/30], Batch [3900/6000], Loss: 0.0288
Epoch [3/30], Batch [4000/6000], Loss: 0.0673
Epoch [3/30], Batch [4100/6000], Loss: 0.0415
Epoch [3/30], Batch [4200/6000], Loss: 0.0333
Epoch [3/30], Batch [4300/6000], Loss: 0.0321
Epoch [3/30], Batch [4400/6000], Loss: 0.0290
Epoch [3/30], Batch [4500/6000], Loss: 0.0418
Epoch [3/30], Batch [4600/6000], Loss: 0.0279
Epoch [3/30], Batch [4700/6000], Loss: 0.0574
Epoch [3/30], Batch [4800/6000], Loss: 0.0361
Epoch [3/30], Batch [4900/6000], Loss: 0.0296
Epoch [3/30], Batch [5000/6000], Loss: 0.0472
Epoch [3/30], Batch [5100/6000], Loss: 0.0398
Epoch [3/30], Batch [5200/6000], Loss: 0.0298
Epoch [3/30], Batch [5300/6000], Loss: 0.0284
Epoch [3/30], Batch [5400/6000], Loss: 0.0199
Epoch [3/30], Batch [5500/6000], Loss: 0.0648
Epoch [3/30], Batch [5600/6000], Loss: 0.0381
Epoch [3/30], Batch [5700/6000], Loss: 0.0296
Epoch [3/30], Batch [5800/6000], Loss: 0.0311
Epoch [3/30], Batch [5900/6000], Loss: 0.0275
Epoch [3/30], Loss: 0.0379
Epoch [4/30], Batch [0/6000], Loss: 0.0607
Epoch [4/30], Batch [100/6000], Loss: 0.0298
Epoch [4/30], Batch [200/6000], Loss: 0.0307
Epoch [4/30], Batch [300/6000], Loss: 0.0736
Epoch [4/30], Batch [400/6000], Loss: 0.0272
Epoch [4/30], Batch [500/6000], Loss: 0.0348
Epoch [4/30], Batch [600/6000], Loss: 0.0316
Epoch [4/30], Batch [700/6000], Loss: 0.0538
Epoch [4/30], Batch [800/6000], Loss: 0.0271
Epoch [4/30], Batch [900/6000], Loss: 0.0318
Epoch [4/30], Batch [1000/6000], Loss: 0.0256
Epoch [4/30], Batch [1100/6000], Loss: 0.0506
Epoch [4/30], Batch [1200/6000], Loss: 0.0265
Epoch [4/30], Batch [1300/6000], Loss: 0.0354
Epoch [4/30], Batch [1400/6000], Loss: 0.0303
Epoch [4/30], Batch [1500/6000], Loss: 0.0296
Epoch [4/30], Batch [1600/6000], Loss: 0.0308
Epoch [4/30], Batch [1700/6000], Loss: 0.0772
Epoch [4/30], Batch [1800/6000], Loss: 0.0353
Epoch [4/30], Batch [1900/6000], Loss: 0.0257
Epoch [4/30], Batch [2000/6000], Loss: 0.0396
Epoch [4/30], Batch [2100/6000], Loss: 0.0289
Epoch [4/30], Batch [2200/6000], Loss: 0.0324
Epoch [4/30], Batch [2300/6000], Loss: 0.0490
Epoch [4/30], Batch [2400/6000], Loss: 0.0276
Epoch [4/30], Batch [2500/6000], Loss: 0.0244
Epoch [4/30], Batch [2600/6000], Loss: 0.0335
Epoch [4/30], Batch [2700/6000], Loss: 0.0336
Epoch [4/30], Batch [2800/6000], Loss: 0.0303
Epoch [4/30], Batch [2900/6000], Loss: 0.0303
Epoch [4/30], Batch [3000/6000], Loss: 0.0362
Epoch [4/30], Batch [3100/6000], Loss: 0.0399
Epoch [4/30], Batch [3200/6000], Loss: 0.0247
Epoch [4/30], Batch [3300/6000], Loss: 0.0267
Epoch [4/30], Batch [3400/6000], Loss: 0.0221
Epoch [4/30], Batch [3500/6000], Loss: 0.0255
Epoch [4/30], Batch [3600/6000], Loss: 0.0588
Epoch [4/30], Batch [3700/6000], Loss: 0.0290
Epoch [4/30], Batch [3800/6000], Loss: 0.0377
Epoch [4/30], Batch [3900/6000], Loss: 0.0463
Epoch [4/30], Batch [4000/6000], Loss: 0.0371
Epoch [4/30], Batch [4100/6000], Loss: 0.0314
Epoch [4/30], Batch [4200/6000], Loss: 0.0296
Epoch [4/30], Batch [4300/6000], Loss: 0.0226
Epoch [4/30], Batch [4400/6000], Loss: 0.0417
Epoch [4/30], Batch [4500/6000], Loss: 0.0253
Epoch [4/30], Batch [4600/6000], Loss: 0.0252
Epoch [4/30], Batch [4700/6000], Loss: 0.0363
Epoch [4/30], Batch [4800/6000], Loss: 0.0329
Epoch [4/30], Batch [4900/6000], Loss: 0.0258
Epoch [4/30], Batch [5000/6000], Loss: 0.0204
Epoch [4/30], Batch [5100/6000], Loss: 0.0276
Epoch [4/30], Batch [5200/6000], Loss: 0.0359
Epoch [4/30], Batch [5300/6000], Loss: 0.0445
Epoch [4/30], Batch [5400/6000], Loss: 0.0281
Epoch [4/30], Batch [5500/6000], Loss: 0.0333
Epoch [4/30], Batch [5600/6000], Loss: 0.0291
Epoch [4/30], Batch [5700/6000], Loss: 0.0339
Epoch [4/30], Batch [5800/6000], Loss: 0.0229
Epoch [4/30], Batch [5900/6000], Loss: 0.0845
Epoch [4/30], Loss: 0.0332
Epoch [5/30], Batch [0/6000], Loss: 0.0246
Epoch [5/30], Batch [100/6000], Loss: 0.0260
Epoch [5/30], Batch [200/6000], Loss: 0.0321
Epoch [5/30], Batch [300/6000], Loss: 0.0292
Epoch [5/30], Batch [400/6000], Loss: 0.0321
Epoch [5/30], Batch [500/6000], Loss: 0.0168
Epoch [5/30], Batch [600/6000], Loss: 0.0277
Epoch [5/30], Batch [700/6000], Loss: 0.0234
Epoch [5/30], Batch [800/6000], Loss: 0.0371
Epoch [5/30], Batch [900/6000], Loss: 0.0359
Epoch [5/30], Batch [1000/6000], Loss: 0.0380
Epoch [5/30], Batch [1100/6000], Loss: 0.0474
Epoch [5/30], Batch [1200/6000], Loss: 0.0255
Epoch [5/30], Batch [1300/6000], Loss: 0.0214
Epoch [5/30], Batch [1400/6000], Loss: 0.0314
Epoch [5/30], Batch [1500/6000], Loss: 0.0427
Epoch [5/30], Batch [1600/6000], Loss: 0.0192
Epoch [5/30], Batch [1700/6000], Loss: 0.0296
Epoch [5/30], Batch [1800/6000], Loss: 0.0220
Epoch [5/30], Batch [1900/6000], Loss: 0.0309
Epoch [5/30], Batch [2000/6000], Loss: 0.0231
Epoch [5/30], Batch [2100/6000], Loss: 0.0244
Epoch [5/30], Batch [2200/6000], Loss: 0.0219
Epoch [5/30], Batch [2300/6000], Loss: 0.0283
Epoch [5/30], Batch [2400/6000], Loss: 0.0244
Epoch [5/30], Batch [2500/6000], Loss: 0.0197
Epoch [5/30], Batch [2600/6000], Loss: 0.0385
Epoch [5/30], Batch [2700/6000], Loss: 0.0268
Epoch [5/30], Batch [2800/6000], Loss: 0.0255
Epoch [5/30], Batch [2900/6000], Loss: 0.0266
Epoch [5/30], Batch [3000/6000], Loss: 0.0239
Epoch [5/30], Batch [3100/6000], Loss: 0.0252
Epoch [5/30], Batch [3200/6000], Loss: 0.0257
Epoch [5/30], Batch [3300/6000], Loss: 0.0446
Epoch [5/30], Batch [3400/6000], Loss: 0.0275
Epoch [5/30], Batch [3500/6000], Loss: 0.0256
Epoch [5/30], Batch [3600/6000], Loss: 0.0341
Epoch [5/30], Batch [3700/6000], Loss: 0.0224
Epoch [5/30], Batch [3800/6000], Loss: 0.0315
Epoch [5/30], Batch [3900/6000], Loss: 0.0483
Epoch [5/30], Batch [4000/6000], Loss: 0.0198
Epoch [5/30], Batch [4100/6000], Loss: 0.0260
Epoch [5/30], Batch [4200/6000], Loss: 0.0188
Epoch [5/30], Batch [4300/6000], Loss: 0.0367
Epoch [5/30], Batch [4400/6000], Loss: 0.0312
Epoch [5/30], Batch [4500/6000], Loss: 0.0322
Epoch [5/30], Batch [4600/6000], Loss: 0.0185
Epoch [5/30], Batch [4700/6000], Loss: 0.0255
Epoch [5/30], Batch [4800/6000], Loss: 0.0241
Epoch [5/30], Batch [4900/6000], Loss: 0.0200
Epoch [5/30], Batch [5000/6000], Loss: 0.0247
Epoch [5/30], Batch [5100/6000], Loss: 0.0278
Epoch [5/30], Batch [5200/6000], Loss: 0.0245
Epoch [5/30], Batch [5300/6000], Loss: 0.0269
Epoch [5/30], Batch [5400/6000], Loss: 0.0320
Epoch [5/30], Batch [5500/6000], Loss: 0.0268
Epoch [5/30], Batch [5600/6000], Loss: 0.0250
Epoch [5/30], Batch [5700/6000], Loss: 0.0236
Epoch [5/30], Batch [5800/6000], Loss: 0.0254
Epoch [5/30], Batch [5900/6000], Loss: 0.0350
Epoch [5/30], Loss: 0.0300
Epoch [6/30], Batch [0/6000], Loss: 0.0322
Epoch [6/30], Batch [100/6000], Loss: 0.0249
Epoch [6/30], Batch [200/6000], Loss: 0.0192
Epoch [6/30], Batch [300/6000], Loss: 0.0192
Epoch [6/30], Batch [400/6000], Loss: 0.0322
Epoch [6/30], Batch [500/6000], Loss: 0.0292
Epoch [6/30], Batch [600/6000], Loss: 0.0266
Epoch [6/30], Batch [700/6000], Loss: 0.0368
Epoch [6/30], Batch [800/6000], Loss: 0.0264
Epoch [6/30], Batch [900/6000], Loss: 0.0254
Epoch [6/30], Batch [1000/6000], Loss: 0.0288
Epoch [6/30], Batch [1100/6000], Loss: 0.0274
Epoch [6/30], Batch [1200/6000], Loss: 0.0271
Epoch [6/30], Batch [1300/6000], Loss: 0.0221
Epoch [6/30], Batch [1400/6000], Loss: 0.0259
Epoch [6/30], Batch [1500/6000], Loss: 0.0228
Epoch [6/30], Batch [1600/6000], Loss: 0.0259
Epoch [6/30], Batch [1700/6000], Loss: 0.0326
Epoch [6/30], Batch [1800/6000], Loss: 0.0350
Epoch [6/30], Batch [1900/6000], Loss: 0.0213
Epoch [6/30], Batch [2000/6000], Loss: 0.0208
Epoch [6/30], Batch [2100/6000], Loss: 0.0294
Epoch [6/30], Batch [2200/6000], Loss: 0.0431
Epoch [6/30], Batch [2300/6000], Loss: 0.0275
Epoch [6/30], Batch [2400/6000], Loss: 0.0296
Epoch [6/30], Batch [2500/6000], Loss: 0.0226
Epoch [6/30], Batch [2600/6000], Loss: 0.0286
Epoch [6/30], Batch [2700/6000], Loss: 0.0225
Epoch [6/30], Batch [2800/6000], Loss: 0.0199
Epoch [6/30], Batch [2900/6000], Loss: 0.0406
Epoch [6/30], Batch [3000/6000], Loss: 0.0207
Epoch [6/30], Batch [3100/6000], Loss: 0.0337
Epoch [6/30], Batch [3200/6000], Loss: 0.0185
Epoch [6/30], Batch [3300/6000], Loss: 0.0274
Epoch [6/30], Batch [3400/6000], Loss: 0.0155
Epoch [6/30], Batch [3500/6000], Loss: 0.0277
Epoch [6/30], Batch [3600/6000], Loss: 0.0304
Epoch [6/30], Batch [3700/6000], Loss: 0.0223
Epoch [6/30], Batch [3800/6000], Loss: 0.0243
Epoch [6/30], Batch [3900/6000], Loss: 0.0226
Epoch [6/30], Batch [4000/6000], Loss: 0.0243
Epoch [6/30], Batch [4100/6000], Loss: 0.0232
Epoch [6/30], Batch [4200/6000], Loss: 0.0235
Epoch [6/30], Batch [4300/6000], Loss: 0.0383
Epoch [6/30], Batch [4400/6000], Loss: 0.0222
Epoch [6/30], Batch [4500/6000], Loss: 0.0201
Epoch [6/30], Batch [4600/6000], Loss: 0.0259
Epoch [6/30], Batch [4700/6000], Loss: 0.0440
Epoch [6/30], Batch [4800/6000], Loss: 0.0289
Epoch [6/30], Batch [4900/6000], Loss: 0.0238
Epoch [6/30], Batch [5000/6000], Loss: 0.0270
Epoch [6/30], Batch [5100/6000], Loss: 0.0245
Epoch [6/30], Batch [5200/6000], Loss: 0.0244
Epoch [6/30], Batch [5300/6000], Loss: 0.0376
Epoch [6/30], Batch [5400/6000], Loss: 0.0241
Epoch [6/30], Batch [5500/6000], Loss: 0.0230
Epoch [6/30], Batch [5600/6000], Loss: 0.0206
Epoch [6/30], Batch [5700/6000], Loss: 0.0250
Epoch [6/30], Batch [5800/6000], Loss: 0.0287
Epoch [6/30], Batch [5900/6000], Loss: 0.0238
Epoch [6/30], Loss: 0.0278
Epoch [7/30], Batch [0/6000], Loss: 0.0253
Epoch [7/30], Batch [100/6000], Loss: 0.0285
Epoch [7/30], Batch [200/6000], Loss: 0.0213
Epoch [7/30], Batch [300/6000], Loss: 0.0359
Epoch [7/30], Batch [400/6000], Loss: 0.0272
Epoch [7/30], Batch [500/6000], Loss: 0.0256
Epoch [7/30], Batch [600/6000], Loss: 0.0262
Epoch [7/30], Batch [700/6000], Loss: 0.0205
Epoch [7/30], Batch [800/6000], Loss: 0.0214
Epoch [7/30], Batch [900/6000], Loss: 0.0203
Epoch [7/30], Batch [1000/6000], Loss: 0.0266
Epoch [7/30], Batch [1100/6000], Loss: 0.0191
Epoch [7/30], Batch [1200/6000], Loss: 0.0182
Epoch [7/30], Batch [1300/6000], Loss: 0.0414
Epoch [7/30], Batch [1400/6000], Loss: 0.0218
Epoch [7/30], Batch [1500/6000], Loss: 0.0267
Epoch [7/30], Batch [1600/6000], Loss: 0.0243
Epoch [7/30], Batch [1700/6000], Loss: 0.0208
Epoch [7/30], Batch [1800/6000], Loss: 0.0264
Epoch [7/30], Batch [1900/6000], Loss: 0.0212
Epoch [7/30], Batch [2000/6000], Loss: 0.0232
Epoch [7/30], Batch [2100/6000], Loss: 0.0392
Epoch [7/30], Batch [2200/6000], Loss: 0.0229
Epoch [7/30], Batch [2300/6000], Loss: 0.0190
Epoch [7/30], Batch [2400/6000], Loss: 0.0254
Epoch [7/30], Batch [2500/6000], Loss: 0.0289
Epoch [7/30], Batch [2600/6000], Loss: 0.0229
Epoch [7/30], Batch [2700/6000], Loss: 0.0203
Epoch [7/30], Batch [2800/6000], Loss: 0.0248
Epoch [7/30], Batch [2900/6000], Loss: 0.0261
Epoch [7/30], Batch [3000/6000], Loss: 0.0260
Epoch [7/30], Batch [3100/6000], Loss: 0.0309
Epoch [7/30], Batch [3200/6000], Loss: 0.0352
Epoch [7/30], Batch [3300/6000], Loss: 0.0245
Epoch [7/30], Batch [3400/6000], Loss: 0.0219
Epoch [7/30], Batch [3500/6000], Loss: 0.0181
Epoch [7/30], Batch [3600/6000], Loss: 0.0308
Epoch [7/30], Batch [3700/6000], Loss: 0.0176
Epoch [7/30], Batch [3800/6000], Loss: 0.0228
Epoch [7/30], Batch [3900/6000], Loss: 0.0269
Epoch [7/30], Batch [4000/6000], Loss: 0.0201
Epoch [7/30], Batch [4100/6000], Loss: 0.0328
Epoch [7/30], Batch [4200/6000], Loss: 0.0226
Epoch [7/30], Batch [4300/6000], Loss: 0.0233
Epoch [7/30], Batch [4400/6000], Loss: 0.0252
Epoch [7/30], Batch [4500/6000], Loss: 0.0218
Epoch [7/30], Batch [4600/6000], Loss: 0.0240
Epoch [7/30], Batch [4700/6000], Loss: 0.0257
Epoch [7/30], Batch [4800/6000], Loss: 0.0291
Epoch [7/30], Batch [4900/6000], Loss: 0.0285
Epoch [7/30], Batch [5000/6000], Loss: 0.0214
Epoch [7/30], Batch [5100/6000], Loss: 0.0203
Epoch [7/30], Batch [5200/6000], Loss: 0.0283
Epoch [7/30], Batch [5300/6000], Loss: 0.0355
Epoch [7/30], Batch [5400/6000], Loss: 0.0256
Epoch [7/30], Batch [5500/6000], Loss: 0.0392
Epoch [7/30], Batch [5600/6000], Loss: 0.0235
Epoch [7/30], Batch [5700/6000], Loss: 0.0204
Epoch [7/30], Batch [5800/6000], Loss: 0.0331
Epoch [7/30], Batch [5900/6000], Loss: 0.0268
Epoch [7/30], Loss: 0.0260
Epoch [8/30], Batch [0/6000], Loss: 0.0231
Epoch [8/30], Batch [100/6000], Loss: 0.0232
Epoch [8/30], Batch [200/6000], Loss: 0.0218
Epoch [8/30], Batch [300/6000], Loss: 0.0168
Epoch [8/30], Batch [400/6000], Loss: 0.0364
Epoch [8/30], Batch [500/6000], Loss: 0.0278
Epoch [8/30], Batch [600/6000], Loss: 0.0218
Epoch [8/30], Batch [700/6000], Loss: 0.0199
Epoch [8/30], Batch [800/6000], Loss: 0.0298
Epoch [8/30], Batch [900/6000], Loss: 0.0260
Epoch [8/30], Batch [1000/6000], Loss: 0.0294
Epoch [8/30], Batch [1100/6000], Loss: 0.0211
Epoch [8/30], Batch [1200/6000], Loss: 0.0182
Epoch [8/30], Batch [1300/6000], Loss: 0.0207
Epoch [8/30], Batch [1400/6000], Loss: 0.0175
Epoch [8/30], Batch [1500/6000], Loss: 0.0208
Epoch [8/30], Batch [1600/6000], Loss: 0.0232
Epoch [8/30], Batch [1700/6000], Loss: 0.0248
Epoch [8/30], Batch [1800/6000], Loss: 0.0286
Epoch [8/30], Batch [1900/6000], Loss: 0.0166
Epoch [8/30], Batch [2000/6000], Loss: 0.0415
Epoch [8/30], Batch [2100/6000], Loss: 0.0383
Epoch [8/30], Batch [2200/6000], Loss: 0.0186
Epoch [8/30], Batch [2300/6000], Loss: 0.0248
Epoch [8/30], Batch [2400/6000], Loss: 0.0204
Epoch [8/30], Batch [2500/6000], Loss: 0.0198
Epoch [8/30], Batch [2600/6000], Loss: 0.0256
Epoch [8/30], Batch [2700/6000], Loss: 0.0242
Epoch [8/30], Batch [2800/6000], Loss: 0.0244
Epoch [8/30], Batch [2900/6000], Loss: 0.0401
Epoch [8/30], Batch [3000/6000], Loss: 0.0241
Epoch [8/30], Batch [3100/6000], Loss: 0.0180
Epoch [8/30], Batch [3200/6000], Loss: 0.0235
Epoch [8/30], Batch [3300/6000], Loss: 0.0186
Epoch [8/30], Batch [3400/6000], Loss: 0.0205
Epoch [8/30], Batch [3500/6000], Loss: 0.0284
Epoch [8/30], Batch [3600/6000], Loss: 0.0224
Epoch [8/30], Batch [3700/6000], Loss: 0.0206
Epoch [8/30], Batch [3800/6000], Loss: 0.0199
Epoch [8/30], Batch [3900/6000], Loss: 0.0263
Epoch [8/30], Batch [4000/6000], Loss: 0.0215
Epoch [8/30], Batch [4100/6000], Loss: 0.0204
Epoch [8/30], Batch [4200/6000], Loss: 0.0343
Epoch [8/30], Batch [4300/6000], Loss: 0.0325
Epoch [8/30], Batch [4400/6000], Loss: 0.0342
Epoch [8/30], Batch [4500/6000], Loss: 0.0332
Epoch [8/30], Batch [4600/6000], Loss: 0.0256
Epoch [8/30], Batch [4700/6000], Loss: 0.0198
Epoch [8/30], Batch [4800/6000], Loss: 0.0190
Epoch [8/30], Batch [4900/6000], Loss: 0.0225
Epoch [8/30], Batch [5000/6000], Loss: 0.0198
Epoch [8/30], Batch [5100/6000], Loss: 0.0340
Epoch [8/30], Batch [5200/6000], Loss: 0.0225
Epoch [8/30], Batch [5300/6000], Loss: 0.0385
Epoch [8/30], Batch [5400/6000], Loss: 0.0453
Epoch [8/30], Batch [5500/6000], Loss: 0.0252
Epoch [8/30], Batch [5600/6000], Loss: 0.0231
Epoch [8/30], Batch [5700/6000], Loss: 0.0313
Epoch [8/30], Batch [5800/6000], Loss: 0.0189
Epoch [8/30], Batch [5900/6000], Loss: 0.0182
Epoch [8/30], Loss: 0.0243
Epoch [9/30], Batch [0/6000], Loss: 0.0171
Epoch [9/30], Batch [100/6000], Loss: 0.0209
Epoch [9/30], Batch [200/6000], Loss: 0.0202
Epoch [9/30], Batch [300/6000], Loss: 0.0188
Epoch [9/30], Batch [400/6000], Loss: 0.0131
Epoch [9/30], Batch [500/6000], Loss: 0.0242
Epoch [9/30], Batch [600/6000], Loss: 0.0188
Epoch [9/30], Batch [700/6000], Loss: 0.0220
Epoch [9/30], Batch [800/6000], Loss: 0.0299
Epoch [9/30], Batch [900/6000], Loss: 0.0233
Epoch [9/30], Batch [1000/6000], Loss: 0.0226
Epoch [9/30], Batch [1100/6000], Loss: 0.0286
Epoch [9/30], Batch [1200/6000], Loss: 0.0311
Epoch [9/30], Batch [1300/6000], Loss: 0.0183
Epoch [9/30], Batch [1400/6000], Loss: 0.0220
Epoch [9/30], Batch [1500/6000], Loss: 0.0311
Epoch [9/30], Batch [1600/6000], Loss: 0.0229
Epoch [9/30], Batch [1700/6000], Loss: 0.0203
Epoch [9/30], Batch [1800/6000], Loss: 0.0227
Epoch [9/30], Batch [1900/6000], Loss: 0.0210
Epoch [9/30], Batch [2000/6000], Loss: 0.0192
Epoch [9/30], Batch [2100/6000], Loss: 0.0243
Epoch [9/30], Batch [2200/6000], Loss: 0.0232
Epoch [9/30], Batch [2300/6000], Loss: 0.0281
Epoch [9/30], Batch [2400/6000], Loss: 0.0204
Epoch [9/30], Batch [2500/6000], Loss: 0.0182
Epoch [9/30], Batch [2600/6000], Loss: 0.0183
Epoch [9/30], Batch [2700/6000], Loss: 0.0234
Epoch [9/30], Batch [2800/6000], Loss: 0.0234
Epoch [9/30], Batch [2900/6000], Loss: 0.0189
Epoch [9/30], Batch [3000/6000], Loss: 0.0176
Epoch [9/30], Batch [3100/6000], Loss: 0.0184
Epoch [9/30], Batch [3200/6000], Loss: 0.0238
Epoch [9/30], Batch [3300/6000], Loss: 0.0183
Epoch [9/30], Batch [3400/6000], Loss: 0.0179
Epoch [9/30], Batch [3500/6000], Loss: 0.0241
Epoch [9/30], Batch [3600/6000], Loss: 0.0216
Epoch [9/30], Batch [3700/6000], Loss: 0.0192
Epoch [9/30], Batch [3800/6000], Loss: 0.0275
Epoch [9/30], Batch [3900/6000], Loss: 0.0205
Epoch [9/30], Batch [4000/6000], Loss: 0.0239
Epoch [9/30], Batch [4100/6000], Loss: 0.0211
Epoch [9/30], Batch [4200/6000], Loss: 0.0231
Epoch [9/30], Batch [4300/6000], Loss: 0.0229
Epoch [9/30], Batch [4400/6000], Loss: 0.0169
Epoch [9/30], Batch [4500/6000], Loss: 0.0184
Epoch [9/30], Batch [4600/6000], Loss: 0.0207
Epoch [9/30], Batch [4700/6000], Loss: 0.0769
Epoch [9/30], Batch [4800/6000], Loss: 0.0230
Epoch [9/30], Batch [4900/6000], Loss: 0.0210
Epoch [9/30], Batch [5000/6000], Loss: 0.0203
Epoch [9/30], Batch [5100/6000], Loss: 0.0205
Epoch [9/30], Batch [5200/6000], Loss: 0.0301
Epoch [9/30], Batch [5300/6000], Loss: 0.0201
Epoch [9/30], Batch [5400/6000], Loss: 0.0206
Epoch [9/30], Batch [5500/6000], Loss: 0.0260
Epoch [9/30], Batch [5600/6000], Loss: 0.0217
Epoch [9/30], Batch [5700/6000], Loss: 0.0206
Epoch [9/30], Batch [5800/6000], Loss: 0.0188
Epoch [9/30], Batch [5900/6000], Loss: 0.0246
Epoch [9/30], Loss: 0.0229
Epoch [10/30], Batch [0/6000], Loss: 0.0187
Epoch [10/30], Batch [100/6000], Loss: 0.0185
Epoch [10/30], Batch [200/6000], Loss: 0.0250
Epoch [10/30], Batch [300/6000], Loss: 0.0181
Epoch [10/30], Batch [400/6000], Loss: 0.0187
Epoch [10/30], Batch [500/6000], Loss: 0.0161
Epoch [10/30], Batch [600/6000], Loss: 0.0192
Epoch [10/30], Batch [700/6000], Loss: 0.0236
Epoch [10/30], Batch [800/6000], Loss: 0.0203
Epoch [10/30], Batch [900/6000], Loss: 0.0205
Epoch [10/30], Batch [1000/6000], Loss: 0.0285
Epoch [10/30], Batch [1100/6000], Loss: 0.0199
Epoch [10/30], Batch [1200/6000], Loss: 0.0181
Epoch [10/30], Batch [1300/6000], Loss: 0.0244
Epoch [10/30], Batch [1400/6000], Loss: 0.0185
Epoch [10/30], Batch [1500/6000], Loss: 0.0189
Epoch [10/30], Batch [1600/6000], Loss: 0.0171
Epoch [10/30], Batch [1700/6000], Loss: 0.0327
Epoch [10/30], Batch [1800/6000], Loss: 0.0207
Epoch [10/30], Batch [1900/6000], Loss: 0.0200
Epoch [10/30], Batch [2000/6000], Loss: 0.0252
Epoch [10/30], Batch [2100/6000], Loss: 0.0217
Epoch [10/30], Batch [2200/6000], Loss: 0.0321
Epoch [10/30], Batch [2300/6000], Loss: 0.0194
Epoch [10/30], Batch [2400/6000], Loss: 0.0182
Epoch [10/30], Batch [2500/6000], Loss: 0.0256
Epoch [10/30], Batch [2600/6000], Loss: 0.0144
Epoch [10/30], Batch [2700/6000], Loss: 0.0339
Epoch [10/30], Batch [2800/6000], Loss: 0.0184
Epoch [10/30], Batch [2900/6000], Loss: 0.0287
Epoch [10/30], Batch [3000/6000], Loss: 0.0176
Epoch [10/30], Batch [3100/6000], Loss: 0.0429
Epoch [10/30], Batch [3200/6000], Loss: 0.0296
Epoch [10/30], Batch [3300/6000], Loss: 0.0183
Epoch [10/30], Batch [3400/6000], Loss: 0.0323
Epoch [10/30], Batch [3500/6000], Loss: 0.0260
Epoch [10/30], Batch [3600/6000], Loss: 0.0197
Epoch [10/30], Batch [3700/6000], Loss: 0.0234
Epoch [10/30], Batch [3800/6000], Loss: 0.0195
Epoch [10/30], Batch [3900/6000], Loss: 0.0203
Epoch [10/30], Batch [4000/6000], Loss: 0.0194
Epoch [10/30], Batch [4100/6000], Loss: 0.0255
Epoch [10/30], Batch [4200/6000], Loss: 0.0173
Epoch [10/30], Batch [4300/6000], Loss: 0.0190
Epoch [10/30], Batch [4400/6000], Loss: 0.0213
Epoch [10/30], Batch [4500/6000], Loss: 0.0196
Epoch [10/30], Batch [4600/6000], Loss: 0.0189
Epoch [10/30], Batch [4700/6000], Loss: 0.0181
Epoch [10/30], Batch [4800/6000], Loss: 0.0198
Epoch [10/30], Batch [4900/6000], Loss: 0.0160
Epoch [10/30], Batch [5000/6000], Loss: 0.0284
Epoch [10/30], Batch [5100/6000], Loss: 0.0171
Epoch [10/30], Batch [5200/6000], Loss: 0.0215
Epoch [10/30], Batch [5300/6000], Loss: 0.0194
Epoch [10/30], Batch [5400/6000], Loss: 0.0203
Epoch [10/30], Batch [5500/6000], Loss: 0.0373
Epoch [10/30], Batch [5600/6000], Loss: 0.0183
Epoch [10/30], Batch [5700/6000], Loss: 0.0196
Epoch [10/30], Batch [5800/6000], Loss: 0.0206
Epoch [10/30], Batch [5900/6000], Loss: 0.0196
Epoch [10/30], Loss: 0.0217
Epoch [11/30], Batch [0/6000], Loss: 0.0148
Epoch [11/30], Batch [100/6000], Loss: 0.0228
Epoch [11/30], Batch [200/6000], Loss: 0.0210
Epoch [11/30], Batch [300/6000], Loss: 0.0192
Epoch [11/30], Batch [400/6000], Loss: 0.0191
Epoch [11/30], Batch [500/6000], Loss: 0.0192
Epoch [11/30], Batch [600/6000], Loss: 0.0205
Epoch [11/30], Batch [700/6000], Loss: 0.0173
Epoch [11/30], Batch [800/6000], Loss: 0.0175
Epoch [11/30], Batch [900/6000], Loss: 0.0226
Epoch [11/30], Batch [1000/6000], Loss: 0.0157
Epoch [11/30], Batch [1100/6000], Loss: 0.0178
Epoch [11/30], Batch [1200/6000], Loss: 0.0169
Epoch [11/30], Batch [1300/6000], Loss: 0.0240
Epoch [11/30], Batch [1400/6000], Loss: 0.0160
Epoch [11/30], Batch [1500/6000], Loss: 0.0202
Epoch [11/30], Batch [1600/6000], Loss: 0.0173
Epoch [11/30], Batch [1700/6000], Loss: 0.0179
Epoch [11/30], Batch [1800/6000], Loss: 0.0209
Epoch [11/30], Batch [1900/6000], Loss: 0.0178
Epoch [11/30], Batch [2000/6000], Loss: 0.0220
Epoch [11/30], Batch [2100/6000], Loss: 0.0181
Epoch [11/30], Batch [2200/6000], Loss: 0.0185
Epoch [11/30], Batch [2300/6000], Loss: 0.0296
Epoch [11/30], Batch [2400/6000], Loss: 0.0218
Epoch [11/30], Batch [2500/6000], Loss: 0.0295
Epoch [11/30], Batch [2600/6000], Loss: 0.0274
Epoch [11/30], Batch [2700/6000], Loss: 0.0218
Epoch [11/30], Batch [2800/6000], Loss: 0.0154
Epoch [11/30], Batch [2900/6000], Loss: 0.0176
Epoch [11/30], Batch [3000/6000], Loss: 0.0152
Epoch [11/30], Batch [3100/6000], Loss: 0.0154
Epoch [11/30], Batch [3200/6000], Loss: 0.0251
Epoch [11/30], Batch [3300/6000], Loss: 0.0201
Epoch [11/30], Batch [3400/6000], Loss: 0.0222
Epoch [11/30], Batch [3500/6000], Loss: 0.0169
Epoch [11/30], Batch [3600/6000], Loss: 0.0199
Epoch [11/30], Batch [3700/6000], Loss: 0.0166
Epoch [11/30], Batch [3800/6000], Loss: 0.0165
Epoch [11/30], Batch [3900/6000], Loss: 0.0258
Epoch [11/30], Batch [4000/6000], Loss: 0.0181
Epoch [11/30], Batch [4100/6000], Loss: 0.0169
Epoch [11/30], Batch [4200/6000], Loss: 0.0173
Epoch [11/30], Batch [4300/6000], Loss: 0.0200
Epoch [11/30], Batch [4400/6000], Loss: 0.0153
Epoch [11/30], Batch [4500/6000], Loss: 0.0306
Epoch [11/30], Batch [4600/6000], Loss: 0.0153
Epoch [11/30], Batch [4700/6000], Loss: 0.0212
Epoch [11/30], Batch [4800/6000], Loss: 0.0156
Epoch [11/30], Batch [4900/6000], Loss: 0.0267
Epoch [11/30], Batch [5000/6000], Loss: 0.0172
Epoch [11/30], Batch [5100/6000], Loss: 0.0205
Epoch [11/30], Batch [5200/6000], Loss: 0.0175
Epoch [11/30], Batch [5300/6000], Loss: 0.0197
Epoch [11/30], Batch [5400/6000], Loss: 0.0228
Epoch [11/30], Batch [5500/6000], Loss: 0.0162
Epoch [11/30], Batch [5600/6000], Loss: 0.0173
Epoch [11/30], Batch [5700/6000], Loss: 0.0160
Epoch [11/30], Batch [5800/6000], Loss: 0.0158
Epoch [11/30], Batch [5900/6000], Loss: 0.0197
Epoch [11/30], Loss: 0.0207
Epoch [12/30], Batch [0/6000], Loss: 0.0210
Epoch [12/30], Batch [100/6000], Loss: 0.0173
Epoch [12/30], Batch [200/6000], Loss: 0.0187
Epoch [12/30], Batch [300/6000], Loss: 0.0178
Epoch [12/30], Batch [400/6000], Loss: 0.0172
Epoch [12/30], Batch [500/6000], Loss: 0.0190
Epoch [12/30], Batch [600/6000], Loss: 0.0122
Epoch [12/30], Batch [700/6000], Loss: 0.0210
Epoch [12/30], Batch [800/6000], Loss: 0.0198
Epoch [12/30], Batch [900/6000], Loss: 0.0200
Epoch [12/30], Batch [1000/6000], Loss: 0.0195
Epoch [12/30], Batch [1100/6000], Loss: 0.0207
Epoch [12/30], Batch [1200/6000], Loss: 0.0227
Epoch [12/30], Batch [1300/6000], Loss: 0.0161
Epoch [12/30], Batch [1400/6000], Loss: 0.0158
Epoch [12/30], Batch [1500/6000], Loss: 0.0182
Epoch [12/30], Batch [1600/6000], Loss: 0.0190
Epoch [12/30], Batch [1700/6000], Loss: 0.0168
Epoch [12/30], Batch [1800/6000], Loss: 0.0136
Epoch [12/30], Batch [1900/6000], Loss: 0.0187
Epoch [12/30], Batch [2000/6000], Loss: 0.0199
Epoch [12/30], Batch [2100/6000], Loss: 0.0211
Epoch [12/30], Batch [2200/6000], Loss: 0.0158
Epoch [12/30], Batch [2300/6000], Loss: 0.0154
Epoch [12/30], Batch [2400/6000], Loss: 0.0178
Epoch [12/30], Batch [2500/6000], Loss: 0.0211
Epoch [12/30], Batch [2600/6000], Loss: 0.0185
Epoch [12/30], Batch [2700/6000], Loss: 0.0185
Epoch [12/30], Batch [2800/6000], Loss: 0.0164
Epoch [12/30], Batch [2900/6000], Loss: 0.0167
Epoch [12/30], Batch [3000/6000], Loss: 0.0185
Epoch [12/30], Batch [3100/6000], Loss: 0.0179
Epoch [12/30], Batch [3200/6000], Loss: 0.0181
Epoch [12/30], Batch [3300/6000], Loss: 0.0208
Epoch [12/30], Batch [3400/6000], Loss: 0.0149
Epoch [12/30], Batch [3500/6000], Loss: 0.0158
Epoch [12/30], Batch [3600/6000], Loss: 0.0145
Epoch [12/30], Batch [3700/6000], Loss: 0.0185
Epoch [12/30], Batch [3800/6000], Loss: 0.0146
Epoch [12/30], Batch [3900/6000], Loss: 0.0194
Epoch [12/30], Batch [4000/6000], Loss: 0.0200
Epoch [12/30], Batch [4100/6000], Loss: 0.0213
Epoch [12/30], Batch [4200/6000], Loss: 0.0236
Epoch [12/30], Batch [4300/6000], Loss: 0.0275
Epoch [12/30], Batch [4400/6000], Loss: 0.0166
Epoch [12/30], Batch [4500/6000], Loss: 0.0192
Epoch [12/30], Batch [4600/6000], Loss: 0.0316
Epoch [12/30], Batch [4700/6000], Loss: 0.0152
Epoch [12/30], Batch [4800/6000], Loss: 0.0236
Epoch [12/30], Batch [4900/6000], Loss: 0.0140
Epoch [12/30], Batch [5000/6000], Loss: 0.0175
Epoch [12/30], Batch [5100/6000], Loss: 0.0197
Epoch [12/30], Batch [5200/6000], Loss: 0.0205
Epoch [12/30], Batch [5300/6000], Loss: 0.0207
Epoch [12/30], Batch [5400/6000], Loss: 0.0216
Epoch [12/30], Batch [5500/6000], Loss: 0.0177
Epoch [12/30], Batch [5600/6000], Loss: 0.0189
Epoch [12/30], Batch [5700/6000], Loss: 0.0171
Epoch [12/30], Batch [5800/6000], Loss: 0.0174
Epoch [12/30], Batch [5900/6000], Loss: 0.0175
Epoch [12/30], Loss: 0.0198
Epoch [13/30], Batch [0/6000], Loss: 0.0125
Epoch [13/30], Batch [100/6000], Loss: 0.0169
Epoch [13/30], Batch [200/6000], Loss: 0.0224
Epoch [13/30], Batch [300/6000], Loss: 0.0173
Epoch [13/30], Batch [400/6000], Loss: 0.0128
Epoch [13/30], Batch [500/6000], Loss: 0.0241
Epoch [13/30], Batch [600/6000], Loss: 0.0166
Epoch [13/30], Batch [700/6000], Loss: 0.0233
Epoch [13/30], Batch [800/6000], Loss: 0.0160
Epoch [13/30], Batch [900/6000], Loss: 0.0364
Epoch [13/30], Batch [1000/6000], Loss: 0.0198
Epoch [13/30], Batch [1100/6000], Loss: 0.0199
Epoch [13/30], Batch [1200/6000], Loss: 0.0192
Epoch [13/30], Batch [1300/6000], Loss: 0.0164
Epoch [13/30], Batch [1400/6000], Loss: 0.0200
Epoch [13/30], Batch [1500/6000], Loss: 0.0254
Epoch [13/30], Batch [1600/6000], Loss: 0.0155
Epoch [13/30], Batch [1700/6000], Loss: 0.0214
Epoch [13/30], Batch [1800/6000], Loss: 0.0201
Epoch [13/30], Batch [1900/6000], Loss: 0.0158
Epoch [13/30], Batch [2000/6000], Loss: 0.0163
Epoch [13/30], Batch [2100/6000], Loss: 0.0151
Epoch [13/30], Batch [2200/6000], Loss: 0.0215
Epoch [13/30], Batch [2300/6000], Loss: 0.0139
Epoch [13/30], Batch [2400/6000], Loss: 0.0151
Epoch [13/30], Batch [2500/6000], Loss: 0.0190
Epoch [13/30], Batch [2600/6000], Loss: 0.0179
Epoch [13/30], Batch [2700/6000], Loss: 0.0212
Epoch [13/30], Batch [2800/6000], Loss: 0.0192
Epoch [13/30], Batch [2900/6000], Loss: 0.0360
Epoch [13/30], Batch [3000/6000], Loss: 0.0180
Epoch [13/30], Batch [3100/6000], Loss: 0.0211
Epoch [13/30], Batch [3200/6000], Loss: 0.0183
Epoch [13/30], Batch [3300/6000], Loss: 0.0203
Epoch [13/30], Batch [3400/6000], Loss: 0.0184
Epoch [13/30], Batch [3500/6000], Loss: 0.0322
Epoch [13/30], Batch [3600/6000], Loss: 0.0164
Epoch [13/30], Batch [3700/6000], Loss: 0.0196
Epoch [13/30], Batch [3800/6000], Loss: 0.0169
Epoch [13/30], Batch [3900/6000], Loss: 0.0179
Epoch [13/30], Batch [4000/6000], Loss: 0.0105
Epoch [13/30], Batch [4100/6000], Loss: 0.0178
Epoch [13/30], Batch [4200/6000], Loss: 0.0180
Epoch [13/30], Batch [4300/6000], Loss: 0.0144
Epoch [13/30], Batch [4400/6000], Loss: 0.0253
Epoch [13/30], Batch [4500/6000], Loss: 0.0127
Epoch [13/30], Batch [4600/6000], Loss: 0.0181
Epoch [13/30], Batch [4700/6000], Loss: 0.0175
Epoch [13/30], Batch [4800/6000], Loss: 0.0172
Epoch [13/30], Batch [4900/6000], Loss: 0.0177
Epoch [13/30], Batch [5000/6000], Loss: 0.0189
Epoch [13/30], Batch [5100/6000], Loss: 0.0171
Epoch [13/30], Batch [5200/6000], Loss: 0.0223
Epoch [13/30], Batch [5300/6000], Loss: 0.0191
Epoch [13/30], Batch [5400/6000], Loss: 0.0170
Epoch [13/30], Batch [5500/6000], Loss: 0.0427
Epoch [13/30], Batch [5600/6000], Loss: 0.0116
Epoch [13/30], Batch [5700/6000], Loss: 0.0159
Epoch [13/30], Batch [5800/6000], Loss: 0.0162
Epoch [13/30], Batch [5900/6000], Loss: 0.0186
Epoch [13/30], Loss: 0.0189
Epoch [14/30], Batch [0/6000], Loss: 0.0187
Epoch [14/30], Batch [100/6000], Loss: 0.0178
Epoch [14/30], Batch [200/6000], Loss: 0.0157
Epoch [14/30], Batch [300/6000], Loss: 0.0142
Epoch [14/30], Batch [400/6000], Loss: 0.0145
Epoch [14/30], Batch [500/6000], Loss: 0.0153
Epoch [14/30], Batch [600/6000], Loss: 0.0258
Epoch [14/30], Batch [700/6000], Loss: 0.0159
Epoch [14/30], Batch [800/6000], Loss: 0.0149
Epoch [14/30], Batch [900/6000], Loss: 0.0154
Epoch [14/30], Batch [1000/6000], Loss: 0.0140
Epoch [14/30], Batch [1100/6000], Loss: 0.0173
Epoch [14/30], Batch [1200/6000], Loss: 0.0151
Epoch [14/30], Batch [1300/6000], Loss: 0.0186
Epoch [14/30], Batch [1400/6000], Loss: 0.0177
Epoch [14/30], Batch [1500/6000], Loss: 0.0165
Epoch [14/30], Batch [1600/6000], Loss: 0.0175
Epoch [14/30], Batch [1700/6000], Loss: 0.0130
Epoch [14/30], Batch [1800/6000], Loss: 0.0153
Epoch [14/30], Batch [1900/6000], Loss: 0.0134
Epoch [14/30], Batch [2000/6000], Loss: 0.0144
Epoch [14/30], Batch [2100/6000], Loss: 0.0131
Epoch [14/30], Batch [2200/6000], Loss: 0.0203
Epoch [14/30], Batch [2300/6000], Loss: 0.0140
Epoch [14/30], Batch [2400/6000], Loss: 0.0150
Epoch [14/30], Batch [2500/6000], Loss: 0.0194
Epoch [14/30], Batch [2600/6000], Loss: 0.0179
Epoch [14/30], Batch [2700/6000], Loss: 0.0158
Epoch [14/30], Batch [2800/6000], Loss: 0.0195
Epoch [14/30], Batch [2900/6000], Loss: 0.0225
Epoch [14/30], Batch [3000/6000], Loss: 0.0140
Epoch [14/30], Batch [3100/6000], Loss: 0.0156
Epoch [14/30], Batch [3200/6000], Loss: 0.0171
Epoch [14/30], Batch [3300/6000], Loss: 0.0163
Epoch [14/30], Batch [3400/6000], Loss: 0.0143
Epoch [14/30], Batch [3500/6000], Loss: 0.0142
Epoch [14/30], Batch [3600/6000], Loss: 0.0173
Epoch [14/30], Batch [3700/6000], Loss: 0.0151
Epoch [14/30], Batch [3800/6000], Loss: 0.0210
Epoch [14/30], Batch [3900/6000], Loss: 0.0176
Epoch [14/30], Batch [4000/6000], Loss: 0.0198
Epoch [14/30], Batch [4100/6000], Loss: 0.0151
Epoch [14/30], Batch [4200/6000], Loss: 0.0164
Epoch [14/30], Batch [4300/6000], Loss: 0.0182
Epoch [14/30], Batch [4400/6000], Loss: 0.0187
Epoch [14/30], Batch [4500/6000], Loss: 0.0245
Epoch [14/30], Batch [4600/6000], Loss: 0.0211
Epoch [14/30], Batch [4700/6000], Loss: 0.0175
Epoch [14/30], Batch [4800/6000], Loss: 0.0131
Epoch [14/30], Batch [4900/6000], Loss: 0.0151
Epoch [14/30], Batch [5000/6000], Loss: 0.0257
Epoch [14/30], Batch [5100/6000], Loss: 0.0141
Epoch [14/30], Batch [5200/6000], Loss: 0.0153
Epoch [14/30], Batch [5300/6000], Loss: 0.0143
Epoch [14/30], Batch [5400/6000], Loss: 0.0183
Epoch [14/30], Batch [5500/6000], Loss: 0.0153
Epoch [14/30], Batch [5600/6000], Loss: 0.0166
Epoch [14/30], Batch [5700/6000], Loss: 0.0209
Epoch [14/30], Batch [5800/6000], Loss: 0.0200
Epoch [14/30], Batch [5900/6000], Loss: 0.0160
Epoch [14/30], Loss: 0.0183
Epoch [15/30], Batch [0/6000], Loss: 0.0192
Epoch [15/30], Batch [100/6000], Loss: 0.0401
Epoch [15/30], Batch [200/6000], Loss: 0.0165
Epoch [15/30], Batch [300/6000], Loss: 0.0181
Epoch [15/30], Batch [400/6000], Loss: 0.0313
Epoch [15/30], Batch [500/6000], Loss: 0.0155
Epoch [15/30], Batch [600/6000], Loss: 0.0149
Epoch [15/30], Batch [700/6000], Loss: 0.0186
Epoch [15/30], Batch [800/6000], Loss: 0.0207
Epoch [15/30], Batch [900/6000], Loss: 0.0197
Epoch [15/30], Batch [1000/6000], Loss: 0.0187
Epoch [15/30], Batch [1100/6000], Loss: 0.0176
Epoch [15/30], Batch [1200/6000], Loss: 0.0199
Epoch [15/30], Batch [1300/6000], Loss: 0.0160
Epoch [15/30], Batch [1400/6000], Loss: 0.0177
Epoch [15/30], Batch [1500/6000], Loss: 0.0176
Epoch [15/30], Batch [1600/6000], Loss: 0.0181
Epoch [15/30], Batch [1700/6000], Loss: 0.0157
Epoch [15/30], Batch [1800/6000], Loss: 0.0147
Epoch [15/30], Batch [1900/6000], Loss: 0.0158
Epoch [15/30], Batch [2000/6000], Loss: 0.0161
Epoch [15/30], Batch [2100/6000], Loss: 0.0180
Epoch [15/30], Batch [2200/6000], Loss: 0.0191
Epoch [15/30], Batch [2300/6000], Loss: 0.0289
Epoch [15/30], Batch [2400/6000], Loss: 0.0198
Epoch [15/30], Batch [2500/6000], Loss: 0.0142
Epoch [15/30], Batch [2600/6000], Loss: 0.0181
Epoch [15/30], Batch [2700/6000], Loss: 0.0203
Epoch [15/30], Batch [2800/6000], Loss: 0.0151
Epoch [15/30], Batch [2900/6000], Loss: 0.0130
Epoch [15/30], Batch [3000/6000], Loss: 0.0140
Epoch [15/30], Batch [3100/6000], Loss: 0.0143
Epoch [15/30], Batch [3200/6000], Loss: 0.0178
Epoch [15/30], Batch [3300/6000], Loss: 0.0182
Epoch [15/30], Batch [3400/6000], Loss: 0.0171
Epoch [15/30], Batch [3500/6000], Loss: 0.0187
Epoch [15/30], Batch [3600/6000], Loss: 0.0156
Epoch [15/30], Batch [3700/6000], Loss: 0.0164
Epoch [15/30], Batch [3800/6000], Loss: 0.0237
Epoch [15/30], Batch [3900/6000], Loss: 0.0148
Epoch [15/30], Batch [4000/6000], Loss: 0.0194
Epoch [15/30], Batch [4100/6000], Loss: 0.0180
Epoch [15/30], Batch [4200/6000], Loss: 0.0166
Epoch [15/30], Batch [4300/6000], Loss: 0.0142
Epoch [15/30], Batch [4400/6000], Loss: 0.0133
Epoch [15/30], Batch [4500/6000], Loss: 0.0162
Epoch [15/30], Batch [4600/6000], Loss: 0.0152
Epoch [15/30], Batch [4700/6000], Loss: 0.0165
Epoch [15/30], Batch [4800/6000], Loss: 0.0146
Epoch [15/30], Batch [4900/6000], Loss: 0.0217
Epoch [15/30], Batch [5000/6000], Loss: 0.0190
Epoch [15/30], Batch [5100/6000], Loss: 0.0145
Epoch [15/30], Batch [5200/6000], Loss: 0.0193
Epoch [15/30], Batch [5300/6000], Loss: 0.0223
Epoch [15/30], Batch [5400/6000], Loss: 0.0183
Epoch [15/30], Batch [5500/6000], Loss: 0.0168
Epoch [15/30], Batch [5600/6000], Loss: 0.0198
Epoch [15/30], Batch [5700/6000], Loss: 0.0142
Epoch [15/30], Batch [5800/6000], Loss: 0.0143
Epoch [15/30], Batch [5900/6000], Loss: 0.0152
Epoch [15/30], Loss: 0.0176
Epoch [16/30], Batch [0/6000], Loss: 0.0134
Epoch [16/30], Batch [100/6000], Loss: 0.0133
Epoch [16/30], Batch [200/6000], Loss: 0.0240
Epoch [16/30], Batch [300/6000], Loss: 0.0161
Epoch [16/30], Batch [400/6000], Loss: 0.0196
Epoch [16/30], Batch [500/6000], Loss: 0.0190
Epoch [16/30], Batch [600/6000], Loss: 0.0109
Epoch [16/30], Batch [700/6000], Loss: 0.0182
Epoch [16/30], Batch [800/6000], Loss: 0.0153
Epoch [16/30], Batch [900/6000], Loss: 0.0184
Epoch [16/30], Batch [1000/6000], Loss: 0.0163
Epoch [16/30], Batch [1100/6000], Loss: 0.0156
Epoch [16/30], Batch [1200/6000], Loss: 0.0190
Epoch [16/30], Batch [1300/6000], Loss: 0.0130
Epoch [16/30], Batch [1400/6000], Loss: 0.0142
Epoch [16/30], Batch [1500/6000], Loss: 0.0212
Epoch [16/30], Batch [1600/6000], Loss: 0.0151
Epoch [16/30], Batch [1700/6000], Loss: 0.0165
Epoch [16/30], Batch [1800/6000], Loss: 0.0178
Epoch [16/30], Batch [1900/6000], Loss: 0.0200
Epoch [16/30], Batch [2000/6000], Loss: 0.0155
Epoch [16/30], Batch [2100/6000], Loss: 0.0101
Epoch [16/30], Batch [2200/6000], Loss: 0.0194
Epoch [16/30], Batch [2300/6000], Loss: 0.0160
Epoch [16/30], Batch [2400/6000], Loss: 0.0176
Epoch [16/30], Batch [2500/6000], Loss: 0.0182
Epoch [16/30], Batch [2600/6000], Loss: 0.0174
Epoch [16/30], Batch [2700/6000], Loss: 0.0188
Epoch [16/30], Batch [2800/6000], Loss: 0.0198
Epoch [16/30], Batch [2900/6000], Loss: 0.0156
Epoch [16/30], Batch [3000/6000], Loss: 0.0214
Epoch [16/30], Batch [3100/6000], Loss: 0.0178
Epoch [16/30], Batch [3200/6000], Loss: 0.0150
Epoch [16/30], Batch [3300/6000], Loss: 0.0273
Epoch [16/30], Batch [3400/6000], Loss: 0.0162
Epoch [16/30], Batch [3500/6000], Loss: 0.0160
Epoch [16/30], Batch [3600/6000], Loss: 0.0247
Epoch [16/30], Batch [3700/6000], Loss: 0.0138
Epoch [16/30], Batch [3800/6000], Loss: 0.0213
Epoch [16/30], Batch [3900/6000], Loss: 0.0149
Epoch [16/30], Batch [4000/6000], Loss: 0.0152
Epoch [16/30], Batch [4100/6000], Loss: 0.0179
Epoch [16/30], Batch [4200/6000], Loss: 0.0168
Epoch [16/30], Batch [4300/6000], Loss: 0.0151
Epoch [16/30], Batch [4400/6000], Loss: 0.0141
Epoch [16/30], Batch [4500/6000], Loss: 0.0123
Epoch [16/30], Batch [4600/6000], Loss: 0.0161
Epoch [16/30], Batch [4700/6000], Loss: 0.0182
Epoch [16/30], Batch [4800/6000], Loss: 0.0138
Epoch [16/30], Batch [4900/6000], Loss: 0.0222
Epoch [16/30], Batch [5000/6000], Loss: 0.0163
Epoch [16/30], Batch [5100/6000], Loss: 0.0155
Epoch [16/30], Batch [5200/6000], Loss: 0.0165
Epoch [16/30], Batch [5300/6000], Loss: 0.0159
Epoch [16/30], Batch [5400/6000], Loss: 0.0159
Epoch [16/30], Batch [5500/6000], Loss: 0.0156
Epoch [16/30], Batch [5600/6000], Loss: 0.0161
Epoch [16/30], Batch [5700/6000], Loss: 0.0111
Epoch [16/30], Batch [5800/6000], Loss: 0.0166
Epoch [16/30], Batch [5900/6000], Loss: 0.0187
Epoch [16/30], Loss: 0.0170
Epoch [17/30], Batch [0/6000], Loss: 0.0127
Epoch [17/30], Batch [100/6000], Loss: 0.0143
Epoch [17/30], Batch [200/6000], Loss: 0.0200
Epoch [17/30], Batch [300/6000], Loss: 0.0151
Epoch [17/30], Batch [400/6000], Loss: 0.0162
Epoch [17/30], Batch [500/6000], Loss: 0.0156
Epoch [17/30], Batch [600/6000], Loss: 0.0196
Epoch [17/30], Batch [700/6000], Loss: 0.0143
Epoch [17/30], Batch [800/6000], Loss: 0.0174
Epoch [17/30], Batch [900/6000], Loss: 0.0119
Epoch [17/30], Batch [1000/6000], Loss: 0.0197
Epoch [17/30], Batch [1100/6000], Loss: 0.0187
Epoch [17/30], Batch [1200/6000], Loss: 0.0149
Epoch [17/30], Batch [1300/6000], Loss: 0.0137
Epoch [17/30], Batch [1400/6000], Loss: 0.0181
Epoch [17/30], Batch [1500/6000], Loss: 0.0148
Epoch [17/30], Batch [1600/6000], Loss: 0.0155
Epoch [17/30], Batch [1700/6000], Loss: 0.0158
Epoch [17/30], Batch [1800/6000], Loss: 0.0144
Epoch [17/30], Batch [1900/6000], Loss: 0.0134
Epoch [17/30], Batch [2000/6000], Loss: 0.0162
Epoch [17/30], Batch [2100/6000], Loss: 0.0139
Epoch [17/30], Batch [2200/6000], Loss: 0.0342
Epoch [17/30], Batch [2300/6000], Loss: 0.0161
Epoch [17/30], Batch [2400/6000], Loss: 0.0166
Epoch [17/30], Batch [2500/6000], Loss: 0.0122
Epoch [17/30], Batch [2600/6000], Loss: 0.0148
Epoch [17/30], Batch [2700/6000], Loss: 0.0145
Epoch [17/30], Batch [2800/6000], Loss: 0.0133
Epoch [17/30], Batch [2900/6000], Loss: 0.0336
Epoch [17/30], Batch [3000/6000], Loss: 0.0198
Epoch [17/30], Batch [3100/6000], Loss: 0.0288
Epoch [17/30], Batch [3200/6000], Loss: 0.0166
Epoch [17/30], Batch [3300/6000], Loss: 0.0144
Epoch [17/30], Batch [3400/6000], Loss: 0.0186
Epoch [17/30], Batch [3500/6000], Loss: 0.0281
Epoch [17/30], Batch [3600/6000], Loss: 0.0177
Epoch [17/30], Batch [3700/6000], Loss: 0.0186
Epoch [17/30], Batch [3800/6000], Loss: 0.0138
Epoch [17/30], Batch [3900/6000], Loss: 0.0144
Epoch [17/30], Batch [4000/6000], Loss: 0.0122
Epoch [17/30], Batch [4100/6000], Loss: 0.0350
Epoch [17/30], Batch [4200/6000], Loss: 0.0160
Epoch [17/30], Batch [4300/6000], Loss: 0.0126
Epoch [17/30], Batch [4400/6000], Loss: 0.0146
Epoch [17/30], Batch [4500/6000], Loss: 0.0132
Epoch [17/30], Batch [4600/6000], Loss: 0.0144
Epoch [17/30], Batch [4700/6000], Loss: 0.0178
Epoch [17/30], Batch [4800/6000], Loss: 0.0122
Epoch [17/30], Batch [4900/6000], Loss: 0.0173
Epoch [17/30], Batch [5000/6000], Loss: 0.0151
Epoch [17/30], Batch [5100/6000], Loss: 0.0151
Epoch [17/30], Batch [5200/6000], Loss: 0.0158
Epoch [17/30], Batch [5300/6000], Loss: 0.0149
Epoch [17/30], Batch [5400/6000], Loss: 0.0126
Epoch [17/30], Batch [5500/6000], Loss: 0.0143
Epoch [17/30], Batch [5600/6000], Loss: 0.0154
Epoch [17/30], Batch [5700/6000], Loss: 0.0143
Epoch [17/30], Batch [5800/6000], Loss: 0.0173
Epoch [17/30], Batch [5900/6000], Loss: 0.0149
Epoch [17/30], Loss: 0.0165
Epoch [18/30], Batch [0/6000], Loss: 0.0154
Epoch [18/30], Batch [100/6000], Loss: 0.0169
Epoch [18/30], Batch [200/6000], Loss: 0.0159
Epoch [18/30], Batch [300/6000], Loss: 0.0127
Epoch [18/30], Batch [400/6000], Loss: 0.0167
Epoch [18/30], Batch [500/6000], Loss: 0.0165
Epoch [18/30], Batch [600/6000], Loss: 0.0151
Epoch [18/30], Batch [700/6000], Loss: 0.0145
Epoch [18/30], Batch [800/6000], Loss: 0.0149
Epoch [18/30], Batch [900/6000], Loss: 0.0197
Epoch [18/30], Batch [1000/6000], Loss: 0.0160
Epoch [18/30], Batch [1100/6000], Loss: 0.0151
Epoch [18/30], Batch [1200/6000], Loss: 0.0154
Epoch [18/30], Batch [1300/6000], Loss: 0.0156
Epoch [18/30], Batch [1400/6000], Loss: 0.0150
Epoch [18/30], Batch [1500/6000], Loss: 0.0151
Epoch [18/30], Batch [1600/6000], Loss: 0.0130
Epoch [18/30], Batch [1700/6000], Loss: 0.0153
Epoch [18/30], Batch [1800/6000], Loss: 0.0175
Epoch [18/30], Batch [1900/6000], Loss: 0.0167
Epoch [18/30], Batch [2000/6000], Loss: 0.0186
Epoch [18/30], Batch [2100/6000], Loss: 0.0152
Epoch [18/30], Batch [2200/6000], Loss: 0.0178
Epoch [18/30], Batch [2300/6000], Loss: 0.0136
Epoch [18/30], Batch [2400/6000], Loss: 0.0151
Epoch [18/30], Batch [2500/6000], Loss: 0.0146
Epoch [18/30], Batch [2600/6000], Loss: 0.0138
Epoch [18/30], Batch [2700/6000], Loss: 0.0169
Epoch [18/30], Batch [2800/6000], Loss: 0.0220
Epoch [18/30], Batch [2900/6000], Loss: 0.0127
Epoch [18/30], Batch [3000/6000], Loss: 0.0161
Epoch [18/30], Batch [3100/6000], Loss: 0.0171
Epoch [18/30], Batch [3200/6000], Loss: 0.0207
Epoch [18/30], Batch [3300/6000], Loss: 0.0189
Epoch [18/30], Batch [3400/6000], Loss: 0.0147
Epoch [18/30], Batch [3500/6000], Loss: 0.0190
Epoch [18/30], Batch [3600/6000], Loss: 0.0160
Epoch [18/30], Batch [3700/6000], Loss: 0.0104
Epoch [18/30], Batch [3800/6000], Loss: 0.0152
Epoch [18/30], Batch [3900/6000], Loss: 0.0156
Epoch [18/30], Batch [4000/6000], Loss: 0.0195
Epoch [18/30], Batch [4100/6000], Loss: 0.0177
Epoch [18/30], Batch [4200/6000], Loss: 0.0164
Epoch [18/30], Batch [4300/6000], Loss: 0.0106
Epoch [18/30], Batch [4400/6000], Loss: 0.0160
Epoch [18/30], Batch [4500/6000], Loss: 0.0172
Epoch [18/30], Batch [4600/6000], Loss: 0.0269
Epoch [18/30], Batch [4700/6000], Loss: 0.0177
Epoch [18/30], Batch [4800/6000], Loss: 0.0160
Epoch [18/30], Batch [4900/6000], Loss: 0.0367
Epoch [18/30], Batch [5000/6000], Loss: 0.0150
Epoch [18/30], Batch [5100/6000], Loss: 0.0123
Epoch [18/30], Batch [5200/6000], Loss: 0.0139
Epoch [18/30], Batch [5300/6000], Loss: 0.0146
Epoch [18/30], Batch [5400/6000], Loss: 0.0153
Epoch [18/30], Batch [5500/6000], Loss: 0.0136
Epoch [18/30], Batch [5600/6000], Loss: 0.0160
Epoch [18/30], Batch [5700/6000], Loss: 0.0143
Epoch [18/30], Batch [5800/6000], Loss: 0.0134
Epoch [18/30], Batch [5900/6000], Loss: 0.0213
Epoch [18/30], Loss: 0.0161
Epoch [19/30], Batch [0/6000], Loss: 0.0167
Epoch [19/30], Batch [100/6000], Loss: 0.0139
Epoch [19/30], Batch [200/6000], Loss: 0.0137
Epoch [19/30], Batch [300/6000], Loss: 0.0125
Epoch [19/30], Batch [400/6000], Loss: 0.0168
Epoch [19/30], Batch [500/6000], Loss: 0.0167
Epoch [19/30], Batch [600/6000], Loss: 0.0199
Epoch [19/30], Batch [700/6000], Loss: 0.0156
Epoch [19/30], Batch [800/6000], Loss: 0.0090
Epoch [19/30], Batch [900/6000], Loss: 0.0149
Epoch [19/30], Batch [1000/6000], Loss: 0.0095
Epoch [19/30], Batch [1100/6000], Loss: 0.0131
Epoch [19/30], Batch [1200/6000], Loss: 0.0144
Epoch [19/30], Batch [1300/6000], Loss: 0.0156
Epoch [19/30], Batch [1400/6000], Loss: 0.0159
Epoch [19/30], Batch [1500/6000], Loss: 0.0174
Epoch [19/30], Batch [1600/6000], Loss: 0.0160
Epoch [19/30], Batch [1700/6000], Loss: 0.0165
Epoch [19/30], Batch [1800/6000], Loss: 0.0116
Epoch [19/30], Batch [1900/6000], Loss: 0.0189
Epoch [19/30], Batch [2000/6000], Loss: 0.0148
Epoch [19/30], Batch [2100/6000], Loss: 0.0140
Epoch [19/30], Batch [2200/6000], Loss: 0.0152
Epoch [19/30], Batch [2300/6000], Loss: 0.0169
Epoch [19/30], Batch [2400/6000], Loss: 0.0129
Epoch [19/30], Batch [2500/6000], Loss: 0.0216
Epoch [19/30], Batch [2600/6000], Loss: 0.0154
Epoch [19/30], Batch [2700/6000], Loss: 0.0158
Epoch [19/30], Batch [2800/6000], Loss: 0.0178
Epoch [19/30], Batch [2900/6000], Loss: 0.0127
Epoch [19/30], Batch [3000/6000], Loss: 0.0148
Epoch [19/30], Batch [3100/6000], Loss: 0.0117
Epoch [19/30], Batch [3200/6000], Loss: 0.0140
Epoch [19/30], Batch [3300/6000], Loss: 0.0194
Epoch [19/30], Batch [3400/6000], Loss: 0.0145
Epoch [19/30], Batch [3500/6000], Loss: 0.0125
Epoch [19/30], Batch [3600/6000], Loss: 0.0147
Epoch [19/30], Batch [3700/6000], Loss: 0.0176
Epoch [19/30], Batch [3800/6000], Loss: 0.0160
Epoch [19/30], Batch [3900/6000], Loss: 0.0151
Epoch [19/30], Batch [4000/6000], Loss: 0.0157
Epoch [19/30], Batch [4100/6000], Loss: 0.0141
Epoch [19/30], Batch [4200/6000], Loss: 0.0134
Epoch [19/30], Batch [4300/6000], Loss: 0.0155
Epoch [19/30], Batch [4400/6000], Loss: 0.0135
Epoch [19/30], Batch [4500/6000], Loss: 0.0266
Epoch [19/30], Batch [4600/6000], Loss: 0.0129
Epoch [19/30], Batch [4700/6000], Loss: 0.0128
Epoch [19/30], Batch [4800/6000], Loss: 0.0131
Epoch [19/30], Batch [4900/6000], Loss: 0.0134
Epoch [19/30], Batch [5000/6000], Loss: 0.0171
Epoch [19/30], Batch [5100/6000], Loss: 0.0149
Epoch [19/30], Batch [5200/6000], Loss: 0.0142
Epoch [19/30], Batch [5300/6000], Loss: 0.0509
Epoch [19/30], Batch [5400/6000], Loss: 0.0375
Epoch [19/30], Batch [5500/6000], Loss: 0.0162
Epoch [19/30], Batch [5600/6000], Loss: 0.0129
Epoch [19/30], Batch [5700/6000], Loss: 0.0137
Epoch [19/30], Batch [5800/6000], Loss: 0.0117
Epoch [19/30], Batch [5900/6000], Loss: 0.0152
Epoch [19/30], Loss: 0.0156
Epoch [20/30], Batch [0/6000], Loss: 0.0140
Epoch [20/30], Batch [100/6000], Loss: 0.0154
Epoch [20/30], Batch [200/6000], Loss: 0.0150
Epoch [20/30], Batch [300/6000], Loss: 0.0207
Epoch [20/30], Batch [400/6000], Loss: 0.0287
Epoch [20/30], Batch [500/6000], Loss: 0.0155
Epoch [20/30], Batch [600/6000], Loss: 0.0136
Epoch [20/30], Batch [700/6000], Loss: 0.0158
Epoch [20/30], Batch [800/6000], Loss: 0.0139
Epoch [20/30], Batch [900/6000], Loss: 0.0127
Epoch [20/30], Batch [1000/6000], Loss: 0.0106
Epoch [20/30], Batch [1100/6000], Loss: 0.0111
Epoch [20/30], Batch [1200/6000], Loss: 0.0175
Epoch [20/30], Batch [1300/6000], Loss: 0.0106
Epoch [20/30], Batch [1400/6000], Loss: 0.0167
Epoch [20/30], Batch [1500/6000], Loss: 0.0201
Epoch [20/30], Batch [1600/6000], Loss: 0.0137
Epoch [20/30], Batch [1700/6000], Loss: 0.0145
Epoch [20/30], Batch [1800/6000], Loss: 0.0152
Epoch [20/30], Batch [1900/6000], Loss: 0.0121
Epoch [20/30], Batch [2000/6000], Loss: 0.0111
Epoch [20/30], Batch [2100/6000], Loss: 0.0137
Epoch [20/30], Batch [2200/6000], Loss: 0.0144
Epoch [20/30], Batch [2300/6000], Loss: 0.0355
Epoch [20/30], Batch [2400/6000], Loss: 0.0152
Epoch [20/30], Batch [2500/6000], Loss: 0.0171
Epoch [20/30], Batch [2600/6000], Loss: 0.0162
Epoch [20/30], Batch [2700/6000], Loss: 0.0171
Epoch [20/30], Batch [2800/6000], Loss: 0.0135
Epoch [20/30], Batch [2900/6000], Loss: 0.0157
Epoch [20/30], Batch [3000/6000], Loss: 0.0138
Epoch [20/30], Batch [3100/6000], Loss: 0.0141
Epoch [20/30], Batch [3200/6000], Loss: 0.0153
Epoch [20/30], Batch [3300/6000], Loss: 0.0120
Epoch [20/30], Batch [3400/6000], Loss: 0.0149
Epoch [20/30], Batch [3500/6000], Loss: 0.0120
Epoch [20/30], Batch [3600/6000], Loss: 0.0116
Epoch [20/30], Batch [3700/6000], Loss: 0.0169
Epoch [20/30], Batch [3800/6000], Loss: 0.0129
Epoch [20/30], Batch [3900/6000], Loss: 0.0193
Epoch [20/30], Batch [4000/6000], Loss: 0.0122
Epoch [20/30], Batch [4100/6000], Loss: 0.0123
Epoch [20/30], Batch [4200/6000], Loss: 0.0140
Epoch [20/30], Batch [4300/6000], Loss: 0.0108
Epoch [20/30], Batch [4400/6000], Loss: 0.0113
Epoch [20/30], Batch [4500/6000], Loss: 0.0119
Epoch [20/30], Batch [4600/6000], Loss: 0.0319
Epoch [20/30], Batch [4700/6000], Loss: 0.0142
Epoch [20/30], Batch [4800/6000], Loss: 0.0115
Epoch [20/30], Batch [4900/6000], Loss: 0.0220
Epoch [20/30], Batch [5000/6000], Loss: 0.0170
Epoch [20/30], Batch [5100/6000], Loss: 0.0146
Epoch [20/30], Batch [5200/6000], Loss: 0.0188
Epoch [20/30], Batch [5300/6000], Loss: 0.0232
Epoch [20/30], Batch [5400/6000], Loss: 0.0132
Epoch [20/30], Batch [5500/6000], Loss: 0.0144
Epoch [20/30], Batch [5600/6000], Loss: 0.0120
Epoch [20/30], Batch [5700/6000], Loss: 0.0120
Epoch [20/30], Batch [5800/6000], Loss: 0.0186
Epoch [20/30], Batch [5900/6000], Loss: 0.0126
Epoch [20/30], Loss: 0.0152
Epoch [21/30], Batch [0/6000], Loss: 0.0169
Epoch [21/30], Batch [100/6000], Loss: 0.0143
Epoch [21/30], Batch [200/6000], Loss: 0.0108
Epoch [21/30], Batch [300/6000], Loss: 0.0146
Epoch [21/30], Batch [400/6000], Loss: 0.0136
Epoch [21/30], Batch [500/6000], Loss: 0.0136
Epoch [21/30], Batch [600/6000], Loss: 0.0120
Epoch [21/30], Batch [700/6000], Loss: 0.0166
Epoch [21/30], Batch [800/6000], Loss: 0.0113
Epoch [21/30], Batch [900/6000], Loss: 0.0197
Epoch [21/30], Batch [1000/6000], Loss: 0.0184
Epoch [21/30], Batch [1100/6000], Loss: 0.0150
Epoch [21/30], Batch [1200/6000], Loss: 0.0154
Epoch [21/30], Batch [1300/6000], Loss: 0.0217
Epoch [21/30], Batch [1400/6000], Loss: 0.0166
Epoch [21/30], Batch [1500/6000], Loss: 0.0131
Epoch [21/30], Batch [1600/6000], Loss: 0.0143
Epoch [21/30], Batch [1700/6000], Loss: 0.0197
Epoch [21/30], Batch [1800/6000], Loss: 0.0140
Epoch [21/30], Batch [1900/6000], Loss: 0.0128
Epoch [21/30], Batch [2000/6000], Loss: 0.0168
Epoch [21/30], Batch [2100/6000], Loss: 0.0181
Epoch [21/30], Batch [2200/6000], Loss: 0.0157
Epoch [21/30], Batch [2300/6000], Loss: 0.0144
Epoch [21/30], Batch [2400/6000], Loss: 0.0103
Epoch [21/30], Batch [2500/6000], Loss: 0.0147
Epoch [21/30], Batch [2600/6000], Loss: 0.0098
Epoch [21/30], Batch [2700/6000], Loss: 0.0104
Epoch [21/30], Batch [2800/6000], Loss: 0.0144
Epoch [21/30], Batch [2900/6000], Loss: 0.0153
Epoch [21/30], Batch [3000/6000], Loss: 0.0139
Epoch [21/30], Batch [3100/6000], Loss: 0.0138
Epoch [21/30], Batch [3200/6000], Loss: 0.0142
Epoch [21/30], Batch [3300/6000], Loss: 0.0165
Epoch [21/30], Batch [3400/6000], Loss: 0.0145
Epoch [21/30], Batch [3500/6000], Loss: 0.0153
Epoch [21/30], Batch [3600/6000], Loss: 0.0197
Epoch [21/30], Batch [3700/6000], Loss: 0.0122
Epoch [21/30], Batch [3800/6000], Loss: 0.0174
Epoch [21/30], Batch [3900/6000], Loss: 0.0204
Epoch [21/30], Batch [4000/6000], Loss: 0.0110
Epoch [21/30], Batch [4100/6000], Loss: 0.0101
Epoch [21/30], Batch [4200/6000], Loss: 0.0124
Epoch [21/30], Batch [4300/6000], Loss: 0.0167
Epoch [21/30], Batch [4400/6000], Loss: 0.0161
Epoch [21/30], Batch [4500/6000], Loss: 0.0426
Epoch [21/30], Batch [4600/6000], Loss: 0.0250
Epoch [21/30], Batch [4700/6000], Loss: 0.0152
Epoch [21/30], Batch [4800/6000], Loss: 0.0123
Epoch [21/30], Batch [4900/6000], Loss: 0.0172
Epoch [21/30], Batch [5000/6000], Loss: 0.0153
Epoch [21/30], Batch [5100/6000], Loss: 0.0130
Epoch [21/30], Batch [5200/6000], Loss: 0.0183
Epoch [21/30], Batch [5300/6000], Loss: 0.0157
Epoch [21/30], Batch [5400/6000], Loss: 0.0134
Epoch [21/30], Batch [5500/6000], Loss: 0.0141
Epoch [21/30], Batch [5600/6000], Loss: 0.0146
Epoch [21/30], Batch [5700/6000], Loss: 0.0124
Epoch [21/30], Batch [5800/6000], Loss: 0.0109
Epoch [21/30], Batch [5900/6000], Loss: 0.0105
Epoch [21/30], Loss: 0.0149
Epoch [22/30], Batch [0/6000], Loss: 0.0137
Epoch [22/30], Batch [100/6000], Loss: 0.0184
Epoch [22/30], Batch [200/6000], Loss: 0.0158
Epoch [22/30], Batch [300/6000], Loss: 0.0106
Epoch [22/30], Batch [400/6000], Loss: 0.0175
Epoch [22/30], Batch [500/6000], Loss: 0.0137
Epoch [22/30], Batch [600/6000], Loss: 0.0149
Epoch [22/30], Batch [700/6000], Loss: 0.0119
Epoch [22/30], Batch [800/6000], Loss: 0.0155
Epoch [22/30], Batch [900/6000], Loss: 0.0153
Epoch [22/30], Batch [1000/6000], Loss: 0.0313
Epoch [22/30], Batch [1100/6000], Loss: 0.0151
Epoch [22/30], Batch [1200/6000], Loss: 0.0168
Epoch [22/30], Batch [1300/6000], Loss: 0.0140
Epoch [22/30], Batch [1400/6000], Loss: 0.0100
Epoch [22/30], Batch [1500/6000], Loss: 0.0161
Epoch [22/30], Batch [1600/6000], Loss: 0.0242
Epoch [22/30], Batch [1700/6000], Loss: 0.0139
Epoch [22/30], Batch [1800/6000], Loss: 0.0131
Epoch [22/30], Batch [1900/6000], Loss: 0.0173
Epoch [22/30], Batch [2000/6000], Loss: 0.0127
Epoch [22/30], Batch [2100/6000], Loss: 0.0149
Epoch [22/30], Batch [2200/6000], Loss: 0.0123
Epoch [22/30], Batch [2300/6000], Loss: 0.0127
Epoch [22/30], Batch [2400/6000], Loss: 0.0108
Epoch [22/30], Batch [2500/6000], Loss: 0.0121
Epoch [22/30], Batch [2600/6000], Loss: 0.0124
Epoch [22/30], Batch [2700/6000], Loss: 0.0135
Epoch [22/30], Batch [2800/6000], Loss: 0.0153
Epoch [22/30], Batch [2900/6000], Loss: 0.0126
Epoch [22/30], Batch [3000/6000], Loss: 0.0166
Epoch [22/30], Batch [3100/6000], Loss: 0.0087
Epoch [22/30], Batch [3200/6000], Loss: 0.0136
Epoch [22/30], Batch [3300/6000], Loss: 0.0147
Epoch [22/30], Batch [3400/6000], Loss: 0.0188
Epoch [22/30], Batch [3500/6000], Loss: 0.0133
Epoch [22/30], Batch [3600/6000], Loss: 0.0142
Epoch [22/30], Batch [3700/6000], Loss: 0.0156
Epoch [22/30], Batch [3800/6000], Loss: 0.0124
Epoch [22/30], Batch [3900/6000], Loss: 0.0117
Epoch [22/30], Batch [4000/6000], Loss: 0.0116
Epoch [22/30], Batch [4100/6000], Loss: 0.0101
Epoch [22/30], Batch [4200/6000], Loss: 0.0147
Epoch [22/30], Batch [4300/6000], Loss: 0.0122
Epoch [22/30], Batch [4400/6000], Loss: 0.0170
Epoch [22/30], Batch [4500/6000], Loss: 0.0155
Epoch [22/30], Batch [4600/6000], Loss: 0.0114
Epoch [22/30], Batch [4700/6000], Loss: 0.0170
Epoch [22/30], Batch [4800/6000], Loss: 0.0140
Epoch [22/30], Batch [4900/6000], Loss: 0.0101
Epoch [22/30], Batch [5000/6000], Loss: 0.0152
Epoch [22/30], Batch [5100/6000], Loss: 0.0134
Epoch [22/30], Batch [5200/6000], Loss: 0.0148
Epoch [22/30], Batch [5300/6000], Loss: 0.0144
Epoch [22/30], Batch [5400/6000], Loss: 0.0141
Epoch [22/30], Batch [5500/6000], Loss: 0.0125
Epoch [22/30], Batch [5600/6000], Loss: 0.0159
Epoch [22/30], Batch [5700/6000], Loss: 0.0163
Epoch [22/30], Batch [5800/6000], Loss: 0.0171
Epoch [22/30], Batch [5900/6000], Loss: 0.0146
Epoch [22/30], Loss: 0.0145
Epoch [23/30], Batch [0/6000], Loss: 0.0148
Epoch [23/30], Batch [100/6000], Loss: 0.0105
Epoch [23/30], Batch [200/6000], Loss: 0.0112
Epoch [23/30], Batch [300/6000], Loss: 0.0137
Epoch [23/30], Batch [400/6000], Loss: 0.0133
Epoch [23/30], Batch [500/6000], Loss: 0.0127
Epoch [23/30], Batch [600/6000], Loss: 0.0112
Epoch [23/30], Batch [700/6000], Loss: 0.0120
Epoch [23/30], Batch [800/6000], Loss: 0.0144
Epoch [23/30], Batch [900/6000], Loss: 0.0149
Epoch [23/30], Batch [1000/6000], Loss: 0.0173
Epoch [23/30], Batch [1100/6000], Loss: 0.0116
Epoch [23/30], Batch [1200/6000], Loss: 0.0128
Epoch [23/30], Batch [1300/6000], Loss: 0.0129
Epoch [23/30], Batch [1400/6000], Loss: 0.0124
Epoch [23/30], Batch [1500/6000], Loss: 0.0194
Epoch [23/30], Batch [1600/6000], Loss: 0.0114
Epoch [23/30], Batch [1700/6000], Loss: 0.0103
Epoch [23/30], Batch [1800/6000], Loss: 0.0139
Epoch [23/30], Batch [1900/6000], Loss: 0.0125
Epoch [23/30], Batch [2000/6000], Loss: 0.0154
Epoch [23/30], Batch [2100/6000], Loss: 0.0151
Epoch [23/30], Batch [2200/6000], Loss: 0.0164
Epoch [23/30], Batch [2300/6000], Loss: 0.0156
Epoch [23/30], Batch [2400/6000], Loss: 0.0104
Epoch [23/30], Batch [2500/6000], Loss: 0.0144
Epoch [23/30], Batch [2600/6000], Loss: 0.0123
Epoch [23/30], Batch [2700/6000], Loss: 0.0108
Epoch [23/30], Batch [2800/6000], Loss: 0.0180
Epoch [23/30], Batch [2900/6000], Loss: 0.0142
Epoch [23/30], Batch [3000/6000], Loss: 0.0150
Epoch [23/30], Batch [3100/6000], Loss: 0.0157
Epoch [23/30], Batch [3200/6000], Loss: 0.0152
Epoch [23/30], Batch [3300/6000], Loss: 0.0160
Epoch [23/30], Batch [3400/6000], Loss: 0.0133
Epoch [23/30], Batch [3500/6000], Loss: 0.0142
Epoch [23/30], Batch [3600/6000], Loss: 0.0132
Epoch [23/30], Batch [3700/6000], Loss: 0.0122
Epoch [23/30], Batch [3800/6000], Loss: 0.0167
Epoch [23/30], Batch [3900/6000], Loss: 0.0134
Epoch [23/30], Batch [4000/6000], Loss: 0.0122
Epoch [23/30], Batch [4100/6000], Loss: 0.0143
Epoch [23/30], Batch [4200/6000], Loss: 0.0169
Epoch [23/30], Batch [4300/6000], Loss: 0.0108
Epoch [23/30], Batch [4400/6000], Loss: 0.0117
Epoch [23/30], Batch [4500/6000], Loss: 0.0190
Epoch [23/30], Batch [4600/6000], Loss: 0.0267
Epoch [23/30], Batch [4700/6000], Loss: 0.0119
Epoch [23/30], Batch [4800/6000], Loss: 0.0161
Epoch [23/30], Batch [4900/6000], Loss: 0.0113
Epoch [23/30], Batch [5000/6000], Loss: 0.0126
Epoch [23/30], Batch [5100/6000], Loss: 0.0127
Epoch [23/30], Batch [5200/6000], Loss: 0.0137
Epoch [23/30], Batch [5300/6000], Loss: 0.0141
Epoch [23/30], Batch [5400/6000], Loss: 0.0157
Epoch [23/30], Batch [5500/6000], Loss: 0.0143
Epoch [23/30], Batch [5600/6000], Loss: 0.0143
Epoch [23/30], Batch [5700/6000], Loss: 0.0121
Epoch [23/30], Batch [5800/6000], Loss: 0.0138
Epoch [23/30], Batch [5900/6000], Loss: 0.0133
Epoch [23/30], Loss: 0.0142
Epoch [24/30], Batch [0/6000], Loss: 0.0165
Epoch [24/30], Batch [100/6000], Loss: 0.0139
Epoch [24/30], Batch [200/6000], Loss: 0.0165
Epoch [24/30], Batch [300/6000], Loss: 0.0182
Epoch [24/30], Batch [400/6000], Loss: 0.0128
Epoch [24/30], Batch [500/6000], Loss: 0.0235
Epoch [24/30], Batch [600/6000], Loss: 0.0170
Epoch [24/30], Batch [700/6000], Loss: 0.0155
Epoch [24/30], Batch [800/6000], Loss: 0.0112
Epoch [24/30], Batch [900/6000], Loss: 0.0149
Epoch [24/30], Batch [1000/6000], Loss: 0.0110
Epoch [24/30], Batch [1100/6000], Loss: 0.0099
Epoch [24/30], Batch [1200/6000], Loss: 0.0156
Epoch [24/30], Batch [1300/6000], Loss: 0.0138
Epoch [24/30], Batch [1400/6000], Loss: 0.0118
Epoch [24/30], Batch [1500/6000], Loss: 0.0133
Epoch [24/30], Batch [1600/6000], Loss: 0.0156
Epoch [24/30], Batch [1700/6000], Loss: 0.0185
Epoch [24/30], Batch [1800/6000], Loss: 0.0090
Epoch [24/30], Batch [1900/6000], Loss: 0.0113
Epoch [24/30], Batch [2000/6000], Loss: 0.0130
Epoch [24/30], Batch [2100/6000], Loss: 0.0117
Epoch [24/30], Batch [2200/6000], Loss: 0.0185
Epoch [24/30], Batch [2300/6000], Loss: 0.0179
Epoch [24/30], Batch [2400/6000], Loss: 0.0147
Epoch [24/30], Batch [2500/6000], Loss: 0.0121
Epoch [24/30], Batch [2600/6000], Loss: 0.0179
Epoch [24/30], Batch [2700/6000], Loss: 0.0094
Epoch [24/30], Batch [2800/6000], Loss: 0.0152
Epoch [24/30], Batch [2900/6000], Loss: 0.0135
Epoch [24/30], Batch [3000/6000], Loss: 0.0165
Epoch [24/30], Batch [3100/6000], Loss: 0.0155
Epoch [24/30], Batch [3200/6000], Loss: 0.0114
Epoch [24/30], Batch [3300/6000], Loss: 0.0227
Epoch [24/30], Batch [3400/6000], Loss: 0.0113
Epoch [24/30], Batch [3500/6000], Loss: 0.0111
Epoch [24/30], Batch [3600/6000], Loss: 0.0145
Epoch [24/30], Batch [3700/6000], Loss: 0.0115
Epoch [24/30], Batch [3800/6000], Loss: 0.0139
Epoch [24/30], Batch [3900/6000], Loss: 0.0141
Epoch [24/30], Batch [4000/6000], Loss: 0.0103
Epoch [24/30], Batch [4100/6000], Loss: 0.0158
Epoch [24/30], Batch [4200/6000], Loss: 0.0218
Epoch [24/30], Batch [4300/6000], Loss: 0.0135
Epoch [24/30], Batch [4400/6000], Loss: 0.0122
Epoch [24/30], Batch [4500/6000], Loss: 0.0135
Epoch [24/30], Batch [4600/6000], Loss: 0.0162
Epoch [24/30], Batch [4700/6000], Loss: 0.0129
Epoch [24/30], Batch [4800/6000], Loss: 0.0116
Epoch [24/30], Batch [4900/6000], Loss: 0.0118
Epoch [24/30], Batch [5000/6000], Loss: 0.0138
Epoch [24/30], Batch [5100/6000], Loss: 0.0139
Epoch [24/30], Batch [5200/6000], Loss: 0.0080
Epoch [24/30], Batch [5300/6000], Loss: 0.0160
Epoch [24/30], Batch [5400/6000], Loss: 0.0135
Epoch [24/30], Batch [5500/6000], Loss: 0.0129
Epoch [24/30], Batch [5600/6000], Loss: 0.0113
Epoch [24/30], Batch [5700/6000], Loss: 0.0116
Epoch [24/30], Batch [5800/6000], Loss: 0.0132
Epoch [24/30], Batch [5900/6000], Loss: 0.0125
Epoch [24/30], Loss: 0.0140
Epoch [25/30], Batch [0/6000], Loss: 0.0173
Epoch [25/30], Batch [100/6000], Loss: 0.0129
Epoch [25/30], Batch [200/6000], Loss: 0.0282
Epoch [25/30], Batch [300/6000], Loss: 0.0171
Epoch [25/30], Batch [400/6000], Loss: 0.0162
Epoch [25/30], Batch [500/6000], Loss: 0.0142
Epoch [25/30], Batch [600/6000], Loss: 0.0128
Epoch [25/30], Batch [700/6000], Loss: 0.0143
Epoch [25/30], Batch [800/6000], Loss: 0.0128
Epoch [25/30], Batch [900/6000], Loss: 0.0144
Epoch [25/30], Batch [1000/6000], Loss: 0.0123
Epoch [25/30], Batch [1100/6000], Loss: 0.0123
Epoch [25/30], Batch [1200/6000], Loss: 0.0116
Epoch [25/30], Batch [1300/6000], Loss: 0.0156
Epoch [25/30], Batch [1400/6000], Loss: 0.0125
Epoch [25/30], Batch [1500/6000], Loss: 0.0129
Epoch [25/30], Batch [1600/6000], Loss: 0.0118
Epoch [25/30], Batch [1700/6000], Loss: 0.0126
Epoch [25/30], Batch [1800/6000], Loss: 0.0119
Epoch [25/30], Batch [1900/6000], Loss: 0.0131
Epoch [25/30], Batch [2000/6000], Loss: 0.0116
Epoch [25/30], Batch [2100/6000], Loss: 0.0132
Epoch [25/30], Batch [2200/6000], Loss: 0.0123
Epoch [25/30], Batch [2300/6000], Loss: 0.0112
Epoch [25/30], Batch [2400/6000], Loss: 0.0111
Epoch [25/30], Batch [2500/6000], Loss: 0.0173
Epoch [25/30], Batch [2600/6000], Loss: 0.0096
Epoch [25/30], Batch [2700/6000], Loss: 0.0128
Epoch [25/30], Batch [2800/6000], Loss: 0.0114
Epoch [25/30], Batch [2900/6000], Loss: 0.0122
Epoch [25/30], Batch [3000/6000], Loss: 0.0135
Epoch [25/30], Batch [3100/6000], Loss: 0.0152
Epoch [25/30], Batch [3200/6000], Loss: 0.0139
Epoch [25/30], Batch [3300/6000], Loss: 0.0117
Epoch [25/30], Batch [3400/6000], Loss: 0.0121
Epoch [25/30], Batch [3500/6000], Loss: 0.0127
Epoch [25/30], Batch [3600/6000], Loss: 0.0139
Epoch [25/30], Batch [3700/6000], Loss: 0.0116
Epoch [25/30], Batch [3800/6000], Loss: 0.0135
Epoch [25/30], Batch [3900/6000], Loss: 0.0141
Epoch [25/30], Batch [4000/6000], Loss: 0.0176
Epoch [25/30], Batch [4100/6000], Loss: 0.0108
Epoch [25/30], Batch [4200/6000], Loss: 0.0125
Epoch [25/30], Batch [4300/6000], Loss: 0.0182
Epoch [25/30], Batch [4400/6000], Loss: 0.0149
Epoch [25/30], Batch [4500/6000], Loss: 0.0163
Epoch [25/30], Batch [4600/6000], Loss: 0.0087
Epoch [25/30], Batch [4700/6000], Loss: 0.0123
Epoch [25/30], Batch [4800/6000], Loss: 0.0123
Epoch [25/30], Batch [4900/6000], Loss: 0.0129
Epoch [25/30], Batch [5000/6000], Loss: 0.0137
Epoch [25/30], Batch [5100/6000], Loss: 0.0144
Epoch [25/30], Batch [5200/6000], Loss: 0.0138
Epoch [25/30], Batch [5300/6000], Loss: 0.0122
Epoch [25/30], Batch [5400/6000], Loss: 0.0146
Epoch [25/30], Batch [5500/6000], Loss: 0.0128
Epoch [25/30], Batch [5600/6000], Loss: 0.0422
Epoch [25/30], Batch [5700/6000], Loss: 0.0132
Epoch [25/30], Batch [5800/6000], Loss: 0.0147
Epoch [25/30], Batch [5900/6000], Loss: 0.0088
Epoch [25/30], Loss: 0.0136
Epoch [26/30], Batch [0/6000], Loss: 0.0132
Epoch [26/30], Batch [100/6000], Loss: 0.0110
Epoch [26/30], Batch [200/6000], Loss: 0.0100
Epoch [26/30], Batch [300/6000], Loss: 0.0199
Epoch [26/30], Batch [400/6000], Loss: 0.0123
Epoch [26/30], Batch [500/6000], Loss: 0.0133
Epoch [26/30], Batch [600/6000], Loss: 0.0108
Epoch [26/30], Batch [700/6000], Loss: 0.0109
Epoch [26/30], Batch [800/6000], Loss: 0.0110
Epoch [26/30], Batch [900/6000], Loss: 0.0152
Epoch [26/30], Batch [1000/6000], Loss: 0.0121
Epoch [26/30], Batch [1100/6000], Loss: 0.0131
Epoch [26/30], Batch [1200/6000], Loss: 0.0132
Epoch [26/30], Batch [1300/6000], Loss: 0.0140
Epoch [26/30], Batch [1400/6000], Loss: 0.0126
Epoch [26/30], Batch [1500/6000], Loss: 0.0101
Epoch [26/30], Batch [1600/6000], Loss: 0.0107
Epoch [26/30], Batch [1700/6000], Loss: 0.0116
Epoch [26/30], Batch [1800/6000], Loss: 0.0133
Epoch [26/30], Batch [1900/6000], Loss: 0.0127
Epoch [26/30], Batch [2000/6000], Loss: 0.0121
Epoch [26/30], Batch [2100/6000], Loss: 0.0152
Epoch [26/30], Batch [2200/6000], Loss: 0.0127
Epoch [26/30], Batch [2300/6000], Loss: 0.0128
Epoch [26/30], Batch [2400/6000], Loss: 0.0146
Epoch [26/30], Batch [2500/6000], Loss: 0.0146
Epoch [26/30], Batch [2600/6000], Loss: 0.0157
Epoch [26/30], Batch [2700/6000], Loss: 0.0131
Epoch [26/30], Batch [2800/6000], Loss: 0.0132
Epoch [26/30], Batch [2900/6000], Loss: 0.0135
Epoch [26/30], Batch [3000/6000], Loss: 0.0143
Epoch [26/30], Batch [3100/6000], Loss: 0.0117
Epoch [26/30], Batch [3200/6000], Loss: 0.0129
Epoch [26/30], Batch [3300/6000], Loss: 0.0136
Epoch [26/30], Batch [3400/6000], Loss: 0.0152
Epoch [26/30], Batch [3500/6000], Loss: 0.0118
Epoch [26/30], Batch [3600/6000], Loss: 0.0138
Epoch [26/30], Batch [3700/6000], Loss: 0.0121
Epoch [26/30], Batch [3800/6000], Loss: 0.0149
Epoch [26/30], Batch [3900/6000], Loss: 0.0137
Epoch [26/30], Batch [4000/6000], Loss: 0.0148
Epoch [26/30], Batch [4100/6000], Loss: 0.0144
Epoch [26/30], Batch [4200/6000], Loss: 0.0126
Epoch [26/30], Batch [4300/6000], Loss: 0.0102
Epoch [26/30], Batch [4400/6000], Loss: 0.0148
Epoch [26/30], Batch [4500/6000], Loss: 0.0150
Epoch [26/30], Batch [4600/6000], Loss: 0.0153
Epoch [26/30], Batch [4700/6000], Loss: 0.0152
Epoch [26/30], Batch [4800/6000], Loss: 0.0177
Epoch [26/30], Batch [4900/6000], Loss: 0.0112
Epoch [26/30], Batch [5000/6000], Loss: 0.0134
Epoch [26/30], Batch [5100/6000], Loss: 0.0127
Epoch [26/30], Batch [5200/6000], Loss: 0.0123
Epoch [26/30], Batch [5300/6000], Loss: 0.0162
Epoch [26/30], Batch [5400/6000], Loss: 0.0107
Epoch [26/30], Batch [5500/6000], Loss: 0.0101
Epoch [26/30], Batch [5600/6000], Loss: 0.0127
Epoch [26/30], Batch [5700/6000], Loss: 0.0156
Epoch [26/30], Batch [5800/6000], Loss: 0.0188
Epoch [26/30], Batch [5900/6000], Loss: 0.0132
Epoch [26/30], Loss: 0.0134
Epoch [27/30], Batch [0/6000], Loss: 0.0137
Epoch [27/30], Batch [100/6000], Loss: 0.0102
Epoch [27/30], Batch [200/6000], Loss: 0.0144
Epoch [27/30], Batch [300/6000], Loss: 0.0154
Epoch [27/30], Batch [400/6000], Loss: 0.0152
Epoch [27/30], Batch [500/6000], Loss: 0.0119
Epoch [27/30], Batch [600/6000], Loss: 0.0111
Epoch [27/30], Batch [700/6000], Loss: 0.0110
Epoch [27/30], Batch [800/6000], Loss: 0.0089
Epoch [27/30], Batch [900/6000], Loss: 0.0161
Epoch [27/30], Batch [1000/6000], Loss: 0.0121
Epoch [27/30], Batch [1100/6000], Loss: 0.0135
Epoch [27/30], Batch [1200/6000], Loss: 0.0127
Epoch [27/30], Batch [1300/6000], Loss: 0.0143
Epoch [27/30], Batch [1400/6000], Loss: 0.0085
Epoch [27/30], Batch [1500/6000], Loss: 0.0152
Epoch [27/30], Batch [1600/6000], Loss: 0.0139
Epoch [27/30], Batch [1700/6000], Loss: 0.0145
Epoch [27/30], Batch [1800/6000], Loss: 0.0122
Epoch [27/30], Batch [1900/6000], Loss: 0.0151
Epoch [27/30], Batch [2000/6000], Loss: 0.0136
Epoch [27/30], Batch [2100/6000], Loss: 0.0102
Epoch [27/30], Batch [2200/6000], Loss: 0.0135
Epoch [27/30], Batch [2300/6000], Loss: 0.0135
Epoch [27/30], Batch [2400/6000], Loss: 0.0118
Epoch [27/30], Batch [2500/6000], Loss: 0.0126
Epoch [27/30], Batch [2600/6000], Loss: 0.0150
Epoch [27/30], Batch [2700/6000], Loss: 0.0121
Epoch [27/30], Batch [2800/6000], Loss: 0.0141
Epoch [27/30], Batch [2900/6000], Loss: 0.0113
Epoch [27/30], Batch [3000/6000], Loss: 0.0200
Epoch [27/30], Batch [3100/6000], Loss: 0.0169
Epoch [27/30], Batch [3200/6000], Loss: 0.0155
Epoch [27/30], Batch [3300/6000], Loss: 0.0117
Epoch [27/30], Batch [3400/6000], Loss: 0.0121
Epoch [27/30], Batch [3500/6000], Loss: 0.0142
Epoch [27/30], Batch [3600/6000], Loss: 0.0100
Epoch [27/30], Batch [3700/6000], Loss: 0.0131
Epoch [27/30], Batch [3800/6000], Loss: 0.0097
Epoch [27/30], Batch [3900/6000], Loss: 0.0132
Epoch [27/30], Batch [4000/6000], Loss: 0.0105
Epoch [27/30], Batch [4100/6000], Loss: 0.0166
Epoch [27/30], Batch [4200/6000], Loss: 0.0139
Epoch [27/30], Batch [4300/6000], Loss: 0.0130
Epoch [27/30], Batch [4400/6000], Loss: 0.0144
Epoch [27/30], Batch [4500/6000], Loss: 0.0144
Epoch [27/30], Batch [4600/6000], Loss: 0.0131
Epoch [27/30], Batch [4700/6000], Loss: 0.0127
Epoch [27/30], Batch [4800/6000], Loss: 0.0126
Epoch [27/30], Batch [4900/6000], Loss: 0.0147
Epoch [27/30], Batch [5000/6000], Loss: 0.0146
Epoch [27/30], Batch [5100/6000], Loss: 0.0200
Epoch [27/30], Batch [5200/6000], Loss: 0.0110
Epoch [27/30], Batch [5300/6000], Loss: 0.0266
Epoch [27/30], Batch [5400/6000], Loss: 0.0162
Epoch [27/30], Batch [5500/6000], Loss: 0.0128
Epoch [27/30], Batch [5600/6000], Loss: 0.0113
Epoch [27/30], Batch [5700/6000], Loss: 0.0102
Epoch [27/30], Batch [5800/6000], Loss: 0.0109
Epoch [27/30], Batch [5900/6000], Loss: 0.0116
Epoch [27/30], Loss: 0.0132
Epoch [28/30], Batch [0/6000], Loss: 0.0130
Epoch [28/30], Batch [100/6000], Loss: 0.0161
Epoch [28/30], Batch [200/6000], Loss: 0.0119
Epoch [28/30], Batch [300/6000], Loss: 0.0155
Epoch [28/30], Batch [400/6000], Loss: 0.0138
Epoch [28/30], Batch [500/6000], Loss: 0.0111
Epoch [28/30], Batch [600/6000], Loss: 0.0142
Epoch [28/30], Batch [700/6000], Loss: 0.0104
Epoch [28/30], Batch [800/6000], Loss: 0.0090
Epoch [28/30], Batch [900/6000], Loss: 0.0115
Epoch [28/30], Batch [1000/6000], Loss: 0.0096
Epoch [28/30], Batch [1100/6000], Loss: 0.0670
Epoch [28/30], Batch [1200/6000], Loss: 0.0117
Epoch [28/30], Batch [1300/6000], Loss: 0.0147
Epoch [28/30], Batch [1400/6000], Loss: 0.0117
Epoch [28/30], Batch [1500/6000], Loss: 0.0125
Epoch [28/30], Batch [1600/6000], Loss: 0.0129
Epoch [28/30], Batch [1700/6000], Loss: 0.0135
Epoch [28/30], Batch [1800/6000], Loss: 0.0122
Epoch [28/30], Batch [1900/6000], Loss: 0.0125
Epoch [28/30], Batch [2000/6000], Loss: 0.0102
Epoch [28/30], Batch [2100/6000], Loss: 0.0102
Epoch [28/30], Batch [2200/6000], Loss: 0.0135
Epoch [28/30], Batch [2300/6000], Loss: 0.0096
Epoch [28/30], Batch [2400/6000], Loss: 0.0178
Epoch [28/30], Batch [2500/6000], Loss: 0.0174
Epoch [28/30], Batch [2600/6000], Loss: 0.0131
Epoch [28/30], Batch [2700/6000], Loss: 0.0136
Epoch [28/30], Batch [2800/6000], Loss: 0.0103
Epoch [28/30], Batch [2900/6000], Loss: 0.0105
Epoch [28/30], Batch [3000/6000], Loss: 0.0169
Epoch [28/30], Batch [3100/6000], Loss: 0.0146
Epoch [28/30], Batch [3200/6000], Loss: 0.0108
Epoch [28/30], Batch [3300/6000], Loss: 0.0120
Epoch [28/30], Batch [3400/6000], Loss: 0.0159
Epoch [28/30], Batch [3500/6000], Loss: 0.0097
Epoch [28/30], Batch [3600/6000], Loss: 0.0109
Epoch [28/30], Batch [3700/6000], Loss: 0.0138
Epoch [28/30], Batch [3800/6000], Loss: 0.0163
Epoch [28/30], Batch [3900/6000], Loss: 0.0114
Epoch [28/30], Batch [4000/6000], Loss: 0.0141
Epoch [28/30], Batch [4100/6000], Loss: 0.0116
Epoch [28/30], Batch [4200/6000], Loss: 0.0085
Epoch [28/30], Batch [4300/6000], Loss: 0.0108
Epoch [28/30], Batch [4400/6000], Loss: 0.0093
Epoch [28/30], Batch [4500/6000], Loss: 0.0097
Epoch [28/30], Batch [4600/6000], Loss: 0.0114
Epoch [28/30], Batch [4700/6000], Loss: 0.0128
Epoch [28/30], Batch [4800/6000], Loss: 0.0136
Epoch [28/30], Batch [4900/6000], Loss: 0.0135
Epoch [28/30], Batch [5000/6000], Loss: 0.0121
Epoch [28/30], Batch [5100/6000], Loss: 0.0112
Epoch [28/30], Batch [5200/6000], Loss: 0.0145
Epoch [28/30], Batch [5300/6000], Loss: 0.0112
Epoch [28/30], Batch [5400/6000], Loss: 0.0130
Epoch [28/30], Batch [5500/6000], Loss: 0.0125
Epoch [28/30], Batch [5600/6000], Loss: 0.0109
Epoch [28/30], Batch [5700/6000], Loss: 0.0126
Epoch [28/30], Batch [5800/6000], Loss: 0.0118
Epoch [28/30], Batch [5900/6000], Loss: 0.0133
Epoch [28/30], Loss: 0.0129
Epoch [29/30], Batch [0/6000], Loss: 0.0143
Epoch [29/30], Batch [100/6000], Loss: 0.0128
Epoch [29/30], Batch [200/6000], Loss: 0.0112
Epoch [29/30], Batch [300/6000], Loss: 0.0104
Epoch [29/30], Batch [400/6000], Loss: 0.0096
Epoch [29/30], Batch [500/6000], Loss: 0.0114
Epoch [29/30], Batch [600/6000], Loss: 0.0118
Epoch [29/30], Batch [700/6000], Loss: 0.0127
Epoch [29/30], Batch [800/6000], Loss: 0.0143
Epoch [29/30], Batch [900/6000], Loss: 0.0134
Epoch [29/30], Batch [1000/6000], Loss: 0.0116
Epoch [29/30], Batch [1100/6000], Loss: 0.0090
Epoch [29/30], Batch [1200/6000], Loss: 0.0128
Epoch [29/30], Batch [1300/6000], Loss: 0.0132
Epoch [29/30], Batch [1400/6000], Loss: 0.0137
Epoch [29/30], Batch [1500/6000], Loss: 0.0109
Epoch [29/30], Batch [1600/6000], Loss: 0.0076
Epoch [29/30], Batch [1700/6000], Loss: 0.0111
Epoch [29/30], Batch [1800/6000], Loss: 0.0198
Epoch [29/30], Batch [1900/6000], Loss: 0.0141
Epoch [29/30], Batch [2000/6000], Loss: 0.0121
Epoch [29/30], Batch [2100/6000], Loss: 0.0140
Epoch [29/30], Batch [2200/6000], Loss: 0.0130
Epoch [29/30], Batch [2300/6000], Loss: 0.0096
Epoch [29/30], Batch [2400/6000], Loss: 0.0106
Epoch [29/30], Batch [2500/6000], Loss: 0.0104
Epoch [29/30], Batch [2600/6000], Loss: 0.0127
Epoch [29/30], Batch [2700/6000], Loss: 0.0122
Epoch [29/30], Batch [2800/6000], Loss: 0.0119
Epoch [29/30], Batch [2900/6000], Loss: 0.0115
Epoch [29/30], Batch [3000/6000], Loss: 0.0149
Epoch [29/30], Batch [3100/6000], Loss: 0.0137
Epoch [29/30], Batch [3200/6000], Loss: 0.0126
Epoch [29/30], Batch [3300/6000], Loss: 0.0123
Epoch [29/30], Batch [3400/6000], Loss: 0.0104
Epoch [29/30], Batch [3500/6000], Loss: 0.0133
Epoch [29/30], Batch [3600/6000], Loss: 0.0132
Epoch [29/30], Batch [3700/6000], Loss: 0.0159
Epoch [29/30], Batch [3800/6000], Loss: 0.0151
Epoch [29/30], Batch [3900/6000], Loss: 0.0142
Epoch [29/30], Batch [4000/6000], Loss: 0.0086
Epoch [29/30], Batch [4100/6000], Loss: 0.0151
Epoch [29/30], Batch [4200/6000], Loss: 0.0131
Epoch [29/30], Batch [4300/6000], Loss: 0.0121
Epoch [29/30], Batch [4400/6000], Loss: 0.0106
Epoch [29/30], Batch [4500/6000], Loss: 0.0112
Epoch [29/30], Batch [4600/6000], Loss: 0.0121
Epoch [29/30], Batch [4700/6000], Loss: 0.0124
Epoch [29/30], Batch [4800/6000], Loss: 0.0121
Epoch [29/30], Batch [4900/6000], Loss: 0.0145
Epoch [29/30], Batch [5000/6000], Loss: 0.0141
Epoch [29/30], Batch [5100/6000], Loss: 0.0168
Epoch [29/30], Batch [5200/6000], Loss: 0.0168
Epoch [29/30], Batch [5300/6000], Loss: 0.0114
Epoch [29/30], Batch [5400/6000], Loss: 0.0121
Epoch [29/30], Batch [5500/6000], Loss: 0.0112
Epoch [29/30], Batch [5600/6000], Loss: 0.0133
Epoch [29/30], Batch [5700/6000], Loss: 0.0137
Epoch [29/30], Batch [5800/6000], Loss: 0.0110
Epoch [29/30], Batch [5900/6000], Loss: 0.0107
Epoch [29/30], Loss: 0.0128
Epoch [30/30], Batch [0/6000], Loss: 0.0115
Epoch [30/30], Batch [100/6000], Loss: 0.0213
Epoch [30/30], Batch [200/6000], Loss: 0.0133
Epoch [30/30], Batch [300/6000], Loss: 0.0139
Epoch [30/30], Batch [400/6000], Loss: 0.0115
Epoch [30/30], Batch [500/6000], Loss: 0.0146
Epoch [30/30], Batch [600/6000], Loss: 0.0132
Epoch [30/30], Batch [700/6000], Loss: 0.0125
Epoch [30/30], Batch [800/6000], Loss: 0.0118
Epoch [30/30], Batch [900/6000], Loss: 0.0123
Epoch [30/30], Batch [1000/6000], Loss: 0.0114
Epoch [30/30], Batch [1100/6000], Loss: 0.0129
Epoch [30/30], Batch [1200/6000], Loss: 0.0110
Epoch [30/30], Batch [1300/6000], Loss: 0.0134
Epoch [30/30], Batch [1400/6000], Loss: 0.0108
Epoch [30/30], Batch [1500/6000], Loss: 0.0107
Epoch [30/30], Batch [1600/6000], Loss: 0.0135
Epoch [30/30], Batch [1700/6000], Loss: 0.0131
Epoch [30/30], Batch [1800/6000], Loss: 0.0124
Epoch [30/30], Batch [1900/6000], Loss: 0.0120
Epoch [30/30], Batch [2000/6000], Loss: 0.0147
Epoch [30/30], Batch [2100/6000], Loss: 0.0128
Epoch [30/30], Batch [2200/6000], Loss: 0.0122
Epoch [30/30], Batch [2300/6000], Loss: 0.0117
Epoch [30/30], Batch [2400/6000], Loss: 0.0150
Epoch [30/30], Batch [2500/6000], Loss: 0.0124
Epoch [30/30], Batch [2600/6000], Loss: 0.0137
Epoch [30/30], Batch [2700/6000], Loss: 0.0121
Epoch [30/30], Batch [2800/6000], Loss: 0.0087
Epoch [30/30], Batch [2900/6000], Loss: 0.0157
Epoch [30/30], Batch [3000/6000], Loss: 0.0105
Epoch [30/30], Batch [3100/6000], Loss: 0.0112
Epoch [30/30], Batch [3200/6000], Loss: 0.0127
Epoch [30/30], Batch [3300/6000], Loss: 0.0088
Epoch [30/30], Batch [3400/6000], Loss: 0.0094
Epoch [30/30], Batch [3500/6000], Loss: 0.0097
Epoch [30/30], Batch [3600/6000], Loss: 0.0128
Epoch [30/30], Batch [3700/6000], Loss: 0.0106
Epoch [30/30], Batch [3800/6000], Loss: 0.0105
Epoch [30/30], Batch [3900/6000], Loss: 0.0139
Epoch [30/30], Batch [4000/6000], Loss: 0.0111
Epoch [30/30], Batch [4100/6000], Loss: 0.0157
Epoch [30/30], Batch [4200/6000], Loss: 0.0128
Epoch [30/30], Batch [4300/6000], Loss: 0.0157
Epoch [30/30], Batch [4400/6000], Loss: 0.0090
Epoch [30/30], Batch [4500/6000], Loss: 0.0105
Epoch [30/30], Batch [4600/6000], Loss: 0.0146
Epoch [30/30], Batch [4700/6000], Loss: 0.0095
Epoch [30/30], Batch [4800/6000], Loss: 0.0110
Epoch [30/30], Batch [4900/6000], Loss: 0.0114
Epoch [30/30], Batch [5000/6000], Loss: 0.0127
Epoch [30/30], Batch [5100/6000], Loss: 0.0115
Epoch [30/30], Batch [5200/6000], Loss: 0.0116
Epoch [30/30], Batch [5300/6000], Loss: 0.0090
Epoch [30/30], Batch [5400/6000], Loss: 0.0104
Epoch [30/30], Batch [5500/6000], Loss: 0.0161
Epoch [30/30], Batch [5600/6000], Loss: 0.0110
Epoch [30/30], Batch [5700/6000], Loss: 0.0129
Epoch [30/30], Batch [5800/6000], Loss: 0.0129
Epoch [30/30], Batch [5900/6000], Loss: 0.0130
Epoch [30/30], Loss: 0.0125
Test Loss: 0.0163, Accuracy: 98.05%
Visualization saved to adversarial_figures/reconstruction.png
  Output probs: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 1/300:
  Label Loss: 1.0550
  Image Loss: 0.0109
  Total Loss: 0.5384
  Image grad max: 0.03103257715702057
  Output probs: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 2/300:
  Label Loss: 0.9757
  Image Loss: 0.0107
  Total Loss: 0.4986
  Image grad max: 0.032615773379802704
  Output probs: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 3/300:
  Label Loss: 0.8929
  Image Loss: 0.0106
  Total Loss: 0.4571
  Image grad max: 0.034041017293930054
  Output probs: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 4/300:
  Label Loss: 0.8058
  Image Loss: 0.0106
  Total Loss: 0.4135
  Image grad max: 0.03410523012280464
  Output probs: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
Adversarial Training Loop 5/300:
  Label Loss: 0.7154
  Image Loss: 0.0108
  Total Loss: 0.3685
  Image grad max: 0.035549771040678024
  Output probs: [[0.999 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]]
Adversarial Training Loop 6/300:
  Label Loss: 0.6233
  Image Loss: 0.0110
  Total Loss: 0.3227
  Image grad max: 0.03713194653391838
  Output probs: [[0.998 0.    0.    0.    0.    0.    0.    0.001 0.    0.001]]
Adversarial Training Loop 7/300:
  Label Loss: 0.5316
  Image Loss: 0.0114
  Total Loss: 0.2772
  Image grad max: 0.038283251225948334
  Output probs: [[0.993 0.    0.    0.    0.    0.002 0.    0.002 0.    0.002]]
Adversarial Training Loop 8/300:
  Label Loss: 0.4411
  Image Loss: 0.0119
  Total Loss: 0.2324
  Image grad max: 0.039310552179813385
  Output probs: [[0.966 0.    0.001 0.001 0.    0.022 0.    0.005 0.    0.005]]
Adversarial Training Loop 9/300:
  Label Loss: 0.3537
  Image Loss: 0.0125
  Total Loss: 0.1893
  Image grad max: 0.038455456495285034
  Output probs: [[0.816 0.    0.004 0.005 0.    0.151 0.    0.011 0.001 0.012]]
Adversarial Training Loop 10/300:
  Label Loss: 0.2819
  Image Loss: 0.0131
  Total Loss: 0.1541
  Image grad max: 0.03147431090474129
  Output probs: [[0.442 0.    0.008 0.019 0.    0.499 0.    0.014 0.003 0.015]]
Adversarial Training Loop 11/300:
  Label Loss: 0.2586
  Image Loss: 0.0139
  Total Loss: 0.1432
  Image grad max: 0.03835169970989227
  Output probs: [[0.21  0.    0.012 0.04  0.    0.707 0.    0.011 0.006 0.013]]
Adversarial Training Loop 12/300:
  Label Loss: 0.2631
  Image Loss: 0.0145
  Total Loss: 0.1461
  Image grad max: 0.05164286121726036
  Output probs: [[0.146 0.    0.02  0.068 0.    0.73  0.    0.01  0.012 0.013]]
Adversarial Training Loop 13/300:
  Label Loss: 0.2476
  Image Loss: 0.0150
  Total Loss: 0.1388
  Image grad max: 0.05363127216696739
  Output probs: [[0.147 0.    0.035 0.104 0.    0.662 0.    0.011 0.025 0.016]]
Adversarial Training Loop 14/300:
  Label Loss: 0.2102
  Image Loss: 0.0154
  Total Loss: 0.1204
  Image grad max: 0.04996278136968613
  Output probs: [[0.187 0.    0.06  0.138 0.    0.53  0.    0.012 0.053 0.02 ]]
Adversarial Training Loop 15/300:
  Label Loss: 0.1614
  Image Loss: 0.0156
  Total Loss: 0.0963
  Image grad max: 0.04265466332435608
  Output probs: [[0.259 0.    0.091 0.147 0.    0.363 0.    0.012 0.101 0.026]]
Adversarial Training Loop 16/300:
  Label Loss: 0.1128
  Image Loss: 0.0158
  Total Loss: 0.0723
  Image grad max: 0.03534156084060669
  Output probs: [[0.352 0.    0.109 0.121 0.    0.21  0.    0.011 0.165 0.031]]
Adversarial Training Loop 17/300:
  Label Loss: 0.0728
  Image Loss: 0.0160
  Total Loss: 0.0525
  Image grad max: 0.025435147807002068
  Output probs: [[0.443 0.    0.102 0.078 0.    0.106 0.    0.008 0.23  0.033]]
Adversarial Training Loop 18/300:
  Label Loss: 0.0449
  Image Loss: 0.0162
  Total Loss: 0.0387
  Image grad max: 0.016703862696886063
  Output probs: [[0.507 0.    0.079 0.044 0.    0.05  0.    0.005 0.284 0.032]]
Adversarial Training Loop 19/300:
  Label Loss: 0.0276
  Image Loss: 0.0165
  Total Loss: 0.0302
  Image grad max: 0.011421424336731434
  Output probs: [[0.538 0.    0.054 0.023 0.    0.023 0.    0.003 0.33  0.029]]
Adversarial Training Loop 20/300:
  Label Loss: 0.0171
  Image Loss: 0.0167
  Total Loss: 0.0252
  Image grad max: 0.009115603752434254
  Output probs: [[0.54  0.    0.036 0.012 0.    0.011 0.    0.002 0.375 0.025]]
Adversarial Training Loop 21/300:
  Label Loss: 0.0106
  Image Loss: 0.0170
  Total Loss: 0.0222
  Image grad max: 0.007351350504904985
  Output probs: [[0.52  0.    0.023 0.006 0.    0.005 0.    0.001 0.422 0.022]]
Adversarial Training Loop 22/300:
  Label Loss: 0.0065
  Image Loss: 0.0172
  Total Loss: 0.0205
  Image grad max: 0.004537968430668116
  Output probs: [[0.49  0.    0.015 0.003 0.    0.003 0.    0.001 0.47  0.018]]
Adversarial Training Loop 23/300:
  Label Loss: 0.0042
  Image Loss: 0.0175
  Total Loss: 0.0196
  Image grad max: 0.002015357604250312
  Output probs: [[0.459 0.    0.01  0.002 0.    0.002 0.    0.    0.511 0.016]]
Adversarial Training Loop 24/300:
  Label Loss: 0.0032
  Image Loss: 0.0178
  Total Loss: 0.0194
  Image grad max: 0.0029474913608282804
  Output probs: [[0.438 0.    0.007 0.001 0.    0.001 0.    0.    0.539 0.014]]
Adversarial Training Loop 25/300:
  Label Loss: 0.0029
  Image Loss: 0.0180
  Total Loss: 0.0195
  Image grad max: 0.004686328116804361
  Output probs: [[0.43  0.    0.005 0.001 0.    0.001 0.    0.    0.552 0.012]]
Adversarial Training Loop 26/300:
  Label Loss: 0.0026
  Image Loss: 0.0182
  Total Loss: 0.0196
  Image grad max: 0.005426559131592512
  Output probs: [[0.438 0.    0.003 0.    0.    0.    0.    0.    0.547 0.011]]
Adversarial Training Loop 27/300:
  Label Loss: 0.0022
  Image Loss: 0.0184
  Total Loss: 0.0195
  Image grad max: 0.004896017722785473
  Output probs: [[0.46  0.    0.002 0.    0.    0.    0.    0.    0.527 0.01 ]]
Adversarial Training Loop 28/300:
  Label Loss: 0.0016
  Image Loss: 0.0185
  Total Loss: 0.0193
  Image grad max: 0.003190369810909033
  Output probs: [[0.49  0.    0.002 0.    0.    0.    0.    0.    0.498 0.009]]
Adversarial Training Loop 29/300:
  Label Loss: 0.0012
  Image Loss: 0.0186
  Total Loss: 0.0192
  Image grad max: 0.0012565248180180788
  Output probs: [[0.521 0.    0.001 0.    0.    0.    0.    0.    0.469 0.009]]
Adversarial Training Loop 30/300:
  Label Loss: 0.0012
  Image Loss: 0.0186
  Total Loss: 0.0192
  Image grad max: 0.002545124851167202
  Output probs: [[0.543 0.    0.001 0.    0.    0.    0.    0.    0.448 0.008]]
Adversarial Training Loop 31/300:
  Label Loss: 0.0014
  Image Loss: 0.0187
  Total Loss: 0.0194
  Image grad max: 0.00448612030595541
  Output probs: [[0.552 0.    0.001 0.    0.    0.    0.    0.    0.439 0.008]]
Adversarial Training Loop 32/300:
  Label Loss: 0.0016
  Image Loss: 0.0187
  Total Loss: 0.0195
  Image grad max: 0.005298896227031946
  Output probs: [[0.547 0.    0.001 0.    0.    0.    0.    0.    0.444 0.007]]
Adversarial Training Loop 33/300:
  Label Loss: 0.0014
  Image Loss: 0.0188
  Total Loss: 0.0195
  Image grad max: 0.004846918862313032
  Output probs: [[0.53  0.    0.001 0.    0.    0.    0.    0.    0.462 0.007]]
Adversarial Training Loop 34/300:
  Label Loss: 0.0011
  Image Loss: 0.0188
  Total Loss: 0.0193
  Image grad max: 0.0033019583206623793
  Output probs: [[0.507 0.    0.001 0.    0.    0.    0.    0.    0.485 0.007]]
Adversarial Training Loop 35/300:
  Label Loss: 0.0008
  Image Loss: 0.0189
  Total Loss: 0.0193
  Image grad max: 0.0013516696635633707
  Output probs: [[0.484 0.    0.001 0.    0.    0.    0.    0.    0.508 0.006]]
Adversarial Training Loop 36/300:
  Label Loss: 0.0008
  Image Loss: 0.0189
  Total Loss: 0.0193
  Image grad max: 0.0016148602589964867
  Output probs: [[0.469 0.    0.001 0.    0.    0.    0.    0.    0.524 0.006]]
Adversarial Training Loop 37/300:
  Label Loss: 0.0009
  Image Loss: 0.0189
  Total Loss: 0.0193
  Image grad max: 0.0027374562341719866
  Output probs: [[0.465 0.    0.    0.    0.    0.    0.    0.    0.528 0.006]]
Adversarial Training Loop 38/300:
  Label Loss: 0.0009
  Image Loss: 0.0188
  Total Loss: 0.0193
  Image grad max: 0.0030501983128488064
  Output probs: [[0.474 0.    0.    0.    0.    0.    0.    0.    0.519 0.006]]
Adversarial Training Loop 39/300:
  Label Loss: 0.0008
  Image Loss: 0.0188
  Total Loss: 0.0192
  Image grad max: 0.0023578170221298933
  Output probs: [[0.491 0.    0.    0.    0.    0.    0.    0.    0.502 0.006]]
Adversarial Training Loop 40/300:
  Label Loss: 0.0007
  Image Loss: 0.0187
  Total Loss: 0.0190
  Image grad max: 0.0012304912088438869
  Output probs: [[0.511 0.    0.    0.    0.    0.    0.    0.    0.483 0.006]]
Adversarial Training Loop 41/300:
  Label Loss: 0.0007
  Image Loss: 0.0186
  Total Loss: 0.0189
  Image grad max: 0.0015285093104466796
  Output probs: [[0.527 0.    0.    0.    0.    0.    0.    0.    0.467 0.006]]
Adversarial Training Loop 42/300:
  Label Loss: 0.0008
  Image Loss: 0.0185
  Total Loss: 0.0189
  Image grad max: 0.0029211388900876045
  Output probs: [[0.533 0.    0.    0.    0.    0.    0.    0.    0.461 0.005]]
Adversarial Training Loop 43/300:
  Label Loss: 0.0009
  Image Loss: 0.0184
  Total Loss: 0.0188
  Image grad max: 0.0035455054603517056
  Output probs: [[0.53  0.    0.    0.    0.    0.    0.    0.    0.464 0.005]]
Adversarial Training Loop 44/300:
  Label Loss: 0.0008
  Image Loss: 0.0183
  Total Loss: 0.0187
  Image grad max: 0.0031984196975827217
  Output probs: [[0.518 0.    0.    0.    0.    0.    0.    0.    0.476 0.005]]
Adversarial Training Loop 45/300:
  Label Loss: 0.0007
  Image Loss: 0.0182
  Total Loss: 0.0186
  Image grad max: 0.0020706276409327984
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.    0.    0.492 0.005]]
Adversarial Training Loop 46/300:
  Label Loss: 0.0006
  Image Loss: 0.0181
  Total Loss: 0.0184
  Image grad max: 0.0011240255553275347
  Output probs: [[0.488 0.    0.    0.    0.    0.    0.    0.    0.506 0.005]]
Adversarial Training Loop 47/300:
  Label Loss: 0.0006
  Image Loss: 0.0180
  Total Loss: 0.0183
  Image grad max: 0.0014608942437916994
  Output probs: [[0.481 0.    0.    0.    0.    0.    0.    0.    0.513 0.005]]
Adversarial Training Loop 48/300:
  Label Loss: 0.0006
  Image Loss: 0.0179
  Total Loss: 0.0182
  Image grad max: 0.0018822751007974148
  Output probs: [[0.483 0.    0.    0.    0.    0.    0.    0.    0.511 0.005]]
Adversarial Training Loop 49/300:
  Label Loss: 0.0006
  Image Loss: 0.0178
  Total Loss: 0.0181
  Image grad max: 0.0017436188645660877
  Output probs: [[0.493 0.    0.    0.    0.    0.    0.    0.    0.501 0.005]]
Adversarial Training Loop 50/300:
  Label Loss: 0.0006
  Image Loss: 0.0176
  Total Loss: 0.0179
  Image grad max: 0.001190170762129128
  Output probs: [[0.506 0.    0.    0.    0.    0.    0.    0.    0.488 0.005]]
Adversarial Training Loop 51/300:
  Label Loss: 0.0006
  Image Loss: 0.0175
  Total Loss: 0.0178
  Image grad max: 0.0012757438234984875
  Output probs: [[0.517 0.    0.    0.    0.    0.    0.    0.    0.477 0.005]]
Adversarial Training Loop 52/300:
  Label Loss: 0.0007
  Image Loss: 0.0173
  Total Loss: 0.0177
  Image grad max: 0.002037168713286519
  Output probs: [[0.522 0.    0.    0.    0.    0.    0.    0.    0.472 0.005]]
Adversarial Training Loop 53/300:
  Label Loss: 0.0007
  Image Loss: 0.0172
  Total Loss: 0.0175
  Image grad max: 0.0025138729251921177
  Output probs: [[0.52  0.    0.    0.    0.    0.    0.    0.    0.475 0.005]]
Adversarial Training Loop 54/300:
  Label Loss: 0.0007
  Image Loss: 0.0171
  Total Loss: 0.0174
  Image grad max: 0.0022817375138401985
  Output probs: [[0.511 0.    0.    0.    0.    0.    0.    0.    0.483 0.005]]
Adversarial Training Loop 55/300:
  Label Loss: 0.0006
  Image Loss: 0.0169
  Total Loss: 0.0172
  Image grad max: 0.0015098126605153084
  Output probs: [[0.501 0.    0.    0.    0.    0.    0.    0.    0.493 0.005]]
Adversarial Training Loop 56/300:
  Label Loss: 0.0006
  Image Loss: 0.0168
  Total Loss: 0.0171
  Image grad max: 0.001101248781196773
  Output probs: [[0.493 0.    0.    0.    0.    0.    0.    0.    0.501 0.005]]
Adversarial Training Loop 57/300:
  Label Loss: 0.0006
  Image Loss: 0.0167
  Total Loss: 0.0169
  Image grad max: 0.0011730487458407879
  Output probs: [[0.49  0.    0.    0.    0.    0.    0.    0.    0.504 0.005]]
Adversarial Training Loop 58/300:
  Label Loss: 0.0006
  Image Loss: 0.0165
  Total Loss: 0.0168
  Image grad max: 0.001321702729910612
  Output probs: [[0.494 0.    0.    0.    0.    0.    0.    0.    0.501 0.005]]
Adversarial Training Loop 59/300:
  Label Loss: 0.0006
  Image Loss: 0.0164
  Total Loss: 0.0166
  Image grad max: 0.0011608776403591037
  Output probs: [[0.501 0.    0.    0.    0.    0.    0.    0.    0.493 0.005]]
Adversarial Training Loop 60/300:
  Label Loss: 0.0006
  Image Loss: 0.0162
  Total Loss: 0.0165
  Image grad max: 0.0010852976702153683
  Output probs: [[0.509 0.    0.    0.    0.    0.    0.    0.    0.485 0.005]]
Adversarial Training Loop 61/300:
  Label Loss: 0.0006
  Image Loss: 0.0160
  Total Loss: 0.0163
  Image grad max: 0.0013969262363389134
  Output probs: [[0.514 0.    0.    0.    0.    0.    0.    0.    0.48  0.005]]
Adversarial Training Loop 62/300:
  Label Loss: 0.0006
  Image Loss: 0.0159
  Total Loss: 0.0162
  Image grad max: 0.0017986870370805264
  Output probs: [[0.515 0.    0.    0.    0.    0.    0.    0.    0.48  0.005]]
Adversarial Training Loop 63/300:
  Label Loss: 0.0006
  Image Loss: 0.0157
  Total Loss: 0.0161
  Image grad max: 0.0018281934317201376
  Output probs: [[0.51  0.    0.    0.    0.    0.    0.    0.    0.484 0.005]]
Adversarial Training Loop 64/300:
  Label Loss: 0.0006
  Image Loss: 0.0156
  Total Loss: 0.0159
  Image grad max: 0.0014431410236284137
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.    0.    0.491 0.005]]
Adversarial Training Loop 65/300:
  Label Loss: 0.0006
  Image Loss: 0.0155
  Total Loss: 0.0158
  Image grad max: 0.0011116662062704563
  Output probs: [[0.498 0.    0.    0.    0.    0.    0.    0.    0.497 0.005]]
Adversarial Training Loop 66/300:
  Label Loss: 0.0006
  Image Loss: 0.0153
  Total Loss: 0.0156
  Image grad max: 0.0010979725047945976
  Output probs: [[0.495 0.    0.    0.    0.    0.    0.    0.    0.499 0.005]]
Adversarial Training Loop 67/300:
  Label Loss: 0.0006
  Image Loss: 0.0152
  Total Loss: 0.0155
  Image grad max: 0.0011195963015779853
  Output probs: [[0.497 0.    0.    0.    0.    0.    0.001 0.    0.497 0.005]]
Adversarial Training Loop 68/300:
  Label Loss: 0.0006
  Image Loss: 0.0150
  Total Loss: 0.0153
  Image grad max: 0.001098400680348277
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.493 0.005]]
Adversarial Training Loop 69/300:
  Label Loss: 0.0006
  Image Loss: 0.0149
  Total Loss: 0.0152
  Image grad max: 0.0010450382251292467
  Output probs: [[0.507 0.    0.    0.    0.    0.    0.001 0.    0.487 0.005]]
Adversarial Training Loop 70/300:
  Label Loss: 0.0006
  Image Loss: 0.0147
  Total Loss: 0.0150
  Image grad max: 0.0012764802668243647
  Output probs: [[0.511 0.    0.    0.    0.    0.    0.001 0.    0.484 0.005]]
Adversarial Training Loop 71/300:
  Label Loss: 0.0006
  Image Loss: 0.0146
  Total Loss: 0.0149
  Image grad max: 0.0014637695858255029
  Output probs: [[0.511 0.    0.    0.    0.    0.    0.001 0.    0.484 0.005]]
Adversarial Training Loop 72/300:
  Label Loss: 0.0006
  Image Loss: 0.0145
  Total Loss: 0.0148
  Image grad max: 0.0014666583156213164
  Output probs: [[0.508 0.    0.    0.    0.    0.    0.001 0.    0.487 0.005]]
Adversarial Training Loop 73/300:
  Label Loss: 0.0006
  Image Loss: 0.0143
  Total Loss: 0.0146
  Image grad max: 0.0012841557618230581
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.491 0.005]]
Adversarial Training Loop 74/300:
  Label Loss: 0.0006
  Image Loss: 0.0142
  Total Loss: 0.0145
  Image grad max: 0.0010538749629631639
  Output probs: [[0.499 0.    0.    0.    0.    0.    0.001 0.    0.495 0.005]]
Adversarial Training Loop 75/300:
  Label Loss: 0.0006
  Image Loss: 0.0141
  Total Loss: 0.0144
  Image grad max: 0.0010475462768226862
  Output probs: [[0.498 0.    0.    0.    0.    0.    0.001 0.    0.496 0.005]]
Adversarial Training Loop 76/300:
  Label Loss: 0.0006
  Image Loss: 0.0139
  Total Loss: 0.0142
  Image grad max: 0.0010548480786383152
  Output probs: [[0.5   0.    0.    0.    0.    0.    0.001 0.    0.494 0.005]]
Adversarial Training Loop 77/300:
  Label Loss: 0.0006
  Image Loss: 0.0138
  Total Loss: 0.0141
  Image grad max: 0.0010306299664080143
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.49  0.005]]
Adversarial Training Loop 78/300:
  Label Loss: 0.0006
  Image Loss: 0.0137
  Total Loss: 0.0140
  Image grad max: 0.0010787488427013159
  Output probs: [[0.507 0.    0.    0.    0.    0.    0.001 0.    0.487 0.005]]
Adversarial Training Loop 79/300:
  Label Loss: 0.0006
  Image Loss: 0.0135
  Total Loss: 0.0138
  Image grad max: 0.001242955680936575
  Output probs: [[0.508 0.    0.    0.    0.    0.    0.001 0.    0.486 0.005]]
Adversarial Training Loop 80/300:
  Label Loss: 0.0006
  Image Loss: 0.0134
  Total Loss: 0.0137
  Image grad max: 0.001312430016696453
  Output probs: [[0.507 0.    0.    0.    0.    0.    0.001 0.    0.487 0.005]]
Adversarial Training Loop 81/300:
  Label Loss: 0.0006
  Image Loss: 0.0133
  Total Loss: 0.0136
  Image grad max: 0.0012579798931255937
  Output probs: [[0.505 0.    0.    0.    0.    0.    0.001 0.    0.489 0.005]]
Adversarial Training Loop 82/300:
  Label Loss: 0.0006
  Image Loss: 0.0132
  Total Loss: 0.0135
  Image grad max: 0.0011159770656377077
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.492 0.005]]
Adversarial Training Loop 83/300:
  Label Loss: 0.0006
  Image Loss: 0.0130
  Total Loss: 0.0133
  Image grad max: 0.000987074337899685
  Output probs: [[0.5   0.    0.    0.    0.    0.    0.001 0.    0.494 0.005]]
Adversarial Training Loop 84/300:
  Label Loss: 0.0006
  Image Loss: 0.0129
  Total Loss: 0.0132
  Image grad max: 0.0009996831649914384
  Output probs: [[0.501 0.    0.    0.    0.    0.    0.001 0.    0.493 0.005]]
Adversarial Training Loop 85/300:
  Label Loss: 0.0006
  Image Loss: 0.0128
  Total Loss: 0.0131
  Image grad max: 0.0009894700488075614
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.491 0.005]]
Adversarial Training Loop 86/300:
  Label Loss: 0.0006
  Image Loss: 0.0127
  Total Loss: 0.0130
  Image grad max: 0.0010120050283148885
  Output probs: [[0.505 0.    0.    0.    0.    0.    0.001 0.    0.489 0.005]]
Adversarial Training Loop 87/300:
  Label Loss: 0.0006
  Image Loss: 0.0126
  Total Loss: 0.0129
  Image grad max: 0.00113097729627043
  Output probs: [[0.507 0.    0.    0.    0.    0.    0.001 0.    0.487 0.005]]
Adversarial Training Loop 88/300:
  Label Loss: 0.0006
  Image Loss: 0.0125
  Total Loss: 0.0128
  Image grad max: 0.0011994170490652323
  Output probs: [[0.506 0.    0.    0.    0.    0.    0.001 0.    0.487 0.005]]
Adversarial Training Loop 89/300:
  Label Loss: 0.0006
  Image Loss: 0.0123
  Total Loss: 0.0127
  Image grad max: 0.0011828168062493205
  Output probs: [[0.505 0.    0.    0.    0.    0.    0.001 0.    0.489 0.005]]
Adversarial Training Loop 90/300:
  Label Loss: 0.0006
  Image Loss: 0.0122
  Total Loss: 0.0126
  Image grad max: 0.0010952972806990147
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.491 0.005]]
Adversarial Training Loop 91/300:
  Label Loss: 0.0006
  Image Loss: 0.0121
  Total Loss: 0.0124
  Image grad max: 0.0009886871557682753
  Output probs: [[0.501 0.    0.    0.    0.    0.    0.001 0.    0.492 0.005]]
Adversarial Training Loop 92/300:
  Label Loss: 0.0006
  Image Loss: 0.0120
  Total Loss: 0.0123
  Image grad max: 0.000951720227021724
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.492 0.005]]
Adversarial Training Loop 93/300:
  Label Loss: 0.0006
  Image Loss: 0.0119
  Total Loss: 0.0122
  Image grad max: 0.0009447078336961567
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.491 0.005]]
Adversarial Training Loop 94/300:
  Label Loss: 0.0006
  Image Loss: 0.0118
  Total Loss: 0.0121
  Image grad max: 0.0009991489350795746
  Output probs: [[0.505 0.    0.    0.    0.    0.    0.001 0.    0.489 0.005]]
Adversarial Training Loop 95/300:
  Label Loss: 0.0007
  Image Loss: 0.0117
  Total Loss: 0.0120
  Image grad max: 0.001078940462321043
  Output probs: [[0.505 0.    0.    0.    0.    0.    0.001 0.    0.488 0.005]]
Adversarial Training Loop 96/300:
  Label Loss: 0.0007
  Image Loss: 0.0116
  Total Loss: 0.0119
  Image grad max: 0.001121699926443398
  Output probs: [[0.505 0.    0.    0.    0.    0.    0.001 0.    0.488 0.005]]
Adversarial Training Loop 97/300:
  Label Loss: 0.0007
  Image Loss: 0.0115
  Total Loss: 0.0118
  Image grad max: 0.0011044784914702177
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.49  0.005]]
Adversarial Training Loop 98/300:
  Label Loss: 0.0007
  Image Loss: 0.0114
  Total Loss: 0.0117
  Image grad max: 0.0010403611231595278
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.491 0.005]]
Adversarial Training Loop 99/300:
  Label Loss: 0.0007
  Image Loss: 0.0113
  Total Loss: 0.0116
  Image grad max: 0.0009700509253889322
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.492 0.005]]
Adversarial Training Loop 100/300:
  Label Loss: 0.0007
  Image Loss: 0.0112
  Total Loss: 0.0116
  Image grad max: 0.0009307240252383053
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.491 0.005]]
Adversarial Training Loop 101/300:
  Label Loss: 0.0007
  Image Loss: 0.0111
  Total Loss: 0.0115
  Image grad max: 0.0009461031877435744
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 102/300:
  Label Loss: 0.0007
  Image Loss: 0.0110
  Total Loss: 0.0114
  Image grad max: 0.0009993317071348429
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 103/300:
  Label Loss: 0.0007
  Image Loss: 0.0109
  Total Loss: 0.0113
  Image grad max: 0.0010509942658245564
  Output probs: [[0.505 0.    0.    0.    0.    0.    0.001 0.    0.488 0.006]]
Adversarial Training Loop 104/300:
  Label Loss: 0.0007
  Image Loss: 0.0109
  Total Loss: 0.0112
  Image grad max: 0.0010717143304646015
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 105/300:
  Label Loss: 0.0007
  Image Loss: 0.0108
  Total Loss: 0.0111
  Image grad max: 0.0010432108538225293
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 106/300:
  Label Loss: 0.0007
  Image Loss: 0.0107
  Total Loss: 0.0110
  Image grad max: 0.00098372099455446
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.491 0.006]]
Adversarial Training Loop 107/300:
  Label Loss: 0.0007
  Image Loss: 0.0106
  Total Loss: 0.0109
  Image grad max: 0.0009308960288763046
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.491 0.006]]
Adversarial Training Loop 108/300:
  Label Loss: 0.0007
  Image Loss: 0.0105
  Total Loss: 0.0109
  Image grad max: 0.0009132663835771382
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.491 0.006]]
Adversarial Training Loop 109/300:
  Label Loss: 0.0007
  Image Loss: 0.0104
  Total Loss: 0.0108
  Image grad max: 0.0009358786046504974
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 110/300:
  Label Loss: 0.0007
  Image Loss: 0.0104
  Total Loss: 0.0107
  Image grad max: 0.0009786017471924424
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 111/300:
  Label Loss: 0.0007
  Image Loss: 0.0103
  Total Loss: 0.0106
  Image grad max: 0.0010108551941812038
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 112/300:
  Label Loss: 0.0007
  Image Loss: 0.0102
  Total Loss: 0.0106
  Image grad max: 0.0010110100265592337
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 113/300:
  Label Loss: 0.0007
  Image Loss: 0.0101
  Total Loss: 0.0105
  Image grad max: 0.0009791416814550757
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 114/300:
  Label Loss: 0.0007
  Image Loss: 0.0100
  Total Loss: 0.0104
  Image grad max: 0.000935276155360043
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 115/300:
  Label Loss: 0.0007
  Image Loss: 0.0100
  Total Loss: 0.0103
  Image grad max: 0.0009071888634935021
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 116/300:
  Label Loss: 0.0007
  Image Loss: 0.0099
  Total Loss: 0.0103
  Image grad max: 0.0009105532662943006
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 117/300:
  Label Loss: 0.0007
  Image Loss: 0.0098
  Total Loss: 0.0102
  Image grad max: 0.0009334839414805174
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 118/300:
  Label Loss: 0.0007
  Image Loss: 0.0098
  Total Loss: 0.0101
  Image grad max: 0.0009568073437549174
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 119/300:
  Label Loss: 0.0007
  Image Loss: 0.0097
  Total Loss: 0.0101
  Image grad max: 0.0009632438886910677
  Output probs: [[0.504 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 120/300:
  Label Loss: 0.0007
  Image Loss: 0.0096
  Total Loss: 0.0100
  Image grad max: 0.000947236199863255
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 121/300:
  Label Loss: 0.0007
  Image Loss: 0.0096
  Total Loss: 0.0099
  Image grad max: 0.0009184053633362055
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 122/300:
  Label Loss: 0.0007
  Image Loss: 0.0095
  Total Loss: 0.0099
  Image grad max: 0.0008937360835261643
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 123/300:
  Label Loss: 0.0008
  Image Loss: 0.0094
  Total Loss: 0.0098
  Image grad max: 0.000886572408489883
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 124/300:
  Label Loss: 0.0008
  Image Loss: 0.0094
  Total Loss: 0.0097
  Image grad max: 0.0009019981371238828
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 125/300:
  Label Loss: 0.0008
  Image Loss: 0.0093
  Total Loss: 0.0097
  Image grad max: 0.0009212170261889696
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 126/300:
  Label Loss: 0.0008
  Image Loss: 0.0092
  Total Loss: 0.0096
  Image grad max: 0.0009432160295546055
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 127/300:
  Label Loss: 0.0008
  Image Loss: 0.0092
  Total Loss: 0.0096
  Image grad max: 0.0009254109463654459
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 128/300:
  Label Loss: 0.0008
  Image Loss: 0.0091
  Total Loss: 0.0095
  Image grad max: 0.0008917596423998475
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 129/300:
  Label Loss: 0.0008
  Image Loss: 0.0091
  Total Loss: 0.0095
  Image grad max: 0.00087225396418944
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 130/300:
  Label Loss: 0.0008
  Image Loss: 0.0090
  Total Loss: 0.0094
  Image grad max: 0.0008690287359058857
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 131/300:
  Label Loss: 0.0008
  Image Loss: 0.0090
  Total Loss: 0.0093
  Image grad max: 0.0008922733250074089
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 132/300:
  Label Loss: 0.0008
  Image Loss: 0.0089
  Total Loss: 0.0093
  Image grad max: 0.0009234552853740752
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 133/300:
  Label Loss: 0.0008
  Image Loss: 0.0088
  Total Loss: 0.0092
  Image grad max: 0.0009350685868412256
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 134/300:
  Label Loss: 0.0008
  Image Loss: 0.0088
  Total Loss: 0.0092
  Image grad max: 0.0009204940288327634
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 135/300:
  Label Loss: 0.0008
  Image Loss: 0.0087
  Total Loss: 0.0091
  Image grad max: 0.0008937863749451935
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.49  0.006]]
Adversarial Training Loop 136/300:
  Label Loss: 0.0008
  Image Loss: 0.0087
  Total Loss: 0.0091
  Image grad max: 0.0008838874055072665
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 137/300:
  Label Loss: 0.0008
  Image Loss: 0.0086
  Total Loss: 0.0091
  Image grad max: 0.0008902025292627513
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 138/300:
  Label Loss: 0.0008
  Image Loss: 0.0086
  Total Loss: 0.0090
  Image grad max: 0.0009074182016775012
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 139/300:
  Label Loss: 0.0008
  Image Loss: 0.0086
  Total Loss: 0.0090
  Image grad max: 0.0009230492869392037
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 140/300:
  Label Loss: 0.0008
  Image Loss: 0.0085
  Total Loss: 0.0089
  Image grad max: 0.0009270173031836748
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 141/300:
  Label Loss: 0.0008
  Image Loss: 0.0085
  Total Loss: 0.0089
  Image grad max: 0.0009186920942738652
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 142/300:
  Label Loss: 0.0008
  Image Loss: 0.0084
  Total Loss: 0.0088
  Image grad max: 0.0009057653369382024
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 143/300:
  Label Loss: 0.0008
  Image Loss: 0.0084
  Total Loss: 0.0088
  Image grad max: 0.0008986523025669158
  Output probs: [[0.502 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 144/300:
  Label Loss: 0.0008
  Image Loss: 0.0083
  Total Loss: 0.0087
  Image grad max: 0.0009041347657330334
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 145/300:
  Label Loss: 0.0008
  Image Loss: 0.0083
  Total Loss: 0.0087
  Image grad max: 0.0009221283253282309
  Output probs: [[0.503 0.    0.    0.    0.    0.    0.001 0.    0.489 0.006]]
Adversarial Training Loop 146/300:
  Label Loss: 0.0008
  Image Loss: 0.0082
  Total Loss: 0.0087
  Image grad max: 0.0009346186998300254
  Output probs: [[0.503 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 147/300:
  Label Loss: 0.0008
  Image Loss: 0.0082
  Total Loss: 0.0086
  Image grad max: 0.0009337127557955682
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 148/300:
  Label Loss: 0.0009
  Image Loss: 0.0082
  Total Loss: 0.0086
  Image grad max: 0.000922360282856971
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 149/300:
  Label Loss: 0.0009
  Image Loss: 0.0081
  Total Loss: 0.0085
  Image grad max: 0.0009132843697443604
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 150/300:
  Label Loss: 0.0009
  Image Loss: 0.0081
  Total Loss: 0.0085
  Image grad max: 0.0009151102276518941
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 151/300:
  Label Loss: 0.0009
  Image Loss: 0.0080
  Total Loss: 0.0085
  Image grad max: 0.0009327087900601327
  Output probs: [[0.503 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 152/300:
  Label Loss: 0.0009
  Image Loss: 0.0080
  Total Loss: 0.0084
  Image grad max: 0.0009585116640664637
  Output probs: [[0.503 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 153/300:
  Label Loss: 0.0009
  Image Loss: 0.0080
  Total Loss: 0.0084
  Image grad max: 0.0009597748867236078
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 154/300:
  Label Loss: 0.0009
  Image Loss: 0.0079
  Total Loss: 0.0084
  Image grad max: 0.0009431113139726222
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 155/300:
  Label Loss: 0.0009
  Image Loss: 0.0079
  Total Loss: 0.0083
  Image grad max: 0.0009219283820129931
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 156/300:
  Label Loss: 0.0009
  Image Loss: 0.0079
  Total Loss: 0.0083
  Image grad max: 0.000911849841941148
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 157/300:
  Label Loss: 0.0009
  Image Loss: 0.0078
  Total Loss: 0.0083
  Image grad max: 0.00092056900030002
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 158/300:
  Label Loss: 0.0009
  Image Loss: 0.0078
  Total Loss: 0.0082
  Image grad max: 0.0009409572230651975
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 159/300:
  Label Loss: 0.0009
  Image Loss: 0.0078
  Total Loss: 0.0082
  Image grad max: 0.000957225973252207
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 160/300:
  Label Loss: 0.0009
  Image Loss: 0.0077
  Total Loss: 0.0082
  Image grad max: 0.0009613073198124766
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 161/300:
  Label Loss: 0.0009
  Image Loss: 0.0077
  Total Loss: 0.0081
  Image grad max: 0.0009590759291313589
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 162/300:
  Label Loss: 0.0009
  Image Loss: 0.0077
  Total Loss: 0.0081
  Image grad max: 0.000950074172578752
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 163/300:
  Label Loss: 0.0009
  Image Loss: 0.0076
  Total Loss: 0.0081
  Image grad max: 0.0009383975411765277
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.001 0.    0.489 0.006]]
Adversarial Training Loop 164/300:
  Label Loss: 0.0009
  Image Loss: 0.0076
  Total Loss: 0.0081
  Image grad max: 0.0009340543765574694
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 165/300:
  Label Loss: 0.0009
  Image Loss: 0.0076
  Total Loss: 0.0080
  Image grad max: 0.0009407021570950747
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 166/300:
  Label Loss: 0.0009
  Image Loss: 0.0075
  Total Loss: 0.0080
  Image grad max: 0.0009540801984257996
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 167/300:
  Label Loss: 0.0009
  Image Loss: 0.0075
  Total Loss: 0.0080
  Image grad max: 0.000963103084359318
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 168/300:
  Label Loss: 0.0009
  Image Loss: 0.0075
  Total Loss: 0.0080
  Image grad max: 0.0009610321139916778
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 169/300:
  Label Loss: 0.0009
  Image Loss: 0.0075
  Total Loss: 0.0079
  Image grad max: 0.0009471745579503477
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 170/300:
  Label Loss: 0.0009
  Image Loss: 0.0074
  Total Loss: 0.0079
  Image grad max: 0.0009407198522239923
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 171/300:
  Label Loss: 0.0009
  Image Loss: 0.0074
  Total Loss: 0.0079
  Image grad max: 0.0009453705279156566
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 172/300:
  Label Loss: 0.0009
  Image Loss: 0.0074
  Total Loss: 0.0079
  Image grad max: 0.0009571819682605565
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 173/300:
  Label Loss: 0.0009
  Image Loss: 0.0074
  Total Loss: 0.0078
  Image grad max: 0.0009670097497291863
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 174/300:
  Label Loss: 0.0009
  Image Loss: 0.0073
  Total Loss: 0.0078
  Image grad max: 0.000972370442468673
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 175/300:
  Label Loss: 0.0009
  Image Loss: 0.0073
  Total Loss: 0.0078
  Image grad max: 0.0009662833763286471
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 176/300:
  Label Loss: 0.0009
  Image Loss: 0.0073
  Total Loss: 0.0078
  Image grad max: 0.0009567136294208467
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 177/300:
  Label Loss: 0.0010
  Image Loss: 0.0073
  Total Loss: 0.0077
  Image grad max: 0.000951381865888834
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 178/300:
  Label Loss: 0.0010
  Image Loss: 0.0072
  Total Loss: 0.0077
  Image grad max: 0.0009554041316732764
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 179/300:
  Label Loss: 0.0010
  Image Loss: 0.0072
  Total Loss: 0.0077
  Image grad max: 0.0009652073495090008
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 180/300:
  Label Loss: 0.0010
  Image Loss: 0.0072
  Total Loss: 0.0077
  Image grad max: 0.0009729034500196576
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 181/300:
  Label Loss: 0.0010
  Image Loss: 0.0072
  Total Loss: 0.0076
  Image grad max: 0.0009716051863506436
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 182/300:
  Label Loss: 0.0010
  Image Loss: 0.0071
  Total Loss: 0.0076
  Image grad max: 0.0009678427595645189
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 183/300:
  Label Loss: 0.0010
  Image Loss: 0.0071
  Total Loss: 0.0076
  Image grad max: 0.0009721569949761033
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 184/300:
  Label Loss: 0.0010
  Image Loss: 0.0071
  Total Loss: 0.0076
  Image grad max: 0.0009728348231874406
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 185/300:
  Label Loss: 0.0010
  Image Loss: 0.0071
  Total Loss: 0.0076
  Image grad max: 0.0009702883544377983
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 186/300:
  Label Loss: 0.0010
  Image Loss: 0.0071
  Total Loss: 0.0075
  Image grad max: 0.000967500323895365
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 187/300:
  Label Loss: 0.0010
  Image Loss: 0.0070
  Total Loss: 0.0075
  Image grad max: 0.000966259918641299
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 188/300:
  Label Loss: 0.0010
  Image Loss: 0.0070
  Total Loss: 0.0075
  Image grad max: 0.0009686726843938231
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 189/300:
  Label Loss: 0.0010
  Image Loss: 0.0070
  Total Loss: 0.0075
  Image grad max: 0.0009735424537211657
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.488 0.006]]
Adversarial Training Loop 190/300:
  Label Loss: 0.0010
  Image Loss: 0.0070
  Total Loss: 0.0075
  Image grad max: 0.0009778154781088233
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.488 0.006]]
Adversarial Training Loop 191/300:
  Label Loss: 0.0010
  Image Loss: 0.0070
  Total Loss: 0.0075
  Image grad max: 0.0009773533092811704
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 192/300:
  Label Loss: 0.0010
  Image Loss: 0.0070
  Total Loss: 0.0074
  Image grad max: 0.0009698075009509921
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 193/300:
  Label Loss: 0.0010
  Image Loss: 0.0069
  Total Loss: 0.0074
  Image grad max: 0.0009664761601015925
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 194/300:
  Label Loss: 0.0010
  Image Loss: 0.0069
  Total Loss: 0.0074
  Image grad max: 0.0009672365267761052
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.489 0.006]]
Adversarial Training Loop 195/300:
  Label Loss: 0.0010
  Image Loss: 0.0069
  Total Loss: 0.0074
  Image grad max: 0.0009715965716168284
  Output probs: [[0.502 0.    0.    0.    0.    0.001 0.002 0.    0.488 0.006]]
Adversarial Training Loop 196/300:
  Label Loss: 0.0010
  Image Loss: 0.0069
  Total Loss: 0.0074
  Image grad max: 0.0009785154834389687
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 197/300:
  Label Loss: 0.0010
  Image Loss: 0.0069
  Total Loss: 0.0074
  Image grad max: 0.0009820627747103572
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 198/300:
  Label Loss: 0.0010
  Image Loss: 0.0068
  Total Loss: 0.0073
  Image grad max: 0.000979625852778554
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 199/300:
  Label Loss: 0.0010
  Image Loss: 0.0068
  Total Loss: 0.0073
  Image grad max: 0.0009740078821778297
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 200/300:
  Label Loss: 0.0010
  Image Loss: 0.0068
  Total Loss: 0.0073
  Image grad max: 0.000971395755186677
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 201/300:
  Label Loss: 0.0010
  Image Loss: 0.0068
  Total Loss: 0.0073
  Image grad max: 0.0009730015881359577
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 202/300:
  Label Loss: 0.0010
  Image Loss: 0.0068
  Total Loss: 0.0073
  Image grad max: 0.0009785775328055024
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 203/300:
  Label Loss: 0.0010
  Image Loss: 0.0068
  Total Loss: 0.0073
  Image grad max: 0.0009816663805395365
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 204/300:
  Label Loss: 0.0010
  Image Loss: 0.0068
  Total Loss: 0.0073
  Image grad max: 0.0009806329617276788
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 205/300:
  Label Loss: 0.0010
  Image Loss: 0.0067
  Total Loss: 0.0072
  Image grad max: 0.000979299540631473
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 206/300:
  Label Loss: 0.0010
  Image Loss: 0.0067
  Total Loss: 0.0072
  Image grad max: 0.000977567513473332
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 207/300:
  Label Loss: 0.0010
  Image Loss: 0.0067
  Total Loss: 0.0072
  Image grad max: 0.0009788487805053592
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 208/300:
  Label Loss: 0.0010
  Image Loss: 0.0067
  Total Loss: 0.0072
  Image grad max: 0.0009831140050664544
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 209/300:
  Label Loss: 0.0010
  Image Loss: 0.0067
  Total Loss: 0.0072
  Image grad max: 0.0009844864252954721
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 210/300:
  Label Loss: 0.0010
  Image Loss: 0.0067
  Total Loss: 0.0072
  Image grad max: 0.0009845683816820383
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 211/300:
  Label Loss: 0.0010
  Image Loss: 0.0067
  Total Loss: 0.0072
  Image grad max: 0.0009814092190936208
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 212/300:
  Label Loss: 0.0010
  Image Loss: 0.0067
  Total Loss: 0.0071
  Image grad max: 0.0009782785782590508
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 213/300:
  Label Loss: 0.0010
  Image Loss: 0.0066
  Total Loss: 0.0071
  Image grad max: 0.0009797210805118084
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 214/300:
  Label Loss: 0.0010
  Image Loss: 0.0066
  Total Loss: 0.0071
  Image grad max: 0.000983159989118576
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 215/300:
  Label Loss: 0.0010
  Image Loss: 0.0066
  Total Loss: 0.0071
  Image grad max: 0.0009851350914686918
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 216/300:
  Label Loss: 0.0010
  Image Loss: 0.0066
  Total Loss: 0.0071
  Image grad max: 0.0009839992271736264
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 217/300:
  Label Loss: 0.0010
  Image Loss: 0.0066
  Total Loss: 0.0071
  Image grad max: 0.000981520744971931
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 218/300:
  Label Loss: 0.0010
  Image Loss: 0.0066
  Total Loss: 0.0071
  Image grad max: 0.0009803223656490445
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 219/300:
  Label Loss: 0.0010
  Image Loss: 0.0066
  Total Loss: 0.0071
  Image grad max: 0.0009813817450776696
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 220/300:
  Label Loss: 0.0010
  Image Loss: 0.0066
  Total Loss: 0.0071
  Image grad max: 0.000983900623396039
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 221/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.000985045451670885
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 222/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.0009842616273090243
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 223/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.0009822865249589086
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 224/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.0009805698646232486
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 225/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.000981637043878436
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 226/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.0009840427665039897
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 227/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.0009856057586148381
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 228/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.00098462775349617
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 229/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.0009825617307797074
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 230/300:
  Label Loss: 0.0010
  Image Loss: 0.0065
  Total Loss: 0.0070
  Image grad max: 0.0009815571829676628
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 231/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009824701119214296
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 232/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009844094747677445
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 233/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009849595371633768
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 234/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009831227362155914
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 235/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009760122047737241
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 236/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009764519054442644
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 237/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009840769926086068
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 238/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009901159210130572
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.488 0.006]]
Adversarial Training Loop 239/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009882597951218486
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 240/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009812397183850408
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 241/300:
  Label Loss: 0.0010
  Image Loss: 0.0064
  Total Loss: 0.0069
  Image grad max: 0.0009765889262780547
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 242/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009789520408958197
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 243/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009849976049736142
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 244/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.000988391344435513
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 245/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009857052937150002
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 246/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009804707951843739
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 247/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009777576196938753
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 248/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.000980872311629355
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 249/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009854206582531333
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 250/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009866729378700256
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 251/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009830979397520423
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 252/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009792785858735442
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 253/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009791760239750147
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 254/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0068
  Image grad max: 0.0009822917636483908
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 255/300:
  Label Loss: 0.0010
  Image Loss: 0.0063
  Total Loss: 0.0067
  Image grad max: 0.0009849055204540491
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 256/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.000984647893346846
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 257/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.000981683493591845
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 258/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009796017548069358
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 259/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009810071205720305
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 260/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009833788499236107
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 261/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009839395061135292
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 262/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009821690618991852
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 263/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.000980147160589695
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 264/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009800129337236285
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 265/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009819518309086561
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 266/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009831666247919202
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 267/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009827212197706103
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 268/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009813373908400536
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 269/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009800203843042254
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 270/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009805455338209867
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 271/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0067
  Image grad max: 0.0009817244717851281
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 272/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0066
  Image grad max: 0.000982392462901771
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 273/300:
  Label Loss: 0.0010
  Image Loss: 0.0062
  Total Loss: 0.0066
  Image grad max: 0.0009817607933655381
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 274/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009805647423490882
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 275/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009794014040380716
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 276/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009802582208067179
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 277/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009816967649385333
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 278/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009794646175578237
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 279/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009750426979735494
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 280/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009768061572685838
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 281/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.000982268014922738
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 282/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009847292676568031
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 283/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009816179517656565
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 284/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009769626194611192
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 285/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009753078920766711
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 286/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009745876304805279
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 287/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.000979616423137486
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 288/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009834800148382783
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 289/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009825670858845115
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 290/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009777110535651445
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 291/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009753380436450243
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 292/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.0009773584315553308
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 293/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0066
  Image grad max: 0.000980595126748085
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 294/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0065
  Image grad max: 0.000981972087174654
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 295/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0065
  Image grad max: 0.0009823960717767477
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 296/300:
  Label Loss: 0.0010
  Image Loss: 0.0061
  Total Loss: 0.0065
  Image grad max: 0.000978940981440246
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 297/300:
  Label Loss: 0.0010
  Image Loss: 0.0060
  Total Loss: 0.0065
  Image grad max: 0.0009758579544723034
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 298/300:
  Label Loss: 0.0010
  Image Loss: 0.0060
  Total Loss: 0.0065
  Image grad max: 0.0009767147712409496
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 299/300:
  Label Loss: 0.0010
  Image Loss: 0.0060
  Total Loss: 0.0065
  Image grad max: 0.0009798293467611074
  Output probs: [[0.502 0.    0.    0.    0.    0.002 0.002 0.    0.489 0.006]]
Adversarial Training Loop 300/300:
  Label Loss: 0.0010
  Image Loss: 0.0060
  Total Loss: 0.0065
  Image grad max: 0.0009814697550609708
Visualization saved to adversarial_figures/adversarial_testing.png
